<!doctype html><html lang=en><head><meta charset=UTF-8><meta http-equiv=X-UA-Compatible content="ie=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=author content><meta name=description content="While cleaning up some tech debt, a curious issue cropped up. Nginx was running in an alpine container as a front end load balancer. It had a dynamic config that got periodically updated by a sidecar, and had filebeat shipping logs out to a central collector but otherwise was just a very simple Nginx config.
Every now and then the container would crash, it would automatically recover fast enough no alarms were lost and the clients would just resend their requests."><meta name=keywords content="blog,programming,linux,systems,personal,rust,philosophy,linux,nginx,operations,diagnostics"><meta name=robots content="noodp"><meta name=theme-color content><link rel=canonical href=https://stelfox.net/blog/2019/10/fixing-hung-nginx-workers/><title>Fixing Hung Nginx Workers :: ./Sam_Stelfox.sh â€” A simple place for my collected thoughts and notes.
</title><link rel=stylesheet href=/main.949191c1dcc9c4a887997048b240354e47152016d821198f89448496ba42e491.css integrity="sha256-lJGRwdzJxKiHmXBIskA1TkcVIBbYIRmPiUSElrpC5JE="><link rel=apple-touch-icon sizes=180x180 href=/apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/favicon-16x16.png><link rel=manifest href=/site.webmanifest><link rel="shortcut icon" href=/favicon.ico><meta itemprop=name content="Fixing Hung Nginx Workers"><meta itemprop=description content="While cleaning up some tech debt, a curious issue cropped up. Nginx was running in an alpine container as a front end load balancer. It had a dynamic config that got periodically updated by a sidecar, and had filebeat shipping logs out to a central collector but otherwise was just a very simple Nginx config.
Every now and then the container would crash, it would automatically recover fast enough no alarms were lost and the clients would just resend their requests."><meta itemprop=datePublished content="2019-10-25T11:26:31-05:00"><meta itemprop=dateModified content="2019-10-25T11:26:31-05:00"><meta itemprop=wordCount content="781"><meta itemprop=keywords content="Linux,Nginx,Operations,Diagnostics"><meta name=twitter:card content="summary"><meta name=twitter:title content="Fixing Hung Nginx Workers"><meta name=twitter:description content="While cleaning up some tech debt, a curious issue cropped up. Nginx was running in an alpine container as a front end load balancer. It had a dynamic config that got periodically updated by a sidecar, and had filebeat shipping logs out to a central collector but otherwise was just a very simple Nginx config.
Every now and then the container would crash, it would automatically recover fast enough no alarms were lost and the clients would just resend their requests."><meta property="article:published_time" content="2019-10-25 11:26:31 -0500 -0500"></head><body><div class=container><header class=header><span class=header__inner><a href=/ style=text-decoration:none><div class=logo><span class=logo__mark>></span>
<span class=logo__text>./Sam_Stelfox.sh</span>
<span class=logo__cursor></span></div></a><span class=header__right><nav class=menu><ul class=menu__inner><li><a href=/about/>About</a></li><li><a href=/notes/>Notes</a></li><li><a href=/blog/>Posts</a></li></ul></nav><span class=menu-trigger><svg viewBox="0 0 24 24"><path d="M0 0h24v24H0z" fill="none"/><path d="M3 18h18v-2H3v2zm0-5h18v-2H3v2zm0-7v2h18V6H3z"/></svg></span></span></span></header><div class=content><main class=post><div class=post-info></p></div><article><h2 class=post-title><a href=https://stelfox.net/blog/2019/10/fixing-hung-nginx-workers/>Fixing Hung Nginx Workers</a></h2><div class=post-content><p>While cleaning up some tech debt, a curious issue cropped up. Nginx was running in an alpine container as a front end load balancer. It had a dynamic config that got periodically updated by a sidecar, and had filebeat shipping logs out to a central collector but otherwise was just a very simple Nginx config.</p><p>Every now and then the container would crash, it would automatically recover fast enough no alarms were lost and the clients would just resend their requests. No data was lost, everything failed gracefully, but it's still a pretty crazy thing to leave happening.</p><p>There was nothing in the log besides some client errors and configuration reloading notifications. The external metrics showed the container throwing some kind of memory party before finally burning out and crashing. It was a pretty steady trend upwards indicative of some kind of a memory leak. The graph below illustrates this. Each of those drops is either the container crashing or me manually restarting it to test behavior. Can you figure out when I fixed the issue?</p><p><img alt="Graph showing nginx memory usage. The graph goes from chaotic to steady" src=/images/nginx_memory_consumption.png></p><p>To figure out what was going on I had to get an invitation that container's party.</p><p>I don't think there is great tooling for inspecting individual processes inside of containers. As much as we'd like to think of containers as isolated applications, its not uncommon for them to run <a href=https://github.com/krallin/tini>a minimal init container</a>, or be composed of separate different processes themselves that handle different things (Nginx has a master process and spawns off many worker processes). We still need the data on those processes.</p><p>The common solution to getting this data seems to be to "just exec into the container". In general exec should be restricted. If you find yourself running exec in a container, you're missing tooling or metrics that should avoid that (which is the case here).</p><p>Denying exec is probably a longer discussion but the short of it is: If you can exec into the container, you can extract its secrets, configuration, and access any services that it's allowed to (likely escalating your personal privileges within the cluster). This is without considering the ramifications of exec'ing into a privileged container.</p><p>In any event, not all container clusters solution allow external execution (looking at you AWS ECS, its the one good thing I have to say about you).</p><p>I chose to modify the container to log the data I needed instead. I setup a wrapper that started and backgrounded the following script:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt id=hl-0-1><a class=lnlinks href=#hl-0-1> 1</a>
</span><span class=lnt id=hl-0-2><a class=lnlinks href=#hl-0-2> 2</a>
</span><span class=lnt id=hl-0-3><a class=lnlinks href=#hl-0-3> 3</a>
</span><span class=lnt id=hl-0-4><a class=lnlinks href=#hl-0-4> 4</a>
</span><span class=lnt id=hl-0-5><a class=lnlinks href=#hl-0-5> 5</a>
</span><span class=lnt id=hl-0-6><a class=lnlinks href=#hl-0-6> 6</a>
</span><span class=lnt id=hl-0-7><a class=lnlinks href=#hl-0-7> 7</a>
</span><span class=lnt id=hl-0-8><a class=lnlinks href=#hl-0-8> 8</a>
</span><span class=lnt id=hl-0-9><a class=lnlinks href=#hl-0-9> 9</a>
</span><span class=lnt id=hl-0-10><a class=lnlinks href=#hl-0-10>10</a>
</span><span class=lnt id=hl-0-11><a class=lnlinks href=#hl-0-11>11</a>
</span><span class=lnt id=hl-0-12><a class=lnlinks href=#hl-0-12>12</a>
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl><span class=cp>#!/bin/bash
</span></span></span><span class=line><span class=cl><span class=cp></span>
</span></span><span class=line><span class=cl><span class=k>while</span> <span class=o>[</span> <span class=m>1</span> <span class=o>]</span><span class=p>;</span> <span class=k>do</span>
</span></span><span class=line><span class=cl>  <span class=c1># This generates a JSON output that can be pulled via the ECS logs of the top</span>
</span></span><span class=line><span class=cl>  <span class=c1># memory consuming processes using what we have available in alpine.</span>
</span></span><span class=line><span class=cl>  ps -o pid,time,rss,vsz,args <span class=p>|</span>
</span></span><span class=line><span class=cl>    tail -n +2 <span class=p>|</span>
</span></span><span class=line><span class=cl>    awk <span class=s1>&#39;memstat: { print &#34;{\&#34;pid\&#34;:&#34; $1 &#34;,\&#34;time\&#34;:\&#34;&#34; $2 &#34;\&#34;,\&#34;rss\&#34;:&#34; $3 &#34;,\&#34;vsz\&#34;:&#34;
</span></span></span><span class=line><span class=cl><span class=s1>$4 &#34;,\&#34;cmd\&#34;:\&#34;&#34; substr($0, index($0,$5)) &#34;\&#34;}&#34; }&#39;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>  sleep <span class=m>300</span>
</span></span><span class=line><span class=cl><span class=k>done</span>
</span></span></code></pre></td></tr></table></div></div><p>I'm slightly abusing awk to generate JSON I can parse later, but it works and it doesn't need to be fancy.</p><p>Each message has a prefix of "memstat:" so I can easily filter and extract that data from the live log stream. After a couple of hours (the time it generally took for the party to get going) the issue was pretty obvious. Inside the container there was a growing number of processes with the name "nginx: worker process is shutting down". Some Googling turned up <a href=https://forum.nginx.org/read.php?2,262403,262403>a post in the Nginx mailing list</a> from a while ago.</p><p>The responses claimed that it was the result of a third party module, but this is stock Nginx (version 1.15.8 at the time) which was at least three years older than the post. It did indicate a clue though: config reloading.</p><p>I don't know why Nginx believed connections were still open, we have request and connect timeouts both in this config and upstream. Some of these processes were sticking around for hours (I never actually saw one exit after getting into this hung state). Every config reload was dropping a zombie worker or two into the container, permanently consuming ~30Mb of RAM. When it hit the configured threshold, the party gets stopped.</p><p>An option was added in the <a href=http://nginx.org/en/docs/ngx_core_module.html#worker_shutdown_timeout>Nginx core module</a> to handle situations where the worker wouldn't close, but this definitely seems like a bug of some kind. There is a newer version of Nginx (1.17.5 at the time of the writing) that could very well have fixed this issue.</p><p>Adding "worker_shutdown_timeout 60s;" to the main Nginx config solved the issue (60 seconds matches our request and connect timeouts so nothing valid should last longer than that). Sure enough Nginx went back to stable, and predictably low memory usage.</p></div></article><hr><div class=post-info><p><svg width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-tag meta-icon"><path d="M20.59 13.41l-7.17 7.17a2 2 0 01-2.83.0L2 12V2h10l8.59 8.59a2 2 0 010 2.82z"/><line x1="7" y1="7" x2="7" y2="7"/></svg>
<span class=tag><a href=https://stelfox.net/tags/linux/>linux</a></span>
<span class=tag><a href=https://stelfox.net/tags/nginx/>nginx</a></span>
<span class=tag><a href=https://stelfox.net/tags/operations/>operations</a></span>
<span class=tag><a href=https://stelfox.net/tags/diagnostics/>diagnostics</a></span></p><p><svg width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-git-commit"><circle cx="12" cy="12" r="4"/><line x1="1.05" y1="12" x2="7" y2="12"/><line x1="17.01" y1="12" x2="22.96" y2="12"/></svg><a href=ac3c177eb78019063bdce225211b16f9e6b96848 target=_blank rel=noopener>ac3c177e</a> @ 2024-07-15</p></div></main></div><footer class=footer></footer></div><script type=text/javascript src=/bundle.min.e89fda0f29b95d33f6f4224dd9e5cf69d84aff3818be2b0d73e731689cc374261b016d17d46f8381962fb4a1577ba3017b1f23509d894f6e66431f988c00889e.js integrity="sha512-6J/aDym5XTP29CJN2eXPadhK/zgYvisNc+cxaJzDdCYbAW0X1G+DgZYvtKFXe6MBex8jUJ2JT25mQx+YjACIng=="></script></body></html>
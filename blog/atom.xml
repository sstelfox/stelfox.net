<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en">
	<title>Sam Stelfox&#x27;s Thoughts &amp; Notes - Blog Posts</title>
	<subtitle>Thought&#x27;s from a software engineer, systems architect, and Linux gubernƒÅre.</subtitle>
	<link href="https://stelfox.net/blog/atom.xml" rel="self" type="application/atom+xml"/>
  <link href="https://stelfox.net/blog/"/>
	<generator uri="https://www.getzola.org/">Zola</generator>
	<updated>2020-02-23T20:09:02-04:00</updated>
	<id>https://stelfox.net/blog/atom.xml</id>
	<entry xml:lang="en">
		<title>Logical Volume in Use</title>
		<published>2020-02-23T20:09:02-04:00</published>
		<updated>2020-02-23T20:09:02-04:00</updated>
		<link href="https://stelfox.net/blog/2020/logical-volume-in-use/" type="text/html"/>
		<id>https://stelfox.net/blog/2020/logical-volume-in-use/</id>
		<content type="html">&lt;p&gt;While attempting to automate some filesytem creation that involved LVM I kept
running into an issue occasionally with some holding open the logical volumes.
I would attempt to disable the volume using the following command:&lt;&#x2F;p&gt;
&lt;span id=&quot;continue-reading&quot;&gt;&lt;&#x2F;span&gt;&lt;pre&gt;&lt;code&gt;$ lvchange -an system&#x2F;storage
Logical volume system&#x2F;storage contains a filesystem in use.
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;All of the mounts for the filesystems that were on the volume were unmounted,
so it must have been a process. The trick to finding this out is to query all
the processes mount files to find out what is holding this open. In my case
since I&#x27;m searching for the &lt;code&gt;system&lt;&#x2F;code&gt; volume I&#x27;ll need to filter out systemd.
The following the command will find the offending processes:&lt;&#x2F;p&gt;
&lt;pre&gt;&lt;code&gt;$ grep system &#x2F;proc&#x2F;*&#x2F;mounts | grep -vE &#x27;(cgroup|systemd)&#x27;
&#x2F;proc&#x2F;338&#x2F;mounts:&#x2F;dev&#x2F;mapper&#x2F;system-storage &#x2F;mnt&#x2F;storage_target xfs rw,noatime,attr2,inode64,logbufs=8,logbsize=32k,noquota 0 0
&#x2F;proc&#x2F;421&#x2F;mounts:&#x2F;dev&#x2F;mapper&#x2F;system-storage &#x2F;mnt&#x2F;storage_target xfs rw,noatime,attr2,inode64,logbufs=8,logbsize=32k,noquota 0 0
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;From here we need to identify the offending processes using the PID identifiers
from the previous output:&lt;&#x2F;p&gt;
&lt;pre&gt;&lt;code&gt;$ ps -q 338,421 -o comm=
systemd-udevd
systemd-logind
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;Disabling, shutting down, or kill the offending processes allow the LVM to be
disabled and properly removed. Of course it was systemd...&lt;&#x2F;p&gt;
</content>
	</entry>
	<entry xml:lang="en">
		<title>Extracting Dracut Built initramfs</title>
		<published>2020-02-18T18:42:02-05:00</published>
		<updated>2020-02-18T18:42:02-05:00</updated>
		<link href="https://stelfox.net/blog/2020/extracting-dracut-built-initramfs/" type="text/html"/>
		<id>https://stelfox.net/blog/2020/extracting-dracut-built-initramfs/</id>
		<content type="html">&lt;p&gt;It&#x27;s been a hot second since I&#x27;ve dived into the lands of initramfs and since
then it seems like things have gotten more complicated. This is the way of
things in tech and usually has a good reason. The simple way that used to work
wonders (and is still required) to start with, was to identify if the file is
compressed:&lt;&#x2F;p&gt;
&lt;span id=&quot;continue-reading&quot;&gt;&lt;&#x2F;span&gt;&lt;pre&gt;&lt;code&gt;$ file &#x2F;boot&#x2F;initramfs-current.img
&#x2F;boot&#x2F;initramfs-current.img: ASCII cpio archive (SVR4 with no CRC)
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;In this case the file appears to be entirely uncompressed which is convenient
and likely exactly what you&#x27;ll experience for reasons I&#x27;m about to get to. This
could indicate that there is a type of compression in place (such as gzip or
the like) in which case your initramfs has likely not been generated by a
modern version of Dracut and this post isn&#x27;t right for you.&lt;&#x2F;p&gt;
&lt;p&gt;Next let&#x27;s extract the contents into a new temporary directory:&lt;&#x2F;p&gt;
&lt;pre&gt;&lt;code&gt;$ mkdir init_tmp
$ cd init_tmp
$ cpio -mvid &lt; &#x2F;boot&#x2F;initramfs-current.img
.
early_cpio
kernel
kernel&#x2F;x86
kernel&#x2F;x86&#x2F;microcode
kernel&#x2F;x86&#x2F;microcode&#x2F;AuthenticAMD.bin
62 blocks
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;This is where things got weird and I got caught up. No &lt;code&gt;&#x2F;init&lt;&#x2F;code&gt;? No tools for
dealing with LVM, filesystems, device scanning? No core system directories like
&lt;code&gt;&#x2F;dev&lt;&#x2F;code&gt;, &lt;code&gt;&#x2F;sys&lt;&#x2F;code&gt;, or &lt;code&gt;&#x2F;proc&lt;&#x2F;code&gt;? What is going on here. We can also quickly see the
size doesn&#x27;t match what we extracted:&lt;&#x2F;p&gt;
&lt;pre&gt;&lt;code&gt;$ du --max-depth=1 -h
32K     .&#x2F;kernel
36K     .
$ ls -lh &#x2F;boot&#x2F;initramfs-current.img
-rwxr-xr-x 1 root root 13M Feb 18 15:13 &#x2F;boot&#x2F;initramfs-current.img
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;Turns out this is an early optimization by dracut to get updated microcode to
the processor before we pass control over to any userspace programs. It&#x27;s
actually pretty slick and I&#x27;ll have to figure out how this works in some future
post. To get to the real initramfs we simply need to skip the first one using
&lt;code&gt;dd&lt;&#x2F;code&gt;.&lt;&#x2F;p&gt;
&lt;p&gt;Previosly when we extracted the CPIO archive it told us how large it was (The
&lt;code&gt;62 blocks&lt;&#x2F;code&gt; at the end). We simply need to skip those then we should be able to
decode the inner archive.&lt;&#x2F;p&gt;
&lt;p&gt;Side note for clarity with the following commands, I switched back to my home
directory and emptied out the extracted contents of the &lt;code&gt;init_tmp&lt;&#x2F;code&gt; directory we
created earlier as that&#x27;s where we want to put the extracted inner initramfs
contents.&lt;&#x2F;p&gt;
&lt;pre&gt;&lt;code&gt;$ dd if=&#x2F;boot&#x2F;initramfs-current.img bs=512 skip=62 of=inner-initramfs-current.img
25995+1 records in
25995+1 records out
13309841 bytes (13 MB, 13 MiB) copied, 0.0848529 s, 157 MB&#x2F;s
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;Once again we need to identify if compression is present:&lt;&#x2F;p&gt;
&lt;pre&gt;&lt;code&gt;$ file inner-initramfs-current.img
inner-initramfs-current.img: LZ4 compressed data (v0.1-v0.9)
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;From here we can decompress it and extract the inner archive much like before
using the appropriate utility (lz4cat does the trick here, your compression may
vary).&lt;&#x2F;p&gt;
&lt;pre&gt;&lt;code&gt;$ cd init_tmp
$ lz4cat ..&#x2F;inner-initramfs-current.img | cpio -mvid
.
bin
bin&#x2F;bash
bin&#x2F;cat
bin&#x2F;chown
bin&#x2F;chroot
...&lt;contents remove for brevity&gt;
var
var&#x2F;lock
var&#x2F;run
var&#x2F;tmp
54581 blocks
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;Bam! That is a lot more like it. I&#x27;m really curious how the kernel boot process
works with that early microcode but this got me where I needed to be and I hope
this helps someone else out.&lt;&#x2F;p&gt;
&lt;p&gt;Additional note: I missed this but apparently dracut ships with a utility
called &lt;code&gt;skipcpio&lt;&#x2F;code&gt; which you can pipe one of these initramfs files through to
skip the extra &lt;code&gt;dd&lt;&#x2F;code&gt; step. Had a friend point that out after I wrote this up...&lt;&#x2F;p&gt;
</content>
	</entry>
	<entry xml:lang="en">
		<title>Blender Loop Select in Cinnamon</title>
		<published>2019-11-27T16:42:02-05:00</published>
		<updated>2019-11-27T16:42:02-05:00</updated>
		<link href="https://stelfox.net/blog/2019/blender-loop-select-in-cinnamon/" type="text/html"/>
		<id>https://stelfox.net/blog/2019/blender-loop-select-in-cinnamon/</id>
		<content type="html">&lt;p&gt;I&#x27;ve recently been playing around with Blender (following &lt;a href=&quot;https:&#x2F;&#x2F;www.youtube.com&#x2F;playlist?list=PLjEaoINr3zgEq0u2MzVgAaHEBt--xLB6U&quot;&gt;this tutorial
series&lt;&#x2F;a&gt;). In Part 4 of the Level 1 series, the host Andrew Price is teaching
about loop selects which very simply is holding down Alt while clicking on a
vertex. The issue wasn&#x27;t working for me though I found quite a few other users
experiencing the issue.&lt;&#x2F;p&gt;
&lt;span id=&quot;continue-reading&quot;&gt;&lt;&#x2F;span&gt;
&lt;p&gt;The most common fix was when people had three button mouse emulation enabled (a
common setting people turn on when using laptops or Macs). I checked this and
it wasn&#x27;t my issue.&lt;&#x2F;p&gt;
&lt;p&gt;I&#x27;m running the Cinnamon desktop environment on Fedora (though I&#x27;m sure this
would universally apply to all Cinnamon instances), which has a &amp;quot;convenience&amp;quot;
feature that allows you to hold down a modifier key inside a window to move or
resize it. By default this is Alt.&lt;&#x2F;p&gt;
&lt;p&gt;To change this open up the &lt;code&gt;System Settings&lt;&#x2F;code&gt; app, navigate to the &lt;code&gt;Windows&lt;&#x2F;code&gt;
section, choose the &lt;code&gt;Behavior&lt;&#x2F;code&gt; tab and change the setting of &lt;code&gt;Special key to move and resize windows&lt;&#x2F;code&gt; to &lt;code&gt;Disabled&lt;&#x2F;code&gt;. Bam, problem solved loop select is
working like a charm.&lt;&#x2F;p&gt;
</content>
	</entry>
	<entry xml:lang="en">
		<title>Fixing Hung Nginx Workers</title>
		<published>2019-10-25T11:26:31-05:00</published>
		<updated>2019-10-25T11:26:31-05:00</updated>
		<link href="https://stelfox.net/blog/2019/fixing-hung-nginx-workers/" type="text/html"/>
		<id>https://stelfox.net/blog/2019/fixing-hung-nginx-workers/</id>
		<content type="html">&lt;p&gt;While cleaning up some tech debt, a curious issue cropped up. Nginx was running
in an alpine container as a front end load balancer. It had a dynamic config
that got periodically updated by a sidecar, and had filebeat shipping logs out
to a central collector but otherwise was just a very simple Nginx config.&lt;&#x2F;p&gt;
&lt;span id=&quot;continue-reading&quot;&gt;&lt;&#x2F;span&gt;
&lt;p&gt;Every now and then the container would crash, it would automatically recover
fast enough no alarms were lost and the clients would just resend their
requests. No data was losts, everything failed gracefully, but it&#x27;s still a
pretty crazy thing to leave happening.&lt;&#x2F;p&gt;
&lt;p&gt;There was nothing in the log besides some client errors and configuration
reloading notifications. The external metrics showed the container throwing
some kind of memory party before finally burning out and crashing. It was a
pretty steady trend upwards indicative of some kind of a memory leak. The graph
below illustrates this. Each of those drops is either the container crashing or
me manually restarting it to test behavior. Can you figure out when I fixed the
issue?&lt;&#x2F;p&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;stelfox.net&#x2F;blog&#x2F;2019&#x2F;fixing-hung-nginx-workers&#x2F;nginx_memory_consumption.png&quot; alt=&quot;Graph showing Nginx memory usage. The graph goes from chaotic to steady&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;p&gt;To figure out what was going on I had to get an invitation that container&#x27;s
party.&lt;&#x2F;p&gt;
&lt;p&gt;I don&#x27;t think there is great tooling for inspecting individual processes inside
of containers. As much as we&#x27;d like to think of containers as isolated
applications, its not uncommon for them to run &lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;krallin&#x2F;tini&quot;&gt;a minimal init container&lt;&#x2F;a&gt;,
or be composed of separate different processes themselves that handle different
things (Nginx has a master process and spawns off many worker processes). We
still need the data on those processes.&lt;&#x2F;p&gt;
&lt;p&gt;The common solution to getting this data seems to be to &amp;quot;just exec into the
container&amp;quot;. In general &lt;code&gt;exec&lt;&#x2F;code&gt; should be restricted. If you find yourself
running exec in a container, you&#x27;re missing tooling or metrics that should
avoid that (which is the case here).&lt;&#x2F;p&gt;
&lt;p&gt;Denying &lt;code&gt;exec&lt;&#x2F;code&gt; is probably a longer discussion but the short of it is: If you
can &lt;code&gt;exec&lt;&#x2F;code&gt; into the container, you can extract its secrets, configuration, and
access any services that it&#x27;s allowed to (likely escalating your personal
privileges within the cluster). This is without considering the ramifications
of &lt;code&gt;exec&lt;&#x2F;code&gt;&#x27;ing into a privileged container.&lt;&#x2F;p&gt;
&lt;p&gt;In any event, not all container clusters solution allow external execution
(looking at you AWS ECS, its the one good thing I have to say about you).&lt;&#x2F;p&gt;
&lt;p&gt;I chose to modify the container to log the data I needed instead. I setup a
wrapper that started and backgrounded the following script:&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;sh&quot; class=&quot;language-sh &quot;&gt;&lt;code class=&quot;language-sh&quot; data-lang=&quot;sh&quot;&gt;#!&#x2F;bin&#x2F;sh

while [ 1 ]; do
  # This generates a JSON output that can be pulled via the ECS logs of the top
  # memory consuming processes using what we have available in alpine.
  ps -o pid,time,rss,vsz,args |
    tail -n +2 |
    awk &#x27;memstat: { print &quot;{\&quot;pid\&quot;:&quot; $1 &quot;,\&quot;time\&quot;:\&quot;&quot; $2 &quot;\&quot;,\&quot;rss\&quot;:&quot; $3 &quot;,\&quot;vsz\&quot;:&quot; $4 &quot;,\&quot;cmd\&quot;:\&quot;&quot; substr($0, index($0,$5)) &quot;\&quot;}&quot; }&#x27;

  sleep 300
done
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;Yes, I&#x27;m abusing &lt;code&gt;awk&lt;&#x2F;code&gt; to generate JSON I can parse later.&lt;&#x2F;p&gt;
&lt;p&gt;Each message has a prefix of &lt;code&gt;memstat:&lt;&#x2F;code&gt; so I can easily filter and extract that
data from the live log stream. After a couple of hours (the time it generally
took for the party to get going) the issue was pretty obvious. Inside the
container there was a growing number of processes with the name &lt;code&gt;nginx: worker process is shutting down&lt;&#x2F;code&gt;. Some Googling turned up &lt;a href=&quot;https:&#x2F;&#x2F;forum.nginx.org&#x2F;read.php?2,262403,262403&quot;&gt;a post in the Nginx mailing
list&lt;&#x2F;a&gt; from a while ago.&lt;&#x2F;p&gt;
&lt;p&gt;The responses claimed that it was the result of a third party module, but this
is stock Nginx (version 1.15.8 at the time) which was at least three years
older than the post. It did indicate a clue though: config reloading.&lt;&#x2F;p&gt;
&lt;p&gt;I don&#x27;t know why Nginx believed connections were still open, we have request
and connect timeouts both in this config and upstream. Some of these processes
were sticking around for hours (I never actually saw one exit after getting
into this hung state). Every config reload was dropping a zombie worker or two
into the container, permanently consuming ~30Mb of RAM. When it hit the
configured threshold, the party gets stopped.&lt;&#x2F;p&gt;
&lt;p&gt;An option was added in the &lt;a href=&quot;http:&#x2F;&#x2F;nginx.org&#x2F;en&#x2F;docs&#x2F;ngx_core_module.html#worker_shutdown_timeout&quot;&gt;Nginx core module&lt;&#x2F;a&gt; to handle situations where
the worker wouldn&#x27;t close, but this definitely seems like a bug of some kind.
There is a newer version of Nginx (1.17.5 at the time of the writing) that
could very well have fixed this issue.&lt;&#x2F;p&gt;
&lt;p&gt;Adding &lt;code&gt;worker_shutdown_timeout 60s;&lt;&#x2F;code&gt; to the main Nginx config solved the issue
(60 seconds matches our request and connect timeouts so nothing valid should
last longer than that). Sure enough Nginx went back to stable, and predictably
low memory usage.&lt;&#x2F;p&gt;
</content>
	</entry>
	<entry xml:lang="en">
		<title>Fixing Dark Input Boxes in Firefox</title>
		<published>2019-04-13T11:53:22-04:00</published>
		<updated>2019-04-13T11:53:22-04:00</updated>
		<link href="https://stelfox.net/blog/2019/fixing-dark-input-boxes-in-firefox/" type="text/html"/>
		<id>https://stelfox.net/blog/2019/fixing-dark-input-boxes-in-firefox/</id>
		<content type="html">&lt;p&gt;I recently began trying out &lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;linuxmint&#x2F;Cinnamon&quot;&gt;Cinnamon&lt;&#x2F;a&gt; as my desktop environment and I&#x27;ve
been thoroughly enjoying it. The only issue I was having was occasionally a
page&#x27;s form input fields would have a dark background while still having dark
text making it impossible to read, and very difficult to write.&lt;&#x2F;p&gt;
&lt;span id=&quot;continue-reading&quot;&gt;&lt;&#x2F;span&gt;
&lt;p&gt;It wasn&#x27;t happening everywhere, and I couldn&#x27;t track down what about a website
would cause the issue. Most prominently for me was when this showed up in AWS&#x27;s
interface.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;stelfox.net&#x2F;blog&#x2F;2019&#x2F;fixing-dark-input-boxes-in-firefox&#x2F;dark_firefox_inputs.png&quot; alt=&quot;Example of Dark Input in AWS Security Groups&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;p&gt;Searching around apparently this was an issue with dark GTK themes and has been
a &lt;a href=&quot;https:&#x2F;&#x2F;bugzilla.mozilla.org&#x2F;show_bug.cgi?id=70315&quot;&gt;known issue for about 18 years&lt;&#x2F;a&gt; and was re-reported again &lt;a href=&quot;https:&#x2F;&#x2F;bugzilla.mozilla.org&#x2F;show_bug.cgi?id=1283086&quot;&gt;about 3 years
ago&lt;&#x2F;a&gt;. There have been a really bad workaround that people have suggested
including &lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;DmitriK&#x2F;darkContrast#text-contrast-for-dark-themes&quot;&gt;extensions&lt;&#x2F;a&gt;, &lt;a href=&quot;https:&#x2F;&#x2F;stackoverflow.com&#x2F;questions&#x2F;19911090&#x2F;firefox-how-to-see-text-in-input-fields-with-black-background-set-in-preference&quot;&gt;custom user CSS&lt;&#x2F;a&gt;, and &lt;a href=&quot;https:&#x2F;&#x2F;medium.com&#x2F;@lsm&#x2F;fix-firefox-dark-text-input-on-ubuntu-18-when-using-gnome-dark-themes-98f253f8ed7f&quot;&gt;using desktop shortcuts
with environment variables&lt;&#x2F;a&gt;.&lt;&#x2F;p&gt;
&lt;p&gt;None of these work well, will be inconsistent based on how you open Firefox, or
might cause other issues with different websites. Generally I&#x27;m not a fan of
using random extensions for security, stability, and performance reasons.
Ultimately Mozilla hasn&#x27;t fixed this issue, but there is a workable override at
this point.&lt;&#x2F;p&gt;
&lt;p&gt;To solve the issue we do need to override the GTK theme with a light one.
Generally people recommend &lt;code&gt;Adwaita&lt;&#x2F;code&gt; for the light theme that fixes it and it
seems to work well for me. You can check and make sure that the theme is
available by checking the &lt;code&gt;&#x2F;usr&#x2F;share&#x2F;themes&lt;&#x2F;code&gt; directory on your system.&lt;&#x2F;p&gt;
&lt;p&gt;When you&#x27;ve confirmed the presence of the theme, open up Firefox and go to the
URL &lt;code&gt;about:config&lt;&#x2F;code&gt;. This override value doesn&#x27;t exist by default so you&#x27;ll need
to right click anywhere in that page and go to &lt;code&gt;New -&amp;gt; String&lt;&#x2F;code&gt; in that context
menu. For the preference name enter &lt;code&gt;widget.content.gtk-theme-override&lt;&#x2F;code&gt; and for
the value enter &lt;code&gt;Adwaita&lt;&#x2F;code&gt;. After refreshing the effected pages the background
color issue should be resolved.&lt;&#x2F;p&gt;
&lt;p&gt;If you&#x27;re making use of a &lt;code&gt;user.js&lt;&#x2F;code&gt; to enforce configuration options you can
add the following line to it address the issue:&lt;&#x2F;p&gt;
&lt;pre&gt;&lt;code&gt;user_pref(&quot;widget.content.gtk-theme-override&quot;, &quot;Adwaita&quot;);
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;A little side note in case someone comes looking for other options. There was
another &lt;code&gt;about:config&lt;&#x2F;code&gt; option that some Stack Overflow answers recommended
changing, &lt;code&gt;widget.content.allow-gtk-dark-theme&lt;&#x2F;code&gt;. The default is &lt;code&gt;false&lt;&#x2F;code&gt; and
they&#x27;re recommending changing it to... &lt;code&gt;false&lt;&#x2F;code&gt;. It&#x27;s not the answer to this
problem.&lt;&#x2F;p&gt;
&lt;p&gt;Hope this helps someone else out. Cheers!&lt;&#x2F;p&gt;
</content>
	</entry>
	<entry xml:lang="en">
		<title>Merging Overlapping Subnets</title>
		<published>2019-03-27T19:11:30-04:00</published>
		<updated>2019-03-27T19:11:30-04:00</updated>
		<link href="https://stelfox.net/blog/2019/merging-overlapping-subnets/" type="text/html"/>
		<id>https://stelfox.net/blog/2019/merging-overlapping-subnets/</id>
		<content type="html">&lt;p&gt;Once upon a time there was a single AWS account. In this AWS account was
several regions but a single VPC. To make sure expansions into other regions
was possible this VPC chose to use the largest private subnet which just so
happened to also be the default (&lt;code&gt;10.0.0.0&#x2F;8&lt;&#x2F;code&gt;).&lt;&#x2F;p&gt;
&lt;span id=&quot;continue-reading&quot;&gt;&lt;&#x2F;span&gt;
&lt;p&gt;Another AWS account enter the picture and while they were single they came to
the same conclusion and followed the best practices and defaults to their
heart&#x27;s content. Normally this wouldn&#x27;t be a problem for either of them, but
they found each other and tied the knot and were happily together for the rest
of time...&lt;&#x2F;p&gt;
&lt;p&gt;But in this story there is a darkness looming. Communication was not everything
either of them desired. There were secret things that couldn&#x27;t be said in
public forums of the internet but they both desperately wanted share. There was
a solution... But it involved dark magicks.&lt;&#x2F;p&gt;
&lt;p&gt;I found myself in a situation where two AWS VPCs needed to communicate
sensitive data between the two, but they were using overlapping IP address
spaces. There was a lot of room available in both, but even some individual IPs
overlapped and renumbering would prove problematic and time consuming.
Eventually these two VPCs were intended to be merged anyway, but business
requirements needed a basic level of communication sooner.&lt;&#x2F;p&gt;
&lt;p&gt;The solution I came up with may be useful for others in a pinch; Two layers of
1:1 NAT were employed allowing each to communicate with what each side seemed
to believe were unique IPs. To do this we need to have a usable IP address that
we can map into without potentially wrecking havoc on access to random sites on
the internet.&lt;&#x2F;p&gt;
&lt;p&gt;I was lucky in that all the hosts that need to talk to each other had addresses
on both sides below &lt;code&gt;10.7.0.x&lt;&#x2F;code&gt;. This is more addresses than are available to
the &lt;code&gt;192.168.0.0&#x2F;16&lt;&#x2F;code&gt; private address space but covers only about 25% of the
&lt;code&gt;172.16.0.0&#x2F;16&lt;&#x2F;code&gt; space. If you&#x27;re in a worse situation where hosts are properly
scattered all over the &lt;code&gt;10.0.0.0&#x2F;8&lt;&#x2F;code&gt; address you can still use this technique
but it will require a bit more manual configuration mapping allocating either
&#x2F;24 to route or in the most extreme case individual host addresses.&lt;&#x2F;p&gt;
&lt;p&gt;Before we go any further, I definitely consider this technique to be a band-aid
for the issue. For longer term connectivity some form of migration should be
planned and executed on. This makes a GREAT and stable band-aid though.&lt;&#x2F;p&gt;
&lt;p&gt;If you&#x27;d like to follow along you&#x27;ll need two VPCs, each with two EC2 instances
to work as the tunnel hosts and likely two more to be test hosts to make use of
the tunnels.&lt;&#x2F;p&gt;
&lt;p&gt;This part is easy, we&#x27;ll use CentOS 7 hosts as a base. You&#x27;ll need to
additionally install the following software:&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;iptables-services&lt;&#x2F;li&gt;
&lt;li&gt;libreswan&lt;&#x2F;li&gt;
&lt;li&gt;tcpdump (optional but invaluable to diagnose issues)&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;If you&#x27;re not on AWS you&#x27;ll also want to make sure that NetworkManager and
firewalld are both &lt;em&gt;&lt;strong&gt;removed from the system&lt;&#x2F;strong&gt;&lt;&#x2F;em&gt;. They will break the
configurations you put in place if left to their own machinations. If you
remove NetworkManager remember to enable the network service. For good measure
here is a minimal DHCP config you can use to configure &lt;code&gt;eth0&lt;&#x2F;code&gt; on your system:&lt;&#x2F;p&gt;
&lt;pre&gt;&lt;code&gt;# &#x2F;etc&#x2F;sysconfig&#x2F;network-scripts&#x2F;ifcfg-eth0

DEVICE=&quot;eth0&quot;
NM_CONTROLLED=&quot;no&quot;
ONBOOT=&quot;yes&quot;
TYPE=&quot;Ethernet&quot;

BOOTPROTO=&quot;dhcp&quot;
IPV4_FAILURE_FATAL=&quot;yes&quot;
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;Let&#x27;s also start with a minimal IPTables ruleset. This is pretty close to the
defaults, but it&#x27;s good to be sure that we&#x27;re all on the same page:&lt;&#x2F;p&gt;
&lt;pre&gt;&lt;code&gt;# &#x2F;etc&#x2F;sysconfig&#x2F;iptables

*nat
:PREROUTING ACCEPT [0:0]
:INPUT ACCEPT [0:0]
:OUTPUT ACCEPT [0:0]
:POSTROUTING ACCEPT [0:0]

# NAT rules will be added here

COMMIT

*filter
:INPUT DROP [0:0]
:FORWARD DROP [0:0]
:OUTPUT ACCEPT [0:0]

-A INPUT -m conntrack --ctstate ESTABLISHED,RELATED -j ACCEPT
-A INPUT -p icmp -j ACCEPT
-A INPUT -i lo -j ACCEPT

-A INPUT -m tcp -p tcp --dport 22 -j ACCEPT

# Filter rules will be added here

-A INPUT -j REJECT --reject-with icmp-host-prohibited
-A FORWARD -j REJECT --reject-with icmp-host-prohibited

COMMIT
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;Place the contents of that file in &lt;code&gt;&#x2F;etc&#x2F;sysconfig&#x2F;iptables&lt;&#x2F;code&gt; as the header
indicates. The differences from the default are mostly in that we have also
defined the &lt;code&gt;nat&lt;&#x2F;code&gt; table and switched the default action on the &lt;code&gt;INPUT&lt;&#x2F;code&gt; and
&lt;code&gt;FORWARD&lt;&#x2F;code&gt; chains to drop. Both the default and this one will reject the traffic
anyways so this doesn&#x27;t actually change the behavior of the firewall.&lt;&#x2F;p&gt;
&lt;p&gt;Defining the &lt;code&gt;nat&lt;&#x2F;code&gt; table doesn&#x27;t change any behavior either, but I&#x27;ll be
referencing it later on in the post and you should add the rules where
indicated by the comment. If you get confused by any of my instructions around
adding the firewall rules, there is a complete ruleset at the end of the post
you can reference directly.&lt;&#x2F;p&gt;
&lt;p&gt;Finally let&#x27;s make sure the firewall is enabled and running:&lt;&#x2F;p&gt;
&lt;pre&gt;&lt;code&gt;systemctl enable iptables.service
systemctl start iptables.service
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;h2 id=&quot;basic-connectivity&quot;&gt;Basic Connectivity&lt;&#x2F;h2&gt;
&lt;p&gt;From this point on it is going to become important to distinguish the two
networks I&#x27;ll be bridging. This method is very symmetric (all the firewalls and
configs should effectively be the same on the two tunnel instances) but there
are a few places where the remote IP and local IPs need to be referenced. Going
forward I&#x27;m going to refer to the two networks as &lt;code&gt;east&lt;&#x2F;code&gt; and &lt;code&gt;west&lt;&#x2F;code&gt; but these
are arbitrary labels.&lt;&#x2F;p&gt;
&lt;p&gt;You&#x27;ll need to collect the public IP from the AWS console for your tunnel hosts
in both the &lt;code&gt;east&lt;&#x2F;code&gt; and &lt;code&gt;west&lt;&#x2F;code&gt;. For me I&#x27;m going to use &lt;code&gt;5.5.5.5&lt;&#x2F;code&gt; for the &lt;code&gt;west&lt;&#x2F;code&gt;
IP and &lt;code&gt;7.7.7.7&lt;&#x2F;code&gt; for the &lt;code&gt;east&lt;&#x2F;code&gt; IP. If you see these in the configs you&#x27;ll want
to replace them with the appropriate values for your networks. If you expect
this to last a long time or will be a business critical tunnel I highly
recommend using an Elastic IP on each of these hosts.&lt;&#x2F;p&gt;
&lt;p&gt;You&#x27;ll need to setup a dedicated security group for each of the tunnel hosts.
To avoid bouncing back and forth between these the security groups as we
progress through the guide I&#x27;m going to put all the rules we&#x27;re going to need
in the following table. These are inbound rules only and can be hardened a bit
(but I&#x27;ll get to that later), let&#x27;s focus on getting this up and running first.&lt;&#x2F;p&gt;
&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th align=&quot;center&quot;&gt;Type&lt;&#x2F;th&gt;&lt;th align=&quot;center&quot;&gt;Protocol&lt;&#x2F;th&gt;&lt;th align=&quot;center&quot;&gt;Port Range&lt;&#x2F;th&gt;&lt;th align=&quot;center&quot;&gt;Source&lt;&#x2F;th&gt;&lt;th align=&quot;center&quot;&gt;Description&lt;&#x2F;th&gt;&lt;&#x2F;tr&gt;&lt;&#x2F;thead&gt;&lt;tbody&gt;
&lt;tr&gt;&lt;td align=&quot;center&quot;&gt;SSH&lt;&#x2F;td&gt;&lt;td align=&quot;center&quot;&gt;TCP&lt;&#x2F;td&gt;&lt;td align=&quot;center&quot;&gt;22&lt;&#x2F;td&gt;&lt;td align=&quot;center&quot;&gt;0.0.0.0&#x2F;0&lt;&#x2F;td&gt;&lt;td align=&quot;center&quot;&gt;SSH Access&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td align=&quot;center&quot;&gt;Custom Protocol&lt;&#x2F;td&gt;&lt;td align=&quot;center&quot;&gt;ESP (50)&lt;&#x2F;td&gt;&lt;td align=&quot;center&quot;&gt;All&lt;&#x2F;td&gt;&lt;td align=&quot;center&quot;&gt;{other public IP}&#x2F;32&lt;&#x2F;td&gt;&lt;td align=&quot;center&quot;&gt;IPSec Encapsulated Packets&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td align=&quot;center&quot;&gt;Custom UDP Rule&lt;&#x2F;td&gt;&lt;td align=&quot;center&quot;&gt;UDP&lt;&#x2F;td&gt;&lt;td align=&quot;center&quot;&gt;500&lt;&#x2F;td&gt;&lt;td align=&quot;center&quot;&gt;{other public IP}&#x2F;32&lt;&#x2F;td&gt;&lt;td align=&quot;center&quot;&gt;IPSec Key Management&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td align=&quot;center&quot;&gt;Custom ICMP Rule - IPv4&lt;&#x2F;td&gt;&lt;td align=&quot;center&quot;&gt;Echo Request&lt;&#x2F;td&gt;&lt;td align=&quot;center&quot;&gt;N&#x2F;A&lt;&#x2F;td&gt;&lt;td align=&quot;center&quot;&gt;0.0.0.0&#x2F;0&lt;&#x2F;td&gt;&lt;td align=&quot;center&quot;&gt;Connectivity Checking&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td align=&quot;center&quot;&gt;All TCP&lt;&#x2F;td&gt;&lt;td align=&quot;center&quot;&gt;TCP&lt;&#x2F;td&gt;&lt;td align=&quot;center&quot;&gt;0-65535&lt;&#x2F;td&gt;&lt;td align=&quot;center&quot;&gt;10.0.0.0&#x2F;8&lt;&#x2F;td&gt;&lt;td align=&quot;center&quot;&gt;Internal TCP Traffic&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td align=&quot;center&quot;&gt;All UDP&lt;&#x2F;td&gt;&lt;td align=&quot;center&quot;&gt;UDP&lt;&#x2F;td&gt;&lt;td align=&quot;center&quot;&gt;0-65535&lt;&#x2F;td&gt;&lt;td align=&quot;center&quot;&gt;10.0.0.0&#x2F;8&lt;&#x2F;td&gt;&lt;td align=&quot;center&quot;&gt;Internal UDP Traffic&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;&#x2F;tbody&gt;&lt;&#x2F;table&gt;
&lt;p&gt;You&#x27;ll want to replace &lt;code&gt;{other public IP}&lt;&#x2F;code&gt; with the public IP of the tunnel
host in the opposite network. For example if this is the security group for the
&lt;code&gt;west&lt;&#x2F;code&gt; tunnel host, you&#x27;d be allowing the traffic from &lt;code&gt;7.7.7.7&lt;&#x2F;code&gt;.&lt;&#x2F;p&gt;
&lt;p&gt;If you&#x27;re doing this in another environment you may also need &lt;code&gt;UDP&#x2F;4500&lt;&#x2F;code&gt; from
the other public IP when NAT traversal is required. AWS EC2 instances are NAT&#x27;d
&lt;a href=&quot;https:&#x2F;&#x2F;stelfox.net&#x2F;blog&#x2F;2019&#x2F;aws-elastic-ip-details&#x2F;&quot;&gt;but we can work around that&lt;&#x2F;a&gt; and will include that later on.&lt;&#x2F;p&gt;
&lt;p&gt;With the security groups in place, and the local firewalls configured make sure
each host can ping each other. If they can great! If not, double check all the
IPs, security group rules, and iptables rules all match what I have documented
here.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;the-ipsec-tunnel&quot;&gt;The IPSec Tunnel&lt;&#x2F;h2&gt;
&lt;p&gt;This tunnel provides strong authentication and encryption for all the traffic
that will be exchanged between the two networks. We&#x27;ve already installed the
required packages we just need to configure the various pieces to get it
running.&lt;&#x2F;p&gt;
&lt;p&gt;First let&#x27;s handle the firewall. In the &lt;code&gt;&#x2F;etc&#x2F;sysconfig&#x2F;iptables&lt;&#x2F;code&gt; file we
standardized on earlier we need to add a couple of rules to each tunnel host
for the IPSec traffic. Add these just after the note for adding filter rules
and before the &lt;code&gt;REJECT&lt;&#x2F;code&gt; rules:&lt;&#x2F;p&gt;
&lt;pre&gt;&lt;code&gt;-A INPUT -p esp -j ACCEPT
-A INPUT -m udp -p udp --sport 500 --dport 500 -j ACCEPT
#-A INPUT -m udp -p udp --sport 4500 --dport 4500 -j ACCEPT
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;This will allow tunneled packets and key exchange through the firewall. If
you&#x27;re not on AWS when setting this up you may need to uncomment that third
rule for NAT traversal packets.&lt;&#x2F;p&gt;
&lt;p&gt;These rules are pretty unrestricted, but we have already narrowed down who will
be able to connect using the security group for these machines. By leaving a
more refined specification out of our definition here our IPTables rules can
remain symmetric on both hosts making automated management through a devops
tool simpler.&lt;&#x2F;p&gt;
&lt;p&gt;Next up there are some specific sysctl settings that need to be adjusted for
the tunneled packets to not be rejected by the kernel. The reason behind the
sysctl settings is pretty well &lt;a href=&quot;https:&#x2F;&#x2F;libreswan.org&#x2F;wiki&#x2F;FAQ&quot;&gt;documented on LibreSwan&#x27;s FAQ&lt;&#x2F;a&gt; if you&#x27;re
curious for why they&#x27;re needed.&lt;&#x2F;p&gt;
&lt;p&gt;You&#x27;ll want to append the following to &lt;code&gt;&#x2F;etc&#x2F;sysctl.conf&lt;&#x2F;code&gt; on both tunnel hosts:&lt;&#x2F;p&gt;
&lt;pre&gt;&lt;code&gt;net.ipv4.conf.default.accept_redirects = 0
net.ipv4.conf.default.send_redirects = 0
net.ipv4.conf.default.rp_filter = 0
net.ipv4.conf.all.accept_redirects = 0
net.ipv4.conf.all.send_redirects = 0
net.ipv4.conf.all.rp_filter = 0

# Annoyingly, this seems to ignore the defaults set above. This should be
# interface that libreswan will be receiving the IPSec connections on
net.ipv4.conf.eth0.rp_filter = 0
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;With that in place run &lt;code&gt;sysctl -p&lt;&#x2F;code&gt; to apply the new settings and &lt;code&gt;systemctl restart iptables.service&lt;&#x2F;code&gt; to update the firewall rules.&lt;&#x2F;p&gt;
&lt;p&gt;We&#x27;ll quickly do a global IPSec config to make sure we&#x27;re on the same page.
Replace the contents of &lt;code&gt;&#x2F;etc&#x2F;ipsec.conf&lt;&#x2F;code&gt; on each tunnel host with the
following:&lt;&#x2F;p&gt;
&lt;pre&gt;&lt;code&gt;# &#x2F;etc&#x2F;ipsec.conf

config setup
  protostack=netkey

include &#x2F;etc&#x2F;ipsec.d&#x2F;*.conf
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;IPSec has a couple of ways of handling authentication. The most secure is
asymmetric encryption using RSA keys which requires each host to have a private
key and knowledge of the other host&#x27;s public key. To these keys on each tunnel
host run the following commands:&lt;&#x2F;p&gt;
&lt;pre&gt;&lt;code&gt;sudo ipsec initnss
sudo ipsec newhostkey --output &#x2F;etc&#x2F;ipsec.secrets
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;Similar to our &lt;code&gt;west&lt;&#x2F;code&gt; and &lt;code&gt;east&lt;&#x2F;code&gt; analogy, IPSec has the concept of &lt;code&gt;left&lt;&#x2F;code&gt; and
&lt;code&gt;right&lt;&#x2F;code&gt; hosts on either side of a tunnel. We&#x27;re going to map them the same way
but need to get the public keys of each host first so they can verify each
other.&lt;&#x2F;p&gt;
&lt;p&gt;On our &lt;code&gt;west&lt;&#x2F;code&gt; host, which will be our &lt;code&gt;left&lt;&#x2F;code&gt; host for the IPSec config retrieve
the public key with the following two commands. The output will be one very
long line that begins with &lt;code&gt;leftrsasigkey=&lt;&#x2F;code&gt; record this entire output.&lt;&#x2F;p&gt;
&lt;pre&gt;&lt;code&gt;CKAID=&quot;$(sudo ipsec showhostkey --list | head -n 1 | awk &#x27;{ print $NF }&#x27;)&quot;
sudo ipsec showhostkey --left --ckaid ${CKAID} | tail -n 1
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;A bit of an explanation of those two commands. The first one extracts the
unique key identifier for the first key present (there shouldn&#x27;t be any
others), while the second gets the actual public key for that identifier. We&#x27;ll
need to repeat the process on our &lt;code&gt;east&lt;&#x2F;code&gt; host slightly modified which will be
our &lt;code&gt;right&lt;&#x2F;code&gt; host:&lt;&#x2F;p&gt;
&lt;pre&gt;&lt;code&gt;CKAID=&quot;$(sudo ipsec showhostkey --list | head -n 1 | awk &#x27;{ print $NF }&#x27;)&quot;
sudo ipsec showhostkey --right --ckaid ${CKAID} | tail -n 1
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;This will also output another long line that this time will begin with
&lt;code&gt;rightrsasigkey=&lt;&#x2F;code&gt; which you should also record. On both hosts you&#x27;ll want to
place the following IPSec tunnel config at &lt;code&gt;&#x2F;etc&#x2F;ipsec.d&#x2F;vpc-link-tunnel.conf&lt;&#x2F;code&gt;:&lt;&#x2F;p&gt;
&lt;pre&gt;&lt;code&gt;conn vpc-link-tunnel
  auto=start
  pfs=yes
  type=transport

  leftid=@west_tunnel_server
  rightid=@east_tunnel_server
  left={west external ip}
  right={east external ip}

  authby=rsasig
  leftrsasigkey={left&#x2F;west sig key}
  rightrsasigkey={right&#x2F;east sig key}
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;Be sure to replace the &lt;code&gt;{west external ip}&lt;&#x2F;code&gt; with the external IP address of our
&lt;code&gt;west&lt;&#x2F;code&gt; server and likewise the &lt;code&gt;{east external ip}&lt;&#x2F;code&gt; with the external IP
address of our &lt;code&gt;east&lt;&#x2F;code&gt; server. Be sure to replace the last two lines with the
output of the two keys we got from our &lt;code&gt;west&lt;&#x2F;code&gt; and &lt;code&gt;east&lt;&#x2F;code&gt; tunnel hosts.&lt;&#x2F;p&gt;
&lt;p&gt;That&#x27;s it for the IPSec configuration, let&#x27;s start the daemon up and verify
that it&#x27;s working on both tunnel servers:&lt;&#x2F;p&gt;
&lt;pre&gt;&lt;code&gt;sudo systemctl enable ipsec.service
sudo systemctl start ipsec.service
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;Let&#x27;s check the IPSec status to make sure it&#x27;s happy:&lt;&#x2F;p&gt;
&lt;pre&gt;&lt;code&gt;sudo ipsec status
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;There will be quite a bit of output but what you&#x27;re looking for is a line that
looks like this:&lt;&#x2F;p&gt;
&lt;pre&gt;&lt;code&gt;000 Total IPsec connections: loaded 1, active 1
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;If the loaded count is 0, double check the presence and file names as well as
the global config. If you&#x27;ve properly loaded the config but it isn&#x27;t coming up
as active, review the contents of &lt;code&gt;&#x2F;var&#x2F;log&#x2F;secure&lt;&#x2F;code&gt; for any IPSec error
messages. If there is an authentication error, most likely the public keys got
copied incorrectly. Make sure that both keys exist in both configs and match
the outputs from the key extraction commands earlier on.&lt;&#x2F;p&gt;
&lt;p&gt;If there are connection issues there are quite a few other bits that could have
gone wrong. Review the firewalls, security groups, and IPSec configs to make
sure the addresses are correct and the protocols are allowed through.&lt;&#x2F;p&gt;
&lt;p&gt;Once the details have been worked out and the tunnel is up, all the traffic
between the two hosts should now be encrypted. This can be verified using
&lt;code&gt;tcpdump&lt;&#x2F;code&gt; and sending a couple pings at the other host. When IPSec is flowing
the traffic will look something along the lines of:&lt;&#x2F;p&gt;
&lt;pre&gt;&lt;code&gt;21:51:02.807688 IP 10.0.1.156 &gt; 7.7.7.7: ESP(spi=0x171f19e9,seq=0xe), length 116
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;Make sure this is working, everything beyond this depends on the IPSec tunnel
up and running correctly.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;the-gre-tunnel&quot;&gt;The GRE Tunnel&lt;&#x2F;h2&gt;
&lt;p&gt;The GRE overlay isn&#x27;t required for this to work and does add 24 bytes of
overhead to each packet but it provides us some benefits.&lt;&#x2F;p&gt;
&lt;p&gt;The first and probably most important is that each end will have a fixed
private IP address as it&#x27;s routing target. If the GRE tunnel is down for any
reason the tunnel host won&#x27;t attempt to send any forwarded traffic to a public
IP address. This provides a layer of security against other misconfigurations.&lt;&#x2F;p&gt;
&lt;p&gt;Since all of the traffic will only be routed to the tunnel endpoint if the GRE
tunnel is up and will always travel over the GRE tunnel we can simplify our
firewall policy around enforcement of encrypted traffic. If we guarantee all
GRE traffic is encrypted over the IPSec tunnel, all traffic using the GRE
tunnel will be encrypted with only a single universal firewall rule.&lt;&#x2F;p&gt;
&lt;p&gt;One final benefit with the firewall is that we get a separate interface we can
use to identify the direction traffic is traveling through our tunnels without
worrying about the details of IP addresses (which will be changing in unusual
ways later on).&lt;&#x2F;p&gt;
&lt;p&gt;If these benefits don&#x27;t justify the 24 byte per packet overhead to you, you&#x27;re
welcome to skip this section but you&#x27;ll need to figure out the changes to the
firewall rules and routing tables on your own later on.&lt;&#x2F;p&gt;
&lt;p&gt;Let&#x27;s start the setup with a safety net. We need to allow the GRE traffic
through the firewall on the tunnel hosts, but we want to make sure that we only
pass if it has been properly encrypted with the IPSec. We can use the iptables
&lt;code&gt;policy&lt;&#x2F;code&gt; module. Add the following rules to the filter section of each of our
firewalls:&lt;&#x2F;p&gt;
&lt;pre&gt;&lt;code&gt;-A INPUT -m policy --dir in --pol ipsec --proto esp -p gre -j ACCEPT
-A OUTPUT -m policy --dir out --pol ipsec --proto esp -p gre -j ACCEPT
-A OUTPUT -p gre -j DROP
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;These three lines are all that is required to enforce that all of our traffic
being routed between the two networks will always be encrypted if they have any
hope of making it.&lt;&#x2F;p&gt;
&lt;p&gt;Restart the firewall so the change can take effect:&lt;&#x2F;p&gt;
&lt;pre&gt;&lt;code&gt;sudo systemctl restart iptables.service
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;Configuring a GRE tunnel on a CentOS box is very simple on each host create a
new file &lt;code&gt;&#x2F;etc&#x2F;sysconfig&#x2F;network-scripts&#x2F;ifcfg-tun0&lt;&#x2F;code&gt; with the following
contents:&lt;&#x2F;p&gt;
&lt;pre&gt;&lt;code&gt;# &#x2F;etc&#x2F;sysconfig&#x2F;network-scripts&#x2F;ifcfg-tun0

DEVICE=tun0
BOOTPROTO=none
ONBOOT=yes
TYPE=GRE

MY_INNER_IPADDR=10.255.254.1
#MY_OUTER_IPADDR={current side external IP}

PEER_INNER_IPADDR=10.255.254.2
PEER_OUTER_IPADDR={opposing side external IP}

# Not needed since we only have one tunnel. Can be any 32 bit numerical value
#KEY=12345678
EOF
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;For completeness I&#x27;ve included &lt;code&gt;MY_OUTER_IPADDR&lt;&#x2F;code&gt; and &lt;code&gt;KEY&lt;&#x2F;code&gt; commented out as
they may be useful for other GRE tunnels but not necessary for this one. For
the &lt;code&gt;west&lt;&#x2F;code&gt; server &lt;code&gt;{current side external IP}&lt;&#x2F;code&gt; should be replaced by the &lt;code&gt;west&lt;&#x2F;code&gt;
tunnel server&#x27;s external IP and &lt;code&gt;{opposing side external IP}&lt;&#x2F;code&gt; with the east
tunnel server&#x27;s external IP. Reverse the settings on the east tunnel server.&lt;&#x2F;p&gt;
&lt;p&gt;On each tunnel host bring the tunnel up:&lt;&#x2F;p&gt;
&lt;pre&gt;&lt;code&gt;sudo ifup tun0
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;The state of the tunnel can be checked with &lt;code&gt;sudo ip addr show tun0&lt;&#x2F;code&gt; which will
have an output similar to the following:&lt;&#x2F;p&gt;
&lt;pre&gt;&lt;code&gt;5: tun0@NONE: &lt;POINTOPOINT,NOARP,UP,LOWER_UP&gt; mtu 8977 qdisc noqueue state UNKNOWN group default qlen 1000
    link&#x2F;gre 0.0.0.0 peer 7.7.7.7
    inet 10.255.254.1 peer 10.255.254.2&#x2F;32 scope global tun0
       valid_lft forever preferred_lft forever
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;You&#x27;re specifically looking for the &lt;code&gt;UP&lt;&#x2F;code&gt; and &lt;code&gt;LOWER_UP&lt;&#x2F;code&gt; flags. You can double
check the tunnel is functioning by pinging &lt;code&gt;10.255.254.1&lt;&#x2F;code&gt; and &lt;code&gt;10.255.254.2&lt;&#x2F;code&gt;
from the &lt;code&gt;east&lt;&#x2F;code&gt; and &lt;code&gt;west&lt;&#x2F;code&gt; tunnel host respectively.&lt;&#x2F;p&gt;
&lt;p&gt;We now have a private encrypted layer 2 tunnel between the two VPC tunnel
hosts, next up is to get other traffic in the VPC passing across the tunnel.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;tunnel-host-routing-and-rewriting&quot;&gt;Tunnel Host Routing and Rewriting&lt;&#x2F;h2&gt;
&lt;p&gt;Up to this point everything has been setting up pretty standard tunnels between
Linux hosts. This is where the magic needs to start happening. Each network
needs to see the other network with a different IP space. I&#x27;ve already
discussed that I&#x27;ll be using &lt;code&gt;172.16.0.0&#x2F;12&lt;&#x2F;code&gt; as our mapping network.&lt;&#x2F;p&gt;
&lt;p&gt;Since we&#x27;re going to start forwarding traffic between networks we need to
enable it in the kernel. On both tunnel hosts the following line needs to be
added to &lt;code&gt;&#x2F;etc&#x2F;sysctl.conf&lt;&#x2F;code&gt; and &lt;code&gt;sysctl -p&lt;&#x2F;code&gt; run again to apply the change:&lt;&#x2F;p&gt;
&lt;pre&gt;&lt;code&gt;net.ipv4.ip_forward = 1
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;We need to ensure each tunnel server routes our mapping network to the other
one. This should be added &#x2F; removed based on the status of our GRE tunnel so
we&#x27;ll add it as a static route in &lt;code&gt;&#x2F;etc&#x2F;sysconfig&#x2F;network-scripts&#x2F;route-tun0&lt;&#x2F;code&gt;.&lt;&#x2F;p&gt;
&lt;p&gt;For our &lt;code&gt;west&lt;&#x2F;code&gt; tunnel server the contents of the file should be:&lt;&#x2F;p&gt;
&lt;pre&gt;&lt;code&gt;172.16.0.0&#x2F;12 via 10.255.254.2
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;For our east tunnel server the contents of the file should be:&lt;&#x2F;p&gt;
&lt;pre&gt;&lt;code&gt;172.16.0.0&#x2F;12 via 10.255.254.1
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;Restart the network (again dealing with a minor disruption) and check the
routing table with the following commands:&lt;&#x2F;p&gt;
&lt;pre&gt;&lt;code&gt;sudo systemctl restart network.service
sudo ip -4 route
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;You should see the new route present in the routing table, but now we have a
problem. If the firewalls allowed us to forward traffic right now, any traffic
either tunnel host received with a destination of &lt;code&gt;172.16.0.0&#x2F;12&lt;&#x2F;code&gt; would ping
pong back and forth across the tunnel until it&#x27;s TTL expired. This would end up
being a nasty traffic amplification issue if we allowed it.&lt;&#x2F;p&gt;
&lt;p&gt;Handling this requires us to rewrite the packet destination received from the
tunnel to the VPC&#x27;s network before the kernel can make a routing decision on
it and thus we use our first firewall incantation in the &lt;code&gt;nat&lt;&#x2F;code&gt; table. On each
tunnel host add the following rule:&lt;&#x2F;p&gt;
&lt;pre&gt;&lt;code&gt;-A PREROUTING -i tun0 -d 172.16.0.0&#x2F;12 -j NETMAP --to 10.0.0.0&#x2F;12
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;Side note: It&#x27;s not documented very well but when the &lt;code&gt;NETMAP&lt;&#x2F;code&gt; is used in the
&lt;code&gt;PREROUTING&lt;&#x2F;code&gt; chain it only effects the destination network. When used in the
&lt;code&gt;POSTROUTING&lt;&#x2F;code&gt; chain it only effects the source address (which we&#x27;ll make use of
later).&lt;&#x2F;p&gt;
&lt;p&gt;While we&#x27;re updating our firewall we should also allow our forwarded traffic.
The following two rules need to be added to the &lt;code&gt;filter&lt;&#x2F;code&gt; section of each tunnel
host:&lt;&#x2F;p&gt;
&lt;pre&gt;&lt;code&gt;-A FORWARD -i eth0 -o tun0 -s 10.0.0.0&#x2F;12 -d 172.16.0.0&#x2F;12 -j ACCEPT
-A FORWARD -i tun0 -o eth0 -s 172.16.0.0&#x2F;12 -d 10.0.0.0&#x2F;12 -j ACCEPT
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;You may notice that I&#x27;m specifying &lt;code&gt;10.0.0.0&#x2F;12&lt;&#x2F;code&gt; instead of &lt;code&gt;10.0.0.0&#x2F;8&lt;&#x2F;code&gt;. This
is a limitation I mentioned at the beginning of this article which worked in my
instance. You can&#x27;t uniquely map a larger network into a smaller network. If
your hosts are more scattered this is where you&#x27;ll need to start duplicating
rules and using smaller subnet masks for targeted groups of hosts. There will
be other rules coming up shortly you&#x27;ll need to update as well.&lt;&#x2F;p&gt;
&lt;p&gt;As part of this our rules won&#x27;t forward traffic coming from our tunnel hosts
subnet of &lt;code&gt;10.255.254.0&#x2F;30&lt;&#x2F;code&gt; as it is way outside of &lt;code&gt;10.0.0.0&#x2F;12&lt;&#x2F;code&gt;. Simply
allowing this subnet won&#x27;t allow us to receive the responses to any traffic
leaving our tunnel hosts for the opposite network as the source address will
appear local to the VPC. We can reserve two more addresses within the range of
&lt;code&gt;172.16.0.0&#x2F;12&lt;&#x2F;code&gt; to work as our tunnel endpoints. This isn&#x27;t strictly necessary
if you really need the two addresses but they make diagnostics significantly
simpler.&lt;&#x2F;p&gt;
&lt;p&gt;We can map our two addresses appropriately using the fixed 1:1 NAT mapping in
the kernel by adding the following rules in the &lt;code&gt;nat&lt;&#x2F;code&gt; section of each tunnel
hosts firewall:&lt;&#x2F;p&gt;
&lt;pre&gt;&lt;code&gt;-A PREROUTING -i tun0 -d 172.31.254.1 -j DNAT --to-destination 10.255.254.1
-A PREROUTING -i tun0 -d 172.31.254.2 -j DNAT --to-destination 10.255.254.2

-A POSTROUTING -o tun0 -d 10.255.254.1 -j SNAT --to-source 172.31.254.1
-A POSTROUTING -o tun0 -d 10.255.254.2 -j SNAT --to-source 172.31.254.2
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;Only half of these rules apply to each tunnel host, but it doesn&#x27;t hurt having
both sets on both hosts and it keeps us symmetrical. You should be able to ping
each of the tunnel hosts equivalent &lt;code&gt;172.31.254.0&#x2F;30&lt;&#x2F;code&gt; address at this point (if
you restart the firewall).&lt;&#x2F;p&gt;
&lt;p&gt;Right now if a client host added a route pointing at either of the tunnel host
for the mapped network it would make it out the opposite tunnel host&#x27;s &lt;code&gt;eth0&lt;&#x2F;code&gt;
interface but it would still have a &lt;code&gt;10.0.0.0&#x2F;12&lt;&#x2F;code&gt; source address and the packet
would never return to the tunnel host, much less the host on the other network.&lt;&#x2F;p&gt;
&lt;p&gt;This is a bit tricky as we only want to rewrite the source address (requiring a
&lt;code&gt;POSTROUTING&lt;&#x2F;code&gt; rule) but only want it to effect mapped traffic addresses coming
in from a normal VPC network, and &lt;code&gt;POSTROUTING&lt;&#x2F;code&gt; can&#x27;t match on source
interface. We want to handle this rewriting before any other changes have
occurred which requires us to do the source address rewriting happen on the
source tunnel host.&lt;&#x2F;p&gt;
&lt;p&gt;To handle this we can use a combination of traffic markers and our handy
&lt;code&gt;NETMAP&lt;&#x2F;code&gt; target. On both of the tunnel hosts add the following two rules to the
&lt;code&gt;nat&lt;&#x2F;code&gt; section:&lt;&#x2F;p&gt;
&lt;pre&gt;&lt;code&gt;-A PREROUTING -i tun0 -d 172.16.0.0&#x2F;12 -s 10.0.0.0&#x2F;12 -j MARK --set-mark 0x01
-A POSTROUTING -o tun0 -m mark --mark 0x01 -s 10.0.0.0&#x2F;12 -j NETMAP --to 172.16.0.0&#x2F;12
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;Let&#x27;s restart the firewall again to make sure all the rules have been properly
applied:&lt;&#x2F;p&gt;
&lt;pre&gt;&lt;code&gt;sudo systemctl restart iptables.service
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;That&#x27;s the last of the changes we need to make to the tunnel hosts, now the
other hosts need to learn how to send their traffic to the other side...&lt;&#x2F;p&gt;
&lt;h2 id=&quot;vpc-routing&quot;&gt;VPC Routing&lt;&#x2F;h2&gt;
&lt;p&gt;Hosts inside a VPC will directly send traffic to any other host within it&#x27;s
defined network. For networks beyond their VPC subnet (such as our
&lt;code&gt;172.16.0.0&#x2F;12&lt;&#x2F;code&gt;) network will send their traffic to their default gateway which
is the VPC router. These routers are configurable within the AWS web console by
going to the &lt;code&gt;VPC&lt;&#x2F;code&gt; section, finding the relevant VPC you&#x27;re using and clicking
on the link to your &lt;code&gt;Main Route Table&lt;&#x2F;code&gt;.&lt;&#x2F;p&gt;
&lt;p&gt;Under the &lt;code&gt;Routes&lt;&#x2F;code&gt; sub-tab on the selected Route Table, click on the &lt;code&gt;Edit routes&lt;&#x2F;code&gt; button. Add &lt;code&gt;172.16.0.0&#x2F;12&lt;&#x2F;code&gt; as a destination to the routes. Click on
the &lt;code&gt;Target&lt;&#x2F;code&gt; drop down, choose &lt;code&gt;Instance&lt;&#x2F;code&gt; and find your VPC tunnel host in the
list. Click &lt;code&gt;Save Routes&lt;&#x2F;code&gt; and allow a minute or two for the route to update.&lt;&#x2F;p&gt;
&lt;p&gt;There is a sneaky potential issue here. If you&#x27;ve gone and done some deep
customization to your VPC, you may have created and specified additional route
tables for specific subnets. You&#x27;ll want to evaluate each of the potential
route tables and add the same route to each one.&lt;&#x2F;p&gt;
&lt;p&gt;There is one final thing generally stopping our traffic from flowing freely. By
default every single EC2 instance drops any traffic that reaches an EC2
instance with a source or destination address that doesn&#x27;t match the IP that
has been assigned to that instance. This is generally a very useful protection,
but we&#x27;ll be shooting out packets with source addresses in the &lt;code&gt;172.16.0.0&#x2F;12&lt;&#x2F;code&gt;
range so need to disable this protection on each of tunnel hosts.&lt;&#x2F;p&gt;
&lt;p&gt;Find your tunnel host in the list of your EC2 instances. Right click on the
instance, go to the &lt;code&gt;Networking&lt;&#x2F;code&gt; sub-menu, and choose &lt;code&gt;Change Source&#x2F;Dest. Check&lt;&#x2F;code&gt;. It will pop up a confirmation, confirm it by clicking on &lt;code&gt;Yes, Disable&lt;&#x2F;code&gt;.&lt;&#x2F;p&gt;
&lt;p&gt;Now the only thing preventing hosts in each VPC from talking to each other is
their respective inbound security groups but the traffic should flow freely.
We&#x27;re effectively done and everything should be happy.&lt;&#x2F;p&gt;
&lt;p&gt;It may not be immediately obvious but you will have to do some math to convert
the IP addresses of the remote subnet into the mapping network. Specifically
you&#x27;ll need to replace the first octet (&lt;code&gt;10&lt;&#x2F;code&gt;) with the mapping network&#x27;s first
octet (&lt;code&gt;172&lt;&#x2F;code&gt;), then add &lt;code&gt;16&lt;&#x2F;code&gt; to the second octet. If the resulting second octet
is greater than &lt;code&gt;31&lt;&#x2F;code&gt; it won&#x27;t be able to traverse the network. The remaining
two octets are left unchanged.&lt;&#x2F;p&gt;
&lt;p&gt;Some examples of what this translation looks like:&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;10.0.0.2&lt;&#x2F;code&gt; becomes &lt;code&gt;172.16.0.2&lt;&#x2F;code&gt;&lt;&#x2F;li&gt;
&lt;li&gt;&lt;code&gt;10.4.1.80&lt;&#x2F;code&gt; becomes &lt;code&gt;172.20.1.80&lt;&#x2F;code&gt;&lt;&#x2F;li&gt;
&lt;li&gt;&lt;code&gt;10.30.56.100&lt;&#x2F;code&gt; becomes &lt;code&gt;172.46.56.100&lt;&#x2F;code&gt; and is unroutable&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;No matter which side of the tunnel you&#x27;re on the other side&#x27;s addresses will
always be mapped this way.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;hardening&quot;&gt;Hardening&lt;&#x2F;h2&gt;
&lt;p&gt;We have some fairly wide open firewall rules for passing traffic on the
tunneling hosts themselves and in the security groups on them. These can
certainly be tightened further and I&#x27;ll even cover some situations in a bit
about when you might want to do that. As it stands right now the internal
private IP addresses of the tunnel host&#x27;s and clients haven&#x27;t matter beyond
whether or not they were in the routable range.&lt;&#x2F;p&gt;
&lt;p&gt;If you use ephemeral containers or autoscaling IP addresses are going to change
frequently. To harden the rules on the tunnel hosts themselves would need to be
updated whenever these addresses change which removes a lot of benefits. Since
we&#x27;re already using a dedicated security group for our tunnel hosts, we can
instead have other security groups reference it directly.&lt;&#x2F;p&gt;
&lt;p&gt;To allow traffic from the opposite VPC side, allow the relevant port&#x27;s traffic
from the tunnel host&#x27;s security group and bam problem solved. This is still
somewhat course granularity of firewalling as you are effectively granting the
entire other VPC access to that service port. In a lot of cases that will be
enough and additional network controls such as inter-service authentication
will be sufficient to mitigating additional issues.&lt;&#x2F;p&gt;
&lt;p&gt;If you do need finer granularity you can start by limiting traffic on the VPC
tunnel&#x27;s inbound security group from the opposite side. If that is not fine
grained enough you can eventually resort to firewall rules in the &lt;code&gt;FORWARD&lt;&#x2F;code&gt;
chain itself.&lt;&#x2F;p&gt;
&lt;p&gt;There is one additional benefit of putting the rules in the forward chain if
your addresses are sufficiently static to deploy rules through it. With
security groups alone, the traffic will traverse the tunnel before being
dropped by the opposing side&#x27;s security group. Likely your service will retry
the connection. These little bits of traffic do add up and will take time.&lt;&#x2F;p&gt;
&lt;p&gt;If you instead firewall with reject packets as they enter the tunnel, a service
will get immediate feedback the traffic won&#x27;t flow. No additional bandwidth is
wasted and the latency will be very small. You can also log these packets with
a &lt;code&gt;LOG&lt;&#x2F;code&gt; target before rejecting them so you can audit and diagnose traffic that
doesn&#x27;t make it through the tunnel.&lt;&#x2F;p&gt;
&lt;p&gt;For those reasons I do prefer to firewall at the tunnel hosts themselves for
sufficiently static services.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;troubleshooting&quot;&gt;Troubleshooting&lt;&#x2F;h2&gt;
&lt;p&gt;I&#x27;ve tried to include basic diagnostics for each piece that we&#x27;ve built up but
if you&#x27;re still having issues getting traffic flowing here is a checklist to
look over that might help diagnose the source of the issue:&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;Restart the tunnel host&#x27;s network&lt;&#x2F;li&gt;
&lt;li&gt;Verify the tunnel host&#x27;s firewalls match the final reference firewall below&lt;&#x2F;li&gt;
&lt;li&gt;Restart the tunnel host&#x27;s firewalls&lt;&#x2F;li&gt;
&lt;li&gt;Make sure each tunnel host can ping the other one&lt;&#x2F;li&gt;
&lt;li&gt;Ensure &lt;code&gt;libreswan&lt;&#x2F;code&gt; service is up and running (&lt;code&gt;&#x2F;var&#x2F;log&#x2F;secure&lt;&#x2F;code&gt; will have
any errors it encounters if the tunnel isn&#x27;t coming up&lt;&#x2F;li&gt;
&lt;li&gt;Verify the GRE tunnel is up by pinging the other end&#x27;s tunnel IP&lt;&#x2F;li&gt;
&lt;li&gt;Check the routing table on both tunnel hosts&lt;&#x2F;li&gt;
&lt;li&gt;Ensure source and destination hosts are within the &lt;code&gt;10.0.0.0&#x2F;12&lt;&#x2F;code&gt; range&lt;&#x2F;li&gt;
&lt;li&gt;Make sure the source &#x2F; destination checking is disabled on the tunnel host&#x27;s
EC2 instances&lt;&#x2F;li&gt;
&lt;li&gt;Check to make sure the VPC routing tables include &lt;code&gt;172.16.0.0&#x2F;12&lt;&#x2F;code&gt; pointing at
the tunnel hosts in both networks.&lt;&#x2F;li&gt;
&lt;li&gt;Check the relevant security groups to make sure all other traffic is allowed
to&#x2F;from the tunnel hosts in each security group&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;If all else fails sniff the traffic on the interfaces you expect for the
packets in each place to make sure they&#x27;re going where you expect. Usually this
makes it pretty clear to me whether packets are even getting to the tunnel
hosts and which interface they either stop or aren&#x27;t being manipulated at.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;&#x2F;h2&gt;
&lt;p&gt;This post was quite a wild ride for me to write up and is probably my longest
post to date. If you&#x27;ve made it this far I&#x27;m incredibly flattered. I hope this
helps other people out there and I would especially love to hear from anyone
that makes use of this information or finds an issue with anything in the post.&lt;&#x2F;p&gt;
&lt;p&gt;Either &lt;a href=&quot;mailto:sam@stelfox.net&quot;&gt;send me an email&lt;&#x2F;a&gt; or &lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;sstelfox&#x2F;stelfox.net&#x2F;issues&#x2F;new&quot;&gt;open an issue&lt;&#x2F;a&gt; for my website&#x27;s public
repository. Cheers!&lt;&#x2F;p&gt;
&lt;h2 id=&quot;reference-firewall&quot;&gt;Reference Firewall&lt;&#x2F;h2&gt;
&lt;p&gt;If you had issues following along with incrementally building up our firewall
(I&#x27;m sorry!) the final firewall you should end up with (comments removed)
should like the following:&lt;&#x2F;p&gt;
&lt;pre&gt;&lt;code&gt;# &#x2F;etc&#x2F;sysconfig&#x2F;iptables

*nat
:PREROUTING ACCEPT [0:0]
:INPUT ACCEPT [0:0]
:OUTPUT ACCEPT [0:0]
:POSTROUTING ACCEPT [0:0]

-A PREROUTING -i tun0 -d 172.31.254.1 -j DNAT --to-destination 10.255.254.1
-A PREROUTING -i tun0 -d 172.31.254.2 -j DNAT --to-destination 10.255.254.2

-A POSTROUTING -o tun0 -d 10.255.254.1 -j SNAT --to-source 172.31.254.1
-A POSTROUTING -o tun0 -d 10.255.254.2 -j SNAT --to-source 172.31.254.2

-A PREROUTING -i tun0 -d 172.16.0.0&#x2F;12 -j NETMAP --to 10.0.0.0&#x2F;12
-A PREROUTING -i tun0 -d 172.16.0.0&#x2F;12 -s 10.0.0.0&#x2F;12 -j MARK --set-mark 0x01
-A POSTROUTING -o tun0 -m mark --mark 0x01 -s 10.0.0.0&#x2F;12 -j NETMAP --to 172.16.0.0&#x2F;12

COMMIT

*filter
:INPUT DROP [0:0]
:FORWARD DROP [0:0]
:OUTPUT ACCEPT [0:0]

-A INPUT -m conntrack --ctstate ESTABLISHED,RELATED -j ACCEPT
-A INPUT -p icmp -j ACCEPT
-A INPUT -i lo -j ACCEPT

-A INPUT -m tcp -p tcp --dport 22 -j ACCEPT

-A INPUT -p esp -j ACCEPT
-A INPUT -m udp -p udp --sport 500 --dport 500 -j ACCEPT

-A INPUT -m policy --dir in --pol ipsec --proto esp -p gre -j ACCEPT
-A OUTPUT -m policy --dir out --pol ipsec --proto esp -p gre -j ACCEPT
-A OUTPUT -p gre -j DROP

-A FORWARD -i eth0 -o tun0 -s 10.0.0.0&#x2F;12 -d 172.16.0.0&#x2F;12 -j ACCEPT
-A FORWARD -i tun0 -o eth0 -s 172.16.0.0&#x2F;12 -d 10.0.0.0&#x2F;12 -j ACCEPT

-A INPUT -j REJECT --reject-with icmp-host-prohibited
-A FORWARD -j REJECT --reject-with icmp-host-prohibited

COMMIT
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
</content>
	</entry>
	<entry xml:lang="en">
		<title>Reflashing Cisco Catalyst With XMODEM</title>
		<published>2019-03-24T23:26:30-04:00</published>
		<updated>2019-03-24T23:26:30-04:00</updated>
		<link href="https://stelfox.net/blog/2019/reflashing-cisco-catalyst-with-xmodem/" type="text/html"/>
		<id>https://stelfox.net/blog/2019/reflashing-cisco-catalyst-with-xmodem/</id>
		<content type="html">&lt;p&gt;One of the Cisco Catalyst 3750 I had to work on recently had it&#x27;s flash
completely wiped. When this happens you can only flash the filesystem using the
XMODEM serial console. This is a fairly well documented process on Windows. On
Linux most of the documented ways involve switching between multiple utilities
and can be tricky. I wanted to documented how I did this and possibly help
other in the same situation.&lt;&#x2F;p&gt;
&lt;span id=&quot;continue-reading&quot;&gt;&lt;&#x2F;span&gt;
&lt;p&gt;I&#x27;m doing this on Fedora, but the only thing specific to Fedora is installing
the packages which can be done with the following command (and are pretty
standard names):&lt;&#x2F;p&gt;
&lt;pre&gt;&lt;code&gt;dnf install lrzsz screen -y
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;From an external perspective the switch was rapidly blinking it&#x27;s &lt;code&gt;SYST&lt;&#x2F;code&gt; light
with all of the other lights off. I connected up a console cable, the serial
port showed up as &lt;code&gt;&#x2F;dev&#x2F;ttyUSB0&lt;&#x2F;code&gt;. We&#x27;ll connect to the serial port using screen
with the following command:&lt;&#x2F;p&gt;
&lt;pre&gt;&lt;code&gt;sudo screen &#x2F;dev&#x2F;ttyUSB0 9600
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;Power cycling the switch gets you the boot messages that show us the issue:&lt;&#x2F;p&gt;
&lt;pre&gt;&lt;code&gt;Using driver version 1 for media type 1
Base ethernet MAC Address: xx:xx:xx:xx:xx:xx
Xmodem file system is available.
The password-recovery mechanism is enabled.
Initializing Flash...
mifs[2]: 0 files, 1 directories
mifs[2]: Total bytes     :    3870720
mifs[2]: Bytes used      :       1024
mifs[2]: Bytes available :    3869696
mifs[2]: mifs fsck took 0 seconds.
mifs[3]: 0 files, 1 directories
mifs[3]: Total bytes     :   27998208
mifs[3]: Bytes used      :       1024
mifs[3]: Bytes available :   27997184
mifs[3]: mifs fsck took 1 seconds.
...done Initializing Flash.
done.
Loading &quot;flash:&#x2F;c3750-ipservicesk9-mz-15.0-2_SE10a.bin&quot;...flash:&#x2F;c3750-ipservicesk9-mz-15.0-2_SE10a.bin: no such file or directory

Error loading &quot;flash:&#x2F;c3750-ipservicesk9-mz-15.0-2_SE10a.bin&quot;

Interrupt within 5 seconds to abort boot process.
Boot process failed...

The system is unable to boot automatically.  The BOOT
environment variable needs to be set to a bootable
image.


switch:
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;We can confirm this isn&#x27;t just a badly named file quickly:&lt;&#x2F;p&gt;
&lt;pre&gt;&lt;code&gt;switch: dir flash:&#x2F;
Directory of flash:&#x2F;&#x2F;


32513024 bytes available (1024 bytes used)

switch:
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;If you you get the following error:&lt;&#x2F;p&gt;
&lt;pre&gt;&lt;code&gt;switch: dir flash:
unable to stat flash:&#x2F;: invalid argument
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;The flash hardware hasn&#x27;t been enabled yet. We need to initialize it with the
&lt;code&gt;flash_init&lt;&#x2F;code&gt; command which will allow us access to the flash again:&lt;&#x2F;p&gt;
&lt;pre&gt;&lt;code&gt;switch: flash_init
Initializing Flash...
flashfs[0]: 0 files, 1 directories
flashfs[0]: 0 orphaned files, 0 orphaned directories
flashfs[0]: Total bytes: 32514048
flashfs[0]: Bytes used: 1024
flashfs[0]: Bytes available: 32513024
flashfs[0]: flashfs fsck took 6 seconds.
...done Initializing Flash.

switch: dir flash:&#x2F;
Directory of flash:&#x2F;&#x2F;


32513024 bytes available (1024 bytes used)

switch:
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;Definitely nothing present. At this point we need to use XMODEM to get the
firmware file. You&#x27;ll need to use your support contract to get the latest
firmware for your model of switch. I made sure this file was in my current
directory (saves some time in a bit). For me this file was named
&lt;code&gt;c3750-ipservicesk9-mz.150-2.SE10a.bin&lt;&#x2F;code&gt; you&#x27;ll want to replace this with the
appropriate filename you pull yourself. Right, carrying on.&lt;&#x2F;p&gt;
&lt;p&gt;XMODEM is a very slow process especially when we use the default speed of 9600
baud. We can speed this process up by 12x by adjusting the baud rate. You can
skip this bit though if you&#x27;re willing to wait several hours for the file to
transfer.&lt;&#x2F;p&gt;
&lt;p&gt;To speed up the baud rate execute the following command, be aware that your
terminal is expected to immediately become responsive and this is fine:&lt;&#x2F;p&gt;
&lt;pre&gt;&lt;code&gt;switch: set BAUD 115200
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;Kill the screen session by pressing &lt;code&gt;Ctrl-a&lt;&#x2F;code&gt; quickly following by a lone &lt;code&gt;k&lt;&#x2F;code&gt;.
It will prompt you to confirm exiting the session, do so with &lt;code&gt;y&lt;&#x2F;code&gt; and start a
new session with the higher baud rate:&lt;&#x2F;p&gt;
&lt;pre&gt;&lt;code&gt;sudo screen &#x2F;dev&#x2F;ttyUSB0 115200
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;Press return and you&#x27;ll get back to the switch prompt. This next bit is tricky
as there is about a ten second timeout between starting the copy command and
needing to being the transfer. If you don&#x27;t quite make it I&#x27;ve got a tip after
the command below:&lt;&#x2F;p&gt;
&lt;pre&gt;&lt;code&gt;switch: copy xmodem:&#x2F; flash:&#x2F;c3750-ipservicesk9-mz.150-2.SE10a.bin
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;Quick press &lt;code&gt;Ctrl-a&lt;&#x2F;code&gt; followed by &lt;code&gt;:&lt;&#x2F;code&gt;. In the prompt that shows up type in the
following:&lt;&#x2F;p&gt;
&lt;pre&gt;&lt;code&gt;exec !! sx -b -X c3750-ipservicesk9-mz.150-2.SE10a.bin
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;If you get an error, it reports as cancelled, it times out just try again. This
time though after pressing &lt;code&gt;Ctrl-a&lt;&#x2F;code&gt; and &lt;code&gt;:&lt;&#x2F;code&gt; you can press your up arrow to
bring up the last run command. Assuming you typed it in correctly this should
give you enough time to start the transfer. It will show you the progress of
the transfer which depending on size will take between twenty and thirty
minutes (or hours if you didn&#x27;t update the baud rate).&lt;&#x2F;p&gt;
&lt;p&gt;Site note: One of the benefits of performing the &lt;code&gt;sx&lt;&#x2F;code&gt; command through screen is
that if anything happens to your terminal the transfer will continue. This was
particularly important for me at the time as I found myself SSH&#x27;d into a laptop
far away from myself over an unstable and shared cell connection running these
commands.&lt;&#x2F;p&gt;
&lt;p&gt;When it&#x27;s complete you&#x27;re going to want to ensure the &lt;code&gt;BOOT&lt;&#x2F;code&gt; variable properly
matches the filename you just transferred:&lt;&#x2F;p&gt;
&lt;pre&gt;&lt;code&gt;set BOOT flash:&#x2F;c3750-ipservicesk9-mz.150-2.SE10a.bin
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;We&#x27;re going to reset the baud rate back to normal (skip this if you didn&#x27;t
update your baud rate) which will cause you to loose the connection again:&lt;&#x2F;p&gt;
&lt;pre&gt;&lt;code&gt;unset BAUD
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;Disconnect as we did before and reconnect at the 9600 rate. One final command
should start the switch and get you back into the good graces of the Cisco CLI:&lt;&#x2F;p&gt;
&lt;pre&gt;&lt;code&gt;switch: boot
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;One final tip, if you want to access the boot menu of the switch I had a hell
of a time trying to figure out how to actually send the break command to
interrupt the boot menu. None of the recommended combinations of &lt;code&gt;Ctrl&lt;&#x2F;code&gt;, &lt;code&gt;Alt&lt;&#x2F;code&gt;,
&lt;code&gt;Shift&lt;&#x2F;code&gt;, &lt;code&gt;b&lt;&#x2F;code&gt;, &lt;code&gt;Break&lt;&#x2F;code&gt;, and &lt;code&gt;Scroll Lock&lt;&#x2F;code&gt;. You can instead press the &lt;code&gt;Mode&lt;&#x2F;code&gt;
button on the front of the switch to emulate this break key.&lt;&#x2F;p&gt;
&lt;p&gt;Cheers friends. Hope this helps another CLI adventurer.&lt;&#x2F;p&gt;
</content>
	</entry>
	<entry xml:lang="en">
		<title>AWS Elastic IP Details</title>
		<published>2019-03-17T16:26:30-05:00</published>
		<updated>2019-03-17T16:26:30-05:00</updated>
		<link href="https://stelfox.net/blog/2019/aws-elastic-ip-details/" type="text/html"/>
		<id>https://stelfox.net/blog/2019/aws-elastic-ip-details/</id>
		<content type="html">&lt;p&gt;Sometimes it becomes important to understand how your cloud provider implements
certain networking details. While working through an issue in AWS I needed to
understand how they handle public IP addressing. While this issue for me was
specific to an Elastic IP all of their public addresses are handled this way
and may bite you even without them. The problems specifically crop up when a
hosted piece of software does NAT traversal detection and changes it&#x27;s behavior
based on the result.&lt;&#x2F;p&gt;
&lt;span id=&quot;continue-reading&quot;&gt;&lt;&#x2F;span&gt;
&lt;p&gt;Amazon attaches public IP address to EC2 instances (and likely other of their
hosted services) through 1:1 NAT mapping of external to internal addresses. All
traffic will reach your instance without issue, but your system&#x27;s native
networking stack will (by default) not become aware of what that public IP is.&lt;&#x2F;p&gt;
&lt;p&gt;In most cases this isn&#x27;t an issue, the traffic ends up at your instance and is
properly changed to the internal address so your system responds without any
additional configuration work.&lt;&#x2F;p&gt;
&lt;p&gt;The first issue is tunneled traffic. I &lt;a href=&quot;https:&#x2F;&#x2F;stelfox.net&#x2F;blog&#x2F;2019&#x2F;fighting-ipsec-on-aws&#x2F;&quot;&gt;experienced this&lt;&#x2F;a&gt; in my last post
with IPSec. The tunneled traffic is not modified by going through a 1:1 NAT and
your host will not recognize the external address inside the tunnel, which in
most cases will result in silently dropped packets (the &amp;quot;oh this isn&#x27;t for me&amp;quot;
syndrome).&lt;&#x2F;p&gt;
&lt;p&gt;There are ways to get your system to respond to tunneled packets with other IP
addresses such as treating it like a high-availability shared virtual server IP
address but I have yet to find a clean way to deal with responding to the
address. IPSec had a way to deal with this built in directly and I&#x27;d refer you
back to &lt;a href=&quot;https:&#x2F;&#x2F;stelfox.net&#x2F;blog&#x2F;2019&#x2F;fighting-ipsec-on-aws&#x2F;&quot;&gt;my other post&lt;&#x2F;a&gt; to read about that.&lt;&#x2F;p&gt;
&lt;p&gt;Usually sniffing the tunneled traffic is enough to diagnose these issues.
What is a bit more pernicious, is when a server changes its behavior when it
detects a NAT in place. This requires deeper understanding of the protocol in
use and in some cases the specific server implementation. Once again this post
is ultimately about IPSec (and specifically libreswan).&lt;&#x2F;p&gt;
&lt;p&gt;While you don&#x27;t have to make any changes to get IPSec working with AWS public
IPs it will detect the NAT and start encapsulating the traffic. The
encapsulation will require an additional port being opened on your security
group firewall but more importantly, it will add variable latency and
additional processing overhead to each packet. Not everything is sensitive to
this but you&#x27;ll be needlessly adding overhead to your network traffic is you
don&#x27;t address it. In my case (AWS VPC to AWS VPC) there aren&#x27;t any meaningful
NATs in place that require this encapsulation to function appropriately.&lt;&#x2F;p&gt;
&lt;p&gt;If you encounter this and are using libreswan on AWS you only need to add the
following two lines to your tunnel&#x27;s config disable this behavior:&lt;&#x2F;p&gt;
&lt;pre&gt;&lt;code&gt;encapsulation=no
nat-keepalive=no
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;I hope this helps someone else diagnose weird behavior when working with public
IP addresses on AWS.&lt;&#x2F;p&gt;
</content>
	</entry>
	<entry xml:lang="en">
		<title>Fighting IPSec on AWS</title>
		<published>2019-03-14T21:26:30-04:00</published>
		<updated>2019-03-14T21:26:30-04:00</updated>
		<link href="https://stelfox.net/blog/2019/fighting-ipsec-on-aws/" type="text/html"/>
		<id>https://stelfox.net/blog/2019/fighting-ipsec-on-aws/</id>
		<content type="html">&lt;p&gt;IPSec is a well known and well understood protocol that is pretty easy to get
setup and going... Most of the time. While setting up an IPSec tunnel to an AWS
host I came across a new and unique experience that didn&#x27;t seem to have an
easily searchable solution.&lt;&#x2F;p&gt;
&lt;span id=&quot;continue-reading&quot;&gt;&lt;&#x2F;span&gt;
&lt;p&gt;I had two CentOS 7 EC2 instances, each set up with their own Elastic IP in a
default VPC. I installed and configured libreswan with the following config:&lt;&#x2F;p&gt;
&lt;pre&gt;&lt;code&gt;conn setup
  protostack=netkey

conn site-tunnel
  auto=start
  pfs=yes

  leftid=@left_tunnel_server_name
  rightid=@right_tunnel_server_name
  left=%defaultroute
  right=&lt;remote-ip&gt;

  authby=rsasig
  leftrsasigkey=&lt;left-key&gt;
  rightrsasigkey=&lt;right-key&gt;
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;The IPSec link came up without issue but pings were timing out without a
response. When sniffing the incoming packets, I was appropriately seeing the
ICMP echo requests coming in encapsulated in the tunnel and being re-injected.
There was never a response being generated.&lt;&#x2F;p&gt;
&lt;p&gt;The traffic capture was also pretty clear as to why. The encapsulated packet
had the elastic IP as the destination which the EC2 instance doesn&#x27;t
&lt;em&gt;ACTUALLY&lt;&#x2F;em&gt; have and wasn&#x27;t aware... So it didn&#x27;t attempt to respond.&lt;&#x2F;p&gt;
&lt;p&gt;I went down a bit of a rabbit hole attempting to get the instance to recognize
that address with limited success. Adding the IP to an additional loopback
interface... IPTables DNAT rules... Some of them worked but they were dirty
hacks that would have been left until something blew up...&lt;&#x2F;p&gt;
&lt;p&gt;The solution was simple and even improved the overall performance of the
link. By default libreswan operates in &lt;code&gt;tunnel&lt;&#x2F;code&gt; mode which sends along the
destination information as well (and can be used for routing network across).
For the use I almost always use these links for, &lt;code&gt;transport&lt;&#x2F;code&gt; is more
appropriate and has less protocol overhead.&lt;&#x2F;p&gt;
&lt;p&gt;By adding the following line to the connection configuration on both sides the
problem vanished:&lt;&#x2F;p&gt;
&lt;pre&gt;&lt;code&gt;  type=transport
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;With any luck the next person that comes across this issue will find this post
and their life will be a tad bit easier.&lt;&#x2F;p&gt;
</content>
	</entry>
	<entry xml:lang="en">
		<title>Hosting Your Own Private Git Repo</title>
		<published>2018-12-21T18:53:30+00:00</published>
		<updated>2018-12-21T18:53:30+00:00</updated>
		<link href="https://stelfox.net/blog/2018/hosting-your-own-private-git/" type="text/html"/>
		<id>https://stelfox.net/blog/2018/hosting-your-own-private-git/</id>
		<content type="html">&lt;p&gt;Git was built and developed with the intention of being a distributed reversion
control system. Most people now use it with one or another central repository
even when working on large teams which is perfectly fine if that model works
for you and your team.&lt;&#x2F;p&gt;
&lt;span id=&quot;continue-reading&quot;&gt;&lt;&#x2F;span&gt;
&lt;p&gt;It can be useful to quickly work with others on private repositories without
requiring them to get on your platform of choice, or for sensitive repositories
keep the repository entirely under your control. Occasionally platforms like
GitHub have service outages, and while you won&#x27;t have access to any of your
integrations a private repository can quickly allow your team to keep
collaborating.&lt;&#x2F;p&gt;
&lt;p&gt;If you&#x27;re using Linux or Mac OS X, setting up a local repo that you can push to
is trivial. You simply create a place for it, initialize it as a git repo, then
push to it like so:&lt;&#x2F;p&gt;
&lt;pre&gt;&lt;code&gt;mkdir -p ~&#x2F;repos&#x2F;private_repo.git
cd ~&#x2F;repos&#x2F;private_repo.git
git init --bare
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;In the directory that contains your current repository add the new repo
destination as an origin and push the contents to it:&lt;&#x2F;p&gt;
&lt;pre&gt;&lt;code&gt;git remote add local ~&#x2F;repos&#x2F;private_repo.git
git push local --all
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;Having another copy of your repository locally doesn&#x27;t do you much good. To
push your repository to another system, the only requirements is an account on
an SSH server that has the git binary installed.&lt;&#x2F;p&gt;
&lt;p&gt;Log in to your SSH server and setup the repository like before:&lt;&#x2F;p&gt;
&lt;pre&gt;&lt;code&gt;ssh user@my-remote-system.example.tld
mkdir ~&#x2F;repos&#x2F;private_repo.git
cd ~&#x2F;repos&#x2F;private_repo.git
git init --bare
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;In the local copy of your current repository you add an origin just like
before, but use a slightly different syntax to indicate its on a remote system
instead:&lt;&#x2F;p&gt;
&lt;pre&gt;&lt;code&gt;git remote add remote user@my-remote-system.example.tld:~&#x2F;repos&#x2F;private_repo.git
git push remote --all
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;To clone from it you would use the same syntax as you did for the origin:&lt;&#x2F;p&gt;
&lt;pre&gt;&lt;code&gt;git clone user@my-remote-system.example.tld:~&#x2F;repos&#x2F;private_repo.git
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;If multiple users are allowed on the SSH hosts and you want to allow them
access to the repository, you&#x27;ll need to place the directory in a place all
users can access and handle permissions on.&lt;&#x2F;p&gt;
&lt;p&gt;You can have as many origins as you&#x27;d like and push &#x2F; pull from them
independently.&lt;&#x2F;p&gt;
</content>
	</entry>
	<entry xml:lang="en">
		<title>Performance Impact of OpenVPN Port Sharing</title>
		<published>2018-11-04T15:09:02-06:00</published>
		<updated>2018-11-04T15:09:02-06:00</updated>
		<link href="https://stelfox.net/blog/2018/performance-impact-of-openvpn-portsharing/" type="text/html"/>
		<id>https://stelfox.net/blog/2018/performance-impact-of-openvpn-portsharing/</id>
		<content type="html">&lt;p&gt;I recently had cause to use OpenVPN on the standard HTTPS port to protect my
traffic. This was done as a compromise with administrators who didn&#x27;t want to
change their egress filtering, but wanted to allow me to continue doing my
normal work.&lt;&#x2F;p&gt;
&lt;span id=&quot;continue-reading&quot;&gt;&lt;&#x2F;span&gt;
&lt;p&gt;I already run several webservers, including this one, and didn&#x27;t want to give
up exclusive access to the precious TCP port 443. The recommended way to deal
with this is to make use of the &lt;code&gt;port-share&lt;&#x2F;code&gt; option built into OpenVPN. This
left me with two choices, run this on an existing server sharing the port with
existing websites, or setup a dedicated server just for this instance of
OpenVPN.&lt;&#x2F;p&gt;
&lt;p&gt;I couldn&#x27;t find any other posts that took a look at how this port sharing
effects the performance of the HTTPS server so I felt like doing a quick
analysis for other curious parties.&lt;&#x2F;p&gt;
&lt;p&gt;I setup a fresh Nginx server with Let&#x27;s Encrypt certificates that mimics my
production setup and used &lt;code&gt;ab&lt;&#x2F;code&gt; to bench the service for 30 seconds. The mean
measured rate was 288.70 +&#x2F;- 14.39 requests per second. Mean request
fullfillment took 3.47 +&#x2F;- 0.17 ms.&lt;&#x2F;p&gt;
&lt;p&gt;After enabling &lt;code&gt;port-share&lt;&#x2F;code&gt; on OpenVPN I reran the exact same test. The result
was 139.76 +&#x2F;- 67.68 requests per second. Mean measured request fullfillment
7.44 +&#x2F;- 4.16 ms.&lt;&#x2F;p&gt;
&lt;p&gt;That is a 51% peak request handling reduction, each request has an additional
4-8ms of latency, and an almost 40x increase of jitter. That is a massive
relative impact but the vast majority of the websites I run need won&#x27;t be
terribly impacted by that additional latency.&lt;&#x2F;p&gt;
&lt;p&gt;I ultimately ended up setting up a seperate server for OpenVPN as I didn&#x27;t want
to mess with known working systems.&lt;&#x2F;p&gt;
</content>
	</entry>
	<entry xml:lang="en">
		<title>Run Your Own DNS-over-TLS Server</title>
		<published>2018-11-02T15:09:02-06:00</published>
		<updated>2018-11-02T15:09:02-06:00</updated>
		<link href="https://stelfox.net/blog/2018/run-your-own-dns-over-tls-server/" type="text/html"/>
		<id>https://stelfox.net/blog/2018/run-your-own-dns-over-tls-server/</id>
		<content type="html">&lt;p&gt;DNS-over-TLS is a relatively new privacy enhancing protocol that encrypts all
of your DNS requests to a trusted server. In an age when airports, and coffee
shops are outsourcing &#x27;free wifi&#x27; to corporate entities that are likely
harvesting as much data as they can this is a nice addition. I largely use VPNs
when connected to these access points which provides at least as good
protection as DNS-over-TLS which has caused me to largely overlook this
development.&lt;&#x2F;p&gt;
&lt;span id=&quot;continue-reading&quot;&gt;&lt;&#x2F;span&gt;
&lt;p&gt;When I found out that Android 9 natively supports this through the &#x27;Private
DNS&#x27; feature I got significantly more interested. I&#x27;ve always wanted to dictate
what DNS server my phone used, not for privacy reasons but rather to provide
another layer of security when not connected to my VPN. When you&#x27;re in control
of your DNS server you can blacklist known malicious domains and as an added
bonus the worst of the ad and tracking networks.&lt;&#x2F;p&gt;
&lt;p&gt;Turns out DNS-over-TLS is incredibly simple. It makes no changes to normal DNS
packets, it just wraps the TCP queries in a TLS tunnel with a preconfigured
server address for certificate validation.&lt;&#x2F;p&gt;
&lt;p&gt;I use unbound for my DNS server of choice which natively supports exposing a
TLS protected port, but with a very limited config. My preferred choice of
dealing with this is instead to use Nginx to proxy locally to unbound, allowing
me to ensure the TLS config matches current best practices and limits access to
the servers TLS private key to a single service.&lt;&#x2F;p&gt;
&lt;p&gt;The downside of using Nginx is that you lose the ability to log which internet
addresses are making requests (as the stream access_log directive doesn&#x27;t seem
to be working for the packaged Nginx server). Since this is an unauthenticated
service I&#x27;m slightly concerned about anonymous use and abuse but will address
that if it ever becomes an issue.&lt;&#x2F;p&gt;
&lt;p&gt;First off we need to ensure we have a working local resolver on our host. I&#x27;ll
use unbound here as its my preference but this will work just as well with Bind
or any other DNS resolver that supports TCP queries. If you want to use another
server, just skip past the unbound config to the Nginx section.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;unbound&quot;&gt;Unbound&lt;&#x2F;h2&gt;
&lt;pre&gt;&lt;code&gt;dnf install unbound -y
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;I clear out some of the garbage default files that ship with the package, and
perform the automated control key creation:&lt;&#x2F;p&gt;
&lt;pre&gt;&lt;code&gt;rm -f &#x2F;etc&#x2F;unbound&#x2F;{conf.d,keys.d,local.d}&#x2F;*
unbound-control-setup
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;I then drop in my config to &lt;code&gt;&#x2F;etc&#x2F;unbound&#x2F;unbound.conf&lt;&#x2F;code&gt;. I&#x27;ve included comments
in the config around options that aren&#x27;t immediately obvious from their names.&lt;&#x2F;p&gt;
&lt;pre&gt;&lt;code&gt;# &#x2F;etc&#x2F;unbound&#x2F;unbound.conf

server:
  verbosity: 1

  interface-automatic: no
  num-threads: 4

  # Disable statistics collection
  statistics-interval: 0
  statistics-cumulative: no

  # Reserve this many ports per-thread to prevent conflicts
  outgoing-range: 4096

  # Restrict the ports being used to match a tighter SELinux policy
  outgoing-port-avoid: 0-32767
  outgoing-port-permit: 32768-60999

  # Per-thread limits for active TCP connections
  outgoing-num-tcp: 100
  incoming-num-tcp: 100

  # Bind to port with SO_REUSEPORT so queries can be distributed over threads
  so-reuseport: yes

  # Maximum UDP response size, this can prevent some fragmentation issues
  max-udp-size: 3072

  # For our TCP clients, allow us to still use UDP to make upstream requests.
  # All of the TLS requests will come in over TLS even if they&#x27;d happily be
  # served in a single UDP packet.
  udp-upstream-without-downstream: yes

  access-control: 127.0.0.1 allow
  access-control: ::1 allow
  access-control: 0.0.0.0&#x2F;0 deny

  chroot: &quot;&quot;
  directory: &quot;&#x2F;etc&#x2F;unbound&quot;
  username: &quot;unbound&quot;

  use-syslog: yes
  log-time-ascii: yes

  pidfile: &quot;&#x2F;var&#x2F;run&#x2F;unbound&#x2F;unbound.pid&quot;

  hide-identity: yes
  hide-version: yes
  hide-trustanchor: yes

  # Perform various hardening on queries
  harden-short-bufsize: yes
  harden-large-queries: yes
  harden-glue: yes
  harden-dnssec-stripped: yes
  harden-below-nxdomain: yes
  harden-referral-path: yes
  harden-algo-downgrade: yes

  # Perform some upstream privacy protection (and in some cases performance
  # improvements) on our queries
  qname-minimisation: yes

  # Aggressive NSEC uses the DNSSEC NSEC chain to synthesize NXDOMAIN and other
  # denials, using information from previous NXDOMAINs answers.
  aggressive-nsec: yes

  # Use 0x20-encoded random bits in the query to foil spoof attempts. This
  # feature is an experimental implementation of draft dns-0x20.
  use-caps-for-id: yes

  # Enforce privacy of these addresses. Strips them away from answers to
  # protect against potential rebind attacks at the expense of potential DNSSEC
  # validation failures. Since we&#x27;re directly serving clients this shouldn&#x27;t be
  # an issue.
  private-address: 10.0.0.0&#x2F;8
  private-address: 172.16.0.0&#x2F;12
  private-address: 192.168.0.0&#x2F;16
  private-address: 169.254.0.0&#x2F;16
  private-address: fd00::&#x2F;8
  private-address: fe80::&#x2F;10
  private-address: ::ffff:0:0&#x2F;96

  # In the event this server gets unwanted replies, it will clear the cache as
  # a safety measure to flush potential poisonings out of it
  unwanted-reply-threshold: 10000

  # Don&#x27;t allow this server to... query this server...
  do-not-query-localhost: yes

  # Don&#x27;t prefetch cache entries that are about to expire
  prefetch: no

  # Round-robin returned RRSET queries
  rrset-roundrobin: yes

  minimal-responses: yes

  module-config: &quot;validator iterator&quot;

  # Perform tests automatically to make sure this resolver is ready for root
  # key rollovers
  root-key-sentinel: yes

  val-clean-additional: yes
  val-log-level: 2

  # Serve expired responses from cache, with TTL 0 in the response,
  # and then attempt to fetch the data afresh.
  serve-expired: yes

  # Enable pulling updated anchor trusts automatically
  auto-trust-anchor-file: &quot;&#x2F;var&#x2F;lib&#x2F;unbound&#x2F;root.key&quot;

  # Trust anchor signaling sends a RFC8145 key tag query after priming.
  trust-anchor-signaling: yes

  # Instruct the auto-trust-anchor-file probing to avoid changes to anchors
  # using 30 days hold down
  add-holddown: 2592000
  del-holddown: 2592000

remote-control:
  control-enable: yes
  control-interface: ::1

  server-key-file: &quot;&#x2F;etc&#x2F;unbound&#x2F;unbound_server.key&quot;
  server-cert-file: &quot;&#x2F;etc&#x2F;unbound&#x2F;unbound_server.pem&quot;

  control-key-file: &quot;&#x2F;etc&#x2F;unbound&#x2F;unbound_control.key&quot;
  control-cert-file: &quot;&#x2F;etc&#x2F;unbound&#x2F;unbound_control.pem&quot;

auth-zone:
  name: &quot;.&quot;
  for-downstream: no
  for-upstream: yes
  fallback-enabled: yes
  master: b.root-servers.net
  master: c.root-servers.net
  master: e.root-servers.net
  master: f.root-servers.net
  master: g.root-servers.net
  master: k.root-servers.net
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;I always need to stress changes like these should be understood before being
blindly used. Make sure you start and enable the service:&lt;&#x2F;p&gt;
&lt;pre&gt;&lt;code&gt;systemctl start unbound.service
systemctl enable unbound.service
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;Perform various queries against the service to ensure it&#x27;s working as intended:&lt;&#x2F;p&gt;
&lt;pre&gt;&lt;code&gt;dig +tcp google.com @::1
dig +tcp reddit.com @::1
dig +tcp stelfox.net @::1
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;If you get reasonable responses from those queries then we have a working
resolver. The next step is to add our TLS layer with Nginx.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;nginx&quot;&gt;Nginx&lt;&#x2F;h2&gt;
&lt;p&gt;With a resolver setup the next bit is to setup Nginx to proxy requests to the
resolver. You need to have a trusted TLS certificate and key for your server&#x27;s
domain name. Let&#x27;s Encrypt works great for this and I use both an ECDSA
cert&#x2F;key pair and an RSA cert&#x2F;key pair but you can use one or the other if that
is your preference. I&#x27;ll cover requesting these certificates in a future post,
in the mean time there are plenty of existing Let&#x27;s Encrypt tutorials.&lt;&#x2F;p&gt;
&lt;p&gt;This can be a bit tricky as anyone reading this might have an Nginx config in
place already. It&#x27;ll be up to you to merge this config with yours. To assist
with this merging I&#x27;ve kept the Nginx config as minimal while still being
completely functional as possible and avoided using includes to increase the
clarity of the config.&lt;&#x2F;p&gt;
&lt;p&gt;On Fedora 28 you&#x27;ll need both the &lt;code&gt;nginx&lt;&#x2F;code&gt; and the &lt;code&gt;nginx-mod-stream&lt;&#x2F;code&gt; package.
After installing them both you&#x27;ll want to place the following config to
&lt;code&gt;&#x2F;etc&#x2F;nginx&#x2F;nginx.conf&lt;&#x2F;code&gt;.&lt;&#x2F;p&gt;
&lt;pre&gt;&lt;code&gt;# &#x2F;etc&#x2F;nginx&#x2F;nginx.conf

error_log &#x2F;var&#x2F;log&#x2F;nginx&#x2F;error.log warn;
user nginx;
worker_processes auto;

events {
  worker_connections 1024;
}

# Needs to be loaded before the http or stream blocks are used
load_module &quot;&#x2F;usr&#x2F;lib64&#x2F;nginx&#x2F;modules&#x2F;ngx_stream_module.so&quot;;

# This is where your normal http block would live
#http {
#}

stream {
  upstream dns_tcp_servers {
    server [::1]:53;
  }

  server {
    listen 853 ssl;
    proxy_pass dns_tcp_servers;

    ssl_certificate &#x2F;etc&#x2F;nginx&#x2F;nginx.ec.crt;
    ssl_certificate_key &#x2F;etc&#x2F;nginx&#x2F;nginx.ec.key;

    ssl_certificate &#x2F;etc&#x2F;nginx&#x2F;nginx.rsa.crt;
    ssl_certificate_key &#x2F;etc&#x2F;nginx&#x2F;nginx.rsa.key;

    ssl_handshake_timeout 30s;
    ssl_session_cache shared:DNSTCP:50m;
    ssl_session_tickets off;
    ssl_session_timeout 1h;

    ssl_protocols TLSv1.2;
    ssl_ciphers &#x27;ECDHE-ECDSA-AES256-GCM-SHA384:ECDHE-RSA-AES256-GCM-SHA384:ECDHE-ECDSA-CHACHA20-POLY1305:ECDHE-RSA-CHACHA20-POLY1305:ECDHE-ECDSA-AES128-GCM-SHA256:ECDHE-RSA-AES128-GCM-SHA256:ECDHE-ECDSA-AES256-SHA384:ECDHE-RSA-AES256-SHA384:ECDHE-ECDSA-AES128-SHA256:ECDHE-RSA-AES128-SHA256&#x27;;
    ssl_prefer_server_ciphers on;
  }
}
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;The SSL configuration is largely what Mozilla currently recommends for a
&#x27;Modern&#x27; config. Any client that supports DNS-over-TLS is new enough that
they&#x27;ll have access to modern queries. You&#x27;ll need to put your key and
certificates in the appropriate locations for that config.&lt;&#x2F;p&gt;
&lt;p&gt;With that in place start up the server:&lt;&#x2F;p&gt;
&lt;pre&gt;&lt;code&gt;systemctl start nginx.service
systemctl enable nginx.service
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;You&#x27;ll need to allow &lt;code&gt;953&#x2F;tcp&lt;&#x2F;code&gt; through your firewall and will likely need to
make changes to your SELinux policy to allow Nginx to listen on that port.&lt;&#x2F;p&gt;
&lt;p&gt;I haven&#x27;t found a good way to directly test the resolver using command line
clients but by configuring my Android phone towards my server and temporarily
adding the following to my unbound server config I can that queries being made
and responded to appropriately:&lt;&#x2F;p&gt;
&lt;pre&gt;&lt;code&gt;log-queries: yes
log-replies: yes
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;The next steps are on you. Configure unbound with your domain blacklists of
choice and voila additional privacy and security for those times when VPNs are
to power hungry.&lt;&#x2F;p&gt;
</content>
	</entry>
	<entry xml:lang="en">
		<title>Weird CloudFlare Behavior</title>
		<published>2018-10-21T22:36:09-06:00</published>
		<updated>2018-10-21T22:36:09-06:00</updated>
		<link href="https://stelfox.net/blog/2018/weird-cloudflare-behavior/" type="text/html"/>
		<id>https://stelfox.net/blog/2018/weird-cloudflare-behavior/</id>
		<content type="html">&lt;p&gt;While working on a replacement webserver, I encountered some odd behavior which
took a bit to track down to CloudFlare. This isn&#x27;t a bug or an issue with
CloudFlare, it was just unexpected.&lt;&#x2F;p&gt;
&lt;span id=&quot;continue-reading&quot;&gt;&lt;&#x2F;span&gt;
&lt;p&gt;The server was configured to respond to &lt;code&gt;www.example.tld&lt;&#x2F;code&gt; as well as
&lt;code&gt;example.tld&lt;&#x2F;code&gt;, to both encrypted and unencrypted connections. Any requests to
the &lt;code&gt;www.&lt;&#x2F;code&gt; domain get redirected to &lt;code&gt;https:&#x2F;&#x2F;example.tld&lt;&#x2F;code&gt;. The config was
roughly:&lt;&#x2F;p&gt;
&lt;pre&gt;&lt;code&gt;server {
  listen 80;
  listen [::]:80;

  listen 443 ssl http2;
  listen [::]:443 ssl http2;

  server_name www.example.tld;

  # Valid &#x2F; Basic SSL settings (omitted for brevity)

  return 301 https:&#x2F;&#x2F;example.tld$request_uri;
}
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;This worked fine. To match this config, I configured the unencrypted root
domain to redirect traffic from http to https with a config like the following:&lt;&#x2F;p&gt;
&lt;pre&gt;&lt;code&gt;server {
  listen 80;
  listen [::]:80;

  server_name example.tld;
  return 301 https:&#x2F;&#x2F;example.tld$request_uri;
}

server {
  listen 443 ssl http2;
  listen [::]:443 ssl http2;

  server_name example.tld;

  # Valid &#x2F; Basic SSL settings (omitted for brevity)

  # Normal site config
}
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;When I attempted to go to the site, it performed the https redirect, but
continued to try to redirect the browser. CloudFlare was hitting my upstream on
the unencrypted port and always getting the redirect message.&lt;&#x2F;p&gt;
&lt;p&gt;I wanted to leave this in place so clients would have the same experience when
I bypassed CloudFlare&#x27;s HTTP CDN proxy. The way I solved this was to simply
turn on &#x27;Full (strict)&#x27; SSL setting to ensure CloudFlare also connected to my
upstream on HTTPS, and relied on the &#x27;Always Use HTTPS&#x27; setting when CloudFlare
was active to maintain the same behavior. If you don&#x27;t have a valid SSL
certificate (such as a self signed certificate) you&#x27;ll have to use &#x27;Full SSL&#x27;
instead.&lt;&#x2F;p&gt;
&lt;p&gt;Funnily enough, this is apparently &lt;a href=&quot;https:&#x2F;&#x2F;support.cloudflare.com&#x2F;hc&#x2F;en-us&#x2F;articles&#x2F;115000219871&quot;&gt;common enough&lt;&#x2F;a&gt; of an issue to be
mentioned directly in the Help for the SSL configuration within CloudFlare.&lt;&#x2F;p&gt;
</content>
	</entry>
	<entry xml:lang="en">
		<title>It&#x27;s Never the Firewall</title>
		<published>2018-10-14T13:36:09-06:00</published>
		<updated>2018-10-14T13:36:09-06:00</updated>
		<link href="https://stelfox.net/blog/2018/its-never-the-firewall/" type="text/html"/>
		<id>https://stelfox.net/blog/2018/its-never-the-firewall/</id>
		<content type="html">&lt;p&gt;This last Thursday I had the privilege of giving a talk at our local Linux User
Group about diagnosing firewall issues on Linux entitled &amp;quot;It&#x27;s Never the
Firewall: Diagnosing Linux Firewall Issues&amp;quot;. I really enjoyed giving the talk,
however, I left a few questions unanswered. While I may do a more extensive
post on everything that I went through in the talk (I have been lax on writing
content for this blog), this post is more to answer the outstanding questions
and of course &lt;a href=&quot;https:&#x2F;&#x2F;stelfox.net&#x2F;blog&#x2F;2018&#x2F;its-never-the-firewall&#x2F;it_is_never_the_firewall.pdf&quot;&gt;to make my slides available&lt;&#x2F;a&gt;.&lt;&#x2F;p&gt;
&lt;span id=&quot;continue-reading&quot;&gt;&lt;&#x2F;span&gt;
&lt;p&gt;As far as I remember the only question I wasn&#x27;t able to answer was about file
descriptors related to TCP connections. It isn&#x27;t exactly a firewall issue but
exhaustion of file descriptors is one of the issues I&#x27;ve seen blamed on the
firewall (which in turn was relevant to the talk).&lt;&#x2F;p&gt;
&lt;p&gt;Established TCP connections consume file descriptors on Linux systems. Each
user session is restricted by a limit defined by the system administrator (or
more likely using the distribution&#x27;s defaults). Some services consume a large
number of file descriptors for their basic operations before any network
connections are involved (&lt;a href=&quot;https:&#x2F;&#x2F;redis.io&#x2F;topics&#x2F;clients&quot;&gt;Redis&lt;&#x2F;a&gt; and &lt;a href=&quot;https:&#x2F;&#x2F;www.elastic.co&#x2F;guide&#x2F;en&#x2F;elasticsearch&#x2F;reference&#x2F;current&#x2F;file-descriptors.html&quot;&gt;Elasticsearch&lt;&#x2F;a&gt; being two common
services that both recommend increasing this limit).&lt;&#x2F;p&gt;
&lt;p&gt;When file descriptors are exhausted there are two common behaviors of
applications depending on how they&#x27;re written. The first and more common
behavior will an immediately terminated connection, which to the user will look
quite similar to a iptables REJECT response. The less common behavior is
waiting until a file descriptor to become available before accepting the
connection, which will result in a hanging incomplete connection much like an
iptables DROP target.&lt;&#x2F;p&gt;
&lt;p&gt;An important caveat here is that I&#x27;m talking about user perception. If you look
at the packets being exchanged, it&#x27;s clear that this isn&#x27;t the case.&lt;&#x2F;p&gt;
&lt;p&gt;A quick way to diagnose if this is an issue is to compare the current number of
open file descriptors by a user to their limit. In my experience most of these
commands are available to unprivileged users which is also handy if there is a
complicated process for using or executing commands with elevated privileges.&lt;&#x2F;p&gt;
&lt;p&gt;First let&#x27;s check what the total number of open file descriptors are on the
system:&lt;&#x2F;p&gt;
&lt;pre&gt;&lt;code&gt;[user@sample-host] ~ $ lsof | wc -l
185779
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;If it is under 10,000 there is a very good chance this is not the problem
you&#x27;re looking for. If you see a large number like the output above, it is
worth investigating more. The host that I sampled that from currently has no
file descriptor issues.&lt;&#x2F;p&gt;
&lt;p&gt;First you&#x27;ll want to identify the main process of the application that is
having issues. Make sure you note the user that the application is running as.
Identify the current configured limits for that process (replace &lt;code&gt;&amp;lt;PID&amp;gt;&lt;&#x2F;code&gt; with
the pid you just found):&lt;&#x2F;p&gt;
&lt;pre&gt;&lt;code&gt;[user@sample-host] ~ $ grep &#x27;open files&#x27; &#x2F;proc&#x2F;&lt;PID&gt;&#x2F;limits
Max open files            1024                 4096                 files
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;This tells the process is currently operating with a soft limit of 1,024 file
descriptors and a hard limit of 4,096. This is where the diagnostics can get a
little fuzzy. File descriptors are limited by user &lt;em&gt;session&lt;&#x2F;em&gt; not by process or
user which is what we can directly query on. To get a rough idea of where we&#x27;re
at lets query those two specifics.&lt;&#x2F;p&gt;
&lt;pre&gt;&lt;code&gt;[user@sample-host] ~ $ lsof -p &lt;PID&gt; | wc -l
5
[user@sample-host] ~ $ lsof -u &lt;USER&gt; | wc -l
721
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;This particular example is enough for us to clearly see file descriptors are
not an issue. The current number of file descriptors open for the user (721) is
below the process&#x27;s soft limit of 1,024. If the number of file descriptors is
closer to our limits you&#x27;ll have to get an exact count of what is going on.
Unfortunately I don&#x27;t know an easy one liner to check this.&lt;&#x2F;p&gt;
&lt;p&gt;To get an exact count you need to enumerate all processes with the same session
identifier (SID) as your target process, query them all for the current file
descriptor consumption, and add them up into a total. As a rule of thumb I
recommend leaving about a 25% overhead for your service during peak load which
can be set in your system limits file for that service&#x27;s user.&lt;&#x2F;p&gt;
</content>
	</entry>
	<entry xml:lang="en">
		<title>SPF and Google Site Verification in Route 53</title>
		<published>2018-06-14T11:36:09-06:00</published>
		<updated>2018-06-14T11:36:09-06:00</updated>
		<link href="https://stelfox.net/blog/2018/spf-and-google-site-verification-in-route-53/" type="text/html"/>
		<id>https://stelfox.net/blog/2018/spf-and-google-site-verification-in-route-53/</id>
		<content type="html">&lt;p&gt;Route53 doesn&#x27;t allow multiple definitions of the same name&#x2F;type pair of DNS
entries which is quite a headache. This is the first time I&#x27;ve had a conflict
of a TXT record in Route53 at the base, specifically both Google&#x27;s site
verification, and SPF records both want to live at the root of the domain. The
site verification record needs to stay around as Google periodically
re-verifies the domain.&lt;&#x2F;p&gt;
&lt;span id=&quot;continue-reading&quot;&gt;&lt;&#x2F;span&gt;
&lt;p&gt;To get this to work you need to quote both the Google verification string and
the SPF record, but you also have to ensure that there is a newline in the
field. The contents of the entry should look something like this:&lt;&#x2F;p&gt;
&lt;pre&gt;&lt;code&gt;&quot;google-site-verification=&lt;auth string&gt;&quot;
&quot;v=spf1 include:_spf.google.com ~all&quot;
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;You can use &lt;a href=&quot;https:&#x2F;&#x2F;toolbox.googleapps.com&#x2F;apps&#x2F;checkmx&#x2F;check&quot;&gt;Google&#x27;s MX Toolbox&lt;&#x2F;a&gt; to verify the SPF record.&lt;&#x2F;p&gt;
</content>
	</entry>
	<entry xml:lang="en">
		<title>Timelapse of a Linux Desktop</title>
		<published>2018-06-08T16:37:39-06:00</published>
		<updated>2018-06-08T16:37:39-06:00</updated>
		<link href="https://stelfox.net/blog/2018/timelapse-of-a-linux-desktop/" type="text/html"/>
		<id>https://stelfox.net/blog/2018/timelapse-of-a-linux-desktop/</id>
		<content type="html">&lt;p&gt;I have the privilege of working full remote. To stay connected with our other
remote workers and the main office we keep a live video conference going all
the time. It&#x27;s pretty convenient and definitely allows me to continue to feel
connected with the company.&lt;&#x2F;p&gt;
&lt;span id=&quot;continue-reading&quot;&gt;&lt;&#x2F;span&gt;
&lt;p&gt;During one of our stand ups, a coworker mentioned that they&#x27;d like to see how
things looked over time. I have three 1440p monitors and largely leave the
video conference on my right most window, which makes recording (an audio free)
timelapse pretty easy with FFmpeg.&lt;&#x2F;p&gt;
&lt;p&gt;There are a couple of stack overflow posts about this but I wanted to use VP9
(really I wanted to use AV1, but it isn&#x27;t well supported yet). This adds a bit
of a trick as VP9 wants to analyze a bit of the video and will break if there
aren&#x27;t enough frames so some additional options are needed to get it to work.&lt;&#x2F;p&gt;
&lt;p&gt;These are the settings that worked well for me:&lt;&#x2F;p&gt;
&lt;pre&gt;&lt;code&gt;ffmpeg -f x11grab -framerate 1 \
  -thread_queue_size 2048 -probesize 10M -analyzeduration 10M \
  -s 2560,1440 -i :1.0+5120,0 \
  -vf settb=\(1&#x2F;30\),setpts=N&#x2F;TB&#x2F;30 -r 30 \
  -s 1280,720 -vcodec vp9 -crf 24 video_conference_$(date +%Y-%m-%d).mkv
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;I organized the parameters so they could easily be explained on a line by line
basis. The first line captures the X11 render output (sorry wayland users,
you&#x27;ll have to look elsewhere) with a framerate of 1&#x2F;second.&lt;&#x2F;p&gt;
&lt;p&gt;The second line holds the parameters that tune the analyze phase for the low
bandwidth and low framerate, these values are byte size not duration (10M is
10Mb not 10 minutes).&lt;&#x2F;p&gt;
&lt;p&gt;That third line will likely need to be customized to your display setup. I
wanted to capture the size of one of my monitors (&lt;code&gt;-s 2560x1440&lt;&#x2F;code&gt;) which is
pretty straight forward. Unusually, my X display is &lt;code&gt;:1&lt;&#x2F;code&gt; (visible by printing
the &lt;code&gt;$DISPLAY&lt;&#x2F;code&gt; environment variable) instead of the normal &lt;code&gt;:0&lt;&#x2F;code&gt;. Finally,
&lt;code&gt;x11grab&lt;&#x2F;code&gt; treats multi-monitor displays as one buffer, so to get my right most
monitor I have to add an X offset of the combined widths of my other two
monitors (that&#x27;s the &lt;code&gt;5120,0&lt;&#x2F;code&gt;).&lt;&#x2F;p&gt;
&lt;p&gt;This fourth line is kind of black magic, but ultimately it&#x27;s adding some
metadata to the stream saying that the output will be 30fps, and don&#x27;t complain
that the original framerate was 1fps.&lt;&#x2F;p&gt;
&lt;p&gt;The last line controls the output, I wanted to resize the video down to 720p
with a reasonable encode quality, using the VP9 codec I mentioned before, and
storing it with a date stamped file name.&lt;&#x2F;p&gt;
&lt;p&gt;When your done recording just Ctrl-C out of it (and give it a few seconds, it
may seem like it hung while writing out the data).&lt;&#x2F;p&gt;
&lt;p&gt;If I was going to make a recommendation, you may want to speed this up to 60fps
instead of 30 as a full workday will be a tediously long 8 minute video to
watch.&lt;&#x2F;p&gt;
</content>
	</entry>
	<entry xml:lang="en">
		<title>Setting Up EdgeRouter PoE on Google Fiber</title>
		<published>2018-05-26T18:41:09-06:00</published>
		<updated>2018-05-26T18:41:09-06:00</updated>
		<link href="https://stelfox.net/blog/2018/edgerouter-poe-on-google-fiber/" type="text/html"/>
		<id>https://stelfox.net/blog/2018/edgerouter-poe-on-google-fiber/</id>
		<content type="html">&lt;p&gt;I recently moved to an area with Google Fiber and jumped on the chance to have
a cheap and fast connection, and I didn&#x27;t need to sell my soul to certain other
companies. I already owned a Ubiquiti EdgeRouter PoE 5 which has been battle
tested at easily routing a gigabit worth of small packets.&lt;&#x2F;p&gt;
&lt;span id=&quot;continue-reading&quot;&gt;&lt;&#x2F;span&gt;
&lt;p&gt;When setting up my service, the representative I talked to told me I was able
to use my own router, but I would still need to get a Google Fiber Network Box.
I confirmed this with the staff who handed me the network box, informing me I
just had to &amp;quot;put it into bridge mode&amp;quot;. Turns out there is no bridge mode, and
no you don&#x27;t need the Fiber Box.&lt;&#x2F;p&gt;
&lt;p&gt;After much Googling and confirming those two points, I came across &lt;a href=&quot;https:&#x2F;&#x2F;comptechrt.blogspot.com&#x2F;2015&#x2F;10&#x2F;google-fiber-with-3rd-party.html&quot;&gt;other&lt;&#x2F;a&gt;
&lt;a href=&quot;https:&#x2F;&#x2F;community.ui.com&#x2F;questions&#x2F;Comcast-IPv6-issues-when-hwnat-enabled-on-ER-X&#x2F;c4d049b3-b4a1-46a7-a157-72b54c3403a5&quot;&gt;people&lt;&#x2F;a&gt; have been in this situation &lt;a href=&quot;https:&#x2F;&#x2F;productforums.google.com&#x2F;forum&#x2F;#!msg&#x2F;fiber&#x2F;AbNh8ij72Mw&#x2F;l0quYBiiCJ8J&quot;&gt;before&lt;&#x2F;a&gt; and have written up their
own very good documentation on very similar setups. A lot of the information is
at least a year old so it was worth going over carefully, and didn&#x27;t go quite
as far as I&#x27;d like.&lt;&#x2F;p&gt;
&lt;p&gt;One thing I will say ahead of time. I&#x27;m not adventuring into the TV service as
I don&#x27;t have it. From the messages I read, if you want this a cheap Gigabit
switch between your router and the fiber jack that you can additionally plug
the TV box into will do the trick (but you want to avoid powering the jack with
PoE if you go this route).&lt;&#x2F;p&gt;
&lt;p&gt;There was some discussion from an official Google representative in one of the
many threads that made a passing mention they would switch to untagged traffic
so I wanted to verify what was happening. For this I &lt;a href=&quot;https:&#x2F;&#x2F;stelfox.net&#x2F;blog&#x2F;2018&#x2F;quick-and-silent-gigabit-packet-interception&#x2F;&quot;&gt;became the wire&lt;&#x2F;a&gt; and
sniffed the traffic directly. The settings on the wire matched the contents of
a deleted post from a Google representative early on in &lt;a href=&quot;https:&#x2F;&#x2F;support.google.com&#x2F;fiber&#x2F;forum&#x2F;AAAAiDLuNlIAbNh8ij72Mw&#x2F;&quot;&gt;this thread&lt;&#x2F;a&gt;
reproduced here for future reference:&lt;&#x2F;p&gt;
&lt;blockquote&gt;
&lt;p&gt;Here&#x27;s the gory details if you really want to use your own router:&lt;&#x2F;p&gt;
&lt;ol&gt;
&lt;li&gt;Traffic in&#x2F;out of the fiberjack is vlan tagged with vlan2.&lt;&#x2F;li&gt;
&lt;li&gt;DHCP traffic should have 802.1p bit = 2&lt;&#x2F;li&gt;
&lt;li&gt;IGMP traffic should have 802.1p bit = 6&lt;&#x2F;li&gt;
&lt;li&gt;All other internet traffic 802.1p bit = 3&lt;&#x2F;li&gt;
&lt;&#x2F;ol&gt;
&lt;p&gt;You can send data without the 802.1p bits but your performance will get
throttled to something like 10Mbit.&lt;&#x2F;p&gt;
&lt;p&gt;NOTE:  This data is subject to change.  We are planning on changing the data
in&#x2F;out of the fiberjack to be untagged, which will then make it really easy
for you to connect your own router.&lt;&#x2F;p&gt;
&lt;p&gt;A word of warning, most consumer routers don&#x27;t have hardware forwarding (that
is my feeble understanding) so you might not be happy with the performance on
your network, and which will also probably affect tv service quality.&lt;&#x2F;p&gt;
&lt;&#x2F;blockquote&gt;
&lt;p&gt;One other piece of information that I gleaned from the traffic capture is that
the IPv6 prefix length of &#x2F;56. If you want to reproduce this, look into the
DHCPv6 messages for a pair of solicit &#x2F; request (actually a response but that
is how it is labeled).&lt;&#x2F;p&gt;
&lt;p&gt;I had some initial concern about VLAN tagging as I suspected it may force
software handling of all routed packets. The EdgeRouter PoE has hardware
offloading for VLAN tagging available so this is not an issue for this router.
For anyone using this as a reference for another router, you&#x27;ll want to make
sure yours supports this.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;basic-connectivity&quot;&gt;Basic Connectivity&lt;&#x2F;h2&gt;
&lt;p&gt;The first task was to get basic IPv4 connectivity going. When I started the
configuration my router was on version 1.9.1.1, which I upgraded to 1.10.1
&lt;em&gt;after&lt;&#x2F;em&gt; I finished all of the configuration. I didn&#x27;t have any issues with the
software, everything seemed very stable.&lt;&#x2F;p&gt;
&lt;p&gt;The basic connectivity just requires tagging outbound traffic with VLAN 2, to
get the full speed the packets additionally need to have the correct 802.1q QoS
tags. The one bonus we&#x27;ll add that is totally unecessary is powering the jack
with PoE since we&#x27;re already going to messing with that interface.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;em&gt;Sidenote: If you want to play with egress values, changing after the fact
requires editing of the &lt;a href=&quot;https:&#x2F;&#x2F;stelfox.net&#x2F;blog&#x2F;2018&#x2F;edgerouter-poe-on-google-fiber&#x2F;#changing-the-egress-qos-value&quot;&gt;config on disk and a full restart&lt;&#x2F;a&gt;. It&#x27;s kind of a
pain to go through a bunch of different options.&lt;&#x2F;em&gt;&lt;&#x2F;p&gt;
&lt;p&gt;The EdgeRouter PoE doesn&#x27;t seem to be able to adjust QoS settings based on the
protocol (I could be wrong I didn&#x27;t look into this too deeply, maybe the
advanced traffic control stuff?), but since I don&#x27;t have the TV service, I
don&#x27;t have the IGMP traffic to worry about. That just leaves DHCPv4 packets.&lt;&#x2F;p&gt;
&lt;p&gt;I confirmed that DHCPv4 will get an address when tagged with a QoS value of 3
(which is what the bulk traffic should be tagged with). Later on, I go into
detail &lt;a href=&quot;https:&#x2F;&#x2F;stelfox.net&#x2F;blog&#x2F;2018&#x2F;edgerouter-poe-on-google-fiber&#x2F;#initial-connectivity-time&quot;&gt;about an issue&lt;&#x2F;a&gt; that I think could be potentially related to this
but I haven&#x27;t done additional testing.&lt;&#x2F;p&gt;
&lt;p&gt;I tried several mapping values for different types of traffic and settled on
mapping everything to a QoS value of 3 (option 3 below). The mappings that I
tried are:&lt;&#x2F;p&gt;
&lt;ol&gt;
&lt;li&gt;0:2&lt;&#x2F;li&gt;
&lt;li&gt;0:3&lt;&#x2F;li&gt;
&lt;li&gt;0:3 1:3 2:3 3:3 4:3 5:3 6:3 7:3&lt;&#x2F;li&gt;
&lt;li&gt;0:3 1:3 2:3 3:3 4:4 5:5 6:6 7:7&lt;&#x2F;li&gt;
&lt;li&gt;0:6&lt;&#x2F;li&gt;
&lt;&#x2F;ol&gt;
&lt;p&gt;To configure the router, connect the fiber jack to eth0 on the router and make
sure the fiber jack doesn&#x27;t have its external power connected. SSH into your
router (this will depend on your current settings, or do a full reset and use
the default static IP setting) and go into configure mode:&lt;&#x2F;p&gt;
&lt;pre&gt;&lt;code&gt;configure
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;There are a lot of changes that need to be made, the one thing you&#x27;ll want to
double check is the name of your firewall rules which should have already been
setup. By default they get named &lt;code&gt;WAN_IN&lt;&#x2F;code&gt;, &lt;code&gt;WANv6_IN&lt;&#x2F;code&gt;, &lt;code&gt;WAN_LOCAL&lt;&#x2F;code&gt;, and
&lt;code&gt;WANv6_LOCAL&lt;&#x2F;code&gt; and I didn&#x27;t need to make any changes to them to get things
working. Because there are so many changes from the default WAN settings, we&#x27;re
going to start from a clean slate:&lt;&#x2F;p&gt;
&lt;pre&gt;&lt;code&gt;delete interfaces ethernet eth0
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;Now we need to setup the physical connection details again and we&#x27;ll add the
PoE configuration to power the jack. Please note that these will not go into
effect until we commit them (which we&#x27;re going to wait to do until later).&lt;&#x2F;p&gt;
&lt;pre&gt;&lt;code&gt;edit interfaces ethernet eth0
set description &quot;Google Fiber Jack&quot;
set duplex auto
set speed auto
set poe output 48v
exit
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;We&#x27;re now going to want to create the VLAN interface on eth0, which is done
through the &lt;code&gt;vif&lt;&#x2F;code&gt; sub-commands off the interface. While not necessary yet,
we&#x27;ll also place the IPv6 firewalls in place, even if you don&#x27;t intend to using
IPv6 having these in place can prevent accidents if it ever gets enabled.&lt;&#x2F;p&gt;
&lt;pre&gt;&lt;code&gt;edit interfaces ethernet eth0 vif 2
set description Internet
set address dhcp
set egress-qos &quot;0:3 1:3 2:3 3:3 4:3 5:3 6:3 7:3&quot;
set firewall in name WAN_IN
set firewall in ipv6-name WANv6_IN
set firewall local name WAN_LOCAL
set firewall local name WANv6_IN
exit
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;To ensure the routing performance is as good as it can be, we want to ensure
the relevant hardware offload settings are configured:&lt;&#x2F;p&gt;
&lt;pre&gt;&lt;code&gt;edit system offload
set ipv4 forwarding enable
set ipv4 vlan enable
exit
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;At this point we need to commit the change to allow the VLAN interface to be
created. Routing won&#x27;t work yet, but this will power the jack and get it
booting. To get basic routing working the outbound interface for NAT will also
need to be updated and recommitted. You&#x27;ll want to double check your NAT rule
number matches mine (5010).&lt;&#x2F;p&gt;
&lt;pre&gt;&lt;code&gt;commit
set service nat rule 5010 outbound-interface eth0.2
commit
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;Now you&#x27;ll have to wait a while for everything to come up, which can be &lt;a href=&quot;https:&#x2F;&#x2F;stelfox.net&#x2F;blog&#x2F;2018&#x2F;edgerouter-poe-on-google-fiber&#x2F;#initial-connectivity-time&quot;&gt;up to
five minutes&lt;&#x2F;a&gt; but your IPv4 connectivity should be all set.&lt;&#x2F;p&gt;
&lt;p&gt;I also setup IPv6 separately using DHCPv6-PD, giving a separate &#x2F;64 network to
each of my internal networks. I&#x27;m going to leave that for another post though.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;changing-the-egress-qos-value&quot;&gt;Changing the egress-qos Value&lt;&#x2F;h2&gt;
&lt;p&gt;If you want to change the &lt;code&gt;egress-qos&lt;&#x2F;code&gt; value after the VLAN interface comes up
you&#x27;ll be presented with the following error message:&lt;&#x2F;p&gt;
&lt;pre&gt;&lt;code&gt;admin@ubnt-router:~$ configure
[edit]
admin@ubnt-router# set interfaces ethernet eth0 vif 2 egress-qos &quot;0:2&quot;
[edit]
admin@ubnt-router# commit
egress-qos can only be set at vlan creation for 2

Commit failed
[edit]
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;To actually change this value, you need to switch to root, and use &lt;code&gt;vi&lt;&#x2F;code&gt; to edit
the &lt;code&gt;&#x2F;config&#x2F;config.boot&lt;&#x2F;code&gt; file and reboot the router itself.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;initial-connectivity-time&quot;&gt;Initial Connectivity Time&lt;&#x2F;h2&gt;
&lt;p&gt;Several time during the course of testing this I assumed I had broken my
connectivity while testing. I was actually experiencing surprisingly long
delays getting connectivity after changes. Part of this is that the PoE to the
fiber jack isn&#x27;t maintained while the router is rebooting, forcing the jack to
boot as well, but a large portion of that is the IPv4 connectivity itself.&lt;&#x2F;p&gt;
&lt;p&gt;The timings I measured during a reboot were:&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;1 minute 18 seconds to first ping of internal LAN interface&lt;&#x2F;li&gt;
&lt;li&gt;An additional 35 seconds before the SSH port was open&lt;&#x2F;li&gt;
&lt;li&gt;19 more seconds before logins were allowed&lt;&#x2F;li&gt;
&lt;li&gt;A further 33 seconds before the router saw the link from the fiber jack come
up&lt;&#x2F;li&gt;
&lt;li&gt;22 seconds until IPv6 connectivity was established (internal hosts now have
IPv6 internet)&lt;&#x2F;li&gt;
&lt;li&gt;An additional 3 minutes 46 seconds before the eth0.2 interface gets an IPv4
address and IPv4 connectivity is available.&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;Total boot time: 5 minutes 53 seconds. That is pretty crazy and it&#x27;s almost all
waiting for an IPv4 address on the link. Either something is really slow on
Google&#x27;s end, or this delay is because the DHCP traffic is tagged with an
incorrect QoS value (I haven&#x27;t tested it).&lt;&#x2F;p&gt;
&lt;p&gt;Either way, reboots once setup are pretty rare and I can wait the almost six
minutes without internet when it happens.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;mss-clamping&quot;&gt;MSS Clamping&lt;&#x2F;h2&gt;
&lt;p&gt;&lt;a href=&quot;https:&#x2F;&#x2F;www.stevejenkins.com&#x2F;blog&#x2F;2015&#x2F;11&#x2F;replace-your-google-fiber-network-box-with-a-ubiquiti-edgerouter-lite&#x2F;&quot;&gt;Steve Jenkins&lt;&#x2F;a&gt;&#x27;s post has the most complete documentation on setting up the
EdgeRouter (and makes his configs &lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;stevejenkins&#x2F;UBNT-EdgeRouter-Example-Configs&quot;&gt;available on GitHub&lt;&#x2F;a&gt;), but I was a tad
confused about him using MSS clamping, which I&#x27;ve left out of my config.&lt;&#x2F;p&gt;
&lt;p&gt;MSS clamping is used to restrict MTU sizes through TCP headers, and is very
useful when tunneling traffic or wrapping in an authentication mechanism such
as PPPoE, MPLS, or GRE. During the crafting of packets, hosts won&#x27;t be aware of
the tunnel which will have its own MTU and packet overhead and thus can hit
some performance snags.&lt;&#x2F;p&gt;
&lt;p&gt;With this config, we are at most adding 4 bytes to each packet as a VLAN tag
but won&#x27;t encounter a tunnel that will break a packet up into multiple tunneled
packets when a packet is to large and rebuild it at the other exactly the same.
Any fragmentation that occurs in transit will be visible to the MSS detection
algorithms and handled correctly without the overhead or limitation of MSS
clamping. If anything I&#x27;d expect it to slow down the connection slightly for
large packets.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;notes-for-the-post&quot;&gt;Notes for the post:&lt;&#x2F;h2&gt;
&lt;p&gt;Raw config:&lt;&#x2F;p&gt;
&lt;pre&gt;&lt;code&gt;&#x2F;* Everything in this section with the one exception are defaults *&#x2F;
firewall {
    all-ping enable
    broadcast-ping disable
    ipv6-name WANv6_IN {
        default-action drop
        description &quot;WAN inbound traffic forwarded to LAN&quot;
        enable-default-log
        rule 10 {
            action accept
            description &quot;Allow established&#x2F;related sessions&quot;
            state {
                established enable
                related enable
            }
        }
        rule 20 {
            action drop
            description &quot;Drop invalid state&quot;
            state {
                invalid enable
            }
        }
        &#x2F;* This is the one thing that was added, but I&#x27;m not sure it&#x27;s necessary... *&#x2F;
        rule 30 {
            action accept
            description &quot;Allow ICMPv6&quot;
            log disable
            protocol icmpv6
        }
    }
    ipv6-name WANv6_LOCAL {
        default-action drop
        description &quot;WAN inbound traffic to the router&quot;
        enable-default-log
        rule 10 {
            action accept
            description &quot;Allow established&#x2F;related sessions&quot;
            state {
                established enable
                related enable
            }
        }
        rule 20 {
            action drop
            description &quot;Drop invalid state&quot;
            state {
                invalid enable
            }
        }
        rule 30 {
            action accept
            description &quot;Allow IPv6 icmp&quot;
            protocol ipv6-icmp
        }
        rule 40 {
            action accept
            description &quot;allow dhcpv6&quot;
            destination {
                port 546
            }
            protocol udp
            source {
                port 547
            }
        }
    }
    ipv6-receive-redirects disable
    ipv6-src-route disable
    ip-src-route disable
    log-martians enable
    name WAN_IN {
        default-action drop
        description &quot;WAN to internal&quot;
        rule 10 {
            action accept
            description &quot;Allow established&#x2F;related&quot;
            state {
                established enable
                related enable
            }
        }
        rule 20 {
            action drop
            description &quot;Drop invalid state&quot;
            state {
                invalid enable
            }
        }
    }
    name WAN_LOCAL {
        default-action drop
        description &quot;WAN to router&quot;
        rule 10 {
            action accept
            description &quot;Allow established&#x2F;related&quot;
            state {
                established enable
                related enable
            }
        }
        rule 20 {
            action drop
            description &quot;Drop invalid state&quot;
            state {
                invalid enable
            }
        }
    }
    receive-redirects disable
    send-redirects enable
    source-validation disable
    syn-cookies enable
}
interfaces {
    ethernet eth0 {
        description &quot;Google Fiber Jack&quot;
        duplex auto
        poe {
            &#x2F;* Power the Fiber jack *&#x2F;
            output 48v
        }
        speed auto
        &#x2F;* Most of this section is relevant *&#x2F;
        vif 2 {
            address dhcp
            description Internet
            dhcpv6-pd {
                &#x2F;* Go over the DNS options even though they&#x27;re not required... *&#x2F;
                no-dns
                pd 0 {
                    interface eth1 {
                        &#x2F;* Relevant option *&#x2F;
                        host-address ::1
                        &#x2F;* Go over the DNS options even though they&#x27;re not required... *&#x2F;
                        no-dns
                        &#x2F;* Relevant options *&#x2F;
                        prefix-id :0
                        service slaac
                    }
                    interface switch0 {
                        &#x2F;* Relevant option *&#x2F;
                        host-address ::1
                        &#x2F;* Go over the DNS options even though they&#x27;re not required... *&#x2F;
                        no-dns
                        &#x2F;* Relevant options *&#x2F;
                        prefix-id :1
                        service slaac
                    }
                    &#x2F;* Google Fiber specific *&#x2F;
                    prefix-length &#x2F;56
                }
                &#x2F;* This is debateable and seems to work either way, cover the discussion *&#x2F;
                rapid-commit enable
            }
            &#x2F;* This is worth going over *&#x2F;
            egress-qos 0:3
            firewall {
                in {
                    ipv6-name WANv6_IN
                    name WAN_IN
                }
                local {
                    ipv6-name WANv6_LOCAL
                    name WAN_LOCAL
                }
            }
        }
    }
    ethernet eth1 {
        &#x2F;* Person specific *&#x2F;
        address 10.186.208.1&#x2F;24
        description DMZ
        duplex auto
        ipv6 {
            &#x2F;* Set automatically *&#x2F;
            dup-addr-detect-transmits 1
            router-advert {
                &#x2F;* Set automatically *&#x2F;
                cur-hop-limit 64
                link-mtu 0
                managed-flag false
                max-interval 600
                &#x2F;* Good to go over but not relevant *&#x2F;
                name-server 2606:4700:4700::1111
                name-server 2606:4700:4700::1001
                other-config-flag false
                &#x2F;* Needs to be set, without it it will be missing from the dhcpv6-pd config, but only if name servers are set *&#x2F;
                prefix ::&#x2F;64 {
                    autonomous-flag true
                    on-link-flag true
                    &#x2F;* Will get set automatically *&#x2F;
                    valid-lifetime 2592000
                }
                &#x2F;* Will get set automatically *&#x2F;
                reachable-time 0
                retrans-timer 0
                send-advert true
            }
        }
        poe {
            output off
        }
        speed auto
    }
    ethernet eth2 {
        description &quot;LAN&quot;
        duplex auto
        poe {
            output off
        }
        speed auto
    }
    ethernet eth3 {
        description &quot;LAN&quot;
        duplex auto
        poe {
            output off
        }
        speed auto
    }
    ethernet eth4 {
        description &quot;LAN&quot;
        duplex auto
        poe {
            output off
        }
        speed auto
    }
    loopback lo {
    }
    switch switch0 {
        &#x2F;* Person specific *&#x2F;
        address 10.202.254.1&#x2F;24
        description &quot;LAN&quot;
        ipv6 {
            &#x2F;* Set automatically *&#x2F;
            dup-addr-detect-transmits 1
            router-advert {
                &#x2F;* Set automatically *&#x2F;
                cur-hop-limit 64
                link-mtu 0
                managed-flag false
                max-interval 600
                &#x2F;* Good to go over but not relevant *&#x2F;
                name-server 2606:4700:4700::1111
                name-server 2606:4700:4700::1001
                &#x2F;* Will get set automatically *&#x2F;
                other-config-flag false
                &#x2F;* Needs to be set, without it it will be missing from the dhcpv6-pd config, but only if name servers are set *&#x2F;
                prefix ::&#x2F;64 {
                    autonomous-flag true
                    on-link-flag true
                    &#x2F;* Will get set automatically *&#x2F;
                    valid-lifetime 2592000
                }
                &#x2F;* Will get set automatically *&#x2F;
                reachable-time 0
                retrans-timer 0
                send-advert true
            }
        }
        mtu 1500
        switch-port {
            interface eth2 {
            }
            interface eth3 {
            }
            interface eth4 {
            }
            vlan-aware disable
        }
    }
}
service {
    dhcp-server {
        disabled false
        hostfile-update disable
        shared-network-name LAN {
            authoritative enable
            &#x2F;* Person specific *&#x2F;
            subnet 10.186.208.0&#x2F;24 {
                &#x2F;* Person specific *&#x2F;
                default-router 10.186.208.1
                &#x2F;* Good to go over but not relevant *&#x2F;
                dns-server 1.1.1.1
                dns-server 1.0.0.1
                &#x2F;* Irrelevant *&#x2F;
                domain-name example.tld
                lease 86400
                &#x2F;* Person specific *&#x2F;
                start 10.186.208.38 {
                    stop 10.186.208.243
                }
            }
        }
        shared-network-name DMZ {
            authoritative enable
            &#x2F;* Person specific *&#x2F;
            subnet 10.202.254.0&#x2F;24 {
                &#x2F;* Person specific *&#x2F;
                default-router 10.202.254.1
                &#x2F;* Good to go over but not relevant *&#x2F;
                dns-server 1.1.1.1
                dns-server 1.0.0.1
                &#x2F;* Irrelevant *&#x2F;
                domain-name example.tld
                lease 86400
                &#x2F;* Person specific *&#x2F;
                start 10.202.254.38 {
                    stop 10.202.254.243
                }
            }
        }
        use-dnsmasq disable
    }
    gui {
        http-port 80
        https-port 443
        &#x2F;* Good practice, but not relevant to Fiber *&#x2F;
        older-ciphers disable
    }
    nat {
        rule 5010 {
            description &quot;masquerade for WAN&quot;
            &#x2F;* Relevant to Fiber config (vif 2) *&#x2F;
            outbound-interface eth0.2
            type masquerade
        }
    }
    &#x2F;* Defaults *&#x2F;
    ssh {
        port 22
        protocol-version v2
    }
    &#x2F;* Next two services are unrelated to Fiber config *&#x2F;
    ubnt-discover {
        disable
    }
    &#x2F;* Need to go over the security trade offs here... *&#x2F;
    upnp2 {
        acl {
            rule 10 {
                action deny
                local-port 0-1024
                subnet 0.0.0.0&#x2F;0
            }
            rule 20 {
                action deny
                external-port 0-1024,1080,5432,8000,8080,8081,8088,8443,8888,9100,9200
                subnet 0.0.0.0&#x2F;0
            }
            rule 30 {
                action deny
                local-port 0-1024,1080,5432,8000,8080,8081,8088,8443,8888,9100,9200
                subnet 0.0.0.0&#x2F;0
            }
            rule 40 {
                action allow
                subnet 10.202.254.0&#x2F;24
            }
            rule 100 {
                action deny
                subnet 0.0.0.0&#x2F;0
            }
        }
        listen-on switch0
        nat-pmp disable
        secure-mode enable
        wan eth0.2
    }
}
system {
    &#x2F;* These next two lines are not relevant to the Fiber config *&#x2F;
    domain-name example.tld
    host-name ubnt-router.example.tld
    &#x2F;* These will always be user specific *&#x2F;
    login {
        user admin {
            authentication {
                encrypted-password ****************
                plaintext-password ****************
            }
            full-name &quot;Admin User&quot;
            level admin
        }
    }
    &#x2F;* While these are not relevant to Google Fiber config it is good to go over *&#x2F;
    name-server 1.1.1.1
    name-server 1.0.0.1
    name-server 2606:4700:4700::1111
    name-server 2606:4700:4700::1001
    &#x2F;* Defaults *&#x2F;
    ntp {
        server 0.ubnt.pool.ntp.org {
        }
        server 1.ubnt.pool.ntp.org {
        }
        server 2.ubnt.pool.ntp.org {
        }
        server 3.ubnt.pool.ntp.org {
        }
    }
    offload {
        &#x2F;* This is not available on the EdgeRouter PoE *&#x2F;
        hwnat disable
        &#x2F;* These are all worth discussing *&#x2F;
        ipsec enable
        ipv4 {
            forwarding enable
            vlan enable
        }
        ipv6 {
            forwarding enable
            vlan enable
        }
    }
    syslog {
        global {
            facility all {
                level notice
            }
        }
        &#x2F;* Irrelevant to Fiber config *&#x2F;
        host 2605:a601:4049:c100:afdb:e15f:c174:5f71 {
            facility all {
                level info
            }
        }
    }
    &#x2F;* Irrelevant to Fiber config *&#x2F;
    time-zone UTC
    &#x2F;* Irrelevant to Fiber config *&#x2F;
    traffic-analysis {
        dpi disable
        export enable
    }
}
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;Additional References:&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;http:&#x2F;&#x2F;itnutt.com&#x2F;how-to-bypass-google-fibers-network-box&#x2F;&quot;&gt;How to Bypass Google Fiber&#x27;s Network Box&lt;&#x2F;a&gt;&lt;&#x2F;li&gt;
&lt;li&gt;&lt;a href=&quot;https:&#x2F;&#x2F;community.ui.com&#x2F;questions&#x2F;Comcast-IPv6-issues-when-hwnat-enabled-on-ER-X&#x2F;c4d049b3-b4a1-46a7-a157-72b54c3403a5#U1850112&quot;&gt;Comcast IPv6 issues when hwnat enabled on ER-X&lt;&#x2F;a&gt;&lt;&#x2F;li&gt;
&lt;li&gt;&lt;a href=&quot;https:&#x2F;&#x2F;community.ui.com&#x2F;questions&#x2F;ERlite-1-5-upnp2-secure-mode&#x2F;f7430063-cb95-41c0-a262-ce8b54c4d286&quot;&gt;ERlite 1.5 upnp2 secure mode?&lt;&#x2F;a&gt;&lt;&#x2F;li&gt;
&lt;li&gt;&lt;a href=&quot;https:&#x2F;&#x2F;community.ui.com&#x2F;questions&#x2F;IPv6-DHCPv6-and-DHCPv6-PD&#x2F;3c7e898a-9f57-4e70-9b38-98bcdb1fec1e&quot;&gt;IPv6 DHCPv6 and DHCPv6-PD&lt;&#x2F;a&gt;&lt;&#x2F;li&gt;
&lt;li&gt;&lt;a href=&quot;https:&#x2F;&#x2F;community.ui.com&#x2F;questions&#x2F;The-generation-of-etc-radvd-conf-is-missing-my-configuration-after-reboot-1-7-0&#x2F;a8b750f3-cf7a-4604-9327-70d451c049f3&quot;&gt;The generation of &#x2F;etc&#x2F;radvd.conf is missing my configuration after reboot - 1.7.0&lt;&#x2F;a&gt;&lt;&#x2F;li&gt;
&lt;li&gt;&lt;a href=&quot;https:&#x2F;&#x2F;community.ui.com&#x2F;questions&#x2F;Updated-Google-Fiber-EdgeRouter-Lite-PoE-IPv4-and-IPv6-config-boot-and-Guide&#x2F;321ccca1-ffdc-4ffd-8a31-6c28b24829a8&quot;&gt;Updated Google Fiber + EdgeRouter Lite &#x2F; PoE IPv4 &amp;amp; IPv6 config.boot and Guide&lt;&#x2F;a&gt;&lt;&#x2F;li&gt;
&lt;li&gt;&lt;a href=&quot;https:&#x2F;&#x2F;kazoo.ga&#x2F;dhcpv6-pd-for-native-ipv6&#x2F;&quot;&gt;DHCPv6 Prefix Delegation for IPv6&lt;&#x2F;a&gt;&lt;&#x2F;li&gt;
&lt;li&gt;&lt;a href=&quot;https:&#x2F;&#x2F;netswat.com&#x2F;blog&#x2F;google-fiber-ubiquitis-edgerouter&#x2F;&quot;&gt;Google Fiber &amp;amp; Ubiquiti‚Äôs Edgerouter&lt;&#x2F;a&gt;&lt;&#x2F;li&gt;
&lt;li&gt;&lt;a href=&quot;https:&#x2F;&#x2F;netswat.com&#x2F;blog&#x2F;wp-content&#x2F;uploads&#x2F;2016&#x2F;07&#x2F;EdgeRouter-TVScript6.txt&quot;&gt;EdgeRouter TVScript&lt;&#x2F;a&gt;&lt;&#x2F;li&gt;
&lt;li&gt;&lt;a href=&quot;https:&#x2F;&#x2F;netswat.com&#x2F;blog&#x2F;wp-content&#x2F;uploads&#x2F;2016&#x2F;07&#x2F;Edge-Setup-Interfacesv3.txt&quot;&gt;Edge Setup Interfaces V3&lt;&#x2F;a&gt;&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
</content>
	</entry>
	<entry xml:lang="en">
		<title>Parsing HTTP Responses in Ruby</title>
		<published>2018-05-23T07:53:19-06:00</published>
		<updated>2018-05-23T07:53:19-06:00</updated>
		<link href="https://stelfox.net/blog/2018/parsing-http-responses-in-ruby/" type="text/html"/>
		<id>https://stelfox.net/blog/2018/parsing-http-responses-in-ruby/</id>
		<content type="html">&lt;p&gt;Normally handling HTTP responses in Ruby is rather straight forward. There is a
native library in Ruby that handles HTTP requests which parses the responses
into a neat data structure that you can then operate on. What if you want to
work on stored HTTP responses outside of a connection though? This was the
situation I found myself in and thanks to a series of unusual decisions in the
Ruby core library I found myself left out in the cold.&lt;&#x2F;p&gt;
&lt;span id=&quot;continue-reading&quot;&gt;&lt;&#x2F;span&gt;
&lt;p&gt;For reference this is in the latest stable Ruby as of this writing (2.5.1).&lt;&#x2F;p&gt;
&lt;p&gt;Let&#x27;s start with a very small HTTP response stored in a variable for us to test
on:&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;ruby&quot; class=&quot;language-ruby &quot;&gt;&lt;code class=&quot;language-ruby&quot; data-lang=&quot;ruby&quot;&gt;raw_http_body = &lt;&lt;-BODY.rstrip
Just the body...
BODY

raw_http_response = &lt;&lt;-RESP.rstrip
HTTP&#x2F;1.1 200 Ok\r
Connection: close\r
Content-Length: #{raw_http_body.bytesize}\r
\r
#{raw_http_body}
RESP
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;The above is a little bit weird but is a minimum reasonable HTTP response. All
lines are approprietly terminated with both a carriage return (explicit) and
a newline (implicit in how the strings are defined). The &lt;code&gt;Content-Length&lt;&#x2F;code&gt;
header is the exact number of bytes present in the body (thus the two &lt;code&gt;#rstrip&lt;&#x2F;code&gt;
calls). The &lt;code&gt;Date&lt;&#x2F;code&gt; header was omitted due to this line in &lt;a href=&quot;https:&#x2F;&#x2F;tools.ietf.org&#x2F;html&#x2F;rfc7231#section-7.1.1.2&quot;&gt;RFC7231&lt;&#x2F;a&gt;&lt;&#x2F;p&gt;
&lt;blockquote&gt;
&lt;p&gt;An origin server MUST NOT send a Date header field if it does not have a
clock capable of providing a reasonable approximation of the current instance
in Coordinated Universal Time.&lt;&#x2F;p&gt;
&lt;&#x2F;blockquote&gt;
&lt;p&gt;...Which the content of this static site does not have.&lt;&#x2F;p&gt;
&lt;p&gt;With our minimal response out of the way how do we go about parsing it? The
&lt;a href=&quot;https:&#x2F;&#x2F;ruby-doc.org&#x2F;stdlib-2.5.1&#x2F;libdoc&#x2F;net&#x2F;http&#x2F;rdoc&#x2F;Net&#x2F;HTTPResponse.html&quot;&gt;Ruby 2.5.1 stdlib documentation&lt;&#x2F;a&gt; doesn&#x27;t specify how it can be created by
end users which usually means it isn&#x27;t intended for use by users of the
language directly and digging through the Ruby source, you&#x27;ll see this is
precisely the case. Which means &lt;em&gt;&lt;strong&gt;Ruby does not have a HTTP response parser
available in it&#x27;s standard library&lt;&#x2F;strong&gt;&lt;&#x2F;em&gt;. This is pretty frustrating, but maybe it
can be worked around.&lt;&#x2F;p&gt;
&lt;p&gt;How does the &lt;code&gt;Net::HTTP&lt;&#x2F;code&gt; library make use of it? Even if the methods aren&#x27;t
listed for public documentation they&#x27;re still public APIs on the class and
should be able to be used without monkey patching right? The response is setup
in &lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;ruby&#x2F;ruby&#x2F;blob&#x2F;v2_5_1&#x2F;lib&#x2F;net&#x2F;http.rb#L958&quot;&gt;the connect method of Net::HTTP&lt;&#x2F;a&gt; and it comes down to a few relevant
lines that can be summarized as:&lt;&#x2F;p&gt;
&lt;ol&gt;
&lt;li&gt;Open a socket to the webserver&lt;&#x2F;li&gt;
&lt;li&gt;Write the formatted request to the socket&lt;&#x2F;li&gt;
&lt;li&gt;Pass the socket to &lt;code&gt;HTTPResponse#read_new&lt;&#x2F;code&gt;&lt;&#x2F;li&gt;
&lt;&#x2F;ol&gt;
&lt;p&gt;So we need a socket like object containing our response, which we can do with
&lt;code&gt;StringIO&lt;&#x2F;code&gt; and pass it to the appropriate method. Let&#x27;s see what happens:&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;ruby&quot; class=&quot;language-ruby &quot;&gt;&lt;code class=&quot;language-ruby&quot; data-lang=&quot;ruby&quot;&gt;require &#x27;net&#x2F;http&#x27;
require &#x27;stringio&#x27;

resp_io = StringIO.new(raw_http_response)
response = Net::HTTPResponse.read_new(resp_io)
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;We get a raised exception:&lt;&#x2F;p&gt;
&lt;blockquote&gt;
&lt;p&gt;Net::HTTPBadResponse: wrong status line: &amp;quot;HTTP&#x2F;1.1 200 Ok\r\n&amp;quot;&lt;&#x2F;p&gt;
&lt;&#x2F;blockquote&gt;
&lt;p&gt;That is definitely a valid status line, so what is going on here? Back to
Ruby&#x27;s source code... &lt;code&gt;Net::HTTPResponse#read_new&lt;&#x2F;code&gt; starts off by calling
&lt;code&gt;Net::HTTPResponse#read_status_line&lt;&#x2F;code&gt; which uses this regex for extracting and
checking the validity of the status line:&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;ruby&quot; class=&quot;language-ruby &quot;&gt;&lt;code class=&quot;language-ruby&quot; data-lang=&quot;ruby&quot;&gt;&#x2F;\AHTTP(?:\&#x2F;(\d+\.\d+))?\s+(\d\d\d)(?:\s+(.*))?\z&#x2F;in
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;I had never seen the &lt;code&gt;&#x2F;n&lt;&#x2F;code&gt; modifier for Ruby&#x27;s regular expressions and it seems
to be completely undocumented. This turned out to be a red herring as it simply
sets &lt;code&gt;Regexp::NOENCODING&lt;&#x2F;code&gt; (had to dig into the
&lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;ruby&#x2F;ruby&#x2F;blob&#x2F;3527c05a8f4e189772cdac17f166bd9626c24661&#x2F;spec&#x2F;ruby&#x2F;core&#x2F;regexp&#x2F;options_spec.rb&quot;&gt;spec&#x2F;ruby&#x2F;core&#x2F;regexp&#x2F;options_spec.rb&lt;&#x2F;a&gt; file to figure that one out).&lt;&#x2F;p&gt;
&lt;p&gt;So why isn&#x27;t that regular expression matching? Spoiler: It&#x27;s the newline (the
carriage return is fine). That is a violation of the HTTP spec, but it is
working normally for Ruby&#x27;s HTTP requests so what gives? Apparently we have to
go deeper...&lt;&#x2F;p&gt;
&lt;p&gt;It&#x27;s getting the header string by calling &lt;code&gt;#readline&lt;&#x2F;code&gt; which on standard &lt;a href=&quot;https:&#x2F;&#x2F;ruby-doc.org&#x2F;core-2.5.1&#x2F;IO.html#method-i-readline&quot;&gt;IO&lt;&#x2F;a&gt;
objects returns the newline (The &lt;code&gt;IO&lt;&#x2F;code&gt; class if the base for &lt;code&gt;StringIO&lt;&#x2F;code&gt;, and
&lt;code&gt;Socket&lt;&#x2F;code&gt; objects in addition to many others). In &lt;a href=&quot;https:&#x2F;&#x2F;blog.bigbinary.com&#x2F;2017&#x2F;03&#x2F;07&#x2F;io-readlines-now-accepts-chomp-flag-as-an-argument.html&quot;&gt;Ruby 2.4&lt;&#x2F;a&gt; and later there
is a chomp flag that changes this behavior but it isn&#x27;t being used in this
case, and it would take the carriage return with it if it was.&lt;&#x2F;p&gt;
&lt;p&gt;So... We must not be operating on an actual &lt;code&gt;IO&lt;&#x2F;code&gt; subclass... And sure enough,
&lt;code&gt;Net::HTTP#connect&lt;&#x2F;code&gt; after getting the raw socket wraps it in a
&lt;code&gt;Net::BufferedIO&lt;&#x2F;code&gt; object which is another internal hidden class. You can see
the definition &lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;ruby&#x2F;ruby&#x2F;blob&#x2F;v2_5_1&#x2F;lib&#x2F;net&#x2F;protocol.rb#L81&quot;&gt;of it here&lt;&#x2F;a&gt; and here is its &lt;code&gt;#readline&lt;&#x2F;code&gt; method:&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;ruby&quot; class=&quot;language-ruby &quot;&gt;&lt;code class=&quot;language-ruby&quot; data-lang=&quot;ruby&quot;&gt;def readline
  readuntil(&quot;\n&quot;).chop
end
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;Yep, for some reason this one private internal API has decided to complicate a
Ruby standard API convention and strip off the trailing carriage return and new
line. Wrapping our &lt;code&gt;StringIO&lt;&#x2F;code&gt; object in a &lt;code&gt;BufferedIO&lt;&#x2F;code&gt; object does solve this
problem but there is no reason for these complications...&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;ruby&quot; class=&quot;language-ruby &quot;&gt;&lt;code class=&quot;language-ruby&quot; data-lang=&quot;ruby&quot;&gt;resp_io = StringIO.new(raw_http_response)
buf_io = Net::BufferedIO.new(resp_io)
response = Net::HTTPResponse.read_new(buf_io)
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;Or does it?&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;ruby&quot; class=&quot;language-ruby &quot;&gt;&lt;code class=&quot;language-ruby&quot; data-lang=&quot;ruby&quot;&gt;response.body
# NoMethodError: undefined method `closed?&#x27; for nil:NilClass
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;We need to pull one more trick from the &lt;code&gt;Net::HTTP#transport_request&lt;&#x2F;code&gt; to get
the body. The first line actually returns the body, but we want to treat this
like a normal HTTPResponse so we want to make sure the &lt;code&gt;#body&lt;&#x2F;code&gt; method works:&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;ruby&quot; class=&quot;language-ruby &quot;&gt;&lt;code class=&quot;language-ruby&quot; data-lang=&quot;ruby&quot;&gt;response.reading_body(buf_io, true) { yield res if block_given? }
response.body
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;There are a couple of differences still from a normal response body. The only
one of particular note to me is that normally the response get it&#x27;s &lt;code&gt;#uri&lt;&#x2F;code&gt; data
from the request. This isn&#x27;t available with the response alone but can be set
pretty easily:&lt;&#x2F;p&gt;
&lt;pre&gt;&lt;code&gt;require &#x27;uri&#x27;
response.uri = URI.parse(&#x27;http:&#x2F;&#x2F;example.tld&#x27;)
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;Altogether this is what it looks like:&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;ruby&quot; class=&quot;language-ruby &quot;&gt;&lt;code class=&quot;language-ruby&quot; data-lang=&quot;ruby&quot;&gt;require &#x27;net&#x2F;http&#x27;
require &#x27;stringio&#x27;
require &#x27;uri&#x27;

raw_http_body = &lt;&lt;-BODY.rstrip
Just the body...
BODY

raw_http_response = &lt;&lt;-RESP.rstrip
HTTP&#x2F;1.1 200 Ok\r
Connection: close\r
Content-Length: #{raw_http_body.bytesize}\r
\r
#{raw_http_body}
RESP

resp_io = StringIO.new(raw_http_response)
buf_io = Net::BufferedIO.new(resp_io)

response = Net::HTTPResponse.read_new(buf_io)
response.reading_body(buf_io, true) { yield res if block_given? }
response.uri = URI.parse(&#x27;http:&#x2F;&#x2F;example.tld&#x27;)

# You now have a valid Net::HTTPResponse object
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
</content>
	</entry>
	<entry xml:lang="en">
		<title>Quick and Silent Gigabit Packet Interception</title>
		<published>2018-05-13T00:55:09-06:00</published>
		<updated>2018-05-13T00:55:09-06:00</updated>
		<link href="https://stelfox.net/blog/2018/quick-and-silent-gigabit-packet-interception/" type="text/html"/>
		<id>https://stelfox.net/blog/2018/quick-and-silent-gigabit-packet-interception/</id>
		<content type="html">&lt;p&gt;I regularly find myself inspecting traffic on Linux systems. Usually I&#x27;m
already on the client or server when doing this (such as when diagnosing weird
low level app behavior, or unknown, or unusual traffic). It has been a while
since I&#x27;ve needed to silently be the wire between two black boxes.&lt;&#x2F;p&gt;
&lt;span id=&quot;continue-reading&quot;&gt;&lt;&#x2F;span&gt;
&lt;p&gt;While verifying link level information about bypassing my Google Fiber Network
Box I needed to be that wire again. Before I connected any wires to anything I
needed to be sure I wouldn&#x27;t accidentally leak traffic as I wasn&#x27;t sure what
would impact the link.&lt;&#x2F;p&gt;
&lt;p&gt;You&#x27;ll need a Linux computer with two gigabit ethernet ports. My last two
laptops haven&#x27;t had any built in ethernet ports, but USB gigabit adapters are
cheap and I already had a bunch.&lt;&#x2F;p&gt;
&lt;p&gt;I went through and disabled the services that would configure network
interfaces (NetworkManager and ModemManager) as well as all of the networking
services on my system, and confirming your firewalls are all going to allow the
traffic through.&lt;&#x2F;p&gt;
&lt;p&gt;I double checked no packets were sent out up on link up by listening on each
respective interface, plugging in to a powered but otherwise disconnected
switch, and bringing the interface up.&lt;&#x2F;p&gt;
&lt;p&gt;Once I&#x27;d verified everything in one root terminal I ran:&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;sh&quot; class=&quot;language-sh &quot;&gt;&lt;code class=&quot;language-sh&quot; data-lang=&quot;sh&quot;&gt;ip link set eth0 promisc on multicast off arp off
ip link set eth1 promisc on multicast off arp off

ip link add name intercept0 type bridge
ip link set intercept0 promisc on multicast off arp off up

ip link set eth0 master intercept0
ip link set eth1 master intercept0
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;In another terminal either as root, or as a user in the &lt;code&gt;wireshark&lt;&#x2F;code&gt; group begin
recording the traffic of interest:&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;sh&quot; class=&quot;language-sh &quot;&gt;&lt;code class=&quot;language-sh&quot; data-lang=&quot;sh&quot;&gt;tshark -i intercept0 -w recording.pcap
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;At this point, connect the cables between the two boxes of interest. When ready
bring the links up:&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;sh&quot; class=&quot;language-sh &quot;&gt;&lt;code class=&quot;language-sh&quot; data-lang=&quot;sh&quot;&gt;ip link set eth0 up
ip link set eth1 up
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;Welcome to being the wire.&lt;&#x2F;p&gt;
</content>
	</entry>
	<entry xml:lang="en">
		<title>Converting OpenLDAP Schemas to LDIF</title>
		<published>2018-03-24T20:20:22-06:00</published>
		<updated>2018-03-24T20:20:22-06:00</updated>
		<link href="https://stelfox.net/blog/2018/converting-openldap-schemas-to-ldif/" type="text/html"/>
		<id>https://stelfox.net/blog/2018/converting-openldap-schemas-to-ldif/</id>
		<content type="html">&lt;p&gt;I&#x27;ve been writing software to work against an OpenLDAP instance, with a highly
customized schema. The operators of the existing system only had the schema
files and searching around found several elaborate ways to convert the files
which I tried with mixed success. After doing the research to figure this out,
it became clear I could probably have used &lt;code&gt;slapcat&lt;&#x2F;code&gt; and have dumped the active
schema directly to LDIF.&lt;&#x2F;p&gt;
&lt;span id=&quot;continue-reading&quot;&gt;&lt;&#x2F;span&gt;
&lt;p&gt;As a sample of how I converted these, I&#x27;ll use the &lt;code&gt;rfc2307bis.schema&lt;&#x2F;code&gt; file
which didn&#x27;t seem to come with a matching LDIF file in the source distribution.
You&#x27;ll need to identify the dependencies of the schema, which I&#x27;ve tended to
just do with trial and error. If a dependency is missing you&#x27;ll receive an
error like the following:&lt;&#x2F;p&gt;
&lt;pre&gt;&lt;code&gt;5ab6f5a6 &#x2F;etc&#x2F;openldap&#x2F;schema&#x2F;cosine.schema: line 1084 objectclass: ObjectClass not found: &quot;person&quot;
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;You can identify the requisite schema file by grep&#x27;ing for the missing object
in the other schema files and adding it to the config. The &lt;code&gt;rfc2307.schema&lt;&#x2F;code&gt;
file depends on the &lt;code&gt;core.schema&lt;&#x2F;code&gt; and &lt;code&gt;cosine.schema&lt;&#x2F;code&gt; files. With this in mind
you can use the following script to convert the LDIF file:&lt;&#x2F;p&gt;
&lt;pre&gt;&lt;code&gt;SCHEMA_CONV_DIR=&quot;$(mktemp -d)&quot;

cat &lt;&lt; EOF &gt; ${SCHEMA_CONV_DIR}&#x2F;convert.conf
include &#x2F;etc&#x2F;openldap&#x2F;schema&#x2F;core.schema
include &#x2F;etc&#x2F;openldap&#x2F;schema&#x2F;cosine.schema
include &#x2F;etc&#x2F;openldap&#x2F;schema&#x2F;rfc2307bis.schema
EOF

slapcat -f ${SCHEMA_CONV_DIR}&#x2F;convert.conf -F ${SCHEMA_CONV_DIR} -n 0 \
  -s &quot;cn={2}rfc2307bis,cn=schema,cn=config&quot; | sed -re &#x27;s&#x2F;\{[0-9]+\}&#x2F;&#x2F;&#x27; \
  -e &#x27;&#x2F;^structuralObjectClass: &#x2F;d&#x27; -e &#x27;&#x2F;^entryUUID: &#x2F;d&#x27; -e &#x27;&#x2F;^creatorsName: &#x2F;d&#x27; \
  -e &#x27;&#x2F;^createTimestamp: &#x2F;d&#x27; -e &#x27;&#x2F;^entryCSN: &#x2F;d&#x27; -e &#x27;&#x2F;^modifiersName: &#x2F;d&#x27; \
  -e &#x27;&#x2F;^modifyTimestamp: &#x2F;d&#x27; -e &#x27;&#x2F;^$&#x2F;d&#x27; &gt; &#x2F;etc&#x2F;openldap&#x2F;schema&#x2F;rfc2307bis.ldif

rm -rf ${SCHEMA_CONV_DIR}
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;One important thing to note is the schema identifier in the slapcat command
&lt;code&gt;cn={2}rfc2307bis,cn=schema,cn=config&lt;&#x2F;code&gt;. The &#x27;{2}&#x27; there will be the line number
from the &lt;code&gt;convert.conf&lt;&#x2F;code&gt; file counting from 0 and will likely be different for
the schemas you&#x27;re converting, and the name will be defined by the contents of
the schema file.&lt;&#x2F;p&gt;
&lt;p&gt;You&#x27;ll also want to pay attention to the file names and make sure the inputs
and outputs match your expectations.&lt;&#x2F;p&gt;
</content>
	</entry>
	<entry xml:lang="en">
		<title>Including LDIF Files in OpenLDAP</title>
		<published>2018-03-24T20:20:22-06:00</published>
		<updated>2018-03-24T20:20:22-06:00</updated>
		<link href="https://stelfox.net/blog/2018/including-ldif-files-in-openldap/" type="text/html"/>
		<id>https://stelfox.net/blog/2018/including-ldif-files-in-openldap/</id>
		<content type="html">&lt;p&gt;While setting up and OpenLDAP server I found my distribution shipped with a
couple of schema files, but no equivalent LDIF files. I found ways to convert
the file using &lt;code&gt;slapcat&lt;&#x2F;code&gt; and &lt;code&gt;slaptest&lt;&#x2F;code&gt; and the files were valid on their own.&lt;&#x2F;p&gt;
&lt;span id=&quot;continue-reading&quot;&gt;&lt;&#x2F;span&gt;
&lt;p&gt;I was specifically trying to bootstrap an OpenLDAP server, with it&#x27;s schema,
from scratch for a CI&#x2F;CD system to test against. To accomplish this I was
making use of the &lt;code&gt;include&lt;&#x2F;code&gt; directive in a configuration LDIF file and saw some
very odd behavior.&lt;&#x2F;p&gt;
&lt;p&gt;When I included the LDIF schema files that shipped with the software, I could
include multiple of them back to back like so:&lt;&#x2F;p&gt;
&lt;pre&gt;&lt;code&gt;include: file:&#x2F;&#x2F;&#x2F;etc&#x2F;openldap&#x2F;schema&#x2F;core.ldif
include: file:&#x2F;&#x2F;&#x2F;etc&#x2F;openldap&#x2F;schema&#x2F;cosine.ldif
include: file:&#x2F;&#x2F;&#x2F;etc&#x2F;openldap&#x2F;schema&#x2F;inetorgperson.ldif
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;When I included my converted file, it would successfully apply it, then just
stop... I assumed at first that there was a syntax error, but there was no
error. Everything contained in the included LDIF was present in the DIT. It had
simply stopped.&lt;&#x2F;p&gt;
&lt;p&gt;After many frustrating attempts, and hours Googling, the result came down to...
there was an empty line at the end of the file. I couldn&#x27;t find any
documentation about this behavior and identified it through dumb luck. For
anyone that comes across this I hope this solves your issue as well.&lt;&#x2F;p&gt;
</content>
	</entry>
	<entry xml:lang="en">
		<title>Cross-Compiling Gentoo for Xilinx Boards</title>
		<published>2017-12-18T17:49:22-05:00</published>
		<updated>2017-12-18T17:49:22-05:00</updated>
		<link href="https://stelfox.net/blog/2017/cross-compiling-gentoo-for-xilinx-boards/" type="text/html"/>
		<id>https://stelfox.net/blog/2017/cross-compiling-gentoo-for-xilinx-boards/</id>
		<content type="html">&lt;p&gt;&lt;em&gt;Note: If you&#x27;ve come here looking to build a root filesystem for 32 bit ARM
devices I suspect everything but the build tuple will be the same. The issues
that need to be worked around largely packaging and profile issues that should
all be the same.&lt;&#x2F;em&gt;&lt;&#x2F;p&gt;
&lt;p&gt;I got a hold of a Zynq 7100 development board, and while I&#x27;ve played with some
embedded ARM microcontrollers such as the STM32F3 series and more basic RISC
style microcontrollers like Atmel&#x27;s SAMD10 and Atmega lines, I&#x27;ve never played
with FPGA development before so I considered this an interesting learning
opportunity.&lt;&#x2F;p&gt;
&lt;span id=&quot;continue-reading&quot;&gt;&lt;&#x2F;span&gt;
&lt;p&gt;To do development of the FPGA and generally use the board at all you have to
shell out $2,995 for a non-transferable license of a proprietary piece of
software called Vivado to develop on the FPGA. For a hobby project just
exploring the board this isn&#x27;t going to fly. There is a 30-day evaluation
version though and there are guides to getting it to work in Linux.&lt;&#x2F;p&gt;
&lt;p&gt;For this post I&#x27;m going to gloss over this part and get to the meat of the
largest issue I had while attempting to bootstrap the Linux portion of this
development board. Xilinx maintains its own very rough hacked together
distribution called &lt;a href=&quot;https:&#x2F;&#x2F;xilinx-wiki.atlassian.net&#x2F;wiki&#x2F;spaces&#x2F;A&#x2F;pages&#x2F;18842250&#x2F;PetaLinux&quot;&gt;PetaLinux&lt;&#x2F;a&gt; which is just a very poorly designed wrapper
around &lt;a href=&quot;https:&#x2F;&#x2F;www.yoctoproject.org&#x2F;&quot;&gt;Yocto Linux&lt;&#x2F;a&gt;.&lt;&#x2F;p&gt;
&lt;p&gt;Unfortunately I haven&#x27;t been fully able to remove PetaLinux from my build, I
still need to use it to integrate the board specifics with the Linux kernel and
in turn compile the Linux kernel, u-boot, and handle the configuration to point
at a root filesystem living on the SD card. PetaLinux&#x27;s incredibly limited
documentation can at least get you this far. This post covers building that
root filesystem and guides around some of the problems the Gentoo cross process
doesn&#x27;t cover.&lt;&#x2F;p&gt;
&lt;p&gt;I want a target that is a bit more inclusive than most embedded Linux root
filesystems (think IoT devices). This device is less constrained than devices
like most OpenWRT capable devices (we&#x27;re not limited to 16MB of space). Let&#x27;s
quickly define some criteria that will determine the successful build of a root
filesystem:&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;It will have all the utilities necessary to support interactive logins&lt;&#x2F;li&gt;
&lt;li&gt;It will have a working file editor&lt;&#x2F;li&gt;
&lt;li&gt;It will have a valid native compiler for itself&lt;&#x2F;li&gt;
&lt;li&gt;It will have a working package manager to allow it to extend itself&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;From this set of goals we will both be able to re-compile everything natively
on the board if we so choose, and get access to the vast majority of packaged
software from the Gentoo repositories as well as easily perform project
development directly.&lt;&#x2F;p&gt;
&lt;p&gt;To get started you will have to have a working Gentoo install to start the
cross compilation from. I&#x27;ve personally had issues with the hardened, SELinux,
and no-multilib profile variants. If you encounter issues I strongly recommend
trying the standard system profile on your build host. I&#x27;m also sure there is
probably some way to get the portage cross tooling working in other
distributions, but I&#x27;ll leave that as an exercise to the reader.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;tooling&quot;&gt;Tooling&lt;&#x2F;h2&gt;
&lt;p&gt;To get started we&#x27;re going to want to setup an overlay specifically for our
cross development. This will allow customization of the profile for the device
later on. This is largely for use beyond this guide and is a good practice to
separate changes from your system and the target board.&lt;&#x2F;p&gt;
&lt;p&gt;These commands are pretty straight forward and do need to be run as root:&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;bash&quot; class=&quot;language-bash &quot;&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;mkdir -p &#x2F;usr&#x2F;local&#x2F;overlay&#x2F;portage-crossdev&#x2F;{profiles,metadata}
echo &#x27;crossdev&#x27; &gt; &#x2F;usr&#x2F;local&#x2F;overlay&#x2F;portage-crossdev&#x2F;profiles&#x2F;repo_name
echo &#x27;masters = gentoo&#x27; &gt;
&#x2F;usr&#x2F;local&#x2F;overlay&#x2F;portage-crossdev&#x2F;metadata&#x2F;layout.conf
chown -R portage:portage &#x2F;usr&#x2F;local&#x2F;overlay&#x2F;portage-crossdev

cat &lt;&lt; EOF &gt; &#x2F;usr&#x2F;local&#x2F;overlay&#x2F;portage-crossdev&#x2F;metadata&#x2F;layout.conf
masters = gentoo
thin-manifests = true
EOF

mkdir -p &#x2F;etc&#x2F;portage&#x2F;repos.conf
cat &lt;&lt; EOF &gt; &#x2F;etc&#x2F;portage&#x2F;repos.conf&#x2F;crossdev.conf
[crossdev]
location = &#x2F;usr&#x2F;local&#x2F;overlay&#x2F;portage-crossdev
priority = 10
masters = gentoo
auto-sync = no
EOF
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;With our overlay setup we now need to install the cross development tools (once
again as root):&lt;&#x2F;p&gt;
&lt;pre&gt;&lt;code&gt;emerge sys-devel&#x2F;crossdev
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;The next step is to build our initial tool chain. Having worked with many tool
chains before this is absolutely the easiest time I&#x27;ve ever had setting one up.
One command will get you all the way to a C&#x2F;C++ compiler, linker, bintools, and
a standard library. The specific tool chain target tuple is for a glibc based
tool chain, on a Xilinx variant of an arm processor (or
arm-xilinx-linux-gnueabi).&lt;&#x2F;p&gt;
&lt;pre&gt;&lt;code&gt;crossdev --stable -s4 -t arm-xilinx-linux-gnueabi
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;This will result in a very bare bones root filesystem in
&lt;code&gt;&#x2F;usr&#x2F;arm-xilinx-linux-gnueabi&lt;&#x2F;code&gt;. This doesn&#x27;t really have anything of value
yet.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;base-system&quot;&gt;Base System&lt;&#x2F;h2&gt;
&lt;p&gt;We need to configure portage and then a profile for our build. First off the
portage configuration, this exists in
&lt;code&gt;&#x2F;etc&#x2F;arm-xilinx-linux-gnueabi&#x2F;etc&#x2F;portage&#x2F;make.conf&lt;&#x2F;code&gt; and varies slightly from
the default.&lt;&#x2F;p&gt;
&lt;pre&gt;&lt;code&gt;CHOST=&#x27;arm-xilinx-linux-gnueabi&#x27;
CBUILD=&#x27;x86_64-pc-linux-gnu&#x27;
ARCH=&#x27;arm&#x27;

HOSTCC=&#x27;x86_64-pc-linux-gnu-gcc&#x27;

CFLAGS=&#x27;-O2 -pipe -fomit-frame-pointer&#x27;
CXXFLAGS=&quot;${CFLAGS}&quot;

ROOT=&quot;&#x2F;usr&#x2F;${CHOST}&#x2F;&quot;

ACCEPT_KEYWORDS=&#x27;arm&#x27;

USE=&quot;${ARCH}&quot;

FEATURES=&#x27;sandbox noman noinfo nodoc&#x27;

# Be sure we dont overwrite pkgs from another repo..
PKGDIR=&quot;${ROOT}packages&#x2F;&quot;
PORTAGE_TMPDIR=&quot;${ROOT}tmp&#x2F;&quot;

ELIBC=&#x27;glibc&#x27;

PKG_CONFIG_PATH=&quot;${ROOT}usr&#x2F;lib&#x2F;pkgconfig&#x2F;&quot;

PYTHON_TARGETS=&#x27;python2_7&#x27;
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;If you pay attention compared to the defaults there are a few changes I&#x27;ve
explicitly made:&lt;&#x2F;p&gt;
&lt;ol&gt;
&lt;li&gt;Do not build packages, we simply don&#x27;t need them&lt;&#x2F;li&gt;
&lt;li&gt;Allow PAM to be included&lt;&#x2F;li&gt;
&lt;li&gt;Reject the testing arm packages (~arm keyword)&lt;&#x2F;li&gt;
&lt;li&gt;Re-enable the file collision protections between packages&lt;&#x2F;li&gt;
&lt;li&gt;Explicitly define our valid python target at 2.7 only&lt;&#x2F;li&gt;
&lt;&#x2F;ol&gt;
&lt;p&gt;The first three align with my original stated goals, building will be allowed
and preferred on the device so we won&#x27;t need to host any packages. We want a
standard interactive login and ideally we want a stable system (as much as
possible). The last one is personal preference as in my build (after this guide
is over) I&#x27;ll be using software that doesn&#x27;t work on Python 3 variants.&lt;&#x2F;p&gt;
&lt;p&gt;The fourth change is something I want to draw specific attention to. This is
disabled by default because the stock ARM profile is inherently broken. It
attempts to force both a complete busybox system in addition to the standard
Gentoo base. The faux busybox binaries directly conflict and you&#x27;ll end up in a
weird mixed state that isn&#x27;t good. This is true of libraries as well which will
result in some core libraries failing to compile (&lt;code&gt;dev-libs&#x2F;gmp&lt;&#x2F;code&gt; was the first
one that failed on me).&lt;&#x2F;p&gt;
&lt;p&gt;To both fix that issue and allow us to have a clean build, I needed to build a
custom Gentoo profile for targeting this device. This minimal profile will work
cleanly for our target.&lt;&#x2F;p&gt;
&lt;pre&gt;&lt;code&gt;rm &#x2F;usr&#x2F;arm-xilinx-linux-gnueabi&#x2F;etc&#x2F;portage&#x2F;make.profile
mkdir &#x2F;usr&#x2F;arm-xilinx-linux-gnueabi&#x2F;etc&#x2F;portage&#x2F;make.profile
echo 5 &gt; &#x2F;usr&#x2F;arm-xilinx-linux-gnueabi&#x2F;etc&#x2F;portage&#x2F;make.profile&#x2F;eabi
cat &lt;&lt; EOF &gt; &#x2F;usr&#x2F;arm-xilinx-linux-gnueabi&#x2F;etc&#x2F;portage&#x2F;make.profile&#x2F;parent
&#x2F;usr&#x2F;portage&#x2F;profiles&#x2F;base
&#x2F;usr&#x2F;portage&#x2F;profiles&#x2F;arch&#x2F;arm&#x2F;armv7a
EOF

mkdir -p &#x2F;usr&#x2F;arm-xilinx-linux-gnueabi&#x2F;etc&#x2F;portage&#x2F;package.accept_keywords
cat &lt;&lt; EOF &gt; &#x2F;usr&#x2F;arm-xilinx-linux-gnueabi&#x2F;etc&#x2F;portage&#x2F;package.accept_keywords&#x2F;system
sys-apps&#x2F;coreutils ~arm
sys-apps&#x2F;sandbox ~arm
EOF
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;There is one final workaround we&#x27;re going to need to put in place before we can
begin compiling our system. Currently &lt;code&gt;sys-apps&#x2F;portage&lt;&#x2F;code&gt; and
&lt;code&gt;dev-python&#x2F;pyxattr&lt;&#x2F;code&gt; are incorrectly packaged and will use the system library
paths rather than those in the chroot.&lt;&#x2F;p&gt;
&lt;p&gt;This prevents both of them from being compiled with native extensions and
incorrectly places the files inside the chroot (They&#x27;re in the 64 bit path on a
32 bit target). This is fixable once we have the root filesystem on the device
but in the meantime we need to set some use flags to avoid the issue:&lt;&#x2F;p&gt;
&lt;pre&gt;&lt;code&gt;mkdir -p &#x2F;usr&#x2F;arm-xilinx-linux-gnueabi&#x2F;etc&#x2F;portage&#x2F;package.use
echo &#x27;sys-apps&#x2F;portage -native-extensions -xattr&#x27; &gt; &#x2F;usr&#x2F;arm-xilinx-linux-gnueabi&#x2F;etc&#x2F;portage&#x2F;package.use&#x2F;portage
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;With that in place sit back grab a cup of your favorite warm beverage and watch
the system compile (seriously this is going to take a hot minute):&lt;&#x2F;p&gt;
&lt;pre&gt;&lt;code&gt;arm-xilinx-linux-gnueabi-emerge --update --newuse --deep @system
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;There is one final gotcha with the root filesystem, the commands to date will
not create several important directories. These can be created with the
following command:&lt;&#x2F;p&gt;
&lt;pre&gt;&lt;code&gt;mkdir -p &#x2F;usr&#x2F;arm-xilinx-linux-gnueabi&#x2F;{dev,home,proc,root,sys}
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;At this point you should have a mostly complete root filesystem and may want to
start diverging from this guide (but pay attention to the kernel modules
section, and the rebuild section). There are a couple of things that won&#x27;t
currently work, specifically:&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;Authentication&lt;&#x2F;li&gt;
&lt;li&gt;Serial Console&lt;&#x2F;li&gt;
&lt;li&gt;Networking&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;h2 id=&quot;authentication-serial-usage&quot;&gt;Authentication &amp;amp; Serial Usage&lt;&#x2F;h2&gt;
&lt;p&gt;First authentication, we need to provide root with a password and PAM needs its
configuration files to function. We can&#x27;t use the native tools (without qemu
binary emulation) to change the password so the fastest way to give root a
password is to pregenerate a password hash and drop it directly into the
relevant files. If you&#x27;d like to keep going you can use the hash below for the
super secure password &#x27;root&#x27; (I don&#x27;t recommend it):&lt;&#x2F;p&gt;
&lt;pre&gt;&lt;code&gt;sed -i &#x27;s`root:[^:]*:`root:$6$ufWqa3MP$CfFwj0M7tW15gUYBRVms3GG2FJTRMAhlpkwV7Bp4aro6mGFHmotMjHoePNoTd1Gf9fgzh&#x2F;jJM3rvJgkGgSjz31:`&#x27; &#x2F;usr&#x2F;arm-xilinx-linux-gnueabi&#x2F;etc&#x2F;shadow
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;And the requisite PAM configuration files:&lt;&#x2F;p&gt;
&lt;pre&gt;&lt;code&gt;arm-xilinx-linux-gnueabi-emerge sys-auth&#x2F;pambase
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;The serial console is going to be more device specific and is a bit tricky to
figure out. To find this out on my board I created a service that collected the
names of all devices under &lt;code&gt;&#x2F;dev&lt;&#x2F;code&gt;, logged them to a file and found mine to be
&lt;code&gt;ttyPS0&lt;&#x2F;code&gt; (also one I&#x27;ve never seen before).&lt;&#x2F;p&gt;
&lt;p&gt;The following command will replace the default serial configuration with one
for this device (you may also have to change the baud rate it&#x27;s running at):&lt;&#x2F;p&gt;
&lt;pre&gt;&lt;code&gt;sed -i &#x27;s`^s0:.*$`ps0:12345:respawn:&#x2F;sbin&#x2F;agetty -L 115200 ttyPS0 vt100`&#x27; &#x2F;usr&#x2F;arm-xilinx-linux-gnueabi&#x2F;etc&#x2F;inittab
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;h2 id=&quot;networking&quot;&gt;Networking&lt;&#x2F;h2&gt;
&lt;pre&gt;&lt;code&gt;arm-xilinx-linux-gnueabi-emerge sys-apps&#x2F;net-tools net-misc&#x2F;netifrc \
  net-misc&#x2F;dhcpcd net-misc&#x2F;iputils sys-apps&#x2F;iproute2
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;To get it to come up by default, since we can&#x27;t use the &lt;code&gt;rc&lt;&#x2F;code&gt; tools natively yet
we can cheat. This assumes your kernel is configured to use the legacy network
names (which are more consistent and predictable :-&#x2F;). This will setup eth0 to
come up automatically and use DHCP to grab an address on the network:&lt;&#x2F;p&gt;
&lt;pre&gt;&lt;code&gt;cat &lt;&lt; &#x27;EOF&#x27; &gt; &#x2F;usr&#x2F;arm-xilinx-linux-gnueabi&#x2F;etc&#x2F;conf.d&#x2F;net
# &#x2F;etc&#x2F;conf.d&#x2F;net
modules=&quot;iproute2&quot;

# Default DHCP config for interfaces
dhcp=&quot;release nonis nontp&quot;

config_eth0=&quot;dhcp&quot;
EOF

echo &#x27;arm-board&#x27; &gt; &#x2F;usr&#x2F;arm-xilinx-linux-gnueabi&#x2F;etc&#x2F;hostname
echo &#x27;hostname=&quot;arm-board.localhost&quot;&#x27; &gt; &#x2F;usr&#x2F;arm-xilinx-linux-gnueabi&#x2F;etc&#x2F;conf.d&#x2F;hostname
cat &lt;&lt; &#x27;EOF&#x27; &gt; &#x2F;usr&#x2F;arm-xilinx-linux-gnueabi&#x2F;etc&#x2F;hosts
# &#x2F;etc&#x2F;hosts

127.0.0.1 localhost4 localhost
::1       localhost6 localhost
EOF

cd &#x2F;usr&#x2F;arm-xilinx-linux-gnueabi&#x2F;etc&#x2F;init.d&#x2F;
ln -s net.lo net.eth0

cd &#x2F;usr&#x2F;arm-xilinx-linux-gnueabi&#x2F;etc&#x2F;runlevels&#x2F;default&#x2F;
ln -s &#x2F;etc&#x2F;init.d&#x2F;net.eth0 net.eth0
rm -f netmount
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;h2 id=&quot;kernel-modules&quot;&gt;Kernel Modules&lt;&#x2F;h2&gt;
&lt;p&gt;Part of the kernel build that has to happen still in the PetaLinux environment
are kernel modules. One of the build artifacts is the root filesystem PetaLinux
thinks you&#x27;re going to use. These contain very important kernel modules which
need to be extracted.&lt;&#x2F;p&gt;
&lt;p&gt;Inside the root of your PetaLinux project after a build you should find a file
&lt;code&gt;images&#x2F;linux&#x2F;rootfs.tar.gz&lt;&#x2F;code&gt; which will have a directory inside it
&lt;code&gt;.&#x2F;lib&#x2F;modules&lt;&#x2F;code&gt;. The contents need to be transferred to your root filesystem.
If you transfer that to the system you&#x27;re building the board root on you can
extract all of the appropriate files using the following command:&lt;&#x2F;p&gt;
&lt;pre&gt;&lt;code&gt;tar -xf rootfs.tar.gz -C &#x2F;usr&#x2F;arm-xilinx-linux-gnueabi&#x2F; .&#x2F;lib&#x2F;modules
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;You can verify they are present by confirming a directory that looks along the
lines of &lt;code&gt;4.9.0-xilinx-v2017.2&lt;&#x2F;code&gt; exists in
&lt;code&gt;&#x2F;usr&#x2F;arm-xilinx-linux-gnueabi&#x2F;lib&#x2F;modules&#x2F;&lt;&#x2F;code&gt;.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;ssh-server&quot;&gt;SSH Server&lt;&#x2F;h2&gt;
&lt;p&gt;If you would additionally like an SSH server running (that supports root
login) there is a bit of a trick. User privilege separation requires a
dedicated user and group named &lt;code&gt;sshd&lt;&#x2F;code&gt; for this to work.&lt;&#x2F;p&gt;
&lt;p&gt;The OpenSSH ebuild doesn&#x27;t create this user and I&#x27;m not entirely sure what
does. For now we can solve this issue by creating the user and group manually.&lt;&#x2F;p&gt;
&lt;pre&gt;&lt;code&gt;arm-xilinx-linux-gnueabi-emerge net-misc&#x2F;openssh

echo &#x27;sshd:x:22:22:added by portage for openssh:&#x2F;var&#x2F;empty:&#x2F;sbin&#x2F;nologin&#x27; &gt;&gt; &#x2F;usr&#x2F;arm-xilinx-linux-gnueabi&#x2F;etc&#x2F;passwd
echo &#x27;sshd:*:0:0:::::&#x27; &gt;&gt; &#x2F;usr&#x2F;arm-xilinx-linux-gnueabi&#x2F;etc&#x2F;shadow
echo &#x27;sshd:x:22:&#x27; &gt;&gt; &#x2F;usr&#x2F;arm-xilinx-linux-gnueabi&#x2F;etc&#x2F;group

cat &lt;&lt; &#x27;EOF&#x27; &gt; &#x2F;usr&#x2F;arm-xilinx-linux-gnueabi&#x2F;etc&#x2F;ssh&#x2F;sshd_config
# &#x2F;etc&#x2F;ssh&#x2F;sshd_config

HostKeyAlgorithms ssh-ed25519,ecdsa-sha2-nistp521,ssh-rsa

ClientAliveInterval 10

UseDNS no

AllowTcpForwarding no
UsePAM yes

PasswordAuthentication yes
PermitRootLogin yes
EOF

cd &#x2F;usr&#x2F;arm-xilinx-linux-gnueabi&#x2F;etc&#x2F;runlevels&#x2F;default&#x2F;
ln -s &#x2F;etc&#x2F;init.d&#x2F;sshd sshd
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;h2 id=&quot;additional-tools&quot;&gt;Additional Tools&lt;&#x2F;h2&gt;
&lt;p&gt;This is a pretty solid foundation for any root Linux system. Everything at this
point is going to preferential and determined by your project requirements. A
few things you may want to include:&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;VIM&lt;&#x2F;li&gt;
&lt;li&gt;NTPd or Chronyd for time keeping&lt;&#x2F;li&gt;
&lt;li&gt;A syslog server (I recommend syslog-ng) and in turn logrotate&lt;&#x2F;li&gt;
&lt;li&gt;Network performance testing tools (such as iperf3)&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;From the above list I wanted both VIM and iperf3 and thus ran the following:&lt;&#x2F;p&gt;
&lt;pre&gt;&lt;code&gt;echo &#x27;net-misc&#x2F;iperf ~arm&#x27; &gt; &#x2F;usr&#x2F;arm-xilinx-linux-gnueabi&#x2F;etc&#x2F;portage&#x2F;package.accept_keywords&#x2F;network_utils
echo &#x27;app-editors&#x2F;vim minimal&#x27; &gt; &#x2F;usr&#x2F;arm-xilinx-linux-gnueabi&#x2F;etc&#x2F;portage&#x2F;package.use&#x2F;vim
arm-xilinx-linux-gnueabi-emerge app-editors&#x2F;vim net-misc&#x2F;iperf
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;h2 id=&quot;rebuilding-on-the-system&quot;&gt;Rebuilding on the System&lt;&#x2F;h2&gt;
&lt;p&gt;Once all the packages you want for your base system are installed, the root may
be in an inconsitent state. It&#x27;s a good idea to run a sync, global use update,
a preserved rebuild, and dependency clean on the board before continuing:&lt;&#x2F;p&gt;
&lt;pre&gt;&lt;code&gt;emerge --sync
arm-xilinx-linux-gnueabi-emerge --update --newuse --deep @world
arm-xilinx-linux-gnueabi-emerge @preserved-rebuild
arm-xilinx-linux-gnueabi-emerge --depclean
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;We now need to get the root filesystem on a live board and rebuilding cleaning
up some of the mismatch package flags and irregularities introduced by the
cross compilation process. At a minimum want to fix the incorrectly built
portage package so everything is usable normally.&lt;&#x2F;p&gt;
&lt;p&gt;Before transferring this it&#x27;s a good idea to preemptively adjust the make
config to no longer be a cross environment, and remove the special case for
portage. This can be done with the following command:&lt;&#x2F;p&gt;
&lt;pre&gt;&lt;code&gt;cat &lt;&lt; &#x27;EOF&#x27; &gt; &#x2F;usr&#x2F;arm-xilinx-linux-gnueabi&#x2F;etc&#x2F;portage&#x2F;make.conf
ARCH=&#x27;arm&#x27;
CFLAGS=&#x27;-O2 -pipe&#x27;
CXXFLAGS=&quot;${CFLAGS}&quot;
CHOST=&#x27;arm-xilinx-linux-gnueabi&#x27;

ACCEPT_KEYWORDS=&#x27;arm&#x27;
FEATURES=&#x27;sandbox noman noinfo nodoc&#x27;
USE=&quot;${ARCH} pam&quot;

ELIBC=&#x27;glibc&#x27;

L10N=&#x27;en&#x27;
LINGUAS=&#x27;en&#x27;

PYTHON_TARGETS=&#x27;python2_7&#x27;
EOF

rm -f &#x2F;usr&#x2F;arm-xilinx-linux-gnueabi&#x2F;etc&#x2F;portage&#x2F;package.use&#x2F;portage
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;We now need to package up our root filesystem:&lt;&#x2F;p&gt;
&lt;pre&gt;&lt;code&gt;tar -cJf ~&#x2F;xilinx_root_non_native.txz -C &#x2F;usr&#x2F;arm-xilinx-linux-gnueabi .
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;This next bit requires the proper settings in PetaLinux and a completed build
(you&#x27;ll need your own BOOT.BIN, image.ub, and system.dtb files). After
inserting an appropriately size SD card (You&#x27;re going to want 4 or 8Gb more
likely than not). For me the device showed up as mmcblk0 on my machine. Confirm
yours before following the next steps:&lt;&#x2F;p&gt;
&lt;pre&gt;&lt;code&gt;dd if=&#x2F;dev&#x2F;zero bs=1M count=1 oflag=sync of=&#x2F;dev&#x2F;mmcblk0

parted --script -a optimal &#x2F;dev&#x2F;mmcblk0 -- mklabel msdos
parted --script -a optimal &#x2F;dev&#x2F;mmcblk0 -- mkpart primary fat32 100 600
parted --script -a optimal &#x2F;dev&#x2F;mmcblk0 -- mkpart primary ext4 600 -1

dd if=&#x2F;dev&#x2F;zero bs=1M count=1 oflag=sync of=&#x2F;dev&#x2F;mmcblk0p1
dd if=&#x2F;dev&#x2F;zero bs=1M count=1 oflag=sync of=&#x2F;dev&#x2F;mmcblk0p2

mkfs.vfat -n BOOT -F 32 &#x2F;dev&#x2F;mmcblk0p1
mkfs.ext4 -L rootfs &#x2F;dev&#x2F;mmcblk0p2

mkdir -p &#x2F;mnt&#x2F;boot
mount &#x2F;dev&#x2F;mmcblk0p1 &#x2F;mnt&#x2F;boot

mkdir -p &#x2F;mnt&#x2F;root
mount &#x2F;dev&#x2F;mmcblk0p2 &#x2F;mnt&#x2F;root
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;You&#x27;ll need to copy BOOT.BIN, image.ub, and system.dtb to &#x2F;mnt&#x2F;boot and
extract the root filesystem into the root directory (compressed version still
lives at ~&#x2F;xilinx_root_non_native.txz).&lt;&#x2F;p&gt;
&lt;pre&gt;&lt;code&gt;tar -xf ~&#x2F;xilinx_root_non_native.txz -C &#x2F;mnt&#x2F;root
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;Ensure the writes complete and cleanly unmount the filesystems:&lt;&#x2F;p&gt;
&lt;pre&gt;&lt;code&gt;sync
umount &#x2F;mnt&#x2F;boot &#x2F;mnt&#x2F;root
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;Stick the microSD card into the board and let it boot up. If you&#x27;re following
this guide you should be able to get to a login screen and be able to login
with root &#x2F; root.&lt;&#x2F;p&gt;
&lt;p&gt;The device should be on the network and you should be able to SSH to the
device. For my board at the very least I haven&#x27;t gotten the hardware clock
working correctly so it needs to be set manually upon every boot. Before we can
compile quite a few of the packages the date needs to be roughly correct. You
can reference the build host&#x27;s time using the following command:&lt;&#x2F;p&gt;
&lt;pre&gt;&lt;code&gt;date +%s
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;And set it on the board using the following command (replacing VALUE with
the value returned above):&lt;&#x2F;p&gt;
&lt;pre&gt;&lt;code&gt;date --set=&quot;@VALUE&quot;
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;We now need to sync the system&#x27;s packages and fix portage. This is where we
have to work around the issue of portage being incorrectly installed by
prefixing any use of the portage python module with a &#x27;64 bit&#x27; path:&lt;&#x2F;p&gt;
&lt;pre&gt;&lt;code&gt;PYTHONPATH=&#x27;&#x2F;usr&#x2F;lib64&#x2F;python2.7&#x2F;site-packages&#x27; env-update

cat &lt;&lt; &#x27;EOF&#x27; &gt; &#x2F;etc&#x2F;locale.gen
en_US ISO-8859-1
en_US.UTF-8 UTF-8
EOF
locale-gen

PYTHONPATH=&#x27;&#x2F;usr&#x2F;lib64&#x2F;python2.7&#x2F;site-packages&#x27; eselect locale set &quot;$(eselect locale list | grep &#x27;en_US.utf8&#x27; | awk &#x27;{ print $1 }&#x27; | grep -oE &#x27;[0-9]+&#x27;)&quot;
PYTHONPATH=&#x27;&#x2F;usr&#x2F;lib64&#x2F;python2.7&#x2F;site-packages&#x27; env-update

. &#x2F;etc&#x2F;profile

PYTHONPATH=&#x27;&#x2F;usr&#x2F;lib64&#x2F;python2.7&#x2F;site-packages&#x27; emerge --sync
PYTHONPATH=&#x27;&#x2F;usr&#x2F;lib64&#x2F;python2.7&#x2F;site-packages&#x27; emerge --oneshot sys-apps&#x2F;portage

# There is a circular dependency that has to be broken during this update
USE=&quot;dev-util&#x2F;pkgconfig internal-glib&quot; emerge dev-util&#x2F;pkgconfig

# With the circular update broken we can update everything (this will recompile
# pkgconfig again)
emerge --update --newuse --deep @world
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;The last will recompile quite a few packages (though not all). I recommend
shutting the system down, removing the SD card and making a clean backup of the
root by performing the following commands once the drive is back in your
machine (this assumes the same device name as before):&lt;&#x2F;p&gt;
&lt;pre&gt;&lt;code&gt;mount &#x2F;dev&#x2F;mmcblk0p2 &#x2F;mnt&#x2F;root
rm -rf &#x2F;mnt&#x2F;root&#x2F;root&#x2F;.bash_history &#x2F;mnt&#x2F;root&#x2F;etc&#x2F;ssh&#x2F;ssh_host* &#x2F;usr&#x2F;portage&#x2F;*
tar -cJf ~&#x2F;xilinx_root_native.txz -C &#x2F;mnt&#x2F;root .
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;You now have a solid base to perform development on and a good backup in case
you mess up. I hope this helps someone else out there.&lt;&#x2F;p&gt;
</content>
	</entry>
	<entry xml:lang="en">
		<title>Converting CPIO Files to Tarballs</title>
		<published>2017-12-04T22:46:56-05:00</published>
		<updated>2017-12-04T22:46:56-05:00</updated>
		<link href="https://stelfox.net/blog/2017/converting-cpio-files-to-tarballs/" type="text/html"/>
		<id>https://stelfox.net/blog/2017/converting-cpio-files-to-tarballs/</id>
		<content type="html">&lt;p&gt;I needed to convert a directory full of CPIO files to tar balls. This quick
script did the trick for me but didn&#x27;t preserve the user &#x2F; group. Running it as
root will preserve the ownership information but that wasn&#x27;t important for my
immediate use case.&lt;&#x2F;p&gt;
&lt;span id=&quot;continue-reading&quot;&gt;&lt;&#x2F;span&gt;&lt;pre&gt;&lt;code&gt;#!&#x2F;bin&#x2F;bash

SRC_DIR=$(pwd)
for i in *.cpio; do
  CPIO_TMP_DIR=&quot;$(mktemp -d &#x2F;tmp&#x2F;cpioconv.XXXX)&quot;
  (cd ${CPIO_TMP_DIR} &amp;&amp; cpio -idm &lt; &quot;${SRC_DIR}&#x2F;${i}&quot; &amp;&amp; tar -cf ${SRC_DIR}&#x2F;${i%%.cpio}.tar .)
  rm -rf ${CPIO_TMP_DIR}
done
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
</content>
	</entry>
	<entry xml:lang="en">
		<title>Unusable Secret Key</title>
		<published>2017-12-04T11:38:01-05:00</published>
		<updated>2017-12-04T11:38:01-05:00</updated>
		<link href="https://stelfox.net/blog/2017/unusable-secret-key/" type="text/html"/>
		<id>https://stelfox.net/blog/2017/unusable-secret-key/</id>
		<content type="html">&lt;p&gt;I use a Yubikey NEO to store subkeys used for signing and authentication. I
started experiencing a weird issue with it. It coincided with me rebuilding my
system so diagnosing it ended up being harder than normal. The behavior I
experienced allowed me to use the key to authenticate (SSH&#x27;ing worked fine) but
any attempt to sign new data resulted in an &#x27;Unusuable secret key&#x27; error. For
git this resulted in the following message:&lt;&#x2F;p&gt;
&lt;span id=&quot;continue-reading&quot;&gt;&lt;&#x2F;span&gt;&lt;pre&gt;&lt;code&gt;gpg: skipped &quot;Sam Stelfox &lt;sstelfox@bedroomprogrammers.net&gt;&quot;: Unusable secret
key
gpg: signing failed: Unusable secret key
error: gpg failed to sign the data
fatal: failed to write commit object
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;There is a second Yubikey I use on occasion that contains my companies software
signing key. When reviewing my available secret keys, it seemed like GPG was
listing those private keys as available when they weren&#x27;t. That was a red
herring and unrelated (though still likely a bug). After resetting my GPG
config as well as the agent, and re-importing my key the private keys were not
be listed at all.&lt;&#x2F;p&gt;
&lt;p&gt;Ultimate the issue was that all of my subkeys were expired and only became
visible when I used the following command:&lt;&#x2F;p&gt;
&lt;pre&gt;&lt;code&gt;$ gpg2 --list-options show-unusable-subkeys --list-keys
&#x2F;home&#x2F;sstelfox&#x2F;.gnupg&#x2F;pubring.kbx
---------------------------------
pub   rsa4096&#x2F;0x30856D4EA0FFBA8F 2016-04-26 [C] [expires: 2019-12-01]
      Key fingerprint = DC75 C8B8 7434 4360 FB30  3FC9 3085 6D4E A0FF BA8F
uid                   [ unknown] Sam Stelfox &lt;sstelfox@bedroomprogrammers.net&gt;
uid                   [ unknown] Sam Stelfox &lt;sam@stelfox.net&gt;
uid                   [ unknown] Sam Stelfox &lt;sam@pwnieexpress.com&gt;
uid                   [ unknown] [jpeg image of size 2803]
sub   rsa2048&#x2F;0x5E2FD479ABDD395A 2016-04-26 [S] [expired: 2017-12-01]
sub   rsa2048&#x2F;0xD87BD950C29A0FA2 2016-04-26 [E] [expired: 2017-12-01]
sub   rsa2048&#x2F;0x4B218A4FB733A150 2016-04-26 [A] [expired: 2017-12-01]
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
</content>
	</entry>
	<entry xml:lang="en">
		<title>XFCE Failed to Connect to Socket</title>
		<published>2017-11-27T17:23:09+05:00</published>
		<updated>2017-11-27T17:23:09+05:00</updated>
		<link href="https://stelfox.net/blog/2017/xfce-failed-to-connect-to-socket/" type="text/html"/>
		<id>https://stelfox.net/blog/2017/xfce-failed-to-connect-to-socket/</id>
		<content type="html">&lt;p&gt;While trying to build up a minimal Gentoo graphical environment I kept running
into an error every time I logged into XFCE from lightdm (I didn&#x27;t try starting
up XFCE any other way). There are tons of blog posts that relate to systemd,
ubuntu, or crouton but none related to Gentoo.&lt;&#x2F;p&gt;
&lt;span id=&quot;continue-reading&quot;&gt;&lt;&#x2F;span&gt;
&lt;p&gt;The first error message that pops up is:&lt;&#x2F;p&gt;
&lt;pre&gt;&lt;code&gt;Unable to contact settings server

Failed to connect to socket &#x2F;tmp&#x2F;dbus-xxxxxxxxx: Connection refused
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;Once you click through there was a second error message, but I believe it was
due to the previous error and not actually an issue:&lt;&#x2F;p&gt;
&lt;pre&gt;&lt;code&gt;Unable to load a failsafe session

Unable to determine failsafe session name. Possible causes: xfconfd isn&#x27;t
running (D-Bus setup problem), environment variable $XDG_CONFIG_DIRS is set
incorrectly (must include &quot;&#x2F;etc&quot;), or xfce4-session is installed incorrectly.
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;Where the x&#x27;s are replaced with a random string. My issue ultimately was dbus
not being compiled with the &#x27;X&#x27; use flag. Adding that flag to &#x27;sys-apps&#x2F;dbus&#x27;
and re-emerging with the new flags got me to a clean desktop. Great success.&lt;&#x2F;p&gt;
</content>
	</entry>
	<entry xml:lang="en">
		<title>Unable to Enter LUKS Passphrase</title>
		<published>2017-11-26T21:49:51-05:00</published>
		<updated>2017-11-26T21:49:51-05:00</updated>
		<link href="https://stelfox.net/blog/2017/unable-to-enter-luks-passphrase/" type="text/html"/>
		<id>https://stelfox.net/blog/2017/unable-to-enter-luks-passphrase/</id>
		<content type="html">&lt;p&gt;While setting up a gentoo install with a full disk encryption, I continuously
got to a point where the passphrase would show up on boot but I was unable to
enter the passphrase. The behavior of the keyboard was also odd, it would
toggle it&#x27;s numlock light every couple of button presses.&lt;&#x2F;p&gt;
&lt;span id=&quot;continue-reading&quot;&gt;&lt;&#x2F;span&gt;
&lt;p&gt;Once again this was an issue that was hard to search for, and most other people
asking it seemed to only get snarky non-answers which seem so prevalent in
forums.&lt;&#x2F;p&gt;
&lt;p&gt;Ultimately the issue was with me not having XHCI support compiled into my
kernel and having my keyboard plugged into a USB 3.0 or 3.1 port. After
enabling and recompiling my kernel the issue immediately cleared up.&lt;&#x2F;p&gt;
</content>
	</entry>
	<entry xml:lang="en">
		<title>Downgrading Glibc in Gentoo</title>
		<published>2017-11-15T12:27:45+00:00</published>
		<updated>2017-11-15T12:27:45+00:00</updated>
		<link href="https://stelfox.net/blog/2017/downgrading-glibc-in-gentoo/" type="text/html"/>
		<id>https://stelfox.net/blog/2017/downgrading-glibc-in-gentoo/</id>
		<content type="html">&lt;p&gt;While refining some automated setup scripts at some point I upgraded to a
testing&#x2F;unstable version of glibc. When I attempted to get the box back on to
the stable version I hit a solid protection mechanism built into the portage
scripts that prevents downgrading glibc. Attempts will give you the following
error message:&lt;&#x2F;p&gt;
&lt;span id=&quot;continue-reading&quot;&gt;&lt;&#x2F;span&gt;&lt;pre&gt;&lt;code&gt; * Sanity check to keep you from breaking your system:
 *  Downgrading glibc is not supported and a sure way to destruction
 * ERROR: sys-libs&#x2F;glibc-2.25-r9::gentoo failed (pretend phase):
 *   aborting to save your system
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;It is correct in that this is a very dangerous thing to do. For my use case
this was a non-production system that I was using to test various
configurations and hardening procedures so didn&#x27;t particularly care if the
packages all became corrupt. I was also confident that my method would be
relatively safe (fully bootstrapping the system).&lt;&#x2F;p&gt;
&lt;p&gt;To disable this check (you should seriously understand what you&#x27;re doing and&#x2F;or
not care about the system you perform this on if you continue) you need to edit
the appropriate eclass file which exists at
&lt;code&gt;&#x2F;usr&#x2F;portage&#x2F;eclass&#x2F;toolchain-glibc.eclass&lt;&#x2F;code&gt;. Locate the following line and
disable it by prefixing it with a hash sign (#).&lt;&#x2F;p&gt;
&lt;pre&gt;&lt;code&gt;die &quot;aborting to save your system&quot;
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;For the version I was editing it was line 507.&lt;&#x2F;p&gt;
&lt;p&gt;To do this downgrade safely I performed the following steps (this will take a
while as it fully boostraps your entire system):&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;Review all use flags and accept keywords to ensure you&#x27;ll be in the state you
want to&lt;&#x2F;li&gt;
&lt;li&gt;Sync the repository metadata (&lt;code&gt;emerge --sync&lt;&#x2F;code&gt;)&lt;&#x2F;li&gt;
&lt;li&gt;Disable the glibc safety check documented above&lt;&#x2F;li&gt;
&lt;li&gt;Run the following commands to re-bootstrap the system and undo the safety
check modification by resyncing:&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;pre&gt;&lt;code&gt;&#x2F;usr&#x2F;portage&#x2F;scripts&#x2F;bootstrap.sh
emerge --emptytree --with-bdeps=y @world
emerge --depclean
emerge --sync
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;Reboot to ensure all running programs are on the correct versions and the
system should be back in a happy state.&lt;&#x2F;p&gt;
</content>
	</entry>
	<entry xml:lang="en">
		<title>File in Wrong Format</title>
		<published>2017-11-13T13:44:00-04:00</published>
		<updated>2017-11-13T13:44:00-04:00</updated>
		<link href="https://stelfox.net/blog/2017/file-in-wrong-format/" type="text/html"/>
		<id>https://stelfox.net/blog/2017/file-in-wrong-format/</id>
		<content type="html">&lt;p&gt;I have been recently attempting to cross compile a custom Gentoo profile
targetting a Xilinx board as I found their distribution to be unmanageable
(PetaLinux as hacked together sub-distro of Yocto). I had several issues with
the default profile (embedded) conflicting with other critical packages. I&#x27;ll
do a detailed post later on for building that entire root filesystem.&lt;&#x2F;p&gt;
&lt;span id=&quot;continue-reading&quot;&gt;&lt;&#x2F;span&gt;
&lt;p&gt;I came across one issue which didn&#x27;t seem to have a well-documented solution.
While compiling several packages (most notably &lt;a href=&quot;https:&#x2F;&#x2F;gmplib.org&#x2F;&quot;&gt;GMP&lt;&#x2F;a&gt; which is required for a
native GCC) I kept coming across a linker issue late in the compilation:&lt;&#x2F;p&gt;
&lt;pre&gt;&lt;code&gt;libtool: link: g++ ...&lt;snip&gt;....&#x2F;.libs&#x2F;libgmp.so: error adding symbols: File in wrong format
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;Since it got so late in the compilation (this is pretty much the last link) I
assumed it was an issue with the linker itself. Digging through the ebuilds,
and wrappers around the developer environment I couldn&#x27;t find anything suspect.&lt;&#x2F;p&gt;
&lt;p&gt;Ultimate I went back and read through the entire build log where I found this
in the configure output:&lt;&#x2F;p&gt;
&lt;pre&gt;&lt;code&gt;checking for arm-xilinx-linux-gnueabi-g++... no
checking for arm-xilinx-linux-gnueabi-c++... no
checking for arm-xilinx-linux-gnueabi-gpp... no
checking for arm-xilinx-linux-gnueabi-aCC... no
checking for arm-xilinx-linux-gnueabi-CC... no
checking for arm-xilinx-linux-gnueabi-cxx... no
checking for arm-xilinx-linux-gnueabi-cc++... no
checking for arm-xilinx-linux-gnueabi-cl.exe... no
checking for arm-xilinx-linux-gnueabi-FCC... no
checking for arm-xilinx-linux-gnueabi-KCC... no
checking for arm-xilinx-linux-gnueabi-RCC... no
checking for arm-xilinx-linux-gnueabi-xlC_r... no
checking for arm-xilinx-linux-gnueabi-xlC... no
checking for g++... g++
configure.wrapped: WARNING: using cross tools not prefixed with host triplet
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;The configure script was unable to find a native g++ compiler for my target
environment and thus fell back on to the system&#x27;s native compiler... which is
not compatible with my binaries. I had only performed a stage 3 crossdev setup.
Stage 4 is when a C++ compiler and libstdc++ becomes available. I was able to
upgrade my crossdev environment in Gentoo using the following command:&lt;&#x2F;p&gt;
&lt;pre&gt;&lt;code&gt;crossdev --stable -s4 -v -t arm-xilinx-linux-gnueabi
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;You will likely want to replace my system triple with your target.&lt;&#x2F;p&gt;
</content>
	</entry>
	<entry xml:lang="en">
		<title>Gentoo Fstab Failure</title>
		<published>2017-11-01T11:12:34-04:00</published>
		<updated>2017-11-01T11:12:34-04:00</updated>
		<link href="https://stelfox.net/blog/2017/gentoo-fstab-failure/" type="text/html"/>
		<id>https://stelfox.net/blog/2017/gentoo-fstab-failure/</id>
		<content type="html">&lt;p&gt;I use Gentoo with OpenRC quite a bit both for my personal servers and as a
compilation test bed for new software since I can control the dependency
versions very tightly.  I have a set of scripts I&#x27;ve been using for quite some
time that handle setting up a hardened, fairly minimal install.&lt;&#x2F;p&gt;
&lt;span id=&quot;continue-reading&quot;&gt;&lt;&#x2F;span&gt;
&lt;p&gt;I recently encountered a weird issue with them that resulted in an esoteric
error that prevented my host from fully booting and leaving the root filesystem
read-only. There also didn&#x27;t seem to be much reliable information on the
problem so I&#x27;m documenting it here in hopes it may help someone else.&lt;&#x2F;p&gt;
&lt;p&gt;In addition to the read-only filesystem, none of the system services would
start up leaving me with the kernel&#x27;s default hostname of &lt;code&gt;(none)&lt;&#x2F;code&gt;. This
includes networking so I had to diagnose this directly via the console. I was
able to login as a user but unable to use &lt;code&gt;sudo&lt;&#x2F;code&gt; to switch to a new user
(though I could still use it to execute commands with elevated privileges).&lt;&#x2F;p&gt;
&lt;p&gt;I was able to verify all the OpenRC services had not started with by executing:&lt;&#x2F;p&gt;
&lt;pre&gt;&lt;code&gt;sudo rc-status
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;I attempted to start the SSH daemon (though any networked service should behave
the same) using the following command:&lt;&#x2F;p&gt;
&lt;pre&gt;&lt;code&gt;sudo service sshd start
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;This was the first time any error had directly presented itself to my screen
(and since my filesystems were read-only, and my logger was stopped I had no on
disk logs to go off from). The error that was repeated over my screen was the
following:&lt;&#x2F;p&gt;
&lt;pre&gt;&lt;code&gt; * Checking local filesystems  ...&#x2F;sbin&#x2F;fsck.xfs: XFS file system.
fsck.fat 4.0 (2016-05-06)
open: No such file or directory

 * Filesystems couldn&#x27;t be fixed
                                         [ !! ]
 * ERROR: fsck failed to start
 * ERROR: cannot start root as fsck would not start
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;This was quite confusing and more than a little concerning as XFS doesn&#x27;t use
&lt;code&gt;fsck&lt;&#x2F;code&gt;, instead it uses &lt;code&gt;xfs_repair&lt;&#x2F;code&gt;. There was also the &amp;quot;No such file or
directory&amp;quot; error which also didn&#x27;t seem to make sense.&lt;&#x2F;p&gt;
&lt;p&gt;Ultimately, after reviewing the &lt;code&gt;fsck&lt;&#x2F;code&gt; init script I found the culprit. There
was a bad line in my &lt;code&gt;&#x2F;etc&#x2F;fstab&lt;&#x2F;code&gt; file pointing at a device that didn&#x27;t exist.
I was able to remount my root filesystem read-write and edit my fstab to fix
the device name in my fstab file:&lt;&#x2F;p&gt;
&lt;pre&gt;&lt;code&gt;sudo mount -o remount,rw &#x2F;
sudoedit &#x2F;etc&#x2F;fstab
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;Tracing things back, this was ultimately caused by the Gentoo installation
medium detecting it&#x27;s only disk as &lt;code&gt;&#x2F;dev&#x2F;sda&lt;&#x2F;code&gt; while the kernel I use exposed it
as &lt;code&gt;&#x2F;dev&#x2F;vda&lt;&#x2F;code&gt;. Since I was already using a GPT filesystem I was able to adjust
my scripts so they reference this particular partition by UUID instead of
directly by device name.&lt;&#x2F;p&gt;
</content>
	</entry>
	<entry xml:lang="en">
		<title>Investigating LVM From Dracut</title>
		<published>2017-10-24T11:45:07-04:00</published>
		<updated>2017-10-24T11:45:07-04:00</updated>
		<link href="https://stelfox.net/blog/2017/investigating-lvm-from-dracut/" type="text/html"/>
		<id>https://stelfox.net/blog/2017/investigating-lvm-from-dracut/</id>
		<content type="html">&lt;p&gt;In my &lt;a href=&quot;https:&#x2F;&#x2F;stelfox.net&#x2F;blog&#x2F;2017&#x2F;visible-yet-missing-logical-volumes&#x2F;&quot;&gt;my last post&lt;&#x2F;a&gt;, I covered finding logical volumes that were missing
from LVM from within a live CD (which is effectively a whole standard
environment). Working with dracut is quite a bit more limited.&lt;&#x2F;p&gt;
&lt;span id=&quot;continue-reading&quot;&gt;&lt;&#x2F;span&gt;
&lt;p&gt;Turns out that the commands I&#x27;m normally used to for operating and inspecting
LVM volumes can all be accessed as a second parameter to the &lt;code&gt;lvm&lt;&#x2F;code&gt; tool like
so:&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;sh&quot; class=&quot;language-sh &quot;&gt;&lt;code class=&quot;language-sh&quot; data-lang=&quot;sh&quot;&gt;$ lvm vgscan
$ lvm pvscan
$ lvm lvscan
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;For my particular issue, it led me to notice that block device of my root
filesystem was missing due to a missing kernel driver...&lt;&#x2F;p&gt;
</content>
	</entry>
	<entry xml:lang="en">
		<title>Visible Yet Missing Logical Volumes</title>
		<published>2017-10-24T10:58:12-04:00</published>
		<updated>2017-10-24T10:58:12-04:00</updated>
		<link href="https://stelfox.net/blog/2017/visible-yet-missing-logical-volumes/" type="text/html"/>
		<id>https://stelfox.net/blog/2017/visible-yet-missing-logical-volumes/</id>
		<content type="html">&lt;p&gt;While working on an automated install script for an embedded board, I hit an
issue with the logical volumes never showing up in &lt;code&gt;&#x2F;dev&#x2F;mapper&lt;&#x2F;code&gt;, and in turn
unable to be mounted. This left me in the dracut emergency shell (after about
three minutes), with little to go on beyond the following error:&lt;&#x2F;p&gt;
&lt;span id=&quot;continue-reading&quot;&gt;&lt;&#x2F;span&gt;&lt;pre&gt;&lt;code&gt;[187.508531] dracut Warning: Could not boot.
[187.510560] dracut Warning: &#x2F;dev&#x2F;disk&#x2F;by-uuid&#x2F;5681-902D does not exist
[187.512534] dracut Warning: &#x2F;dev&#x2F;mapper&#x2F;system-root does not exit
[187.513990] dracut Warning: &#x2F;dev&#x2F;system&#x2F;root does not exist
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;After booting into a live CD I checked to make sure the volume group showed up
under &lt;code&gt;pvscan&lt;&#x2F;code&gt; like so:&lt;&#x2F;p&gt;
&lt;pre&gt;&lt;code&gt;livecd ~ # pvscan
  PV &#x2F;dev&#x2F;mmcblk0p3   VG system     lvm2 [19.50 GiB &#x2F; 0    free]
  Total: 1 [19.50 GiB] &#x2F; in use: 1 [19.50 GiB] &#x2F; in no VG: 0 [0   ]
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;And the logical volumes were also showing up with &lt;code&gt;lvscan&lt;&#x2F;code&gt;:&lt;&#x2F;p&gt;
&lt;pre&gt;&lt;code&gt;livecd ~ # lvscan
  inactive          &#x27;&#x2F;dev&#x2F;system&#x2F;root&#x27; [18.45 GiB] inherit
  inactive          &#x27;&#x2F;dev&#x2F;system&#x2F;swap&#x27; [1.05 GiB] inherit
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;Notice that they are both marked inactive? That&#x27;s our issue. To fix it we can
mark all logical volumes under our volume group as active (replace &lt;code&gt;system&lt;&#x2F;code&gt;
with your volume group name):&lt;&#x2F;p&gt;
&lt;pre&gt;&lt;code&gt;vgchange --activate y system
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;This didn&#x27;t fix the LVM volumes showing up at boot but it did allow me to get
back into my root filesystem as a chroot so I could investigate the issue which
I&#x27;ve finished documenting in &lt;a href=&quot;https:&#x2F;&#x2F;stelfox.net&#x2F;blog&#x2F;2017&#x2F;investigating-lvm-from-dracut&#x2F;&quot;&gt;another post&lt;&#x2F;a&gt;.&lt;&#x2F;p&gt;
</content>
	</entry>
	<entry xml:lang="en">
		<title>Vultr Deny All Firewall</title>
		<published>2017-10-20T17:18:36-04:00</published>
		<updated>2017-10-20T17:18:36-04:00</updated>
		<link href="https://stelfox.net/blog/2017/vultr-deny-all-firewall/" type="text/html"/>
		<id>https://stelfox.net/blog/2017/vultr-deny-all-firewall/</id>
		<content type="html">&lt;p&gt;While setting up new instances on Vultr for testing, I wanted to initially
ensure that no traffic beyond my own could touch the instances. After adding a
matching rule for SSH to my IPv4 address, a default rule shows up that drops
any unspecified traffic. Switching to the IPv6 I wanted to add a drop all rule
(as I wouldn&#x27;t be using IPv6 until the system was up).&lt;&#x2F;p&gt;
&lt;span id=&quot;continue-reading&quot;&gt;&lt;&#x2F;span&gt;
&lt;p&gt;The interface only allows &lt;code&gt;accept&lt;&#x2F;code&gt; rules to be created and additionally you&#x27;ll
be greeted with this message while trying to figure out what to do:&lt;&#x2F;p&gt;
&lt;blockquote&gt;
&lt;p&gt;This firewall ruleset will not be active until at least one rule is added.&lt;&#x2F;p&gt;
&lt;&#x2F;blockquote&gt;
&lt;p&gt;The creative solution I came up with was to add an SSH rule with a custom IP of
&lt;code&gt;::1&#x2F;128&lt;&#x2F;code&gt;. The loopback IPv6 address... The drop all rule showed up and unless
there is some bug in Vultr&#x27;s firewalling nothing should be able to reach these
instances over IPv6.&lt;&#x2F;p&gt;
&lt;p&gt;If you&#x27;re interested in using Vultr, I&#x27;d appreciate it if you consider using my
&lt;a href=&quot;https:&#x2F;&#x2F;www.vultr.com&#x2F;?ref=7199712&quot;&gt;affiliate link&lt;&#x2F;a&gt; to sign up.&lt;&#x2F;p&gt;
</content>
	</entry>
	<entry xml:lang="en">
		<title>Security Principles</title>
		<published>2017-10-19T05:19:12+00:00</published>
		<updated>2017-10-19T05:19:12+00:00</updated>
		<link href="https://stelfox.net/blog/2017/security-principles/" type="text/html"/>
		<id>https://stelfox.net/blog/2017/security-principles/</id>
		<content type="html">&lt;p&gt;While reviewing current security hardening practices put out by several
organizations and attempting to filter the good recommendations from the
outdated legislated requirements, I came across one of the best descriptions of
basic security principles. You can find it in the &lt;a href=&quot;https:&#x2F;&#x2F;nvlpubs.nist.gov&#x2F;nistpubs&#x2F;Legacy&#x2F;SP&#x2F;nistspecialpublication800-123.pdf&quot;&gt;NIST Guide to General Server
Security&lt;&#x2F;a&gt; (published in 2008).&lt;&#x2F;p&gt;
&lt;span id=&quot;continue-reading&quot;&gt;&lt;&#x2F;span&gt;
&lt;p&gt;I&#x27;ve replicated section 2.4 from the linked document (I have removed the
footnotes, but it is otherwise unchanged) in its entirety here for safe keeping
and to hopefully help expose this to more people.&lt;&#x2F;p&gt;
&lt;p&gt;When addressing server security issues, it is an excellent idea to keep in mind
the following general information security principles:&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;Simplicity&lt;&#x2F;strong&gt; --- Security mechanisms (and information systems in general)
should be as simple as possible. Complexity is at the root of many security
issues. &lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;Fail-Safe&lt;&#x2F;strong&gt; --- If a failure occurs, the system should fail in a secure
manner, i.e., security controls and settings remain in effect and are enforced.
It is usually better to lose functionality rather than security.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;Complete Mediation&lt;&#x2F;strong&gt; --- Rather than providing direct access to information,
mediators that enforce access policy should be employed. Common examples of
mediators include file system permissions, proxies, firewalls, and mail
gateways.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;Open Design&lt;&#x2F;strong&gt; --- System security should not depend on the secrecy of the
implementation or its components.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;Separation of Privilege&lt;&#x2F;strong&gt; --- Functions, to the degree possible, should be
separate and provide as much granularity as possible. The concept can apply to
both systems and operators and users. In the case of systems, functions such as
read, edit, write, and execute should be separate. In the case of system
operators and users, roles should be as separate as possible. For example, if
resources allow, the role of system administrator should be separate from that
of the database administrator.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;Least Privilege&lt;&#x2F;strong&gt; --- This principle dictates that each task, process, or
user is granted the minimum rights required to perform its job. By applying
this principle consistently, if a task, process, or user is compromised, the
scope of damage is constrained to the limited resources available to the
compromised entity.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;Psychological Acceptability&lt;&#x2F;strong&gt; --- Users should understand the necessity of
security. This can be provided through training and education. In addition, the
security mechanisms in place should present users with sensible options that
give them the usability they require on a daily basis. If users find the
security mechanisms too cumbersome, they may devise ways to work around or
compromise them. The objective is not to weaken security so it is
understandable and acceptable, but to train and educate users and to design
security mechanisms and policies that are usable and effective.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;Least Common Mechanism&lt;&#x2F;strong&gt; --- When providing a feature for the system, it is
best to have a single process or service gain some function without granting
that same function to other parts of the system. The ability for the Web server
process to access a back-end database, for instance, should not also enable
other applications on the system to access the back-end database.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;Defense-in-Depth&lt;&#x2F;strong&gt; --- Organizations should understand that a single security
mechanism is generally insufficient. Security mechanisms (defenses) need to be
layered so that compromise of a single security mechanism is insufficient to
compromise a host or network. No &amp;quot;silver bullet&amp;quot; exists for information system
security.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;Work Factor&lt;&#x2F;strong&gt; --- Organizations should understand what it would take to
break the system or network‚Äôs security features. The amount of work necessary
for an attacker to break the system or network should exceed the value that the
attacker would gain from a successful compromise.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;Compromise Recording&lt;&#x2F;strong&gt; --- Records and logs should be maintained so that if a
compromise does occur, evidence of the attack is available to the organization.
This information can assist in securing the network and host after the
compromise and aid in identifying the methods and exploits used by the
attacker. This information can be used to better secure the host or network in
the future. In addition, these records and logs can assist organizations in
identifying and prosecuting attackers.&lt;&#x2F;p&gt;
</content>
	</entry>
	<entry xml:lang="en">
		<title>Linux Audit Rule Paths</title>
		<published>2017-10-16T23:20:20-04:00</published>
		<updated>2017-10-16T23:20:20-04:00</updated>
		<link href="https://stelfox.net/blog/2017/linux-audit-rule-paths/" type="text/html"/>
		<id>https://stelfox.net/blog/2017/linux-audit-rule-paths/</id>
		<content type="html">&lt;p&gt;I encountered a little bit of confusion while rewriting my &lt;a href=&quot;https:&#x2F;&#x2F;stelfox.net&#x2F;notes&#x2F;auditd&#x2F;&quot;&gt;auditd rules&lt;&#x2F;a&gt;
which some Googling did not help me solve.&lt;&#x2F;p&gt;
&lt;span id=&quot;continue-reading&quot;&gt;&lt;&#x2F;span&gt;
&lt;p&gt;When monitoring a file or directory there are two forms the rules can take.
They are effectively equivalent in their functionality. The simpler form is the
following format:&lt;&#x2F;p&gt;
&lt;pre&gt;&lt;code&gt;-w &#x2F;etc&#x2F;shadow -p wa
-w &#x2F;boot -p wa
-w &#x2F;etc&#x2F;dont_readme -p r
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;These three rules are all behaving slightly differently. The first will audit
any writes or attribute changes to the shadow file. The second, being a
directory, will be recursively watched for writes and attribute changes
(including the directory itself). The last line will create an audit record
whenever the file is read.&lt;&#x2F;p&gt;
&lt;p&gt;The second form is more consistent with the syscall rule types but more
verbose. Examples are given in the man page for converting between the two
forms. Before I show you an example of those rules, this is the part of the man
page that ultimately confused me:&lt;&#x2F;p&gt;
&lt;blockquote&gt;
&lt;p&gt;If you place a watch on a file, its the same as using the -F path option on a
syscall rule.&lt;&#x2F;p&gt;
&lt;&#x2F;blockquote&gt;
&lt;p&gt;Simple enough, this is what the first line of the first example would look like
in this style:&lt;&#x2F;p&gt;
&lt;pre&gt;&lt;code&gt;-a always,exit -F path=&#x2F;etc&#x2F;shadow -F perm=wa
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;Before changing my rules over to this form I checked a couple to ensure they
were behaving correctly then migrated the rest. After applying the new rules I
found quite a few audit records I&#x27;d expect missing. If I had kept reading (the
next line from the last man page) I would have found this:&lt;&#x2F;p&gt;
&lt;blockquote&gt;
&lt;p&gt;If you place a watch on a directory, its the same as using the -F dir option
on a syscall rule.&lt;&#x2F;p&gt;
&lt;&#x2F;blockquote&gt;
&lt;p&gt;So the correct way to represent the second line in the first example would be
like this:&lt;&#x2F;p&gt;
&lt;pre&gt;&lt;code&gt;-a always,exit -F dir=&#x2F;boot -F perm=wa
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;The long and short of it, is that the watch flag (&lt;code&gt;-w&lt;&#x2F;code&gt;) has different behaviors
depending on what it is pointing at. Having only read the first sentence, I
assumed the &lt;code&gt;path=&lt;&#x2F;code&gt; argument would behave the same as a watch. Attempts to
Google whether or not &lt;code&gt;path=&lt;&#x2F;code&gt; was recursive didn&#x27;t turn up anything. Now that
I&#x27;ve read the man page while writing up this point it is very clearly stated
that it is not.&lt;&#x2F;p&gt;
&lt;p&gt;If anyone else comes across this, &lt;code&gt;path=&lt;&#x2F;code&gt; is not recursive, &lt;code&gt;dir=&lt;&#x2F;code&gt; is. Be
careful when translating your rules.&lt;&#x2F;p&gt;
</content>
	</entry>
	<entry xml:lang="en">
		<title>Vulnerable Smart Cards</title>
		<published>2017-10-16T12:25:58-04:00</published>
		<updated>2017-10-16T12:25:58-04:00</updated>
		<link href="https://stelfox.net/blog/2017/vulnerable-smart-cards/" type="text/html"/>
		<id>https://stelfox.net/blog/2017/vulnerable-smart-cards/</id>
		<content type="html">&lt;p&gt;In addition to the &lt;a href=&quot;https:&#x2F;&#x2F;stelfox.net&#x2F;blog&#x2F;2017&#x2F;a-krack-in-the-defenses&#x2F;&quot;&gt;WiFi vulnerability&lt;&#x2F;a&gt; a much more limited vulnerability
was &lt;a href=&quot;https:&#x2F;&#x2F;crocs.fi.muni.cz&#x2F;public&#x2F;papers&#x2F;rsa_ccs17&quot;&gt;announced&lt;&#x2F;a&gt; around private GPG keys that were generated using
&lt;a href=&quot;https:&#x2F;&#x2F;www.commoncriteriaportal.org&#x2F;files&#x2F;epfiles&#x2F;0782V2a_pdf.pdf&quot;&gt;Infineon&#x27;s RSA Library version v1.02.013&lt;&#x2F;a&gt;.&lt;&#x2F;p&gt;
&lt;span id=&quot;continue-reading&quot;&gt;&lt;&#x2F;span&gt;
&lt;p&gt;The vulnerability lies in shortcuts taken to speed up the key generation using
the library. The performance increase makes the private key vulnerable to
factorization attacks using an extension to &lt;a href=&quot;https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Coppersmith%27s_attack&quot;&gt;Coppersmith&#x27;s attack&lt;&#x2F;a&gt;.&lt;&#x2F;p&gt;
&lt;p&gt;It has been confirmed that YubiKey 4s are effected as are many nations national
ID cards. Earlier versions of YubiKey were not affected (including my
preference the Neo). This brings up a controversial decision made by Yubico a
little over a year ago to switch from tested open source and widely inspected
libraries, to closed source versions.&lt;&#x2F;p&gt;
&lt;p&gt;This led some very respected individuals to accuse Yubico that they were
relying on &lt;a href=&quot;https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Security_through_obscurity&quot;&gt;security through obscurity&lt;&#x2F;a&gt; as core to their security and I have
to agree. Would a FOSS implementation guaranteed this would have been spotted
sooner? No, absolutely not.&lt;&#x2F;p&gt;
&lt;p&gt;If this implementation was open source the fix could be assessed to ensure it
was complete. This very public vulnerability would have also driven passionate
security researchers to look and test other parts of the code, after all
exposing one bug statistically indicates the presence of many more.&lt;&#x2F;p&gt;
&lt;p&gt;One amusing anecdote from the Yubico blog post I&#x27;ve already linked to is this
snippet:&lt;&#x2F;p&gt;
&lt;blockquote&gt;
&lt;p&gt;Yubico has developed the firmware from the ground up. These devices are
loaded by Yubico and cannot be updated.&lt;&#x2F;p&gt;
&lt;&#x2F;blockquote&gt;
&lt;p&gt;It seems they didn&#x27;t roll their own crypto library, which is good but also
belies the first part of this sentence. The latter part is concerning. If they
can&#x27;t be updated all YubiKey 4s out there can not and should never ever be used
for key generation and there is nothing consumers that have already purchased
the devices can do about their existing faulty devices without purchasing a new
one or with any luck have Yubico replace them with a non-vulnerable version as
they did with &lt;a href=&quot;https:&#x2F;&#x2F;developers.yubico.com&#x2F;ykneo-openpgp&#x2F;SecurityAdvisory%202015-04-14.html&quot;&gt;CVE-2015-3298&lt;&#x2F;a&gt;.&lt;&#x2F;p&gt;
&lt;p&gt;While this doesn&#x27;t directly effect me, I did distinctly notice a mention that
this effects TPM chips used in many laptops, and the keys used by Windows
BitLocker. If your laptop uses an affected chip, your full disk encryption
could be broken by a determined individual over the course of a year or a well
financed attacker in under a month.&lt;&#x2F;p&gt;
&lt;p&gt;If you have a GPG key, or the public portion of any RSA key that may be
affected you can test it using a &lt;a href=&quot;https:&#x2F;&#x2F;keychest.net&#x2F;roca&quot;&gt;convenient online analyzer&lt;&#x2F;a&gt;.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;Update:&lt;&#x2F;strong&gt;&lt;&#x2F;em&gt; Yubico is providing mitigation recommendations and optional
&lt;a href=&quot;https:&#x2F;&#x2F;support.yubico.com&#x2F;hc&#x2F;en-us&#x2F;articles&#x2F;360021803580&quot;&gt;YubiKey replacements&lt;&#x2F;a&gt;. There are also reports rolling in that GitHub is
taking the proactive step of disabling all keys that have been found to be weak
according to the ROCA tests (Well done GitHub!).&lt;&#x2F;p&gt;
</content>
	</entry>
	<entry xml:lang="en">
		<title>A KRACK In the Defenses</title>
		<published>2017-10-16T08:23:43-04:00</published>
		<updated>2017-10-16T08:23:43-04:00</updated>
		<link href="https://stelfox.net/blog/2017/a-krack-in-the-defenses/" type="text/html"/>
		<id>https://stelfox.net/blog/2017/a-krack-in-the-defenses/</id>
		<content type="html">&lt;p&gt;An advisory from US CERT has been circulating for the last week about a
protocol level flaw in WPA &amp;amp; WPA2. The advisory itself was:&lt;&#x2F;p&gt;
&lt;span id=&quot;continue-reading&quot;&gt;&lt;&#x2F;span&gt;
&lt;blockquote&gt;
&lt;p&gt;US-CERT has become aware of several key management vulnerabilities in the
4-way handshake of the Wi-Fi Protected Access II (WPA2) security protocol.
The impact of exploiting these vulnerabilities includes decryption, packet
replay, TCP connection hijacking, HTTP content injection, and others. Note
that as protocol-level issues, most or all correct implementations of the
standard will be affected. The CERT&#x2F;CC and the reporting researcher KU
Leuven, will be publicly disclosing these vulnerabilities on 16 October 2017.&lt;&#x2F;p&gt;
&lt;&#x2F;blockquote&gt;
&lt;p&gt;Details of the vulnerability have been &lt;a href=&quot;https:&#x2F;&#x2F;www.krackattacks.com&#x2F;&quot;&gt;released today&lt;&#x2F;a&gt;, and paint a pretty
horrifying picture. Ultimately this is an issue with a mechanism for dealing
with lost packets during the initial 4-way key negotiation and the client&#x27;s
behavior when they receive one of these packets after they&#x27;re session is
already established.&lt;&#x2F;p&gt;
&lt;p&gt;Almost all WiFi devices out there are vulnerable to this attack and patches
should be applied as soon as they are available. Some very quick and important
notes I&#x27;d like to make available for people:&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;This is an active against clients&lt;&#x2F;li&gt;
&lt;li&gt;WPA &amp;amp; WPA2 both personal and enterprise are effected&lt;&#x2F;li&gt;
&lt;li&gt;The WiFi association key (your network&#x27;s passphrase) is safe, this attack
breaks a per-client session key.&lt;&#x2F;li&gt;
&lt;li&gt;Assume all your traffic is being sent in plain text and can be manipulated by
an attacker until you have patched.&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;If you have a VPN available to your client devices, having it active will
protect your traffic from this attack.&lt;&#x2F;p&gt;
</content>
	</entry>
	<entry xml:lang="en">
		<title>The Case of an Empty Executable</title>
		<published>2017-10-12T23:09:01-04:00</published>
		<updated>2017-10-12T23:09:01-04:00</updated>
		<link href="https://stelfox.net/blog/2017/the-case-of-an-empty-executable/" type="text/html"/>
		<id>https://stelfox.net/blog/2017/the-case-of-an-empty-executable/</id>
		<content type="html">&lt;p&gt;I recently came across a &lt;a href=&quot;http:&#x2F;&#x2F;trillian.mit.edu&#x2F;%7Ejc&#x2F;humor&#x2F;ATT_Copyright_true.html&quot;&gt;short article&lt;&#x2F;a&gt; written about a decade ago. It was
a curious thing already as it was hosted in a user&#x27;s home directory off a web
server with the standard &lt;code&gt;~&amp;lt;username&amp;gt;&lt;&#x2F;code&gt; showing up in the URL. The important
part that caught my eye was this:&lt;&#x2F;p&gt;
&lt;blockquote&gt;
&lt;p&gt;The &amp;quot;true&amp;quot; program does nothing; it merely exits with a zero exit status.
This can be done with an empty file that&#x27;s marked executable, and that&#x27;s what
it was in the earliest unix system libraries.&lt;&#x2F;p&gt;
&lt;&#x2F;blockquote&gt;
&lt;span id=&quot;continue-reading&quot;&gt;&lt;&#x2F;span&gt;
&lt;p&gt;Being a curious sort, and presented with an old mystery, I had to try out this
little tidbit of information:&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;sh&quot; class=&quot;language-sh &quot;&gt;&lt;code class=&quot;language-sh&quot; data-lang=&quot;sh&quot;&gt;$ echo -n &gt; test
$ chmod +x test
$ .&#x2F;test
$ echo $?
0
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;Sure enough, an empty file can be successfully run. It obviously doesn&#x27;t have a
known binary header, so it won&#x27;t be interpretted as a valid executable on it&#x27;s
own. Even scripts rely on the shebang header (&lt;code&gt;#!&lt;&#x2F;code&gt;) to be considered valid by
the kernel, so something else has to be executing this. We can confirm the
kernel isn&#x27;t recognizing this by abusing &lt;code&gt;strace&lt;&#x2F;code&gt; into calling an explicit
execve on the file:&lt;&#x2F;p&gt;
&lt;pre&gt;&lt;code&gt;$ strace .&#x2F;test
execve(&quot;.&#x2F;test&quot;, [&quot;.&#x2F;test&quot;], 0x7ffe74e0a720 &#x2F;* 70 vars *&#x2F;) = -1 ENOEXEC (Exec format error)
fstat(2, {st_mode=S_IFCHR|0620, st_rdev=makedev(136, 3), ...}) = 0
write(2, &quot;strace: exec: Exec format error\n&quot;, 32strace: exec: Exec format error
) = 32
getpid()                                = 24047
exit_group(1)                           = ?
+++ exited with 1 +++
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;This is exactly what I was expecting and the &lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;torvalds&#x2F;linux&#x2F;blob&#x2F;c2315c187fa0d3ab363fdebe22718170b40473e3&#x2F;fs&#x2F;binfmt_script.c#L24&quot;&gt;Kernel source&lt;&#x2F;a&gt; reflects
exactly what I&#x27;d expect. I&#x27;ll leave it up to you test the results on an empty
but otherwise valid shell script. With the kernel cleared of any odd behavior I
was left with only one suspect. A little shell by the name of bash.&lt;&#x2F;p&gt;
&lt;p&gt;Bash is a coy devil with several different mechanisms built-in to execute a
program. Likely one of these culprits are being used behind the scenes when we
run a program. A quick trip the &lt;a href=&quot;https:&#x2F;&#x2F;www.gnu.org&#x2F;software&#x2F;bash&#x2F;manual&#x2F;bash.txt&quot;&gt;man page&lt;&#x2F;a&gt; and a late night cup of coffee
narrowed down my search to the following functions:&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;command&lt;&#x2F;li&gt;
&lt;li&gt;eval&lt;&#x2F;li&gt;
&lt;li&gt;exec&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;I decided it was time to talk to each of them one by one and see what was up.
Putting them under the bright light I was surprised that they all were telling
the same story:&lt;&#x2F;p&gt;
&lt;pre&gt;&lt;code&gt;$ command .&#x2F;test
$ echo $?
0
$ eval .&#x2F;test
$ echo $?
0
$ exec .&#x2F;test
-bash: &#x2F;home&#x2F;playground&#x2F;test: Success
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;How sinister! Someone had gotten to them first, I have to go higher into their
organization. This calls for... The Source. I quickly traverse into the builtin
directory and identify the commonality &lt;code&gt;parse_and_execute&lt;&#x2F;code&gt;. This is where it
gets a little fuzzy as bash is a rather complicated code base and I didn&#x27;t want
to spend to much time on this in the middle of the night.&lt;&#x2F;p&gt;
&lt;p&gt;After parsing the file, it does seem to treat it as a script (as expected).
There are two possibilities here and I didn&#x27;t trace down which was true. Either
&lt;code&gt;parse_and_execute&lt;&#x2F;code&gt; is simply returning with a success or it is sending the
contents to &lt;code&gt;execute_command_internal&lt;&#x2F;code&gt;, which in turn defaults to a
successfully return value.&lt;&#x2F;p&gt;
&lt;p&gt;The motive remains unclear, but no harm seems to be getting done so I&#x27;m going
to call this one case closed. It&#x27;d be interesting to see how other shells
behave with empty files.&lt;&#x2F;p&gt;
</content>
	</entry>
	<entry xml:lang="en">
		<title>Hash Cash</title>
		<published>2016-09-14T01:36:59-04:00</published>
		<updated>2016-09-14T01:36:59-04:00</updated>
		<link href="https://stelfox.net/blog/2016/hash-cash/" type="text/html"/>
		<id>https://stelfox.net/blog/2016/hash-cash/</id>
		<content type="html">&lt;p&gt;This is an interesting proof of work concept. The first example I have found of
this in the wild is to prevent abuse for anonymous account registration on an
IRC network.&lt;&#x2F;p&gt;
&lt;span id=&quot;continue-reading&quot;&gt;&lt;&#x2F;span&gt;
&lt;p&gt;I reviewed it&#x27;s source and found that it requests a seed, and payload from a
backend PHP script. It assumes that a target collision will happen within
1,000,000 iterations.&lt;&#x2F;p&gt;
&lt;p&gt;This is broken up into 10 iterations. A pool of four WebWorkers are spawned.
Each one iterates through their assigned iterations, appending the iteration
value to the salt then calculating the SHA256 value for that iteration. When
the iteration is found that matches the payload all workers are stopped and the
found value is injected into a hidden form on the field.&lt;&#x2F;p&gt;
&lt;p&gt;They have it set pretty aggressively at several hours per registration attempt,
that does work pretty well as a mechanism against certain types of abuse,
though with a poisoned ad campaign you could have plenty of registration
attempts from random host all over the internet.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;a href=&quot;https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Hashcash&quot;&gt;Actual HashCash&lt;&#x2F;a&gt; uses an interesting and well thought out header. Each
component needed to validate the work was done is included in the header and
part of that is the data being submitted.&lt;&#x2F;p&gt;
</content>
	</entry>
	<entry xml:lang="en">
		<title>Better Practices With Sudo</title>
		<published>2016-02-26T17:45:22-05:00</published>
		<updated>2016-02-26T17:45:22-05:00</updated>
		<link href="https://stelfox.net/blog/2016/better-practices-with-sudo/" type="text/html"/>
		<id>https://stelfox.net/blog/2016/better-practices-with-sudo/</id>
		<content type="html">&lt;p&gt;I work with a lot of different linux machines from embedded devices, to cloud
servers and open stack hosts. For many of them I&#x27;m either the sole
administrator or one of three or less with administrative access. Where there
are multiple administrative users, we all are generally working as backups to
each other. We use sudo whenever we need to execute a task with privileges on
any of these machines with no direct root login permitted remotely.&lt;&#x2F;p&gt;
&lt;p&gt;I must confess I have established two habits over time that are against best
practices with regard to sudo; Using it to execute a root shell only, and not
restricting which commands can be run with sudo.&lt;&#x2F;p&gt;
&lt;span id=&quot;continue-reading&quot;&gt;&lt;&#x2F;span&gt;
&lt;p&gt;I&#x27;m sure many other administrators commit these sins as well. I&#x27;ve always
gotten sudo to the &#x27;good enough&#x27; point without ever learning how to configure
it properly countless times, which mostly meant leaving the distribution&#x27;s
defaults.&lt;&#x2F;p&gt;
&lt;p&gt;At face value, executing a shell this way doesn&#x27;t seem to pose a problem. We
use auditd to record administrative changes, and the kernel can track our
original login UID and record that in addition to our effective user ID.
Permission to use sudo is still restricted to a subset of trusted
administrators.&lt;&#x2F;p&gt;
&lt;p&gt;Using this default configuration is forming bad habits and after working
through it it&#x27;s not particularly hard to make a drastic improvement on the
granularity of control.&lt;&#x2F;p&gt;
&lt;p&gt;I&#x27;m going to work through the changes I&#x27;ve made slowly building up my final
configuration.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;These changes, if made incorrectly or with the wrong paths to binaries may
effect your ability to get privileged access to the system. I strongly
encourage you to maintain a root shell independent of the shell you are using
to test just in case you need to revert a breaking change.&lt;&#x2F;strong&gt;&lt;&#x2F;em&gt;&lt;&#x2F;p&gt;
&lt;h2 id=&quot;minimal-configuration&quot;&gt;Minimal Configuration&lt;&#x2F;h2&gt;
&lt;p&gt;Rather than looking at what needs to be changed, or removed I prefer to start
with a minimal effective configuration.&lt;&#x2F;p&gt;
&lt;p&gt;Most distribution&#x27;s default sudo configuration pass through environment
variables related to locale and a few others. I have left these out since the
way I see sudo executed most commonly (&lt;code&gt;sudo su -&lt;&#x2F;code&gt;), removes any environment
variables passed through anyway. If you work on multi-lingual systems or
otherwise your administrators make use of multiple system locales, you will
want to re-introduce the locale values used.&lt;&#x2F;p&gt;
&lt;p&gt;My entire starting sudo configuration is the following:&lt;&#x2F;p&gt;
&lt;pre&gt;&lt;code&gt;Defaults env_reset
Defaults !visiblepw

root      ALL=(ALL)     ALL
%wheel    ALL=(root)    ALL
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;This is very similar to most distribution&#x27;s configurations if you ignore the
environment variables and comments. The root user and members of the wheel
group can all execute anything as sudo as long as the user can authenticate
through PAM and the mechanism won&#x27;t display their password.&lt;&#x2F;p&gt;
&lt;p&gt;There is also a small restriction in place that ensures members of the wheel
group will only be executing commands as the root user. Executing as other
user&#x27;s directly should be a special case and added separately.&lt;&#x2F;p&gt;
&lt;p&gt;Usually distributions also include additional sudo configuration by including
all files in &#x2F;etc&#x2F;sudoers.d. This configuration isn&#x27;t going to be terribly long
so we may as well KISS it and not allow the inclusion of other files.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;no-need-for-the-su&quot;&gt;No Need for the su&lt;&#x2F;h2&gt;
&lt;p&gt;The first habit I wanted to break was executing &lt;code&gt;sudo su -&lt;&#x2F;code&gt; instead of &lt;code&gt;sudo -s&lt;&#x2F;code&gt;. Generally when sudo is configured correctly, administrators are supposed
to minimize the number of times dropping to a root shell. There are always
going to be times when a root shell is necessary.&lt;&#x2F;p&gt;
&lt;p&gt;The differences between the two methods of executing a root shell are subtle.
They are creating to different types of shells. Executing &lt;code&gt;sudo su -&lt;&#x2F;code&gt; creates
a login shell, while &lt;code&gt;sudo -s&lt;&#x2F;code&gt; doesn&#x27;t. Both can be subtly changed to provide
the other type (Adding the &lt;code&gt;-i&lt;&#x2F;code&gt; flag to sudo, or removing the &lt;code&gt;-&lt;&#x2F;code&gt; from su).&lt;&#x2F;p&gt;
&lt;p&gt;A login shell resets your environment, spawns the new user&#x27;s default shell (in
this case root&#x27;s default shell) and executes the user&#x27;s profile scripts in
addition to the shell&#x27;s rc files.&lt;&#x2F;p&gt;
&lt;p&gt;By not using a login shell, administrators can keep their preferred shells
while allowing selective bits of their configuration (whitelisted environment
variables) through to the new session.&lt;&#x2F;p&gt;
&lt;p&gt;By removing &lt;code&gt;su&lt;&#x2F;code&gt; from the process, administrators can enforce permitted root
shells just like whitelisting or blacklisting any other binary on the system.
The only way to enforce this transition is to blacklist &lt;code&gt;su&lt;&#x2F;code&gt; directly.&lt;&#x2F;p&gt;
&lt;p&gt;A blacklist is added by creating a command alias that includes the commands to
be blacklisted, then adjusting ACLs to make use of them. These need to be
defined before they&#x27;re used. Generally this means all command aliases are at
the top of the configuration file. The following command alias will be used for our
blacklist. The path to &lt;code&gt;su&lt;&#x2F;code&gt; is valid for CentOS 7, other distributions do
vary.&lt;&#x2F;p&gt;
&lt;pre&gt;&lt;code&gt;Cmnd_Alias BLACKLIST = &#x2F;bin&#x2F;su
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;To enforce the blacklist the wheel group ACL needs to be adjusted to the
following:&lt;&#x2F;p&gt;
&lt;pre&gt;&lt;code&gt;%wheel  ALL=(root)  ALL,!BLACKLIST
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;Now when you try to execute &lt;code&gt;sudo su -&lt;&#x2F;code&gt; you&#x27;ll instead get this warning after
authenticating:&lt;&#x2F;p&gt;
&lt;pre&gt;&lt;code&gt;Sorry, user &lt;username&gt; is not allowed to execute &#x27;&#x2F;bin&#x2F;su -&#x27; as root on &lt;hostname&gt;.
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;This warning will enforce not using the less ideal mechanism.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;brief-interlude-on-blacklists&quot;&gt;Brief Interlude on Blacklists&lt;&#x2F;h2&gt;
&lt;p&gt;I&#x27;m going to be adding several more things to different forms of blacklists
inside sudo. Some of these may be unacceptably inconvenient for some
environments. If you find the explained reason insufficient to justify the
inconvenience and are willing to accept the risk, remove the offender from the
blacklist.&lt;&#x2F;p&gt;
&lt;p&gt;There is also always a risk that programs allowed through the blacklist have
the ability to execute blacklisted applications as root. The blacklist applies
only to direct execution through sudo.&lt;&#x2F;p&gt;
&lt;p&gt;Preventing &#x27;commonly used&#x27; escalation vectors does make it that much harder on
potential attackers and may allow you see an attack in progress through the
logs. This should not be considered perfect though. A good example of these
vectors is the utility &lt;code&gt;awk&lt;&#x2F;code&gt;. If allowed to be executed through sudo an
unrestricted root shell can be acquired with the following command:&lt;&#x2F;p&gt;
&lt;pre&gt;&lt;code&gt;sudo awk &#x27;BEGIN {system(&quot;&#x2F;bin&#x2F;sh&quot;)}&#x27;
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;h2 id=&quot;editing-files-as-root&quot;&gt;Editing Files as Root&lt;&#x2F;h2&gt;
&lt;p&gt;Commonly when I wanted to edit a particular sensitive configuration file, I
would drop to a root shell, then open the file in my preferred editor, possibly
saving along the way until I was done. Less commonly I would open my editor
directly using &lt;code&gt;sudo&lt;&#x2F;code&gt; skipping the shell entirely.&lt;&#x2F;p&gt;
&lt;p&gt;The partially complete saves as part of that workflow, have caused issues
though they&#x27;re temporary. Sudo provides a utility, &lt;code&gt;sudoedit&lt;&#x2F;code&gt;, that covers this
use case. It make a copy of the file to be edited into a temporary directory,
and allows you to edit and save as you like. When you&#x27;re done save the file and
it will replace the real file with the temporary one you&#x27;ve been editing.&lt;&#x2F;p&gt;
&lt;p&gt;Editing the sudoers file itself should be done using the &lt;code&gt;visudo&lt;&#x2F;code&gt; command. And
can be invoked by:&lt;&#x2F;p&gt;
&lt;pre&gt;&lt;code&gt;sudo visudo
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;It&#x27;s a good idea to restrict the list of editors that can be used by visudo
(this doesn&#x27;t affect sudoedit at all) by adding the following line (replace
this with your preferred, colon separated list of editors):&lt;&#x2F;p&gt;
&lt;pre&gt;&lt;code&gt;Defaults editor = &#x2F;bin&#x2F;vim:&#x2F;bin&#x2F;nano
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;h2 id=&quot;user-writable-directories&quot;&gt;User Writable Directories&lt;&#x2F;h2&gt;
&lt;p&gt;Since the blacklist functionality is based on full paths to binaries, there is
a quick way for a user with sudo permissions to bypass the blacklist for a
specific program, copy it somewhere else.&lt;&#x2F;p&gt;
&lt;p&gt;When an attacker gets into a system and downloads a binary off their site they
want to run with privileges. They&#x27;ll have to put it somewhere they have
permission to write to.&lt;&#x2F;p&gt;
&lt;p&gt;This is less of a threat if you always require authentication to use sudo,
trust all your administrators, and are confident their credentials will never
be stolen.&lt;&#x2F;p&gt;
&lt;p&gt;A salve to both problems is simply to prevent sudo from executing files in user
writable directories, and ensuring it has a sane path to lookup trusted
binaries. The following three lines need to be added to the sudoers file:&lt;&#x2F;p&gt;
&lt;pre&gt;&lt;code&gt;Cmnd_Alias USER_WRITEABLE = &#x2F;home&#x2F;*, &#x2F;tmp&#x2F;*, &#x2F;var&#x2F;tmp&#x2F;*
Defaults ignore_dot
Defaults secure_path = &#x2F;sbin:&#x2F;bin:&#x2F;usr&#x2F;sbin:&#x2F;usr&#x2F;bin
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;We also need to modify our wheel ACL to prevent the execution in the aliases
locations. Replace your previous line with the following one:&lt;&#x2F;p&gt;
&lt;pre&gt;&lt;code&gt;%wheel  ALL=(root)  ALL,!BLACKLIST,!USER_WRITEABLE
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;h2 id=&quot;preventing-breakouts&quot;&gt;Preventing Breakouts&lt;&#x2F;h2&gt;
&lt;p&gt;I&#x27;ve already shown that there is a way to abuse individual commands to expose a
root shell. There are a few additional common applications that can regularly
shell out, advanced text editors, pagers, several unix utilities and any
interactive programming shell are easy candidates.&lt;&#x2F;p&gt;
&lt;p&gt;These utilities likely still need to be available for general administrative
purposes, but we don&#x27;t want them in turn executing other programs. Sudo has a
trick up it&#x27;s sleeve for this, the noexec flag.&lt;&#x2F;p&gt;
&lt;p&gt;There are two ways to effectively apply this, a whitelist and a blacklist. I
encourage you to try the whitelist approach first as it does offer
substantially better protection against this potential abuse.&lt;&#x2F;p&gt;
&lt;p&gt;Before applying this it is useful to know how this works and what it&#x27;s
limitations are. Sudo disables the exec call using &lt;code&gt;LD_PRELOAD&lt;&#x2F;code&gt; and defining an
alternate version of the system call. This is largely effective, but will only
work with dynamically linked programs (most coming from a distribution are
going to be dynamically linked).&lt;&#x2F;p&gt;
&lt;h3 id=&quot;whitelisting-programs-w-exec-privileges&quot;&gt;Whitelisting Programs w&#x2F; Exec Privileges&lt;&#x2F;h3&gt;
&lt;p&gt;This is very strict but also very effective. We need to ensure that things we
expect and want to be able to execute other programs (like shells) still can.
Additionally visudo in turn executes your editor, so it to needs to be able to
spawn programs.&lt;&#x2F;p&gt;
&lt;p&gt;Be very sure of the paths in the following change. If you have no shells, or
editors that can be executed as root through sudo you may lock yourself out of
your system privileges.&lt;&#x2F;p&gt;
&lt;pre&gt;&lt;code&gt;Cmnd_Alias SHELLS = &#x2F;bin&#x2F;sh, &#x2F;bin&#x2F;bash
Cmnd_Alias ALLOWED_EXEC = &#x2F;usr&#x2F;sbin&#x2F;visudo
Defaults noexec
Defaults!ALLOWED_EXEC,SHELLS !noexec
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;As you use this in your environment you will probably find programs that behave
incorrectly and will need to be added to the whitelist. This whitelist
(assuming your paths are correct) will at least be enough to allow future
modifications of the sudoers file.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;blacklisting-programs-w-exec-privileges&quot;&gt;Blacklisting Programs w&#x2F; Exec Privileges&lt;&#x2F;h3&gt;
&lt;p&gt;This is a much milder version of the exec restrictions, and won&#x27;t catch unknown
abuses. This will also have the least impact on normal operations to apply and
is better than nothing.&lt;&#x2F;p&gt;
&lt;pre&gt;&lt;code&gt;Cmnd_Alias EDITORS     = &#x2F;bin&#x2F;vim
Cmnd_Alias PAGERS      = &#x2F;bin&#x2F;less, &#x2F;bin&#x2F;more
Cmnd_Alias BREAKOUT    = &#x2F;bin&#x2F;awk, &#x2F;bin&#x2F;find
Cmnd_Alias DEVEL_SHELL = &#x2F;bin&#x2F;perl, &#x2F;bin&#x2F;python, &#x2F;bin&#x2F;ruby

Defaults!EDITORS,PAGERS,BREAKOUT,DEVEL_SHELL noexec
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;h2 id=&quot;ttys&quot;&gt;TTYs!&lt;&#x2F;h2&gt;
&lt;p&gt;Enforcing the use of TTYs generally prevents non-interactive processes from
executing anything as sudo either remotely or locally. Examples of this might
be from cron, apache, or from a remote Jenkins server. In almost all cases
prevention of this type of execution is the ideal behavior.&lt;&#x2F;p&gt;
&lt;p&gt;There are a &lt;a href=&quot;https:&#x2F;&#x2F;unix.stackexchange.com&#x2F;questions&#x2F;65774&#x2F;is-it-okay-to-disable-requiretty&quot;&gt;couple&lt;&#x2F;a&gt; of very &lt;a href=&quot;https:&#x2F;&#x2F;bugzilla.redhat.com&#x2F;show_bug.cgi?id=1020147&quot;&gt;visible&lt;&#x2F;a&gt; search results on this topic that
indicate there isn&#x27;t any security benefit to this, but their are
&lt;a href=&quot;https:&#x2F;&#x2F;superuser.com&#x2F;questions&#x2F;180764&#x2F;sudoers-files-requiretty-flag-security-implications&quot;&gt;exceptions&lt;&#x2F;a&gt; as well. The argument that seems to have the most merit, is
that no special privileges are required to create a PTY. This in turn means an
attacking process could spawn the PTY required, and continue it&#x27;s attack.&lt;&#x2F;p&gt;
&lt;p&gt;The same argument could be used in favor of the option. An attacker would have
learn they need to make this adjustment and actively work around it. As the
administrator you know the option is set and should be able to work around it
more easily than the attacker.&lt;&#x2F;p&gt;
&lt;p&gt;The most common form of pain seems to be remotely executing privileged commands
through ssh. By providing the SSH command being executed the &#x27;-t&#x27; flag twice,
the client will force a PTY allocation even when there is no local tty. Other
more stubborn use cases can be individually exempted.&lt;&#x2F;p&gt;
&lt;p&gt;When the user already has a local TTY, the sudoers man page calls out to an
additional potential attack vector around TTYs under the &#x27;use_pty&#x27; option:&lt;&#x2F;p&gt;
&lt;blockquote&gt;
&lt;p&gt;A malicious program run under sudo could conceivably fork a background
process that retains to the user&#x27;s terminal device after the main program has
finished executing. Use of this option will make that impossible.&lt;&#x2F;p&gt;
&lt;&#x2F;blockquote&gt;
&lt;p&gt;I haven&#x27;t been able to find any attacks that exploit this possibility, but I
have yet to be impacted by turning that feature on within sudo. Making both
changes can be done by adding the following line to the sudoers configuration.&lt;&#x2F;p&gt;
&lt;pre&gt;&lt;code&gt;Defaults requiretty, use_pty
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;h2 id=&quot;notification-of-violation&quot;&gt;Notification of Violation&lt;&#x2F;h2&gt;
&lt;p&gt;Receiving immediate notification when privilege gain has been attempted can be
invaluable to stopping an attacker before they can do any damage. If the linux
system has a properly configured MTA forwarding root&#x27;s email to relevant
parties it is recommended to have failure mailed to them directly to take
action.&lt;&#x2F;p&gt;
&lt;pre&gt;&lt;code&gt;Defaults mail_badpass, mail_no_perms
Defaults mailfrom = root
Defaults mailsub = &quot;Sudo Policy Violation on %H by %u&quot;
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;The overridden subject provides everything but the command itself (which isn&#x27;t
available through the expanded variables) needed to quickly judge a threat at a
glance.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;auditing-interactive-shells&quot;&gt;Auditing Interactive Shells&lt;&#x2F;h2&gt;
&lt;p&gt;With all the protections put in place so far, we still have no visibility or
restrictions on what administrators do with the root shells when they
use them. These should hopefully be relatively few and far between.&lt;&#x2F;p&gt;
&lt;p&gt;Built into sudo is an option to &lt;em&gt;record&lt;&#x2F;em&gt; execution of commands. This has proven
to be valuable to narrow down things that have gone wrong, or see how something
was done before. This may not prove useful as much for an audit tool as a user
with root privileges can purge the recordings and logs.&lt;&#x2F;p&gt;
&lt;p&gt;If auditing is the goal, use of the kernel audit subsystem may be a better
choice, but will only give you the command and arguments executed. This shows
what was displayed to the privileged shell directly. There will be a future
article covering the use of the audit subsystem and centralizing the
information in a future post.&lt;&#x2F;p&gt;
&lt;p&gt;If you didn&#x27;t go the whitelist exec route, to enable this you will need to pull
in the &#x27;SHELLS&#x27; command alias from there to make use of this.&lt;&#x2F;p&gt;
&lt;pre&gt;&lt;code&gt;Defaults!SHELLS log_output
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;Once this is in place you can get a list of recorded sessions using the
command:&lt;&#x2F;p&gt;
&lt;pre&gt;&lt;code&gt;$ sudo sudoreplay -l
Feb 26 17:56:18 2016 : jdoe : TTY=&#x2F;dev&#x2F;pts&#x2F;7 ; CWD=&#x2F;home&#x2F;jdoe ; USER=root ; TSID=000001 ; COMMAND=&#x2F;bin&#x2F;bash
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;To view an individual session provide &lt;code&gt;sudoreplay&lt;&#x2F;code&gt; with the TSID value of the
session like so:&lt;&#x2F;p&gt;
&lt;pre&gt;&lt;code&gt;$ sudo sudoreplay 000001
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;Refer to the man page of &lt;code&gt;sudoreplay&lt;&#x2F;code&gt; for additional tricks such as speeding up
playback.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;final-configuration&quot;&gt;Final Configuration&lt;&#x2F;h2&gt;
&lt;p&gt;Some of the options from above I have combined into a single configuration
line. This uses the stricter whitelist policy for exec privileges.&lt;&#x2F;p&gt;
&lt;pre&gt;&lt;code&gt;# &#x2F;etc&#x2F;sudoers

Cmnd_Alias ALLOWED_EXEC = &#x2F;usr&#x2F;sbin&#x2F;visudo
Cmnd_Alias BLACKLIST = &#x2F;usr&#x2F;bin&#x2F;su
Cmnd_Alias SHELLS = &#x2F;usr&#x2F;bin&#x2F;sh, &#x2F;usr&#x2F;bin&#x2F;bash
Cmnd_Alias USER_WRITEABLE = &#x2F;home&#x2F;*, &#x2F;tmp&#x2F;*, &#x2F;var&#x2F;tmp&#x2F;*

Defaults env_reset, mail_badpass, mail_no_perms, noexec, requiretty, use_pty
Defaults !visiblepw

Defaults editor = &#x2F;usr&#x2F;bin&#x2F;vim
Defaults mailfrom = root
Defaults mailsub = &quot;Sudo Policy Violation on %H by %u&quot;
Defaults secure_path = &#x2F;sbin:&#x2F;bin:&#x2F;usr&#x2F;sbin:&#x2F;usr&#x2F;bin

Defaults!ALLOWED_EXEC,SHELLS !noexec
Defaults!SHELLS log_output

root    ALL=(ALL)   ALL
%wheel  ALL=(root)  ALL,!BLACKLIST,!USER_WRITEABLE
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
</content>
	</entry>
	<entry xml:lang="en">
		<title>Sharing Context Between Dependent Rake Tasks</title>
		<published>2016-02-18T15:46:12-05:00</published>
		<updated>2016-02-18T15:46:12-05:00</updated>
		<link href="https://stelfox.net/blog/2016/sharing-context-between-dependent-rake-tasks/" type="text/html"/>
		<id>https://stelfox.net/blog/2016/sharing-context-between-dependent-rake-tasks/</id>
		<content type="html">&lt;p&gt;I use Rakefiles quite a bit like traditional Makefiles, in that I specify
immediate dependencies for an individual task and Rake will execute all of
them. If a file or directory is the dependency and it exists, the task that
creates it will be skipped. A contrived Rakefile example might look like:&lt;&#x2F;p&gt;
&lt;span id=&quot;continue-reading&quot;&gt;&lt;&#x2F;span&gt;&lt;pre data-lang=&quot;ruby&quot; class=&quot;language-ruby &quot;&gt;&lt;code class=&quot;language-ruby&quot; data-lang=&quot;ruby&quot;&gt;file &#x27;sample&#x27; do |t|
  puts &#x27;Creating sample directory&#x27;
  Dir.mkdir(t.name)
end

file &#x27;sample&#x2F;population.txt&#x27; =&gt; [&#x27;sample&#x27;] do |t|
  puts &#x27;Creating sample population file...&#x27;
  # Perhaps download a dataset? Lets just create the file
  File.write(t.name, &quot;---&gt; Very important data &lt;---\n&quot;)
end

task :process_population =&gt; [&#x27;sample&#x2F;population.txt&#x27;] do
  puts &#x27;Check out our data!&#x27;
  # Do some processing... whatever you need to...
  puts File.read(&#x27;sample&#x2F;population.txt&#x27;)
end
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;The first time you run it you&#x27;ll the following output:&lt;&#x2F;p&gt;
&lt;pre&gt;&lt;code&gt;$ rake process_population
Creating sample directory
Creating sample population file...
Check out our data!
---&gt; Very important data &lt;---
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;And subsequent runs will skip the creation since they&#x27;re already present:&lt;&#x2F;p&gt;
&lt;pre&gt;&lt;code&gt;$ rake process_population
Check out our data!
---&gt; Very important data &lt;---
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;This is fine for statically implementing file contents, but what if you need
additional information to generate the file? With a normal rake task you can
provide bracketed arguments to access additional information like so:&lt;&#x2F;p&gt;
&lt;pre&gt;&lt;code&gt;task :args_example, :word do |t, args|
  puts &quot;The word is: #{args.word}&quot;
end
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;You&#x27;d use it like so:&lt;&#x2F;p&gt;
&lt;pre&gt;&lt;code&gt;$ rake args_example[data]
The word is: data
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;That information isn&#x27;t made available to the dependent tasks though so we need
to broaden our scope a little bit. There is another way to provide arguments to
Rake using key value pairs. This has a bonus that was kind of an obvious
solution once I found it. Rake provides the values of key&#x2F;value pairs to a task
via environment variables. Another contrived example of how to use this
(specifically with a file dependency example):&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;ruby&quot; class=&quot;language-ruby &quot;&gt;&lt;code class=&quot;language-ruby&quot; data-lang=&quot;ruby&quot;&gt;file &#x27;passed_state&#x27; do |t|
  puts &#x27;Creating state file&#x27;
  File.write(t.name, ENV[&#x27;state&#x27;])
end

task :read_state =&gt; [&#x27;passed_state&#x27;] do
  puts File.read(&#x27;passed_state&#x27;)
end
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;pre data-lang=&quot;sh&quot; class=&quot;language-sh &quot;&gt;&lt;code class=&quot;language-sh&quot; data-lang=&quot;sh&quot;&gt;$ rake read_state state=something
Creating state file
something
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;State has been transferred! There is a gotcha, that is handling expiration of
data yourself. Passing in state again with a different value you&#x27;ll see the
problem:&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;sh&quot; class=&quot;language-sh &quot;&gt;&lt;code class=&quot;language-sh&quot; data-lang=&quot;sh&quot;&gt;$ rake read_state state=notsomething
something
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;It won&#x27;t recreate that file again until it&#x27;s removed which you&#x27;ll need to
handle on your own.&lt;&#x2F;p&gt;
</content>
	</entry>
	<entry xml:lang="en">
		<title>Ruby Code Quality Metrics</title>
		<published>2015-04-22T16:47:10-04:00</published>
		<updated>2015-04-22T16:47:10-04:00</updated>
		<link href="https://stelfox.net/blog/2015/ruby-code-quality-metrics/" type="text/html"/>
		<id>https://stelfox.net/blog/2015/ruby-code-quality-metrics/</id>
		<content type="html">&lt;p&gt;I like getting unopinionated feedback on the quality of the code I write.
Sometimes I can get this from other developers but they tend to get annoyed
being asked after every commit whether they consider it an improvement.&lt;&#x2F;p&gt;
&lt;span id=&quot;continue-reading&quot;&gt;&lt;&#x2F;span&gt;
&lt;p&gt;There are a few utilities for Ruby codebases such as &lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;seattlerb&#x2F;flay&quot;&gt;flay&lt;&#x2F;a&gt;, &lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;seattlerb&#x2F;flog&quot;&gt;flog&lt;&#x2F;a&gt;, and
&lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;bbatsov&#x2F;rubocop&quot;&gt;rubocop&lt;&#x2F;a&gt; as well as hosted services such as &lt;a href=&quot;https:&#x2F;&#x2F;codeclimate.com&#x2F;&quot;&gt;Code Climate&lt;&#x2F;a&gt; that can help
you identify chunks of code that can use some work.&lt;&#x2F;p&gt;
&lt;p&gt;While not directly connected to the quality of the code, I also make use of
&lt;a href=&quot;https:&#x2F;&#x2F;yardoc.org&#x2F;&quot;&gt;yard&lt;&#x2F;a&gt; and &lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;colszowka&#x2F;simplecov&quot;&gt;simplecov&lt;&#x2F;a&gt; to assess documentation and test coverage of the
codebases I work on.&lt;&#x2F;p&gt;
&lt;p&gt;Using the tools means very little without some reference or understanding
doesn&#x27;t get you very far. For a while I&#x27;ve been using flog and only comparing
the numbers against other codebases I control. I finally googled around and
found a &lt;a href=&quot;https:&#x2F;&#x2F;jakescruggs.blogspot.com&#x2F;2008&#x2F;08&#x2F;whats-good-flog-score.html&quot;&gt;blog post&lt;&#x2F;a&gt; by a developer named Jake Scruggs from a while ago
(2008).&lt;&#x2F;p&gt;
&lt;p&gt;The blog post includes a rough table for assessing scores on individual methods
reported from the flog utility. From what I can tell the ranges are still
pretty accurate. I&#x27;ve tweaked the descriptions a bit to fit my mental
understanding a bit but the table is here:&lt;&#x2F;p&gt;
&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th&gt;Method Score&lt;&#x2F;th&gt;&lt;th&gt;Description&lt;&#x2F;th&gt;&lt;&#x2F;tr&gt;&lt;&#x2F;thead&gt;&lt;tbody&gt;
&lt;tr&gt;&lt;td&gt;0   - 10&lt;&#x2F;td&gt;&lt;td&gt;Awesome&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;10  - 20&lt;&#x2F;td&gt;&lt;td&gt;Decent&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;20  - 40&lt;&#x2F;td&gt;&lt;td&gt;Might need refactoring&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;40  - 60&lt;&#x2F;td&gt;&lt;td&gt;Should probably review&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;60  - 100&lt;&#x2F;td&gt;&lt;td&gt;Danger&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;100 - 200&lt;&#x2F;td&gt;&lt;td&gt;Raise the alarm&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;200+&lt;&#x2F;td&gt;&lt;td&gt;Seriously what are you doing!?&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;&#x2F;tbody&gt;&lt;&#x2F;table&gt;
&lt;p&gt;I wanted to extend this with a second table providing a scale for the overall
method average with a more aggressive scale (an individual couple of methods
can be justifiably complex but the overall code base shouldn&#x27;t be riddled with
them) but had a hard time working it out.&lt;&#x2F;p&gt;
&lt;p&gt;I&#x27;ve seen some awesome code bases with a score of 6.4 on average, some bad
larger ones with 7.8. Even some mediocre ones around a score of 10.6.&lt;&#x2F;p&gt;
&lt;p&gt;I guess I&#x27;ll have to think more on it...&lt;&#x2F;p&gt;
</content>
	</entry>
	<entry xml:lang="en">
		<title>Creating an Empty Git Branch</title>
		<published>2015-04-13T20:47:40-04:00</published>
		<updated>2015-04-13T20:47:40-04:00</updated>
		<link href="https://stelfox.net/blog/2015/creating-an-empty-git-branch/" type="text/html"/>
		<id>https://stelfox.net/blog/2015/creating-an-empty-git-branch/</id>
		<content type="html">&lt;p&gt;Every now and then I find myself wanting to create a new empty branch in an
existing repository. It&#x27;s useful for things such as &lt;a href=&quot;https:&#x2F;&#x2F;pages.github.com&#x2F;&quot;&gt;GitHub Pages&lt;&#x2F;a&gt; so you&#x27;re
able to keep your content source in the master branch while only keeping the
output in the &lt;code&gt;gh-pages&lt;&#x2F;code&gt; branch. I&#x27;ve also used it for testing a complete
rewrite of a code base without the overhead of creating a new repo and copying
access permissions.&lt;&#x2F;p&gt;
&lt;span id=&quot;continue-reading&quot;&gt;&lt;&#x2F;span&gt;
&lt;p&gt;This is a pretty straight forward trick to do. You create the branch by
indicating you want the new branch to be an orphan by passing the &#x27;--orphan&#x27;
flag like so:&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;sh&quot; class=&quot;language-sh &quot;&gt;&lt;code class=&quot;language-sh&quot; data-lang=&quot;sh&quot;&gt;git checkout --orphan NEW_BRANCH_NAME
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;This leaves all the files in place but effectively uncommitted like you just
initialized a new repository. Add and commit any files you&#x27;d like to keep then
delete the rest, everything will still be preserved in the original branches.&lt;&#x2F;p&gt;
&lt;p&gt;With that done you should be able to easily switch just using a normal
&#x27;checkout&#x27; between your normal branches and this new tree.&lt;&#x2F;p&gt;
</content>
	</entry>
	<entry xml:lang="en">
		<title>Unbuffered Pipe Filters</title>
		<published>2015-02-23T12:49:13-05:00</published>
		<updated>2015-02-23T12:49:13-05:00</updated>
		<link href="https://stelfox.net/blog/2015/unbuffered-pipe-filters/" type="text/html"/>
		<id>https://stelfox.net/blog/2015/unbuffered-pipe-filters/</id>
		<content type="html">&lt;p&gt;I need to filter a live log stream for only relevant events and quickly hit an
issue that I wasn&#x27;t expecting. The &lt;code&gt;grep&lt;&#x2F;code&gt; in my pipe chain was waiting until it
received all the output from the prior command before it began to attempt to
filter it.&lt;&#x2F;p&gt;
&lt;span id=&quot;continue-reading&quot;&gt;&lt;&#x2F;span&gt;
&lt;p&gt;Reading through the grep man page I came across the &lt;code&gt;--line-buffered&lt;&#x2F;code&gt; flag
which provides exactly what I needed. I wasn&#x27;t using the &lt;code&gt;tail&lt;&#x2F;code&gt; command but it
serves really well in this situation to demonstrate the use:&lt;&#x2F;p&gt;
&lt;pre&gt;&lt;code&gt;tail -f &#x2F;var&#x2F;log&#x2F;maillog | grep --line-buffered -i error
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;Hope this saves someone a headache in the future!&lt;&#x2F;p&gt;
</content>
	</entry>
	<entry xml:lang="en">
		<title>Dependency Prelink Issues</title>
		<published>2014-08-12T16:16:14-04:00</published>
		<updated>2014-08-12T16:16:14-04:00</updated>
		<link href="https://stelfox.net/blog/2014/dependency-prelink-issues/" type="text/html"/>
		<id>https://stelfox.net/blog/2014/dependency-prelink-issues/</id>
		<content type="html">&lt;p&gt;While running an aide check on one of my servers after updating it, I started
seeing a large number of very concerning warning messages:&lt;&#x2F;p&gt;
&lt;span id=&quot;continue-reading&quot;&gt;&lt;&#x2F;span&gt;&lt;pre&gt;&lt;code&gt;&#x2F;usr&#x2F;sbin&#x2F;prelink: &#x2F;bin&#x2F;mailx: at least one of file&#x27;s dependencies has changed since prelinking
Error on exit of prelink child process
&#x2F;usr&#x2F;sbin&#x2F;prelink: &#x2F;bin&#x2F;rpm: at least one of file&#x27;s dependencies has changed since prelinking
Error on exit of prelink child process
&#x2F;usr&#x2F;sbin&#x2F;prelink: &#x2F;sbin&#x2F;readahead: at least one of file&#x27;s dependencies has changed since prelinking
Error on exit of prelink child process
&#x2F;usr&#x2F;sbin&#x2F;prelink: &#x2F;lib64&#x2F;libkrb5.so.3.3: at least one of file&#x27;s dependencies has changed since prelinking
Error on exit of prelink child process
&#x2F;usr&#x2F;sbin&#x2F;prelink: &#x2F;lib64&#x2F;libgssapi_krb5.so.2.2: at least one of file&#x27;s dependencies has changed since prelinking
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;The list went on with maybe a total of forty packages and libraries. My initial
reaction was &#x27;Did I get hacked?&#x27;. Before running the updates I ran an aide
verification check which returned no issues and the files that were now
displaying the issue were in the packages that got updated.&lt;&#x2F;p&gt;
&lt;p&gt;What was the next worse scenario? The packages had been tampered with and I
just installed malicious files. This didn&#x27;t seem likely as the packages are all
signed with GPG and an aide check would have caught tampering with my trust
database, the &lt;code&gt;gpg&lt;&#x2F;code&gt; binary, or the aide binary. Still a key could have been
compromised.&lt;&#x2F;p&gt;
&lt;p&gt;After some Googling I came across people with similar issues, (including one
annoyingly paywalled Red Hat article on the issue). Several people simply ended
the conversation on the assumption the user with the issue had been hacked.
Finally I &lt;a href=&quot;https:&#x2F;&#x2F;lists.centos.org&#x2F;pipermail&#x2F;centos&#x2F;2007-December&#x2F;049222.html&quot;&gt;came across one helpful individual&lt;&#x2F;a&gt; with the fix. The binaries
just need to have their prelink cache updated again. This can be accomplished
with the following command on CentOS 6.5 (probably the same on others).&lt;&#x2F;p&gt;
&lt;pre&gt;&lt;code&gt;&#x2F;usr&#x2F;sbin&#x2F;prelink -av -mR
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;&lt;em&gt;Update:&lt;&#x2F;em&gt; Ultimately I decided to follow &lt;a href=&quot;https:&#x2F;&#x2F;stelfox.net&#x2F;notes&#x2F;linux-hardening&#x2F;&quot;&gt;my own advice&lt;&#x2F;a&gt; (search for
prelink) and just simply disabled prelinking too prevent it from interferring
with aide checks and causing other weird issues. The memory trade-off isn&#x27;t
valuable enough for me.&lt;&#x2F;p&gt;
</content>
	</entry>
	<entry xml:lang="en">
		<title>Fast Hex to Decimal in Bash</title>
		<published>2014-08-01T19:50:24-04:00</published>
		<updated>2014-08-01T19:50:24-04:00</updated>
		<link href="https://stelfox.net/blog/2014/fast-hex-to-decimal-in-bash/" type="text/html"/>
		<id>https://stelfox.net/blog/2014/fast-hex-to-decimal-in-bash/</id>
		<content type="html">&lt;p&gt;I needed too turn some hexadecimal values into decimal in a bash script and
found a real easy way too do it. The following is a very short bash script
demonstrating how too turn the hexadecimal string &amp;quot;deadbeefcafe&amp;quot; into it&#x27;s
equivalent decimal value of &amp;quot;244837814094590&amp;quot;.&lt;&#x2F;p&gt;
&lt;span id=&quot;continue-reading&quot;&gt;&lt;&#x2F;span&gt;&lt;pre data-lang=&quot;bash&quot; class=&quot;language-bash &quot;&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;#!&#x2F;bin&#x2F;bash

INPUT=&quot;deadbeefcafe&quot;
OUTPUT=$((0x${INPUT}))

echo $OUTPUT
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
</content>
	</entry>
	<entry xml:lang="en">
		<title>SPF &amp; DKIM Records in Route 53</title>
		<published>2014-07-30T10:46:13-04:00</published>
		<updated>2014-07-30T10:46:13-04:00</updated>
		<link href="https://stelfox.net/blog/2014/spf-and-dkim-records-in-route-53/" type="text/html"/>
		<id>https://stelfox.net/blog/2014/spf-and-dkim-records-in-route-53/</id>
		<content type="html">&lt;p&gt;I&#x27;m going to do a more detailed post on emailing from Amazon&#x27;s infrastructure
soon, but in the meantime I wanted to quickly throw out solutions too a couple
of problems I encountered. These are all specific too Amazon&#x27;s Route 53, and
most are user error (myself).&lt;&#x2F;p&gt;
&lt;span id=&quot;continue-reading&quot;&gt;&lt;&#x2F;span&gt;&lt;h2 id=&quot;spf-invalid-characters-or-format&quot;&gt;SPF Invalid Characters or Format&lt;&#x2F;h2&gt;
&lt;p&gt;After generating my SPF record, I jumped into Route 53, created a new record
pasted in my record, attempted to save and received the following message:&lt;&#x2F;p&gt;
&lt;blockquote&gt;
&lt;p&gt;The record set could not be saved because:&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;The Value field contains invalid characters or is in an invalid format.&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;&#x2F;blockquote&gt;
&lt;p&gt;I was using the SPF record type at a time (see the next section) and assumed
that I had messed up the format of my record in some way. I banged my head
against the wall and through RFCs thoroughly before I found the solution...&lt;&#x2F;p&gt;
&lt;p&gt;Solution: Wrap your SPF records in quotation characters.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;no-spf-record-found-validation-failure&quot;&gt;No SPF Record Found &#x2F; Validation Failure&lt;&#x2F;h2&gt;
&lt;p&gt;Since I have my DMARC policy in place (I&#x27;ll cover this in my email follow up),
I receive daily domain reports from Google whenever something fails validation
about my domain. After switching to Route 53 for DNS the &lt;code&gt;authresult&lt;&#x2F;code&gt; component
started showing up as fail for SPF.&lt;&#x2F;p&gt;
&lt;p&gt;Testing around a few online SPF validators indicated that none of them were
able to see my new SPF record, and there had been plenty of time for it too
propagate.&lt;&#x2F;p&gt;
&lt;p&gt;The SPF resource record type (RRTYPE 99) is available in Route 53 even though
&lt;a href=&quot;https:&#x2F;&#x2F;tools.ietf.org&#x2F;html&#x2F;rfc6686#section-3.1&quot;&gt;the record type has been deprecated&lt;&#x2F;a&gt;. Not being familiar with this
particular decision, I assumed I should be using it &lt;em&gt;instead&lt;&#x2F;em&gt; of the TXT record
I&#x27;ve used for every other domain, and it would be handled correctly or more
intelligently.&lt;&#x2F;p&gt;
&lt;p&gt;Solution: Either switch the SPF record too a TXT record or, my preference,
duplicate it into a TXT record so you have both.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;invalid-dkim-record&quot;&gt;Invalid DKIM record&lt;&#x2F;h2&gt;
&lt;p&gt;This one had me scratching my head for a while. This was my first time
deploying DKIM on a domain that I was not running a Bind name server for.
OpenDKIM is nice enough too generate a Bind record for you which works
perfectly. It&#x27;s output looks like the following:&lt;&#x2F;p&gt;
&lt;pre&gt;&lt;code&gt;default._domainkey.example.tld.   IN      TXT     ( &quot;v=DKIM1; k=rsa; t=y; s=email; &quot;
          &quot;p=MIHfMA0GCSqGSIb3DQEBAQUAA4HNADCByQKBwQC2Cwpa&#x2F;+Xhfkzn0QnyQoxRwoJPb+s51dIt9UtFLMlMFuYa&#x2F;k3GBwZ7UWeyAaQJ3RibSzKV&#x2F;YwgFuMrzyISrLNSuL2k1bQlQQG8nl23Mu9Mowcb+mV2&#x2F;3G7roshK6kOLNA0IV2SBl8&#x2F;0UoNZR&#x2F;x7c1lzVtVqdj0vW1SsJzgGfbt4LGRvCPyjdg+SLpYtOd&#x2F;Li4Y1pvHgSRKQRrklpKeJo&quot;
          &quot;nJQ4+lXWqzYtuX9xdNH46ck2HUl56Ob4cy3&#x2F;gYCJBWrAsCAwEAAQ==&quot; )  ; ----- DKIM key default for example.tld
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;Copying and pasting everything between the parentheses in the value field and
pasting them into Route 53 works flawlessly. The catch? This won&#x27;t be treated
as a single record, but three individual responses. None of which are complete
and valid DKIM records.&lt;&#x2F;p&gt;
&lt;p&gt;This happens because Route 53&#x27;s value field treats newlines as separate
records.&lt;&#x2F;p&gt;
&lt;p&gt;Solution: Turn it into one long string so it isn&#x27;t covering multiple lines
right? Not quite...&lt;&#x2F;p&gt;
&lt;h2 id=&quot;txtrdatatoolong&quot;&gt;TXTRDATATooLong&lt;&#x2F;h2&gt;
&lt;p&gt;Combining the DKIM key into one string like so:&lt;&#x2F;p&gt;
&lt;pre&gt;&lt;code&gt;&quot;v=DKIM1; k=rsa; t=y; s=email; p=MIHfMA0GCSqGSIb3DQEBAQUAA4HNADCByQKBwQC2Cwpa&#x2F;+Xhfkzn0QnyQoxRwoJPb+s51dIt9UtFLMlMFuYa&#x2F;k3GBwZ7UWeyAaQJ3RibSzKV&#x2F;YwgFuMrzyISrLNSuL2k1bQlQQG8nl23Mu9Mowcb+mV2&#x2F;3G7roshK6kOLNA0IV2SBl8&#x2F;0UoNZR&#x2F;x7c1lzVtVqdj0vW1SsJzgGfbt4LGRvCPyjdg+SLpYtOd&#x2F;Li4Y1pvHgSRKQRrklpKeJonJQ4+lXWqzYtuX9xdNH46ck2HUl56Ob4cy3&#x2F;gYCJBWrAsCAwEAAQ==&quot;
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;And attempting to save results in the following error message:&lt;&#x2F;p&gt;
&lt;pre&gt;&lt;code&gt;Invalid Resource Record: FATAL problem: TXTRDATATooLong encountered at ...`&lt;snip&gt;`
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;Now we&#x27;re left in a tricky spot. After some research the reason behind this is
clear, and makes sense. Though it is another poor usability bug in the way
Amazon&#x27;s Route 53 behaves. Individual DNS UDP packets are limited too 255
characters for their response.&lt;&#x2F;p&gt;
&lt;p&gt;Too properly deliver records longer than that DNS servers are supposed to break
up the response into chunks. Properly implemented clients combine these chunks
together (with no spaces, newlines or other characters added). What this means
is that the record can be broken up transparently behind the scenes anywhere in
the message and the client will put it back together correctly.&lt;&#x2F;p&gt;
&lt;p&gt;The Route 53 entry form won&#x27;t handle this for you though, and in hindsight it
looks like Bind might not do it for you though I suspected that was more for
readability of zone files rather than a technical limitation (and I haven&#x27;t
tested whether Bind is intelligent enough too handle just a long string).&lt;&#x2F;p&gt;
&lt;p&gt;Solution: Take the original output of Bind between the parentheses and just
remove the newline characters, leave the quotation marks and spaces between the
sections like the following sample and you&#x27;ll be golden:&lt;&#x2F;p&gt;
&lt;pre&gt;&lt;code&gt;&quot;v=DKIM1; k=rsa; t=y; s=email; &quot; &quot;p=MIHfMA0GCSqGSIb3DQEBAQUAA4HNADCByQKBwQC2Cwpa&#x2F;+Xhfkzn0QnyQoxRwoJPb+s51dIt9UtFLMlMFuYa&#x2F;k3GBwZ7UWeyAaQJ3RibSzKV&#x2F;YwgFuMrzyISrLNSuL2k1bQlQQG8nl23Mu9Mowcb+mV2&#x2F;3G7roshK6kOLNA0IV2SBl8&#x2F;0UoNZR&#x2F;x7c1lzVtVqdj0vW1SsJzgGfbt4LGRvCPyjdg+SLpYtOd&#x2F;Li4Y1pvHgSRKQRrklpKeJo&quot; &quot;nJQ4+lXWqzYtuX9xdNH46ck2HUl56Ob4cy3&#x2F;gYCJBWrAsCAwEAAQ==&quot;
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;Hope this helps someone else!&lt;&#x2F;p&gt;
</content>
	</entry>
	<entry xml:lang="en">
		<title>Unregistering From WhisperPush After Flashing a New ROM</title>
		<published>2014-07-22T21:54:59-04:00</published>
		<updated>2014-07-22T21:54:59-04:00</updated>
		<link href="https://stelfox.net/blog/2014/unregistering-from-whisperpush-after-flashing-a-new-rom/" type="text/html"/>
		<id>https://stelfox.net/blog/2014/unregistering-from-whisperpush-after-flashing-a-new-rom/</id>
		<content type="html">&lt;p&gt;I&#x27;ve been playing around with my Nexus 5 lately. It was quickly rooted and I
began playing with various ROMs that had been pre-built for the Nexus 5. My
first stop was the CyanogenMod. Since I&#x27;d last used CyanogenMod they added a
built-in framework that provides &lt;a href=&quot;https:&#x2F;&#x2F;whispersystems.org&#x2F;blog&#x2F;cyanogen-integration&#x2F;&quot;&gt;transparent text&lt;&#x2F;a&gt; message encryption
called WhisperPush.&lt;&#x2F;p&gt;
&lt;span id=&quot;continue-reading&quot;&gt;&lt;&#x2F;span&gt;
&lt;p&gt;WhisperPush is an implementation of &lt;a href=&quot;https:&#x2F;&#x2F;thoughtcrime.org&#x2F;&quot;&gt;Moxie Marlinspike&#x27;s&lt;&#x2F;a&gt; highly respected
TextSecure and I was very excited at the possibility of using it. I immediately
signed up for the service.&lt;&#x2F;p&gt;
&lt;p&gt;After a day of use I found CyanogenMod far too unstable too use on my primary
device. It locked up multiple times the first day and mobile data simply
wouldn&#x27;t work all day. I promptly formatted and flashed my phone, I haven&#x27;t
settled on a new ROM but that&#x27;s not what this post is about.&lt;&#x2F;p&gt;
&lt;p&gt;It occurred to me after flashing the phone I was still subscribed to
WhisperPush. If anyone that texts me was signed up as well. I&#x27;d never receive
even an encrypted blob, it would just silently fail.&lt;&#x2F;p&gt;
&lt;p&gt;Searching around I found there is very little information on it, and no
official way to unregister, especially after you&#x27;ve wiped your device and no
longer have your credentials. Ultimately I found a fairly easy solution, just
re-register and perform the backdoor steps too de-register.&lt;&#x2F;p&gt;
&lt;p&gt;I wiped my phone again and installed a new copy of the CyanogenMod nightly.
Booted it up and re-enabled WhisperPush. It didn&#x27;t even note that my number was
registered in the past.&lt;&#x2F;p&gt;
&lt;p&gt;I found the solution somewhere in the CyanogenMod forums (though I lost the
link, and I&#x27;m now too lazy to go find it again). You can unregister by
performing the following steps:&lt;&#x2F;p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Connect your computer with ADB too the phone and pair the computer with the
phone.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;Enable developer options by opening the system settings, choosing &#x27;About
phone&#x27; and clicking on the &#x27;Build number&#x27; about 7 times (it will start
counting down).&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;Open up the developer options found in the root of the system settings menu
and enable root access for &#x27;Apps and ADB&#x27;.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;On the computer use &lt;code&gt;adb shell&lt;&#x2F;code&gt; to get a shell on the device.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;Switch to root using the &lt;code&gt;su&lt;&#x2F;code&gt; command.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;Run the following command too view the WhisperPush internal settings:&lt;&#x2F;p&gt;
&lt;pre&gt;&lt;code&gt;cat &#x2F;data&#x2F;user&#x2F;0&#x2F;org.whispersystems.whisperpush&#x2F;shared_prefs&#x2F;org.whispersystems.whisperpush_preferences.xml`
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;Note down the value for &lt;code&gt;pref_registered_number&lt;&#x2F;code&gt; (this should be your phone
number with a preceding &#x27;+&#x27;) and &lt;code&gt;pre_push_password&lt;&#x2F;code&gt;.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;Exit the shell.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;&#x2F;ol&gt;
&lt;p&gt;Finally too unregister we need too make a DELETE request against the
WhisperPush API. The classic HTTP swiss army knife &lt;code&gt;curl&lt;&#x2F;code&gt; is going to help us
on this front. Run the following command on any linux computer with curl
installed, replacing the registered number and registered password with the
value you recorded earlier.&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;bash&quot; class=&quot;language-bash &quot;&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;curl -v -k -X DELETE --basic --user ${pref_registered_number}:${pre_push_password} https:&#x2F;&#x2F;whisperpush.cyanogenmod.org&#x2F;v1&#x2F;accounts&#x2F;gcm
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;Be sure too include the &#x27;+&#x27; in your pref_registered_number. You should end up
with a status code of 204. The output will look something like the following
(credentials removed).&lt;&#x2F;p&gt;
&lt;pre&gt;&lt;code&gt;* About to connect() to whisperpush.cyanogenmod.org port 443 (#0)
*   Trying 54.201.5.27...
* Connected to whisperpush.cyanogenmod.org (54.201.5.27) port 443 (#0)
* Initializing NSS with certpath: sql:&#x2F;etc&#x2F;pki&#x2F;nssdb
* skipping SSL peer certificate verification
* SSL connection using TLS_DHE_RSA_WITH_AES_128_CBC_SHA
* Server certificate:
*   subject: OU=Operations,O=&quot;Cyanogen, Inc.&quot;,E=ops@cyngn.com,C=US,ST=Washington,L=Seattle,CN=whisperpush.cyanogenmod.org
*   start date: Nov 26 05:39:18 2013 GMT
*   expire date: Nov 24 05:39:18 2023 GMT
*   common name: whisperpush.cyanogenmod.org
*   issuer: E=ops@cyngn.com,CN=Authority,OU=Operations,O=&quot;Cyanogen, Inc.&quot;,L=Seattle,ST=Washington,C=US
* Server auth using Basic with user &#x27;${pref_registered_number}&#x27;
&gt; DELETE &#x2F;v1&#x2F;accounts&#x2F;gcm HTTP&#x2F;1.1
&gt; Authorization: Basic ${encoded credentials}
&gt; User-Agent: curl&#x2F;7.29.0
&gt; Host: whisperpush.cyanogenmod.org
&gt; Accept: *&#x2F;*
&gt;
&lt; HTTP&#x2F;1.1 204 No Content
&lt; Server: nginx&#x2F;1.1.19
&lt; Date: Wed, 23 Jul 2014 01:45:25 GMT
&lt; Connection: keep-alive
&lt;
* Connection #0 to host whisperpush.cyanogenmod.org left intact
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;I don&#x27;t have any way too check that I&#x27;m unregistered but it seems too have
worked. Here is hoping this helps some else out in the future.&lt;&#x2F;p&gt;
</content>
	</entry>
	<entry xml:lang="en">
		<title>Using OpenWRT&#x27;s Dnsmasq as a TFTP Server</title>
		<published>2014-07-01T21:26:45-04:00</published>
		<updated>2014-07-01T21:26:45-04:00</updated>
		<link href="https://stelfox.net/blog/2014/using-openwrts-dnsmasq-as-a-tftp-server/" type="text/html"/>
		<id>https://stelfox.net/blog/2014/using-openwrts-dnsmasq-as-a-tftp-server/</id>
		<content type="html">&lt;p&gt;I recently reflashed my primary router to a newer version of OpenWRT and
attempted to follow &lt;a href=&quot;https:&#x2F;&#x2F;stelfox.net&#x2F;blog&#x2F;2013&#x2F;using-dnsmasq-as-a-standalone-tftp-server&#x2F;&quot;&gt;my own directions&lt;&#x2F;a&gt; written in an earlier blog post to
add PXE booting to my local network using the dnsmasq service built in. After
following my advice I found that the dnsmasq service wasn&#x27;t starting.&lt;&#x2F;p&gt;
&lt;span id=&quot;continue-reading&quot;&gt;&lt;&#x2F;span&gt;
&lt;p&gt;Looking into the &lt;code&gt;logread&lt;&#x2F;code&gt; output I finally saw that this was due too a
permission issue. Combining this with the output of &lt;code&gt;ps&lt;&#x2F;code&gt; too identify the user
that dnsmasq was running on I was able to both modify my instructions and use
OpenWRT&#x27;s own configuration system to perform the configuration instead of
modifying the dnsmasq configuration.&lt;&#x2F;p&gt;
&lt;p&gt;First was solving the permissions issue. I created a dedicated directory at
&lt;code&gt;&#x2F;var&#x2F;tftp&lt;&#x2F;code&gt; and changed the ownership to &#x27;nobody&#x27; and &#x27;nogroup&#x27; and mode too
&#x27;0755&#x27;.&lt;&#x2F;p&gt;
&lt;p&gt;Previously I used &lt;code&gt;&#x2F;var&#x2F;lib&#x2F;tftp&lt;&#x2F;code&gt;, however, the default permissions on the
&lt;code&gt;&#x2F;var&#x2F;lib&lt;&#x2F;code&gt; directory is too restrictive and I didn&#x27;t want to reduce the rest of
that directories security posture simply too allow directory traversal.&lt;&#x2F;p&gt;
&lt;p&gt;Next up was getting the TFTP portion of dnsmasq configured and running. Open up
&lt;code&gt;&#x2F;etc&#x2F;config&#x2F;dhcp&lt;&#x2F;code&gt; and under the &lt;code&gt;dnsmasq&lt;&#x2F;code&gt; section add the following lines (or
if these lines already exist adjust the values to match).&lt;&#x2F;p&gt;
&lt;pre&gt;&lt;code&gt;option enable_tftp &#x27;1&#x27;
option tftp_root &#x27;&#x2F;var&#x2F;tftp&#x27;
option dhcp_boot &#x27;pxelinux.0&#x27;
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;Run &lt;code&gt;uci commit dhcp&lt;&#x2F;code&gt; too commit the changes and finally &lt;code&gt;&#x2F;etc&#x2F;init.d&#x2F;dnsmasq restart&lt;&#x2F;code&gt; To apply the changes. You&#x27;ll want too put the &lt;code&gt;pxelinux.0&lt;&#x2F;code&gt; and
associated configuration files into the &lt;code&gt;&#x2F;var&#x2F;tftp&lt;&#x2F;code&gt; directory too complete the
PXE booting configuration.&lt;&#x2F;p&gt;
&lt;p&gt;I&#x27;ll probably write a blog post covering my PXE setup and configuration if I
don&#x27;t get distracted by other projects.&lt;&#x2F;p&gt;
</content>
	</entry>
	<entry xml:lang="en">
		<title>Fixing Erratic BMC Controller on PowerEdge C6100</title>
		<published>2014-06-30T21:43:02-04:00</published>
		<updated>2014-06-30T21:43:02-04:00</updated>
		<link href="https://stelfox.net/blog/2014/fixing-erratic-bmc-controller-on-poweredge-c6100/" type="text/html"/>
		<id>https://stelfox.net/blog/2014/fixing-erratic-bmc-controller-on-poweredge-c6100/</id>
		<content type="html">&lt;p&gt;I randomly started experiencing an issue with one blade in one of my PowerEdge
C6100 blades. It wouldn&#x27;t obey all commands issued too it via IPMI or through
the BMC&#x27;s web interface. Additionally the blade would randomly power on when
off, and the front light would consistently blink as if a hardware fault was
detected.&lt;&#x2F;p&gt;
&lt;span id=&quot;continue-reading&quot;&gt;&lt;&#x2F;span&gt;
&lt;p&gt;This has been bothering me for a while, but it was my spare blade and wasn&#x27;t
affecting my lab in anyway so I&#x27;ve ignored it. I finally needed it for a
project and looked into what may be causing the issue.&lt;&#x2F;p&gt;
&lt;p&gt;A &lt;a href=&quot;https:&#x2F;&#x2F;forums.servethehome.com&#x2F;index.php?threads&#x2F;dell-c6100-anyone-brick-a-board-yet.1448&#x2F;&quot;&gt;thread&lt;&#x2F;a&gt; on the Serve the Home forums lead to me too a solution, even
though my symptoms didn&#x27;t quite match up with what I was experiencing.&lt;&#x2F;p&gt;
&lt;p&gt;I downloaded the &lt;a href=&quot;https:&#x2F;&#x2F;ftp.dell.com&#x2F;Manuals&#x2F;all-products&#x2F;esuprt_ser_stor_net&#x2F;esuprt_cloud_products&#x2F;poweredge-c6100_Owner%27s%20Manual_en-us.pdf&quot;&gt;PowerEdge C6100 Owner&#x27;s Manual&lt;&#x2F;a&gt; for the jumper
information, and found it too be redundant. The board itself has each of the
jumpers clearly labeled.&lt;&#x2F;p&gt;
&lt;p&gt;After pulling the affected chassis out of the server I connected the pins for
the CMOS reset, CMOS password reset, and system reset for about 15 seconds. I
pulled the jumpers, reinstalled the blade and it&#x27;s happy once again. Problem
solved.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;em&gt;Update:&lt;&#x2F;em&gt; After performing a few commands via the web interface the issue
returned. I&#x27;m still looking for a solution too the problem.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;em&gt;Update 2:&lt;&#x2F;em&gt; I&#x27;m now suspecting that the issue may be related too me not
updating the FSB, which is responsible for handling power management of
individual nodes as well as reporting temperature and command response from the
BMC.&lt;&#x2F;p&gt;
</content>
	</entry>
	<entry xml:lang="en">
		<title>AWS Reserved Instance Pricing</title>
		<published>2014-06-06T13:28:11-04:00</published>
		<updated>2014-06-06T13:28:11-04:00</updated>
		<link href="https://stelfox.net/blog/2014/aws-reserved-instance-pricing/" type="text/html"/>
		<id>https://stelfox.net/blog/2014/aws-reserved-instance-pricing/</id>
		<content type="html">&lt;p&gt;The current large project I&#x27;m working on is going to be hosted on AWS and I was
requested to do a cost estimate. Looking into it, it quickly became clear that
reserved instances could potentially save quite a bit of cash but there was a
catch (isn&#x27;t there always?).&lt;&#x2F;p&gt;
&lt;span id=&quot;continue-reading&quot;&gt;&lt;&#x2F;span&gt;
&lt;p&gt;There is an upfront cost for reserving the instance and in exchange you get a
reduced hourly rate. After running the numbers one thing wasn&#x27;t clear too me,
is the upfront cost credit towards running machines or a fee you never see
again?&lt;&#x2F;p&gt;
&lt;p&gt;I immediately assumed the latter based on the numbers for one simple reason. If
you use the &#x27;Light Reserved Instance&#x27; with a 1 year reservation, have your
machine running 24&#x2F;7 the whole year it will cost your &lt;em&gt;more&lt;&#x2F;em&gt; than running the
same instance as &#x27;on demand&#x27;. This was true for their m1.small, m3.medium, and
m3.large which was the only ones I ran the numbers for.&lt;&#x2F;p&gt;
&lt;p&gt;I searched the internet and wasn&#x27;t able to find a solid answer to the question
until I asked Amazon&#x27;s customer service directly.&lt;&#x2F;p&gt;
&lt;p&gt;Ultimately there probably is a price point where 1 year light reserved
instances make sense, and if you&#x27;re looking too run 24&#x2F;7 for the whole year
you&#x27;ll want to do a heavy anyway for the most savings but it still surprised
me.&lt;&#x2F;p&gt;
&lt;p&gt;I&#x27;ll probably do a project later using &lt;a href=&quot;https:&#x2F;&#x2F;d3js.org&#x2F;&quot;&gt;d3.js&lt;&#x2F;a&gt; to get some direct hours run
vs total cost for various instances. It&#x27;ll probably be a fun project.&lt;&#x2F;p&gt;
</content>
	</entry>
	<entry xml:lang="en">
		<title>Modifying the Hosts File in a Docker Container</title>
		<published>2014-06-03T11:43:59-04:00</published>
		<updated>2014-06-03T11:43:59-04:00</updated>
		<link href="https://stelfox.net/blog/2014/modifying-the-hosts-file-in-a-docker-container/" type="text/html"/>
		<id>https://stelfox.net/blog/2014/modifying-the-hosts-file-in-a-docker-container/</id>
		<content type="html">&lt;p&gt;Before I describe the issue that I encountered, let me be very clear. This hack
is &lt;em&gt;potentially dangerous and should absolutely only be done in development
environments&lt;&#x2F;em&gt;. This won&#x27;t affect your host system, only the docker container so
the most damage you&#x27;ll do is prevent hostname and possibly user&#x2F;group lookups
within the container itself.&lt;&#x2F;p&gt;
&lt;span id=&quot;continue-reading&quot;&gt;&lt;&#x2F;span&gt;
&lt;p&gt;Alright with that out of the way, I was actively working on a codebase that
uses subdomains as part of the identifier. Rather than setup a full DNS server,
point my local system at it and load in the domains I wanted to simply modify
the &#x2F;etc&#x2F;hosts file inside the environment.&lt;&#x2F;p&gt;
&lt;p&gt;Docker mounts an &#x2F;etc&#x2F;hosts file inside it&#x27;s containers, read-only, and the
container&#x27;s &#x27;root&#x27; user has had it&#x27;s mount permissions revoked so it&#x27;s not able
to be modified. Other users have encountered this issue, and a &lt;a href=&quot;https:&#x2F;&#x2F;stackoverflow.com&#x2F;questions&#x2F;19414543&#x2F;how-can-i-make-etc-hosts-writable-by-root-in-a-docker-container&quot;&gt;novel
workaround was put forward&lt;&#x2F;a&gt;. The solution however makes use of Perl, and is
specific too Ubuntu base systems.&lt;&#x2F;p&gt;
&lt;p&gt;I&#x27;ll explain the solution after showing a more general way to accomplish the
same thing. Different linux systems will store their libraries in different
directory structures. CentOS is different from Fedora, which is different from
Ubuntu and Debian. All of them name their libraries, in this case we&#x27;re looking
for &#x27;libnss_files.so.2&#x27;.&lt;&#x2F;p&gt;
&lt;p&gt;You can find where your copy of this library lives with the following command.
This should be run inside the docker container that you want to modify the
&#x2F;etc&#x2F;hosts file in.&lt;&#x2F;p&gt;
&lt;pre&gt;&lt;code&gt;find &#x2F; -name libnss_files.so.2 -print 2&gt; &#x2F;dev&#x2F;null
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;Pay attention to the path, multiple files may show up and you want the one that
matches your system&#x27;s running kernel (generally x86_64 systems will have their
libraries in a lib64 directory).&lt;&#x2F;p&gt;
&lt;p&gt;Once you&#x27;ve found this add the following lines to your &lt;code&gt;Dockerfile&lt;&#x2F;code&gt;. Make sure
you modify the path in the copy in the first line to the path of your copy of
the library. Once done you&#x27;ll use the &#x2F;var&#x2F;hosts file to modify your hosts file
instead.&lt;&#x2F;p&gt;
&lt;pre&gt;&lt;code&gt;RUN mkdir -p &#x2F;override_lib &amp;&amp; cp &#x2F;etc&#x2F;hosts &#x2F;var&#x2F; &amp;&amp; cp &#x2F;usr&#x2F;lib64&#x2F;libnss_files.so.2 &#x2F;override_lib
RUN sed -ie &#x27;s:&#x2F;etc&#x2F;hosts:&#x2F;var&#x2F;hosts:g&#x27; &#x2F;override_lib&#x2F;libnss_files.so.2
ENV LD_LIBRARY_PATH &#x2F;override_lib
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;So what is this actually doing? On linux systems, name configurations such as
DNS, username, and group lookups are generally handled by the &lt;code&gt;nss&lt;&#x2F;code&gt; or name
service switch configuration tools including the hosts file. The library that
we&#x27;re copying and modifying is a very specific to reading from files on the
system and includes the default paths to these files.&lt;&#x2F;p&gt;
&lt;p&gt;Generally you have to be very careful when you&#x27;re manipulating strings within
compiled libraries. The length of the string is encoded along with it, so at a
minimum it&#x27;s important that the string is &lt;em&gt;the same length or less&lt;&#x2F;em&gt;. You can
get away with less but it requires additionally writing an end of string
character as well.&lt;&#x2F;p&gt;
&lt;p&gt;Too make this hack simple, we&#x27;re simply replacing the &#x27;etc&#x27; with &#x27;var&#x27;, both
systems directories that regular users generally should have read access but
not write access too.&lt;&#x2F;p&gt;
&lt;p&gt;Finally we need to tell all programs that need to perform lookups using
hostnames in the hosts file to make use of our modified library instead of the
system one. Linux will look for shared libraries at runtime in any paths set in
in the LD_LIBRARY_PATH (colon delimited just like PATH) and this doesn&#x27;t
require any privileges too set.&lt;&#x2F;p&gt;
&lt;p&gt;And the result? An editable hosts file, with no extra services. I can&#x27;t stress
enough though, there could be bad ramifications from modifying libraries this
way. This is definitely not a &#x27;production ready&#x27; hack.&lt;&#x2F;p&gt;
</content>
	</entry>
	<entry xml:lang="en">
		<title>Extracting Content From Markdown</title>
		<published>2014-05-30T18:34:29-04:00</published>
		<updated>2014-05-30T18:34:29-04:00</updated>
		<link href="https://stelfox.net/blog/2014/extracting-content-from-markdown/" type="text/html"/>
		<id>https://stelfox.net/blog/2014/extracting-content-from-markdown/</id>
		<content type="html">&lt;p&gt;Recently I&#x27;ve been playing around with building a pure javascript full text
search engine for static content sites like this one. One of the challenges
with doing this has been working around the Markdown markup embedded in the
written content.&lt;&#x2F;p&gt;
&lt;span id=&quot;continue-reading&quot;&gt;&lt;&#x2F;span&gt;
&lt;p&gt;Most of the markdown syntax can be stripped out simply by removing all
non-alphanumeric characters from the document and move on. This doesn&#x27;t solve
one of the bigger challenges I&#x27;ve experienced... Code blocks. Code blocks have
plenty of regular English-ish words and can easily skew keyword detection
within it.&lt;&#x2F;p&gt;
&lt;p&gt;I didn&#x27;t want to write my own Markdown parser, so I started with the one
already in use by this site&#x27;s renderer (&lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;vmg&#x2F;redcarpet&quot;&gt;redcarpet&lt;&#x2F;a&gt;). Another GitHub user,
&lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;toupeira&quot;&gt;Markus Koller or toupeira on GitHub&lt;&#x2F;a&gt; provided &lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;vmg&#x2F;redcarpet&#x2F;issues&#x2F;79&quot;&gt;the basis&lt;&#x2F;a&gt; for the code
that became the redcarpet &amp;quot;StripDown&amp;quot; formatter, which was designed to
essentially render a Markdown document without the markup.&lt;&#x2F;p&gt;
&lt;p&gt;It does almost exactly what I want, except it still outputs raw code inside the
content. The following code sample includes a modified version that excludes
any code blocks. My content is also formatted inside the markdown documents to
never be longer than 80 lines, this also turns individual paragraphs and list
items into individual lines for paragraph detection.&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;ruby&quot; class=&quot;language-ruby &quot;&gt;&lt;code class=&quot;language-ruby&quot; data-lang=&quot;ruby&quot;&gt;require &#x27;redcarpet&#x27;
require &#x27;redcarpet&#x2F;render_strip&#x27;

class ContentRenderer &lt; Redcarpet::Render::StripDown
  def block_code(*args)
    nil
  end

  def list_item(content, list_type)
    content.gsub(&quot;\n&quot;, &quot; &quot;) + &quot;\n\n&quot;
  end

  def paragraph(text)
    text.gsub(&quot;\n&quot;, &quot; &quot;) + &quot;\n\n&quot;
  end
end

markdown = Redcarpet::Markdown.new(ContentRenderer, fenced_code_blocks: true)
puts markdown.render(File.read(&#x27;sample_markdown_article.md&#x27;))
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;The above code will print out just the content of the markdown formatted file
&#x27;sample_markdown_article.md&#x27;.&lt;&#x2F;p&gt;
</content>
	</entry>
	<entry xml:lang="en">
		<title>PG::Error: ERROR: Type &#x27;Hstore&#x27; Does Not Exist</title>
		<published>2014-05-28T18:00:55-04:00</published>
		<updated>2014-05-28T18:00:55-04:00</updated>
		<link href="https://stelfox.net/blog/2014/pg-error-error-type-hstore-does-not-exist/" type="text/html"/>
		<id>https://stelfox.net/blog/2014/pg-error-error-type-hstore-does-not-exist/</id>
		<content type="html">&lt;p&gt;I&#x27;ve been using the PostgreSQL&#x27;s hstore extension in a Rails application lately
and kept encountering the error that is this post&#x27;s namesake. It would
specifically happen when a database had been dropped, recreated and I freshly
ran the migrations.&lt;&#x2F;p&gt;
&lt;span id=&quot;continue-reading&quot;&gt;&lt;&#x2F;span&gt;
&lt;p&gt;It seems that while Rails 4 supports the HStore datatype, it doesn&#x27;t enable the
extension itself. I&#x27;ve found two ways too solve this issue in wildly different
ways.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;first-solution-enable-hstore-by-default&quot;&gt;First Solution: Enable HStore by Default&lt;&#x2F;h2&gt;
&lt;p&gt;This is the common solution that is recommended too solve this issue. It
enables the HStore extension by default on all newly created databases. Too
understand this you need to know a bit about PostgreSQL&#x27;s behavior.&lt;&#x2F;p&gt;
&lt;p&gt;When a new database is created, PostgreSQL creates a copy of a special
pre-existing database named &lt;code&gt;template1&lt;&#x2F;code&gt; by default. Anything done too this
database will be reflected in all new databases, including enabling extensions.&lt;&#x2F;p&gt;
&lt;p&gt;Too enable the HStore extension on the &lt;code&gt;template1&lt;&#x2F;code&gt; database you can execute the
following command (generally as the &lt;code&gt;postgres&lt;&#x2F;code&gt; user or with your authentication
of choice).&lt;&#x2F;p&gt;
&lt;pre&gt;&lt;code&gt;psql -d template1 -c &#x27;CREATE EXTENSION hstore;&#x27;
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;h2 id=&quot;second-solution-rails-migration&quot;&gt;Second Solution: Rails Migration&lt;&#x2F;h2&gt;
&lt;p&gt;The above solution doesn&#x27;t sit well with me. While it&#x27;s uncommon for any
individual PostgreSQL server to be shared among different applications with
different databases, the possibility is there. Perhaps the application will get
decommissioned and the DBA will simply drop the associated database and roles
instead of setting up a new one.&lt;&#x2F;p&gt;
&lt;p&gt;Disconnecting the requirements of the application from the application itself
always seems to lead too trouble.&lt;&#x2F;p&gt;
&lt;p&gt;Rails already has a mechanism too handle modifications too the database
overtime, migrations. They&#x27;re solid, well tested, and encapsulate not only how
to get the database to a particular state but also how to return it back to
it&#x27;s prior state (generally).&lt;&#x2F;p&gt;
&lt;p&gt;We can also do this without using raw SQL which now also seems a bit... Off to
me. The following is a sample Rails migration that will both enable and disable
the extension:&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;ruby&quot; class=&quot;language-ruby &quot;&gt;&lt;code class=&quot;language-ruby&quot; data-lang=&quot;ruby&quot;&gt;class ManageHstore &lt; ActiveRecord::Migration
  def change
    reversible do |op|
      op.up { enable_extension &#x27;hstore&#x27; }
      op.down { disable_extension &#x27;hstore&#x27; }
    end
  end
end
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;Now the biggest problem with this migration is that too use it, you need too
plan ahead of time too use the extension or not worry about freshly running all
the migrations (generally because you dropped and created the database). This
migration needs to be named so it alphabetically comes before any migration in
your application that makes use of the HStore datatype.&lt;&#x2F;p&gt;
&lt;p&gt;ActiveRecord uses timestamps at the beginning of the migration names to handle
this alphabetic sorting, and such you&#x27;ll want to fake this in before you used
the HStore datatype.&lt;&#x2F;p&gt;
</content>
	</entry>
	<entry xml:lang="en">
		<title>Chain Loading Kernels</title>
		<published>2014-05-23T11:39:16-04:00</published>
		<updated>2014-05-23T11:39:16-04:00</updated>
		<link href="https://stelfox.net/blog/2014/chain-loading-kernels/" type="text/html"/>
		<id>https://stelfox.net/blog/2014/chain-loading-kernels/</id>
		<content type="html">&lt;p&gt;I&#x27;ve found several places where I needed to be able to update my kernels but
for one reason or another can&#x27;t update the kernel that gets booted initially. A
couple of these situations were:&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;Running Custom or Updated Kernels on DigitalOcean (this is one of their
biggest failings IMHO)&lt;&#x2F;li&gt;
&lt;li&gt;Allowing updating of kernels on embedded linux devices that require their
kernel flashed into NVRAM.&lt;&#x2F;li&gt;
&lt;li&gt;Running an embedded system that used an active&#x2F;backup partition scheme for
updating.&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;In all cases the process was pretty much the same, though there were some
custom changes to the preliminary init system depending on what I needed to get
done, especially with the last one which I may cover in a different article.&lt;&#x2F;p&gt;
&lt;span id=&quot;continue-reading&quot;&gt;&lt;&#x2F;span&gt;
&lt;p&gt;In all cases these were done on a Red Hat based distribution like CentOS,
Scientific Linux, RHEL, or even Fedora. For those users of Debian based systems
you&#x27;ll need to adjust the scripts too your system though I can&#x27;t imagine
anything other than the package names changing.&lt;&#x2F;p&gt;
&lt;p&gt;This assumes you already have the kernel and initramfs you want to boot
installed on your local filesystem at &lt;code&gt;&#x2F;boot&#x2F;vmlinuz-custom&lt;&#x2F;code&gt; and
&lt;code&gt;&#x2F;boot&#x2F;initramfs.img&lt;&#x2F;code&gt;.&lt;&#x2F;p&gt;
&lt;p&gt;A quick background on how this works, when the linux kernel is compiled an init
program is configured to be the first thing triggered, by default and in most
situations this will be the executable &lt;code&gt;&#x2F;sbin&#x2F;init&lt;&#x2F;code&gt;. This init process is then
responsible for starting the rest of the daemons and processes that make up the
systems we regularly interact with.&lt;&#x2F;p&gt;
&lt;p&gt;There are tools that allow you too effectively execute another kernel to run in
place of the kernel that is already running. There are some catches though as
the new kernel won&#x27;t always re-initialize all devices (since they&#x27;ve already
been initialized) and that can lead too some weird behaviors with processes
that already have hooks on those devices.&lt;&#x2F;p&gt;
&lt;p&gt;Too prevent any issues you need to load the new kernel as early in the boot
process as possible. Doing this in the init program is pretty much as early as
you can get and makes for a pretty stable system (I&#x27;ve yet to experience any
issues with machines running this way).&lt;&#x2F;p&gt;
&lt;p&gt;There are several different init systems and they all behave a little
differently, as far as I know only systemd supports a means of automatically
executing a different kernel but I am personally not a systemd fan and it would
be too late in the boot process already for me too trust the chain load. You
can reliably chain load kernels regardless of what your normal init system is
though very easily and that&#x27;s what I&#x27;m going to cover here.&lt;&#x2F;p&gt;
&lt;p&gt;You&#x27;ll need to have the kexec tools installed on your system. This is pretty
straight-forward:&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;sh&quot; class=&quot;language-sh &quot;&gt;&lt;code class=&quot;language-sh&quot; data-lang=&quot;sh&quot;&gt;yum install kexec-tools -y
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;Next we&#x27;re going to shift the standard init process off to the side, someplace
still accessible so we can call it later (this will need to be done as root).&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;sh&quot; class=&quot;language-sh &quot;&gt;&lt;code class=&quot;language-sh&quot; data-lang=&quot;sh&quot;&gt;mv &#x2F;sbin&#x2F;init &#x2F;sbin&#x2F;init.original
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;Now we need to create our own init script that will handle detecting if it&#x27;s
the new or old kernel, replacing the kernel if it is indeed an old one, and
starting up the normal init process if it&#x27;s the new kernel.&lt;&#x2F;p&gt;
&lt;p&gt;Now there is a very important catch here, whatever process starts up first is
given PID 1 which is very important in kernel land. Whatever process is PID 1
will inherit all zombie processes on the system and will need to handle them.
Since our shell script is the first thing started up it will get PID 1 for both
the old and new kernel and getting the process handling code correct is not a
trivial issue.&lt;&#x2F;p&gt;
&lt;p&gt;What we really need is to hand over PID 1 to the init process so it can do it&#x27;s
job normally as if the shell script never existed. There is a native function
to do exactly this in these shell scripts: &lt;code&gt;exec&lt;&#x2F;code&gt;.&lt;&#x2F;p&gt;
&lt;p&gt;Our simple shell script to do the chain load looks like this:&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;sh&quot; class=&quot;language-sh &quot;&gt;&lt;code class=&quot;language-sh&quot; data-lang=&quot;sh&quot;&gt;#!&#x2F;bin&#x2F;bash

# Detect if this is the old kernel (not booted with the otherwise meaningless
# &#x27;kexeced&#x27; parameter.
if [ $(grep -q &#x27; kexeced$&#x27; &#x2F;proc&#x2F;cmdline) ]; then
  kexec --load &#x2F;boot&#x2F;vmlinuz-custom --initrd=&#x2F;boot&#x2F;initramfs.img \
    --reuse-cmdline --append=&#x27; kexeced&#x27;
  kexec --exec
fi

# If we made it this far we&#x27;re running on the new kernel, trigger the original
# init binary with all the options passed too this as well as having it take
# over this process&#x27;s PID.
exec &#x2F;sbin&#x2F;init.original &quot;$@&quot;
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;After rebooting you should be in your new kernel which you can verify with
&lt;code&gt;uname -a&lt;&#x2F;code&gt; and also by examining the &lt;code&gt;&#x2F;proc&#x2F;cmdline&lt;&#x2F;code&gt; file for the existence of
the &lt;code&gt;kexeced&lt;&#x2F;code&gt; flag.&lt;&#x2F;p&gt;
&lt;p&gt;If you modify the script above, be very careful as any execution error will
cause your system to die and recovery will only be possible by mounting the
filesystem on another linux system and fixing it.&lt;&#x2F;p&gt;
&lt;p&gt;In a future article I&#x27;ll cover how to use this trick to build an active &#x2F;
backup system allowing you to fall back to a known good system when booting
fails which is incredibly useful for embedded devices in the field that need
updates but are not easy to get too or replace when an update bricks the
system.&lt;&#x2F;p&gt;
</content>
	</entry>
	<entry xml:lang="en">
		<title>Calculating RSA Key Fingerprints in Ruby</title>
		<published>2014-04-21T18:37:04-04:00</published>
		<updated>2014-04-21T18:37:04-04:00</updated>
		<link href="https://stelfox.net/blog/2014/calculating-rsa-key-fingerprints-in-ruby/" type="text/html"/>
		<id>https://stelfox.net/blog/2014/calculating-rsa-key-fingerprints-in-ruby/</id>
		<content type="html">&lt;p&gt;I regularly find myself working on projects that involve the manipulation and
storage of RSA keys. In the past I&#x27;ve never had to worry about identification
or presentation of these keys. Normally I&#x27;ve only got one too three pairs at
most that I&#x27;m manipulating (server, certificate authority, client).&lt;&#x2F;p&gt;
&lt;span id=&quot;continue-reading&quot;&gt;&lt;&#x2F;span&gt;
&lt;p&gt;I&#x27;ve not found myself working on a project that involves presenting the
certificates to users for selection and comparison. The obvious way too do this
is take a page out of other developer&#x27;s books and present the key&#x27;s
fingerprint.&lt;&#x2F;p&gt;
&lt;p&gt;For those unfamiliar with key fingerprints, they are a condensed way to compare
differing RSA with a high probability that if the fingerprints match, so do the
keys. These are generally based on a cryptographic digest function such as SHA1
and MD5, and you&#x27;ll see them most commonly when connecting to a new SSH host
and will look like the following:.&lt;&#x2F;p&gt;
&lt;pre&gt;&lt;code&gt;The authenticity of host &#x27;some.fakedomain.tld (127.0.0.1)&#x27; can&#x27;t be established.
RSA key fingerprint is 0c:6c:dd:32:b5:59:40:1d:ac:05:24:4f:04:bc:e0:f3.
Are you sure you want to continue connecting (yes&#x2F;no)?
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;The string of 32 hex characters presented there can be compared with another
known value to make sure you&#x27;re connecting to the correct SSH server and will
always be the same length regardless of the bit-strength of the keys used.
Without the fingerprint, users would have to compare 256 hex characters for a
1024 bit key, which is a very low security key.&lt;&#x2F;p&gt;
&lt;p&gt;You can calculate the SSH fingerprint for your SSH key or a SSH host key using
the &lt;code&gt;ssh-keygen&lt;&#x2F;code&gt; command like so:&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;sh&quot; class=&quot;language-sh &quot;&gt;&lt;code class=&quot;language-sh&quot; data-lang=&quot;sh&quot;&gt;ssh-keygen -lf ~&#x2F;.ssh&#x2F;id_rsa
ssh-keygen -lf &#x2F;etc&#x2F;ssh&#x2F;ssh_host_key.pub
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;It will work when the path is either a private RSA key or a public key
formatted for SSH authorized key files.&lt;&#x2F;p&gt;
&lt;p&gt;X509 certificates also use a key fingerprint to help identify a certificate&#x27;s
signing authority. What I rapidly learned through this investigation was that
they are calculated slightly differently from SSH fingerprints even if they&#x27;re
in the same format.&lt;&#x2F;p&gt;
&lt;p&gt;I couldn&#x27;t find any good Ruby code that calculated either, and the alternatives
were some dense C++. Luckily SSH fingerprints are pretty documented in
&lt;a href=&quot;https:&#x2F;&#x2F;www.ietf.org&#x2F;rfc&#x2F;rfc4253.txt&quot;&gt;RFC4253&lt;&#x2F;a&gt; and &lt;a href=&quot;https:&#x2F;&#x2F;www.ietf.org&#x2F;rfc&#x2F;rfc4716.txt&quot;&gt;RFC4716&lt;&#x2F;a&gt;. Fingerprints on RSA keys for use with OpenSSL
are less clear, and there is a different method for calculating the
fingerprints of certificates.&lt;&#x2F;p&gt;
&lt;p&gt;Slowly working through the undocumented bits of Ruby&#x27;s OpenSSL wrapper, the
RFCs and a couple of C++ implementations I finally got a set of working
implementations that calculate the following fingerprints in Ruby:&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;MD5 &amp;amp; SHA1 fingerprints for RSA SSH keys&lt;&#x2F;li&gt;
&lt;li&gt;Fingerprints of RSA keys for use with x509 certificates&lt;&#x2F;li&gt;
&lt;li&gt;Fingerprints of x509 certificates&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;The easiest being a regular x509 certificate:&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;ruby&quot; class=&quot;language-ruby &quot;&gt;&lt;code class=&quot;language-ruby&quot; data-lang=&quot;ruby&quot;&gt;require &#x27;openssl&#x27;

path_to_cert = &#x27;&#x2F;tmp&#x2F;sample.crt&#x27;
cert = OpenSSL::X509::Certificate.new(File.read(path_to_cert))
puts OpenSSL::Digest::SHA1.hexdigest(cert.to_der).scan(&#x2F;..&#x2F;).join(&#x27;:&#x27;)
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;You can compare the output of the above code with OpenSSL&#x27;s implementation with
the following command:&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;sh&quot; class=&quot;language-sh &quot;&gt;&lt;code class=&quot;language-sh&quot; data-lang=&quot;sh&quot;&gt;openssl x509 -in &#x2F;tmp&#x2F;sample.crt -noout -fingerprint
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;Please note that case sensitivity doesn&#x27;t matter here (OpenSSL will return
upper case hex codes).&lt;&#x2F;p&gt;
&lt;p&gt;The next one I got working was the SSH fingerprints thanks to the RFCs mentioned
earlier.&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;ruby&quot; class=&quot;language-ruby &quot;&gt;&lt;code class=&quot;language-ruby&quot; data-lang=&quot;ruby&quot;&gt;require &#x27;openssl&#x27;

path_to_key = &#x27;&#x2F;tmp&#x2F;ssh_key&#x27;

key = OpenSSL::PKey::RSA.new(File.read(path_to_key))
data_string = [7].pack(&#x27;N&#x27;) + &#x27;ssh-rsa&#x27; + key.public_key.e.to_s(0) + key.public_key.n.to_s(0)
puts OpenSSL::Digest::MD5.hexdigest(data_string).scan(&#x2F;..&#x2F;).join(&#x27;:&#x27;)
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;&lt;em&gt;Please note: The above only works for RSA SSH keys.&lt;&#x2F;em&gt;&lt;&#x2F;p&gt;
&lt;p&gt;Calculating a SHA1 fingerprint for SSH hosts is as simple as replacing the
&#x27;MD5&#x27; class with &#x27;SHA1&#x27; or any of the other support digest algorithms.&lt;&#x2F;p&gt;
&lt;p&gt;The last one was the hardest to track down and implement, eventually I found
the answer in &lt;a href=&quot;https:&#x2F;&#x2F;www.ietf.org&#x2F;rfc&#x2F;rfc3279.txt&quot;&gt;RFC3279&lt;&#x2F;a&gt; under section 2.3.1 for the format of the public key
I would need to generate before performing a digest calculation on it.&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;ruby&quot; class=&quot;language-ruby &quot;&gt;&lt;code class=&quot;language-ruby&quot; data-lang=&quot;ruby&quot;&gt;require &#x27;openssl&#x27;

path_to_key = &#x27;&#x2F;tmp&#x2F;x509_key.pem&#x27;

key = OpenSSL::PKey::RSA.new(File.read(path_to_key))
data_string = OpenSSL::ASN1::Sequence([
  OpenSSL::ASN1::Integer.new(key.public_key.n),
  OpenSSL::ASN1::Integer.new(key.public_key.e)
])
puts OpenSSL::Digest::SHA1.hexdigest(data_string.to_der).scan(&#x2F;..&#x2F;).join(&#x27;:&#x27;)
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
</content>
	</entry>
	<entry xml:lang="en">
		<title>Disabling Gnome&#x27;s Keyring in Fedora 19</title>
		<published>2014-04-14T10:19:23-04:00</published>
		<updated>2014-04-14T10:19:23-04:00</updated>
		<link href="https://stelfox.net/blog/2014/disabling-gnomes-keyring-in-fedora-19/" type="text/html"/>
		<id>https://stelfox.net/blog/2014/disabling-gnomes-keyring-in-fedora-19/</id>
		<content type="html">&lt;p&gt;An update too Fedora a while ago started causing some unexpected behavior with
my dotfiles. Specifically the way I was handling my SSH agent. My SSH keys when
added to my agent automatically expire after a couple of hours.&lt;&#x2F;p&gt;
&lt;span id=&quot;continue-reading&quot;&gt;&lt;&#x2F;span&gt;
&lt;p&gt;After the update, when that expiration came I started receiving errors in my
shell that looked similar to the following (Since I fixed it I am not able to
get the exact working again):&lt;&#x2F;p&gt;
&lt;pre&gt;&lt;code&gt;Warning: Unable to connect to SSH agent
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;I also noticed that periodically I got a Gnome keyring pop-up asking for my SSH
agent rather than my command-line client. I&#x27;m personally not a big fan of
Gnome, but I deal with because it&#x27;s the default for Fedora, tends to stay out
of your way, and switching to something else is just not a project I&#x27;ve had
time for.&lt;&#x2F;p&gt;
&lt;p&gt;Now Gnome was very much getting in my way. I dealt with it for several months
now and finally got sick of it.&lt;&#x2F;p&gt;
&lt;p&gt;I tracked this down too the &lt;code&gt;gnome-keyring-daemon&lt;&#x2F;code&gt; which was starting up and
clobbering the contents of my &lt;code&gt;SSH_AUTH_SOCK&lt;&#x2F;code&gt; variable along with my
&lt;code&gt;GPG_AGENT_INFO&lt;&#x2F;code&gt; environment. Not very friendly.&lt;&#x2F;p&gt;
&lt;p&gt;There were a couple paths that I could&#x27;ve gone for for solving this situation.
The first, and easiest way to probably have dealt with this was too put some
logic into my &lt;code&gt;~&#x2F;.bashrc&lt;&#x2F;code&gt; file that detected when the &lt;code&gt;gnome-keying-agent&lt;&#x2F;code&gt; was
running, kill it and clean up after it. It might look something like this:&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;sh&quot; class=&quot;language-sh &quot;&gt;&lt;code class=&quot;language-sh&quot; data-lang=&quot;sh&quot;&gt;if [ -n &quot;${GNOME_KEYRING_PID}&quot; ]; then
  if $(kill -0 ${GNOME_KEYRING_PID}); then
    kill -9 ${GNOME_KEYRING_PID}
  fi
fi

unset GNOME_KEYRING_CONTROL SSH_AUTH_SOCK GPG_AGENT_INFO GNOME_KEYRING_PID
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;I share my dotfiles along a lot of different systems and don&#x27;t like
system-specific behavior getting in there. Instead I choose to find what was
starting up the keyring daemon and preventing it from doing so. Without a good
place to start and stubbornly refusing to Google this particular problem I took
the brute force approach of grep for the binary name in the &lt;code&gt;&#x2F;etc&lt;&#x2F;code&gt; directory.&lt;&#x2F;p&gt;
&lt;p&gt;Sure enough in &lt;code&gt;&#x2F;etc&#x2F;xdg&#x2F;autostart&lt;&#x2F;code&gt; I found a series of background daemons that
I definitely did not want nor need running. As root I ran the following command
to purge them from my system:&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;sh&quot; class=&quot;language-sh &quot;&gt;&lt;code class=&quot;language-sh&quot; data-lang=&quot;sh&quot;&gt;cd &#x2F;etc&#x2F;xdg&#x2F;autostart
rm -f gnome-keyring-{gpg,pkcs11,secrets,ssh}.desktop
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;The first solution will keep your system in a default state, but this will
permanently prevent the obnoxious behavior on your system for all users and
prevents you from adding hacks to your bashrc to work around misbehaving
software.&lt;&#x2F;p&gt;
&lt;p&gt;I hope this helps someone else!&lt;&#x2F;p&gt;
</content>
	</entry>
	<entry xml:lang="en">
		<title>One-Liner SSL Certificate Generation</title>
		<published>2014-03-28T14:52:51-04:00</published>
		<updated>2014-03-28T14:52:51-04:00</updated>
		<link href="https://stelfox.net/blog/2014/one-liner-ssl-certificate-generation/" type="text/html"/>
		<id>https://stelfox.net/blog/2014/one-liner-ssl-certificate-generation/</id>
		<content type="html">&lt;p&gt;I regularly find myself in need of generating a quick SSL key and certificate
pair. I&#x27;ve been using a one-liner for a while to generate these certificates.
No annoying user prompts just a quick fast certificate pair.&lt;&#x2F;p&gt;
&lt;span id=&quot;continue-reading&quot;&gt;&lt;&#x2F;span&gt;&lt;pre data-lang=&quot;sh&quot; class=&quot;language-sh &quot;&gt;&lt;code class=&quot;language-sh&quot; data-lang=&quot;sh&quot;&gt;echo -e &quot;XX\n\n \n \n\n$(hostname)\n\n&quot; | openssl req -new -x509 -newkey \
  rsa:2048 -keyout service.key -nodes -days 90 -out service.crt &amp;&gt; &#x2F;dev&#x2F;null
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;The cert uses the hostname of whatever machine you generated it on. It should
under no circumstances be used for a production service. It&#x27;s a 2048 bit key
and only valid for for roughly three months.&lt;&#x2F;p&gt;
</content>
	</entry>
	<entry xml:lang="en">
		<title>Preventing Tmux Lockups</title>
		<published>2014-03-28T12:56:12-04:00</published>
		<updated>2014-03-28T12:56:12-04:00</updated>
		<link href="https://stelfox.net/blog/2014/preventing-tmux-lockups/" type="text/html"/>
		<id>https://stelfox.net/blog/2014/preventing-tmux-lockups/</id>
		<content type="html">&lt;p&gt;Anyone that has used SSH, Tmux or Screen for a while will have inevitably
dumped excessive output to their terminal. Depending on the size of the output
you may have experienced the dreaded lockup. That horrible realization seconds
after you hit the command where signals just stop working and you just have to
sit there and wait for your terminal to catch up.&lt;&#x2F;p&gt;
&lt;span id=&quot;continue-reading&quot;&gt;&lt;&#x2F;span&gt;
&lt;p&gt;There is a piece of remote connection software called Mosh that I&#x27;ve been told
handles this pretty well, but I don&#x27;t yet trust its security model and it
doesn&#x27;t prevent the same thing from happening locally.&lt;&#x2F;p&gt;
&lt;p&gt;This is especially bad if you&#x27;re working in a multi-pane tmux window as it&#x27;s
locks up all the terminals in the same window, and prevents you from changing
to the other windows.&lt;&#x2F;p&gt;
&lt;p&gt;I&#x27;ve had this issue happen to me one too many times but never thought of
looking for a solution until a friend of mine, &lt;a href=&quot;https:&#x2F;&#x2F;gabekoss.com&#x2F;&quot;&gt;Gabe Koss&lt;&#x2F;a&gt;, made a passing
comment along the lines of &amp;quot;Too bad tmux can&#x27;t rate limit the output of a
terminal&amp;quot;.&lt;&#x2F;p&gt;
&lt;p&gt;A quick search through the doc and two relatively recent configuration options
popped out doing exactly what I was looking for (c0-change-internal, and
c0-change-trigger). Googling around for good values, left me wanting. A lot of
people were recommending setting the values to 100 and 250 respectively; These
are the defaults and since I still experience the issue are clearly not working
for me.&lt;&#x2F;p&gt;
&lt;p&gt;To set the variables to something more reasonable I had to understand what they
were doing. A &#x27;C0&#x27; sequence is one that modifies the screen beyond a normal
character sequence, think newlines, carriage returns, backspaces. According to
the tmux man page, the trigger will catch if the number of c0 sequences per
&lt;strong&gt;millisecond&lt;&#x2F;strong&gt; exceeds the number in the configuration file, at which point it
will start displaying an update once every interval number of milliseconds.&lt;&#x2F;p&gt;
&lt;p&gt;I can&#x27;t see faster than my eye&#x27;s refresh rate so that seems like a decent
starting point. According to &lt;a href=&quot;https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Frame_rate&quot;&gt;Wikipedia&lt;&#x2F;a&gt; the human eye&#x2F;brain interface can
process 10-12 images per second but we can notice &#x27;choppiness&#x27; below 48 FPS.
Since I won&#x27;t be reading anything flying by that fast I settled on a maximum
rate of 10 FPS updated in my shell, or an interval of &#x27;100ms&#x27;.&lt;&#x2F;p&gt;
&lt;p&gt;For the trigger I was significantly less scientific, I dropped the trigger by
50, reloaded my tmux configuration, &lt;code&gt;cat&lt;&#x2F;code&gt;&#x27;d a large file and tested whether I
could immediately kill the process and move between panes. I finally settled on
a value of &#x27;75&#x27; for the trigger rate. It does make the output seem a little
choppy but it is significantly nicer to not kill my terminal.&lt;&#x2F;p&gt;
&lt;p&gt;TL;DR Add the following lines to your &lt;code&gt;~&#x2F;.tmux.conf&lt;&#x2F;code&gt; file and you&#x27;ll be in a
much better shape:&lt;&#x2F;p&gt;
&lt;pre&gt;&lt;code&gt;setw -g c0-change-interval 50
setw -g c0-change-trigger  75
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
</content>
	</entry>
	<entry xml:lang="en">
		<title>Finding Ruby Subclasses</title>
		<published>2014-02-20T07:48:24-05:00</published>
		<updated>2014-02-20T07:48:24-05:00</updated>
		<link href="https://stelfox.net/blog/2014/finding-ruby-subclasses/" type="text/html"/>
		<id>https://stelfox.net/blog/2014/finding-ruby-subclasses/</id>
		<content type="html">&lt;p&gt;While working through a problem I found it would be immensely useful to be able
to enumerate all of the current subclasses of a particular class. After
thinking about this for a while I settled on a good old friend of mine,
&lt;code&gt;ObjectSpace&lt;&#x2F;code&gt;.&lt;&#x2F;p&gt;
&lt;span id=&quot;continue-reading&quot;&gt;&lt;&#x2F;span&gt;
&lt;p&gt;For those not familiar with the &lt;code&gt;ObjectSpace&lt;&#x2F;code&gt; module, it is a means to inspect
and access the items being tracked by Ruby&#x27;s garbage collector. This means it
has a hook into every living object, and more dangerously, every near-death
object.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;code&gt;ObjectSpace&lt;&#x2F;code&gt; provides a method for enumerating instances of a specific class,
specifically named &lt;code&gt;each_object&lt;&#x2F;code&gt; which takes a class. With Ruby all classes are
in fact instances of the &lt;code&gt;Class&lt;&#x2F;code&gt; class. This allows us to enumerate every
available class by passing it to the enumerator like so:&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;ruby&quot; class=&quot;language-ruby &quot;&gt;&lt;code class=&quot;language-ruby&quot; data-lang=&quot;ruby&quot;&gt;ObjectSpace.each_object(Class).to_a
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;Alright so we now have an array of every single class that could possibly be
instantiated, how do we narrow it down to just the ones we&#x27;re interested in?
Once again Ruby provides with the &lt;code&gt;ancestors&lt;&#x2F;code&gt; method, combine that with a
select and we can quickly narrow it down. You can see it in the following
example:&lt;&#x2F;p&gt;
&lt;pre&gt;&lt;code&gt;[1] pry(main)&gt; TargetSubclass = Class.new(String)
=&gt; TargetSubclass
[2] pry(main)&gt; ObjectSpace.each_object(Class).select { |k| k.ancestors.include?(String) }
=&gt; [String, TargetSubclass]
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;Hmm, that&#x27;s not quite right though. We have found all the subclasses but we&#x27;ve
also grabbed the parent class. With one small modification we eliminate that as
well.&lt;&#x2F;p&gt;
&lt;pre&gt;&lt;code&gt;[1] pry(main)&gt; TargetSubclass = Class.new(String)
=&gt; TargetSubclass
[2] pry(main)&gt; ObjectSpace.each_object(Class).select { |k| k.ancestors.include?(String) &amp;&amp; k != String }
=&gt; [TargetSubclass]
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;That line is rather long though, and I generally like to avoid multiple tests
in a select block. There is a tad bit of syntactic sugar provided by Ruby
allowing us to accomplish the same thing, our final example is ultimately the
solution I went with:&lt;&#x2F;p&gt;
&lt;pre&gt;&lt;code&gt;[1] pry(main)&gt; TargetSubclass = Class.new(String)
=&gt; TargetSubclass
[2] pry(main)&gt; ObjectSpace.each_object(Class).select { |k| k &lt; String }
=&gt; [TargetSubclass]
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;Putting this into a method:&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;ruby&quot; class=&quot;language-ruby &quot;&gt;&lt;code class=&quot;language-ruby&quot; data-lang=&quot;ruby&quot;&gt;def subclasses(klass)
  ObjectSpace.each_object(Class).select { |k| k &lt; klass }
end
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;If you were so inclined you could extend the &lt;code&gt;Class&lt;&#x2F;code&gt; class with a method to
make this available anywhere like so:&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;ruby&quot; class=&quot;language-ruby &quot;&gt;&lt;code class=&quot;language-ruby&quot; data-lang=&quot;ruby&quot;&gt;class Class
  def self.subclasses
    ObjectSpace.each_object(Class).select { |k| k &lt; self }
  end
end
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;I&#x27;m personally not a fan of extending any of the core classes unless absolutely
necessary, but too each there own.&lt;&#x2F;p&gt;
</content>
	</entry>
	<entry xml:lang="en">
		<title>Creating Crypt Style SHA512 Passwords With Ruby</title>
		<published>2014-02-17T15:28:27-05:00</published>
		<updated>2014-02-17T15:28:27-05:00</updated>
		<link href="https://stelfox.net/blog/2014/creating-crypt-style-sha512-passwords-with-ruby/" type="text/html"/>
		<id>https://stelfox.net/blog/2014/creating-crypt-style-sha512-passwords-with-ruby/</id>
		<content type="html">&lt;p&gt;I needed to generate crypt-style SHA512 passwords in ruby for an &lt;code&gt;&#x2F;etc&#x2F;shadow&lt;&#x2F;code&gt;
file. After a bunch of Googling and messing around with the OpenSSL library I
finally found a very simple built-in way to handle this.&lt;&#x2F;p&gt;
&lt;span id=&quot;continue-reading&quot;&gt;&lt;&#x2F;span&gt;&lt;pre data-lang=&quot;ruby&quot; class=&quot;language-ruby &quot;&gt;&lt;code class=&quot;language-ruby&quot; data-lang=&quot;ruby&quot;&gt;require &#x27;securerandom&#x27;

&#x27;password&#x27;.crypt(&#x27;$6$&#x27; + SecureRandom.random_number(36 ** 8).to_s(36))
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;You&#x27;ll get a string that looks like:&lt;&#x2F;p&gt;
&lt;pre&gt;&lt;code&gt;$6$4dksjo1b$Lt194Dwy7r&#x2F;7WbM8MezYZysmGcxjaiisgTrTBbHkyBZFXeqQTG0J5hep4wLM&#x2F;AmYxlGNLRy0OWATLDZCqjwCk.
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;If you don&#x27;t want to use the &lt;code&gt;SecureRandom&lt;&#x2F;code&gt; module you can replace the random
call with simply &lt;code&gt;rand(36 ** 8)&lt;&#x2F;code&gt; though this isn&#x27;t recommended.&lt;&#x2F;p&gt;
&lt;p&gt;Enjoy!&lt;&#x2F;p&gt;
</content>
	</entry>
	<entry xml:lang="en">
		<title>Setting Linux System Timezone</title>
		<published>2014-02-01T13:50:46-05:00</published>
		<updated>2014-02-01T13:50:46-05:00</updated>
		<link href="https://stelfox.net/blog/2014/setting-linux-system-timezone/" type="text/html"/>
		<id>https://stelfox.net/blog/2014/setting-linux-system-timezone/</id>
		<content type="html">&lt;p&gt;I change the timezone on the linux systems so rarely that I almost always have
to look it up. I&#x27;m writing it up here for my own personal reference. With any
luck it&#x27;ll also help others.&lt;&#x2F;p&gt;
&lt;span id=&quot;continue-reading&quot;&gt;&lt;&#x2F;span&gt;
&lt;p&gt;The system timezone is controlled by the &lt;code&gt;&#x2F;etc&#x2F;localtime&lt;&#x2F;code&gt; file and is generally
symlinked to locale files stored in &lt;code&gt;&#x2F;usr&#x2F;share&#x2F;zoneinfo&lt;&#x2F;code&gt;. Generally I like to
keep my systems on UTC as I my machines are in several timezones and it makes
all the logs have consistent times.&lt;&#x2F;p&gt;
&lt;p&gt;To set the system time to UTC you&#x27;d run the following command as root:&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;sh&quot; class=&quot;language-sh &quot;&gt;&lt;code class=&quot;language-sh&quot; data-lang=&quot;sh&quot;&gt;ln -sf &#x2F;usr&#x2F;share&#x2F;zoneinfo&#x2F;UTC &#x2F;etc&#x2F;localtime
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;Other timezones can be found in the &lt;code&gt;&#x2F;usr&#x2F;share&#x2F;zoneinfo&lt;&#x2F;code&gt; and are generally
broken up by continent with a few exceptions.&lt;&#x2F;p&gt;
&lt;p&gt;As a user it&#x27;s obviously more useful to see the time in my local timezone and
this can be overridden on a per-user basis using the &lt;code&gt;TZ&lt;&#x2F;code&gt; environment variable.
I stick this in my &lt;code&gt;~&#x2F;.bashrc&lt;&#x2F;code&gt; file and it just works transparently:&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;sh&quot; class=&quot;language-sh &quot;&gt;&lt;code class=&quot;language-sh&quot; data-lang=&quot;sh&quot;&gt;export TZ=&quot;America&#x2F;Los_Angeles&quot;
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
</content>
	</entry>
	<entry xml:lang="en">
		<title>Starting Puppet Master on Fedora 19</title>
		<published>2014-01-18T22:47:37-05:00</published>
		<updated>2014-01-18T22:47:37-05:00</updated>
		<link href="https://stelfox.net/blog/2014/starting-puppetmaster-on-fedora-19/" type="text/html"/>
		<id>https://stelfox.net/blog/2014/starting-puppetmaster-on-fedora-19/</id>
		<content type="html">&lt;p&gt;I was trying to get puppet running out of the box on Fedora 19 and found a bug
exists in their systemd service file. After installing &lt;code&gt;puppet&lt;&#x2F;code&gt; and
&lt;code&gt;puppet-server&lt;&#x2F;code&gt;, whenever I tried to start the server with the following
command:&lt;&#x2F;p&gt;
&lt;span id=&quot;continue-reading&quot;&gt;&lt;&#x2F;span&gt;&lt;pre data-lang=&quot;sh&quot; class=&quot;language-sh &quot;&gt;&lt;code class=&quot;language-sh&quot; data-lang=&quot;sh&quot;&gt;systemctl start puppetmaster.service
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;It would hang for a long time and the following error message would show up in
the log:&lt;&#x2F;p&gt;
&lt;pre&gt;&lt;code&gt;Jan 19 03:42:18 puppet-01 puppet-master[1166]: Starting Puppet master version 3.3.1
Jan 19 03:42:18 puppet-01 systemd[1]: PID file &#x2F;run&#x2F;puppet&#x2F;master.pid not readable (yet?) after start.
Jan 19 03:43:07 puppet-01 systemd[1]: puppetmaster.service operation timed out. Terminating.
Jan 19 03:43:07 puppet-01 puppet-master[1166]: Could not run: can&#x27;t be called from trap context
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;Starting puppet directly from the command line using the same command specified
in the service file would work fine, but that wasn&#x27;t really a solution. Turns
out puppet, additionally I would briefly see the &lt;code&gt;puppetmaster&lt;&#x2F;code&gt; service open up
port 8140 before systemd would kill it.&lt;&#x2F;p&gt;
&lt;p&gt;Turns out the systemd service script is looking in the wrong location for the
pid file. All of the pids are stored in &lt;code&gt;&#x2F;var&#x2F;run&#x2F;puppet&#x2F;&lt;&#x2F;code&gt; with a filename of
either &lt;code&gt;agent.pid&lt;&#x2F;code&gt; or &lt;code&gt;master.pid&lt;&#x2F;code&gt; depending on the mode it was run as. The
systemd script, as the log indicates is looking for the pid files in
&lt;code&gt;&#x2F;run&#x2F;puppet&lt;&#x2F;code&gt;.&lt;&#x2F;p&gt;
&lt;p&gt;The real solution would be to bring this too the attention of the script
maintainers, but I haven&#x27;t had a lot of luck going through those processes.
Instead you can work around the issue without any bureaucracy by changing the
&lt;code&gt;rundir&lt;&#x2F;code&gt; configuration option in &lt;code&gt;&#x2F;etc&#x2F;puppet&#x2F;puppet.conf&lt;&#x2F;code&gt; to &lt;code&gt;&#x2F;run&#x2F;puppet&lt;&#x2F;code&gt;,
and creating &lt;code&gt;&#x2F;run&#x2F;puppet&lt;&#x2F;code&gt; (with puppet as the user and group owning the
directory).&lt;&#x2F;p&gt;
&lt;p&gt;After that, voila! The service starts up. You&#x27;d think a QA process would catch
that the service script doesn&#x27;t work...&lt;&#x2F;p&gt;
</content>
	</entry>
	<entry xml:lang="en">
		<title>Playing With the Linux Bluetooth Stack</title>
		<published>2013-12-24T14:53:27-05:00</published>
		<updated>2013-12-24T14:53:27-05:00</updated>
		<link href="https://stelfox.net/blog/2013/playing-with-the-linux-bluetooth-stack/" type="text/html"/>
		<id>https://stelfox.net/blog/2013/playing-with-the-linux-bluetooth-stack/</id>
		<content type="html">&lt;p&gt;List all available bluetooth interfaces:&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;sh&quot; class=&quot;language-sh &quot;&gt;&lt;code class=&quot;language-sh&quot; data-lang=&quot;sh&quot;&gt;hciconfig -a
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;span id=&quot;continue-reading&quot;&gt;&lt;&#x2F;span&gt;
&lt;p&gt;If you get an error like the following:&lt;&#x2F;p&gt;
&lt;pre&gt;&lt;code&gt;Operation not possible due to RF-kill
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;You&#x27;ll need to unblock access to the resource using &lt;code&gt;rfkill&lt;&#x2F;code&gt;. You can unblock
all blocked devices like so:&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;sh&quot; class=&quot;language-sh &quot;&gt;&lt;code class=&quot;language-sh&quot; data-lang=&quot;sh&quot;&gt;rfkill unblock all
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;Before doing any iBeacon stuff you should disable scanning:&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;sh&quot; class=&quot;language-sh &quot;&gt;&lt;code class=&quot;language-sh&quot; data-lang=&quot;sh&quot;&gt;hciconfig hci0 noscan
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;pre data-lang=&quot;sh&quot; class=&quot;language-sh &quot;&gt;&lt;code class=&quot;language-sh&quot; data-lang=&quot;sh&quot;&gt;hcitool -i hci0 cmd 0x08 0x0008 1E 02 01 1A 1A FF 4C 00 02 15 [ 92 77 83 0A B2 EB 49 0F A1 DD 7F E3 8C 49 2E DE ] [ 00 00 ] [ 00 00 ] C5 00
hcitool -i hci0 leadv
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;pre&gt;&lt;code&gt;LE set advertise enable on hci1 returned status 12
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
</content>
	</entry>
	<entry xml:lang="en">
		<title>Updating BMC on Dell PowerEdge C6100</title>
		<published>2013-12-16T21:26:13-05:00</published>
		<updated>2013-12-16T21:26:13-05:00</updated>
		<link href="https://stelfox.net/blog/2013/updating-bmc-on-dell-poweredge-c6100/" type="text/html"/>
		<id>https://stelfox.net/blog/2013/updating-bmc-on-dell-poweredge-c6100/</id>
		<content type="html">&lt;p&gt;I just received my Dell PowerEdge C6100 and found it&#x27;s software quite a bit
outdated. After searching around quite a bit I found the resources lacking for
explaining how to perform these updates. So in this post I&#x27;m going to quickly
cover updating the BMC firmware on each blade.&lt;&#x2F;p&gt;
&lt;span id=&quot;continue-reading&quot;&gt;&lt;&#x2F;span&gt;
&lt;p&gt;The system I received had four different versions of the BMC software
installed, additionally Two were branded as MegaRAC and the others branded as
Dell. This update didn&#x27;t fix the branding (and I&#x27;d love to remove the Dell
branding as it&#x27;s kind of annoying) it did, however, fix a number of other
issues that I was experiencing such as:&lt;&#x2F;p&gt;
&lt;ol&gt;
&lt;li&gt;Console Redirection failing to connect&lt;&#x2F;li&gt;
&lt;li&gt;BMC losing it&#x27;s network connection after a couple of minutes&lt;&#x2F;li&gt;
&lt;li&gt;Slow responses, with occasional failures to load pages&lt;&#x2F;li&gt;
&lt;li&gt;Remote IPMI tools being unable to read sensors status&lt;&#x2F;li&gt;
&lt;&#x2F;ol&gt;
&lt;p&gt;The first step is too download the latest version of of the BMC software from
Dell&#x27;s support site (I&#x27;ve also taken the liberty of &lt;a href=&quot;https:&#x2F;&#x2F;static.stelfox.net&#x2F;files&#x2F;PEC6100BMC130.exe&quot;&gt;hosting a copy
myself&lt;&#x2F;a&gt;). I recommend you go through the process of entering the service tag
of each of the blades and make sure that Dell recognizes them as existing even
if they&#x27;re out of support.&lt;&#x2F;p&gt;
&lt;p&gt;There has been mention of versions of these blades that had custom
modifications for DCS and any attempts to modify the BIOS or BMC will likely
cause you to end up bricking the remote management board or the motherboard.&lt;&#x2F;p&gt;
&lt;p&gt;Even with the regular board there is always a risk of bricking it, though
firmware updates have gotten a lot more reliable and I haven&#x27;t experienced an
incorrectly flashed motherboard in years. You&#x27;ve been warned.&lt;&#x2F;p&gt;
&lt;p&gt;The BMC was fairly straight forward. I installed the 64-bit version of Fedora
19 on a thumb drive, downloaded version 1.30 of the BMC software (get the file
named &lt;code&gt;PEC6100BMC130.exe&lt;&#x2F;code&gt;). The file itself is a self-extracting zip archive
which can be extracted using the regular unzip utility.&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;sh&quot; class=&quot;language-sh &quot;&gt;&lt;code class=&quot;language-sh&quot; data-lang=&quot;sh&quot;&gt;unzip PEC6100BMC130.exe
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;Inside you&#x27;ll find two folders, KCSFlash and SOCFlash should both be put on the
live drive within the KCSFlash. You&#x27;ll need to set the execute bit on the
contents of the linux directory and the linux.sh file. You&#x27;ll also need to
install the &lt;code&gt;glibc.i686&lt;&#x2F;code&gt; package. Afterwards it&#x27;s as simple as booting each
chassis off the drive and as root run the linux.sh script.&lt;&#x2F;p&gt;
&lt;p&gt;If the KCSFlash fails, the SOCFlash will more likely than not work but it is
slightly more dangerous. If you need it mark the &lt;code&gt;linux&#x2F;flash8.sh&lt;&#x2F;code&gt;,
&lt;code&gt;linux&#x2F;socflash&lt;&#x2F;code&gt;, and &lt;code&gt;linux&#x2F;socflash_x64&lt;&#x2F;code&gt; as executable in the SOCFlash folder
and run the flash8.sh script.&lt;&#x2F;p&gt;
&lt;p&gt;After that you&#x27;re going to want to reboot into the BIOS and ensure the IPMI
ethernet port is set to dedicated, as this switched it back to &amp;quot;Shared&amp;quot; on me.&lt;&#x2F;p&gt;
</content>
	</entry>
	<entry xml:lang="en">
		<title>Updating the BIOS on Dell PowerEdge C6100</title>
		<published>2013-12-16T09:39:02-05:00</published>
		<updated>2013-12-16T09:39:02-05:00</updated>
		<link href="https://stelfox.net/blog/2013/updating-the-bios-on-dell-poweredge-c6100/" type="text/html"/>
		<id>https://stelfox.net/blog/2013/updating-the-bios-on-dell-poweredge-c6100/</id>
		<content type="html">&lt;p&gt;The BIOS was quite a bit more complicated and there was a few options that I
had available to try, all of which require either Windows or DOS environments.
I don&#x27;t have any legal copies of Windows to put on my server and didn&#x27;t want to
go through all that effort.&lt;&#x2F;p&gt;
&lt;span id=&quot;continue-reading&quot;&gt;&lt;&#x2F;span&gt;
&lt;p&gt;It really needs to be done within a DOS environment. I downloaded the file
&lt;code&gt;PEC6100BIOS017000.exe&lt;&#x2F;code&gt; from Dell&#x27;s support website (&lt;a href=&quot;http:&#x2F;&#x2F;static.stelfox.net&#x2F;files&#x2F;PEC6100BIOS017000.exe&quot;&gt;locally hosted copy&lt;&#x2F;a&gt;)
as well as the 2.88Mb version of FreeDOS (&lt;a href=&quot;http:&#x2F;&#x2F;static.stelfox.net&#x2F;files&#x2F;FDSTD.288.imz&quot;&gt;locally hosted copy&lt;&#x2F;a&gt;).&lt;&#x2F;p&gt;
</content>
	</entry>
	<entry xml:lang="en">
		<title>Using Dnsmasq as a Standalone TFTP Server</title>
		<published>2013-12-12T18:29:46-05:00</published>
		<updated>2013-12-12T18:29:46-05:00</updated>
		<link href="https://stelfox.net/blog/2013/using-dnsmasq-as-a-standalone-tftp-server/" type="text/html"/>
		<id>https://stelfox.net/blog/2013/using-dnsmasq-as-a-standalone-tftp-server/</id>
		<content type="html">&lt;p&gt;&lt;em&gt;If you&#x27;ve come across this blog post with the intention of setting up TFTP on
an modern version of OpenWRT I have a &lt;a href=&quot;https:&#x2F;&#x2F;stelfox.net&#x2F;blog&#x2F;2014&#x2F;using-openwrts-dnsmasq-as-a-tftp-server&#x2F;&quot;&gt;more recent blog post&lt;&#x2F;a&gt; detailing how
too configure your system.&lt;&#x2F;em&gt;&lt;&#x2F;p&gt;
&lt;p&gt;I found myself in need of a TFTP server but wanted to avoid having all of the
xinet.d packages and services on my system (even if they were disabled). While
looking for alternatives I found out that &lt;code&gt;dnsmasq&lt;&#x2F;code&gt; has a built-in read-only
TFTP server.&lt;&#x2F;p&gt;
&lt;span id=&quot;continue-reading&quot;&gt;&lt;&#x2F;span&gt;
&lt;p&gt;I already have a DNS and DHCP server on my network and didn&#x27;t want dnsmasq to
take on either of those roles so my first challenge was finding a way to
prevent dnsmasq from running those bits of it&#x27;s code, or failing that I would
just firewall off the service. Luckily it&#x27;s quite easy to disable both bits of
functionality.&lt;&#x2F;p&gt;
&lt;p&gt;For DHCP you simply have to leave out any of the dhcp option in the
configuration file, DNS you just tell it to operate on port 0 and it will be
disabled.&lt;&#x2F;p&gt;
&lt;p&gt;So my whole configuration starting out looks like this:&lt;&#x2F;p&gt;
&lt;pre&gt;&lt;code&gt;# Disable DNS
port=0
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;Now I need to configure the TFTP bits of dnsmasq. This too was rather simple
only requiring me to add the following to my already terse configuration file:&lt;&#x2F;p&gt;
&lt;pre&gt;&lt;code&gt;# Enable the TFTP server
enable-tftp
tftp-root=&#x2F;var&#x2F;lib&#x2F;tftp
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;I created the root directory for my TFTP server and started it up with the
following commands:&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;bash&quot; class=&quot;language-bash &quot;&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;mkdir &#x2F;var&#x2F;lib&#x2F;tftp
systemctl enable dnsmasq.service
systemctl start dnsmasq.service
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;Voila, TFTP running and happy. If you have a firewall running you&#x27;ll also want
to open ports &lt;code&gt;69&#x2F;tcp&lt;&#x2F;code&gt; and &lt;code&gt;69&#x2F;udp&lt;&#x2F;code&gt; (though I suspect only the UDP one is
needed).&lt;&#x2F;p&gt;
</content>
	</entry>
	<entry xml:lang="en">
		<title>Configuring PXE Booting on OpenWRT</title>
		<published>2013-12-11T08:40:44-05:00</published>
		<updated>2013-12-11T08:40:44-05:00</updated>
		<link href="https://stelfox.net/blog/2013/configuring-pxe-booting-on-openwrt/" type="text/html"/>
		<id>https://stelfox.net/blog/2013/configuring-pxe-booting-on-openwrt/</id>
		<content type="html">&lt;p&gt;I needed to support PXE booting on my home network. I use OpenWRT as my main
router and DHCP server and it took me a bit of searching how to configure the
BOOTP next server to redirect local clients to my Arch TFTP&#x2F;NFS server for
booting, so I&#x27;m placing the configuration here to help others who might be
looking to do the same thing.&lt;&#x2F;p&gt;
&lt;span id=&quot;continue-reading&quot;&gt;&lt;&#x2F;span&gt;
&lt;p&gt;It&#x27;s worth noting that this isn&#x27;t a guide on setting up PXE booting completely
on an OpenWRT, you&#x27;ll need another system that is running a configured TFTP
server. I&#x27;ll write up how I setup my Arch box as a TFTP server at a later date.&lt;&#x2F;p&gt;
&lt;p&gt;The configuration itself was very simple; You just need to add a couple lines
to &lt;code&gt;&#x2F;etc&#x2F;config&#x2F;dhcp&lt;&#x2F;code&gt;. You&#x27;ll want to replace 10.0.0.45 with whatever your
local TFTP server is.&lt;&#x2F;p&gt;
&lt;pre&gt;&lt;code&gt;config boot linux
  option filename      &#x27;pxelinux.0&#x27;
  option serveraddress &#x27;10.0.0.45&#x27;
  option servername    &#x27;Arch-Pixie&#x27;
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;The filename &lt;code&gt;pxelinux.0&lt;&#x2F;code&gt; is from Syslinux, and the &lt;code&gt;servername&lt;&#x2F;code&gt; has no
technical meaning, but it provides nice information to the clients. In this
case I&#x27;ve used the name of my Arch linux server that I&#x27;ll be booting off of.&lt;&#x2F;p&gt;
&lt;p&gt;Hope this helps someone out. Cheers!&lt;&#x2F;p&gt;
</content>
	</entry>
	<entry xml:lang="en">
		<title>Running Emails Through Ruby</title>
		<published>2013-12-08T09:32:05-05:00</published>
		<updated>2013-12-08T09:32:05-05:00</updated>
		<link href="https://stelfox.net/blog/2013/running-emails-through-ruby/" type="text/html"/>
		<id>https://stelfox.net/blog/2013/running-emails-through-ruby/</id>
		<content type="html">&lt;p&gt;Following up on my &lt;a href=&quot;https:&#x2F;&#x2F;stelfox.net&#x2F;blog&#x2F;2013&#x2F;backing-up-gmail-with-fetchmail&#x2F;&quot;&gt;earlier post&lt;&#x2F;a&gt; where I covered how to backup your Gmail
account using &lt;code&gt;fetchmail&lt;&#x2F;code&gt; and &lt;code&gt;procmail&lt;&#x2F;code&gt;; I wanted to cover how I was
additionally processing received mail through ruby.&lt;&#x2F;p&gt;
&lt;p&gt;This was part of a larger project where I was doing statistical analysis on my
email while evaluating various data stores. To get the emails into the various
data stores, I used the ruby script to parse, process and store the emails as
they came in.&lt;&#x2F;p&gt;
&lt;span id=&quot;continue-reading&quot;&gt;&lt;&#x2F;span&gt;
&lt;p&gt;If you&#x27;re going to be doing any form of mail manipulation or statistics I
highly recommend the &lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;mikel&#x2F;mail&quot;&gt;mail&lt;&#x2F;a&gt; gem. It did almost everything I needed out of
the box, though it didn&#x27;t correctly enumerate any of the additional headers.&lt;&#x2F;p&gt;
&lt;p&gt;Procmail is a highly flexible mail filtering and local delivery agent. Without
much effort you can pass the mail it is handling through a series of filters
which can manipulate and reject mail before eventually delivering it to your
inbox. In light of this, we&#x27;re going to make a filter that simply counts the
total number of emails the script has processed, and add a header to the
message that indicates this count.&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;ruby&quot; class=&quot;language-ruby &quot;&gt;&lt;code class=&quot;language-ruby&quot; data-lang=&quot;ruby&quot;&gt;#!&#x2F;usr&#x2F;bin&#x2F;env ruby

require &#x27;mail&#x27;

# Get the email message from STDIN or a passed filename
message = &quot;&quot;
while input = ARGF.gets
  message += input
end

# Parse the email into a ruby object
msg = Mail.new(message)

# Location of our count file
count_file = &quot;#{ENV[&#x27;HOME&#x27;]}&#x2F;.mail_counter.txt&quot;

# Load or initialize our count value and increment it
count = File.exists?(count_file) ? File.read(count_file).to_i : 0
count += 1

# Update our count on disk
File.write(count_file, count.to_s)

# Add our header with the count
msg.header.fields &lt;&lt; Mail::Field.new(&quot;X-Mail-Counter: #{count}&quot;)

# Output the now modified message back out to $stdout
begin
  $stdout.puts msg.to_s
rescue Errno::EPIPE
  exit(74)
end
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;Make sure you mark the script executable after saving it.&lt;&#x2F;p&gt;
&lt;p&gt;If you followed along with &lt;a href=&quot;https:&#x2F;&#x2F;stelfox.net&#x2F;blog&#x2F;2013&#x2F;backing-up-gmail-with-fetchmail&#x2F;&quot;&gt;my earlier post&lt;&#x2F;a&gt; the only change we need to make
is to add our ruby mail processor as a procmail filter. I&#x27;ve stored the script
in &lt;code&gt;~&#x2F;.bin&#x2F;mail-counter.rb&lt;&#x2F;code&gt;, if you&#x27;ve stored it in a different location you&#x27;ll
want to update your path to reflect that.&lt;&#x2F;p&gt;
&lt;p&gt;Filters in procmail are handled by using the pipe helper. The following is a
minimum working example of a &lt;code&gt;procmailrc&lt;&#x2F;code&gt; file to make use of our filter:&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;bash&quot; class=&quot;language-bash &quot;&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;MAILDIR=$HOME
VERBOSE=on

:0fw
| &#x2F;home&#x2F;sstelfox&#x2F;Documents&#x2F;ruby&#x2F;riak-mail-indexer&#x2F;counter.rb

:0
Maildir&#x2F;
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;Store the above file in &lt;code&gt;~&#x2F;.procmailrc&lt;&#x2F;code&gt;. The next time you run &lt;code&gt;fetchmail&lt;&#x2F;code&gt;
those headers will be added to the messages before being delivered and you can
watch the count increment by looking at the contents of &lt;code&gt;~&#x2F;.mail_counter.txt&lt;&#x2F;code&gt;.&lt;&#x2F;p&gt;
&lt;p&gt;The following are a few additional sources I made use of while writing this
article:&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;http:&#x2F;&#x2F;stackoverflow.com&#x2F;questions&#x2F;273262&#x2F;best-practices-with-stdin-in-ruby&lt;&#x2F;li&gt;
&lt;li&gt;http:&#x2F;&#x2F;www.jstorimer.com&#x2F;blogs&#x2F;workingwithcode&#x2F;7766125-writing-ruby-scripts-that-respect-pipelines&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
</content>
	</entry>
	<entry xml:lang="en">
		<title>Access GET Parameters With Coffeescript</title>
		<published>2013-12-07T18:20:58-05:00</published>
		<updated>2013-12-07T18:20:58-05:00</updated>
		<link href="https://stelfox.net/blog/2013/access-get-parameters-with-coffeescript/" type="text/html"/>
		<id>https://stelfox.net/blog/2013/access-get-parameters-with-coffeescript/</id>
		<content type="html">&lt;p&gt;I&#x27;ve been working on a pure javascript based search engine for this static
website and needed to access a get parameter within the URL.&lt;&#x2F;p&gt;
&lt;span id=&quot;continue-reading&quot;&gt;&lt;&#x2F;span&gt;
&lt;p&gt;I found a few solutions online but they usually made use of jQuery or weren&#x27;t
in coffeescript. A few others would only extract an individual named parameter
at a time. The following will return all of them in Javascript&#x27;s equivalent of
a hash (or dictionary if you prefer) in the form of an object.&lt;&#x2F;p&gt;
&lt;pre&gt;&lt;code&gt;getParams = -&gt;
  query = window.location.search.substring(1)
  raw_vars = query.split(&quot;&amp;&quot;)

  params = {}

  for v in raw_vars
    [key, val] = v.split(&quot;=&quot;)
    params[key] = decodeURIComponent(val)

  params

console.log(getParams())
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;If compiled and included in a page the above will print out the parameters as a
hash object to the console.&lt;&#x2F;p&gt;
</content>
	</entry>
	<entry xml:lang="en">
		<title>Downloading Google Mail and Calendar Data</title>
		<published>2013-12-05T11:04:18-05:00</published>
		<updated>2013-12-05T11:04:18-05:00</updated>
		<link href="https://stelfox.net/blog/2013/downloading-google-mail-and-calendar-data/" type="text/html"/>
		<id>https://stelfox.net/blog/2013/downloading-google-mail-and-calendar-data/</id>
		<content type="html">&lt;p&gt;I [recently posted][1] a guide on backing up your Gmail with &lt;code&gt;fetchmail&lt;&#x2F;code&gt;. This
unfortunately doesn&#x27;t include your calendar data. It seems like backing up was
a hot enough topic that the Google Gmail team are [releasing an official backup
method][2].  It&#x27;s not completely in the wild yet but I definitely look forward
to poking around in it.&lt;&#x2F;p&gt;
&lt;span id=&quot;continue-reading&quot;&gt;&lt;&#x2F;span&gt;
&lt;p&gt;Now if only Google let you download everything they know about you as well...
Would definitely make for an interesting read.&lt;&#x2F;p&gt;
&lt;p&gt;[1]: {{&amp;lt; ref &amp;quot;.&#x2F;2013-11-19-backing-up-gmail-with-fetchmail.md&amp;quot; &amp;gt;}}
[2]: http:&#x2F;&#x2F;gmailblog.blogspot.com&#x2F;2013&#x2F;12&#x2F;download-copy-of-your-gmail-and-google.html&lt;&#x2F;p&gt;
</content>
	</entry>
	<entry xml:lang="en">
		<title>Taking Back the Sky</title>
		<published>2013-12-04T13:18:15-05:00</published>
		<updated>2013-12-04T13:18:15-05:00</updated>
		<link href="https://stelfox.net/blog/2013/taking-back-the-sky/" type="text/html"/>
		<id>https://stelfox.net/blog/2013/taking-back-the-sky/</id>
		<content type="html">&lt;p&gt;During my daily review of various new sources I came across one particular
article that was both concerning and very amusing. Drones have been getting
more and more popular, and more accessible. They&#x27;ve been getting used by the
military, law enforcement, &lt;a href=&quot;http:&#x2F;&#x2F;www.cnn.com&#x2F;2013&#x2F;12&#x2F;02&#x2F;tech&#x2F;innovation&#x2F;amazon-drones-questions&#x2F;&quot;&gt;recently Amazon&lt;&#x2F;a&gt; (though they&#x27;ve abandoned that
for now), you can even purchase one for your iPhone at airports.&lt;&#x2F;p&gt;
&lt;span id=&quot;continue-reading&quot;&gt;&lt;&#x2F;span&gt;
&lt;p&gt;The security of these systems hasn&#x27;t been thoroughly tested publicly, though
&lt;a href=&quot;http:&#x2F;&#x2F;rt.com&#x2F;news&#x2F;iran-us-drone-gulf-216&#x2F;&quot;&gt;there is at least&lt;&#x2F;a&gt; one report of a military drone being stolen already.
With the beginnings of &lt;a href=&quot;http:&#x2F;&#x2F;www.fastcompany.com&#x2F;3019913&#x2F;watch-the-skies-tonight-for-a-taco-delivering-drone-brought-to-you-by-taco-bell&quot;&gt;various commercial uses&lt;&#x2F;a&gt; of drones, expanding beyond
hobbiests exploring &lt;a href=&quot;http:&#x2F;&#x2F;gizmodo.com&#x2F;5947033&#x2F;this-team-of-quadrocopters-can-throw-and-catch-better-than-you&quot;&gt;what they can do with them&lt;&#x2F;a&gt;.&lt;&#x2F;p&gt;
&lt;p&gt;Someone had to start the testing eventually and Samy Kamkar took the lead this
time with his new project &lt;a href=&quot;http:&#x2F;&#x2F;samy.pl&#x2F;skyjack&#x2F;&quot;&gt;Skyjack&lt;&#x2F;a&gt;. The article that tipped me off to this
project can be found &lt;a href=&quot;http:&#x2F;&#x2F;threatpost.com&#x2F;how-to-skyjack-drones-in-an-hour-for-less-than-400&#x2F;103086&quot;&gt;over on Threatpost&lt;&#x2F;a&gt;.&lt;&#x2F;p&gt;
&lt;p&gt;The gist of the project is a drone that can forcibly disconnect other drone&#x27;s
controller and take it&#x27;s place to &amp;quot;steal&amp;quot; the drone.&lt;&#x2F;p&gt;
&lt;p&gt;I can imagine a whole cyberpunk thriller action scene where corporate
anarchists hijack a drone and use it&#x27;s trusted status within the drone&#x27;s
network to hack in and take control of the entire CNC drone system to further
their goals.&lt;&#x2F;p&gt;
</content>
	</entry>
	<entry xml:lang="en">
		<title>Fail Fast in Bash Scripts</title>
		<published>2013-11-26T15:19:40-05:00</published>
		<updated>2013-11-26T15:19:40-05:00</updated>
		<link href="https://stelfox.net/blog/2013/fail-fast-in-bash-scripts/" type="text/html"/>
		<id>https://stelfox.net/blog/2013/fail-fast-in-bash-scripts/</id>
		<content type="html">&lt;p&gt;I found myself writing another bash script that should exit should any of the
few commands within it fail to run. As I began writing some error handling
after each command, and isolating the sections into bash functions I figured
there had to be a better way. After a little Googling and a trip through the
bash man pages sure enough:&lt;&#x2F;p&gt;
&lt;span id=&quot;continue-reading&quot;&gt;&lt;&#x2F;span&gt;&lt;pre data-lang=&quot;sh&quot; class=&quot;language-sh &quot;&gt;&lt;code class=&quot;language-sh&quot; data-lang=&quot;sh&quot;&gt;#!&#x2F;bin&#x2F;bash

function error_handler() {
  echo &quot;Error occurred in script at line: ${1}.&quot;
  echo &quot;Line exited with status: ${2}&quot;
}

trap &#x27;error_handler ${LINENO} $?&#x27; ERR

set -o errexit
set -o errtrace
set -o errpipe
set -o nounset

echo &quot;Everything is running fine...&quot;

# A command outside of a conditional that will always return a exit code of 1
test 1 -eq 0

echo &quot;This will never run, as a command has failed&quot;
echo &quot;Using unset variable ${TEST} will also cause this script to exit&quot;
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;The first piece of that is setting up an error handler that will get run
whenever an error condition occurs with the script. You can use this section to
roll back any changes or cleanup your environment as well as give you some
debug information about the failure.&lt;&#x2F;p&gt;
&lt;p&gt;I&#x27;m then setting a few bash options, The following is a description taken more
or less directly from the bash man pages:&lt;&#x2F;p&gt;
&lt;blockquote&gt;
&lt;p&gt;-o errexit: Exit immediately if a pipeline (which may consist of a single
simple command), a subshell command enclosed in parentheses, or one of the
commands executed as part of a command list enclosed by braces exits with a
non-zero status.&lt;&#x2F;p&gt;
&lt;p&gt;-o errtrace: If set, any trap on ERR is inherited by shell functions, command
substitutions, and commands execute in a subshell environment.&lt;&#x2F;p&gt;
&lt;p&gt;-o nounset: Treat unset variables and parameters other than the special
parameters &amp;quot;@&amp;quot; and &amp;quot;*&amp;quot; as an error when performing parameter expansion.&lt;&#x2F;p&gt;
&lt;&#x2F;blockquote&gt;
&lt;p&gt;If anything goes wrong in the script it will fail once, fail fast, and let you
know where it died.&lt;&#x2F;p&gt;
</content>
	</entry>
	<entry xml:lang="en">
		<title>Using VIM as Your Password Manager</title>
		<published>2013-11-25T15:10:46-05:00</published>
		<updated>2013-11-25T15:10:46-05:00</updated>
		<link href="https://stelfox.net/blog/2013/using-vim-as-your-password-manager/" type="text/html"/>
		<id>https://stelfox.net/blog/2013/using-vim-as-your-password-manager/</id>
		<content type="html">&lt;p&gt;There are all kinds of password managers out there. Everything from web
services that are quite solid and respectable, to native desktop apps.&lt;&#x2F;p&gt;
&lt;p&gt;A lot of these are simply too heavy for me, involve installing software on a
computer to access in addition to sharing the file around, or required you to
remember multiple account details before you could get access to any individual
password.&lt;&#x2F;p&gt;
&lt;span id=&quot;continue-reading&quot;&gt;&lt;&#x2F;span&gt;
&lt;p&gt;Due too the various complexities and lack of matching use cases a couple years
ago I set out to develop my own open-source version of PassPack. In the interim
though I needed a solution for keeping track of my hundreds of various accounts
and their passwords.&lt;&#x2F;p&gt;
&lt;p&gt;Around this time I was ramping up my usage of vim and happened to come across a
very fortunate command entirely by accident. Enter &lt;em&gt;vimcrypt&lt;&#x2F;em&gt;.&lt;&#x2F;p&gt;
&lt;p&gt;For any plaintext file you, while in command mode you can type the command &lt;code&gt;:X&lt;&#x2F;code&gt;
and it will ask you for a password to encrypt your file with. By default this
uses a remarkably weak algorithm called &lt;a href=&quot;https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;PKZIP&quot;&gt;pkzip&lt;&#x2F;a&gt; which isn&#x27;t secure enough
for me to trust it with my keys.&lt;&#x2F;p&gt;
&lt;p&gt;Since vim 7.3 and later, &lt;code&gt;:X&lt;&#x2F;code&gt; has also supported an additional cipher; The much
stronger blowfish algorithm. You can enable this by running the command &lt;code&gt;:set cryptmethod=blowfish&lt;&#x2F;code&gt;. I chose to add the following lines to my &lt;code&gt;~&#x2F;.vimrc&lt;&#x2F;code&gt;
file:&lt;&#x2F;p&gt;
&lt;pre&gt;&lt;code&gt;&quot; When encrypting any file, use the much stronger blowfish algorithm
set cryptmethod=blowfish
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;This was a fantastic interim solution as I have yet to find a development or
production linux system that hasn&#x27;t been excessively locked down (and probably
not somewhere I&#x27;d put my password file anyway) that didn&#x27;t already have vim
installed.&lt;&#x2F;p&gt;
&lt;p&gt;Using this personally required me coming up with a pseudo-file format that
would allow me to quickly and easily find the credentials I needed. I settled
on the simple format shown off below:&lt;&#x2F;p&gt;
&lt;pre&gt;&lt;code&gt;Oneline Account Description
  Site: &lt;URL of Site&#x27;s login page&gt;
  Username: &lt;username for the site&gt;
  Password: &lt;password for the site&gt;
  Email: &lt;email I used to register&gt;

  Login with: &lt;email|username&gt; # Only necessary when I have both

  ** Address on file **
  ** Phone on file **
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;You&#x27;ll notice I also used this to keep track of whether an account had physical
information tied to it. When I moved this made it very quick for me to search
for accounts that I needed to update with my new mailing address.&lt;&#x2F;p&gt;
&lt;p&gt;As with many solutions this &amp;quot;temporary&amp;quot; one became more and more permanent as
my motivation to build the Passpack competitor dwindled. My problem had been
solved and I was no longer compelled to put any effort into a solution.&lt;&#x2F;p&gt;
&lt;p&gt;If this still isn&#x27;t strong enough for your tastes, the &lt;a href=&quot;http:&#x2F;&#x2F;vim.wikia.com&#x2F;wiki&#x2F;Encryption&quot;&gt;vim wiki&lt;&#x2F;a&gt; has some
additional ways you can encrypt your files. These all require additional setup
and failed my requirements in that they generally require additional files or
setup before I can access my passwords.&lt;&#x2F;p&gt;
&lt;p&gt;Hope this helps some other weary CLI warrior some trouble. Cheers!&lt;&#x2F;p&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;Update&lt;&#x2F;strong&gt;&lt;&#x2F;em&gt;: I received a recommendation from a user named &lt;a href=&quot;http:&#x2F;&#x2F;www.reddit.com&#x2F;r&#x2F;vim&#x2F;comments&#x2F;1rg3ji&#x2F;wrote_up_my_thoughts_on_using_vim_as_a_password&#x2F;cdn20o8&quot;&gt;sigzero&lt;&#x2F;a&gt; over
on Reddit. For additional security they added the following line to their
&lt;code&gt;~&#x2F;.vimrc&lt;&#x2F;code&gt; file.&lt;&#x2F;p&gt;
&lt;pre&gt;&lt;code&gt;autocmd BufReadPost * if &amp;key != &quot;&quot; | set noswapfile nowritebackup viminfo= nobackup noshelltemp history=0 secure | endif
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;It disables additional files that vim may write copies to such as swap files
and backups, prevents dangerous shell commands, and prevents vim from storing a
history of commands.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;Update 2&lt;&#x2F;strong&gt;&lt;&#x2F;em&gt;: I received another recommendation from another reddit user,
this time from &lt;a href=&quot;http:&#x2F;&#x2F;www.reddit.com&#x2F;r&#x2F;vim&#x2F;comments&#x2F;1rg3ji&#x2F;wrote_up_my_thoughts_on_using_vim_as_a_password&#x2F;cdnn94z&quot;&gt;NinlyOne&lt;&#x2F;a&gt;.  At their recommendation, I&#x27;ve prepended the
following modeline to my password.  It automatically folds each password entry
to prevent potential shoulder surfing. You can open up an entry using the
command &lt;code&gt;zo&lt;&#x2F;code&gt; and close it back up with &lt;code&gt;zc&lt;&#x2F;code&gt;. It&#x27;s worth noting that this is
tied to my indented file format.&lt;&#x2F;p&gt;
&lt;pre&gt;&lt;code&gt;# vim: fdm=indent fdn=1 sw=2:
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
</content>
	</entry>
	<entry xml:lang="en">
		<title>Backing up Gmail with fetchmail</title>
		<published>2013-11-19T09:55:40-05:00</published>
		<updated>2013-11-19T09:55:40-05:00</updated>
		<link href="https://stelfox.net/blog/2013/backing-up-gmail-with-fetchmail/" type="text/html"/>
		<id>https://stelfox.net/blog/2013/backing-up-gmail-with-fetchmail/</id>
		<content type="html">&lt;p&gt;This morning I found myself in need of a large set of emails to test a
particular set of code. Ideally these emails would be broken out into easily
digestible pieces, and it was strictly for my own personal testing so I wasn&#x27;t
concerned with using my own live data for this test (There will probably be
another post on this project later on).&lt;&#x2F;p&gt;
&lt;span id=&quot;continue-reading&quot;&gt;&lt;&#x2F;span&gt;
&lt;p&gt;Having used &lt;code&gt;fetchmail&lt;&#x2F;code&gt; with good results in the past I decided it was a good
idea to take this opportunity to also backup my Gmail account into the common
&lt;code&gt;Maildir&lt;&#x2F;code&gt; format (which essentially breaks out emails into individual files
meeting my requirements).&lt;&#x2F;p&gt;
&lt;p&gt;The first step was to enable POP access to my account through Gmail&#x27;s
interface. You can accomplish this with the following steps.&lt;&#x2F;p&gt;
&lt;ol&gt;
&lt;li&gt;Login to Gmail&lt;&#x2F;li&gt;
&lt;li&gt;Click on the gear icon&lt;&#x2F;li&gt;
&lt;li&gt;Choose settings&lt;&#x2F;li&gt;
&lt;li&gt;Forwarding and POP&#x2F;IMAP&lt;&#x2F;li&gt;
&lt;li&gt;Enable POP for all mail&lt;&#x2F;li&gt;
&lt;li&gt;When messages are accessed with POP... Keep&amp;quot;&lt;&#x2F;li&gt;
&lt;li&gt;Save Changes.&lt;&#x2F;li&gt;
&lt;&#x2F;ol&gt;
&lt;p&gt;Ensure you have &lt;code&gt;fetchmail&lt;&#x2F;code&gt; and &lt;code&gt;procmail&lt;&#x2F;code&gt; installed. For me on Fedora this can
be accomplished using yum by running the following commands:&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;sh&quot; class=&quot;language-sh &quot;&gt;&lt;code class=&quot;language-sh&quot; data-lang=&quot;sh&quot;&gt;sudo yum install fetchmail procmail -y
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;We need to configure fetchmail to let it know where to retrieve our mail from.
This configuration file lives at &lt;code&gt;$HOME&#x2F;.fetchmailrc&lt;&#x2F;code&gt;. By default fetchmail
will send all retrieved mail to the local SMTP server over a normal TCP
connection. This isn&#x27;t necessary or ideal, rather we&#x27;ll additionally supply a
local mail delivery agent (procmail) to handle processing the mail into the
Maildir format.&lt;&#x2F;p&gt;
&lt;pre&gt;&lt;code&gt;poll pop.gmail.com
protocol pop3
timeout 300
port 995
username &quot;full_email@withdomain.tld&quot; password &quot;yourpassword&quot;
keep
ssl
sslcertck
sslproto TLS1
mda &quot;&#x2F;usr&#x2F;bin&#x2F;procmail -m &#x27;&#x2F;home&#x2F;&lt;username&gt;&#x2F;.procmailrc&#x27;&quot;
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;Be sure to set the permissions on the &lt;code&gt;.fetchmailrc&lt;&#x2F;code&gt; file to 0600:&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;sh&quot; class=&quot;language-sh &quot;&gt;&lt;code class=&quot;language-sh&quot; data-lang=&quot;sh&quot;&gt;chmod 0600 $HOME&#x2F;.fetchmailrc
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;We&#x27;ll now need to configure procmail to properly deliver our mail to the local
&lt;code&gt;Maildir&lt;&#x2F;code&gt; folder. Procmail&#x27;s configuration by default lives in
&lt;code&gt;$HOME&#x2F;.procmailrc&lt;&#x2F;code&gt;&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;sh&quot; class=&quot;language-sh &quot;&gt;&lt;code class=&quot;language-sh&quot; data-lang=&quot;sh&quot;&gt;LOGFILE=$HOME&#x2F;.procmail.log
MAILDIR=$HOME
VERBOSE=on

:0
Maildir&#x2F;
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;With that done, simply run the &lt;code&gt;fetchmail&lt;&#x2F;code&gt; command. In my experience this can
take a while process and it seems like Google limits the number of emails you
can download at a time, so you may need to run the command a couple of times to
get all your emails.&lt;&#x2F;p&gt;
</content>
	</entry>
	<entry xml:lang="en">
		<title>Ruby&#x27;s Option Parser - a More Complete Example</title>
		<published>2012-12-02T22:59:00-05:00</published>
		<updated>2012-12-02T22:59:00-05:00</updated>
		<link href="https://stelfox.net/blog/2012/rubys-option-parser-a-more-complete-example/" type="text/html"/>
		<id>https://stelfox.net/blog/2012/rubys-option-parser-a-more-complete-example/</id>
		<content type="html">&lt;p&gt;Recently while writing a Ruby program I needed to parse some command line
options. Helpfully Ruby provides a module named &lt;code&gt;OptionParser&lt;&#x2F;code&gt; to make this
easy. I found a few parts of the documentation ambiguous and a few others down
right confusing.&lt;&#x2F;p&gt;
&lt;span id=&quot;continue-reading&quot;&gt;&lt;&#x2F;span&gt;
&lt;p&gt;The catch I hit was the required field. In my mind the definition of a required
argument is something that needs to be passed on the command line to continue.
What&lt;code&gt;OptionParser&lt;&#x2F;code&gt; actually means is that a value isn&#x27;t required when the
argument is passed.&lt;code&gt;OptionParser&lt;&#x2F;code&gt; already provides boolean switches, so when
someone would use an optional switch is beyond me.&lt;&#x2F;p&gt;
&lt;p&gt;To make it a little more clear and to have something to work from in the future
I created the following chunk of code that includes a Configuration singleton
that can be used anywhere within your codebase to access the run-time
configuration, a sample parser with a wide range of different types of options,
and it will load configuration from a file named &lt;code&gt;config.yml&lt;&#x2F;code&gt; in the same
directory.&lt;&#x2F;p&gt;
&lt;p&gt;I feel like the following is a much more complete explanation of how
&lt;code&gt;OptionParser&lt;&#x2F;code&gt; is supposed to be used with supporting code.&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;ruby&quot; class=&quot;language-ruby &quot;&gt;&lt;code class=&quot;language-ruby&quot; data-lang=&quot;ruby&quot;&gt;#!&#x2F;usr&#x2F;bin&#x2F;env ruby

# This file provides an example of creating a command line application with a
# wide variety of command line options, parsing and the like as well as global
# configuration singleton that can be relied on throughout a program.
#
# This entire setup lives within the &quot;Example&quot; module. These are really common
# names and it would be a shame to override required functionality in other code
# that wasn&#x27;t properly namespaced.

require &#x27;optparse&#x27;
require &#x27;singleton&#x27;
require &#x27;yaml&#x27;

module Example
  # Defines the available configuration options for the configuration
  ConfigurationStruct = Struct.new(:enum, :list, :required, :optional, :verbose, :float)

  class Configuration
    include Singleton

    # Initialize the configuration and set defaults:
    @@config = ConfigurationStruct.new

    # This is where the defaults are being set
    @@config.enum = :one
    @@config.list = []
    @@config.optional = nil
    @@config.verbose = false

    def self.config
      yield(@@config) if block_given?
      @@config
    end

    # Loads a YAML configuration file and sets each of the configuration values to
    # whats in the file.
    def self.load(file)
      YAML::load_file(file).each do |key, value|
        self.send(&quot;#{key}=&quot;, value)
      end
    end

    # This provides an easy way to dump the configuration as a hash
    def self.to_hash
      Hash[@@config.each_pair.to_a]
    end

    # Pass any other calls (most likely attribute setters&#x2F;getters on to the
    # configuration as a way to easily set&#x2F;get attribute values 
    def self.method_missing(method, *args, &amp;block)
      if @@config.respond_to?(method)
        @@config.send(method, *args, &amp;block)
      else
        raise NoMethodError
      end
    end

    # Handles validating the configuration that has been loaded&#x2F;configured
    def self.validate!
      valid = true

      valid = false if Configuration.required.nil?

      raise ArgumentError unless valid
    end
  end

  class ConfigurationParser
    def self.parse(args)
      opts = OptionParser.new do |parser|

        parser.separator &quot;&quot;
        parser.separator &quot;Specific options:&quot;

        parser.on(&quot;--enum ENUM&quot;, [:one, :two, :three], &quot;This field requires one of a set of predefined values be&quot;, &quot;set. If wrapped in brackets this option can be set to nil.&quot;) do |setting|
          Configuration.enum = setting
        end

        parser.on(&quot;-l&quot;, &quot;--list x,y&quot;, Array, &quot;This command flag takes a comma separated list (without&quot;, &quot;spaces) of values and turns it into an array. This requires&quot;, &quot;at least one argument.&quot;) do |setting|
          Configuration.list = setting
        end

        parser.on(&quot;--[no-]verbose&quot;, &quot;This is a common boolean flag, setting verbosity to either&quot;, &quot;true or false.&quot;) do |setting|
          Configuration.verbose = setting
        end

        parser.on(&quot;--optional [STR]&quot;, &quot;This command doesn&#x27;t require a string to be passed to it, if&quot;, &quot;nothing is passed it will be nil. No error will be raised if&quot;, &quot;nothing is passed to it that logic needs to be handled&quot;, &quot;yourself.&quot;) do |setting|
          Configuration.optional = setting
        end

        parser.on(&quot;-r&quot;, &quot;--required STR&quot;, &quot;This command requires a string to be passed to it.&quot;) do |setting|
          Configuration.required = setting
        end

        parser.on(&quot;--float NUM&quot;, Float, &quot;This command will only accept an integer or a float.&quot;) do |setting|
          Configuration.float = setting
        end

        parser.on_tail(&quot;-h&quot;, &quot;--help&quot;, &quot;--usage&quot;, &quot;Show this usage message and quit.&quot;) do |setting|
          puts parser.help
          exit
        end

        parser.on_tail(&quot;-v&quot;, &quot;--version&quot;, &quot;Show version information about this program and quit.&quot;) do
          puts &quot;Option Parser Example v1.0.0&quot;
          exit
        end
      end

      opts.parse!(args)
    end
  end
end

if File.exists?(&quot;config.yml&quot;)
  Example::Configuration.load(&quot;config.yml&quot;)
end

Example::ConfigurationParser.parse(ARGV)
Example::Configuration.validate!

require &quot;json&quot;
puts JSON.pretty_generate(Example::Configuration.to_hash)
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
</content>
	</entry>
	<entry xml:lang="en">
		<title>Keep Your Gems Updated</title>
		<published>2012-11-27T14:17:00-05:00</published>
		<updated>2012-11-27T14:17:00-05:00</updated>
		<link href="https://stelfox.net/blog/2012/keep-your-gems-updated/" type="text/html"/>
		<id>https://stelfox.net/blog/2012/keep-your-gems-updated/</id>
		<content type="html">&lt;p&gt;I recently went back through my backups recently and found quite a few old
abandoned projects. Looking back on the code I see some things I&#x27;m impressed
with, but the majority of the code I wouldn&#x27;t write today. That&#x27;s not to say
the code is bad, or doesn&#x27;t function. It did exactly what I wanted to
accomplish at the time, just not necessarily in the most efficient way.&lt;&#x2F;p&gt;
&lt;span id=&quot;continue-reading&quot;&gt;&lt;&#x2F;span&gt;
&lt;p&gt;This archive of old code made me start wondering how much old code I&#x27;m using in
the projects that I&#x27;m currently writing. Not code that I&#x27;ve written but code
that I&#x27;m depending on, specifically gems. As of this writing I have 26 active
ruby projects in various states of development all of which make use of RVM and
&lt;code&gt;bundler&lt;&#x2F;code&gt;.&lt;&#x2F;p&gt;
&lt;p&gt;Conveniently enough, &lt;code&gt;bundler&lt;&#x2F;code&gt; provides an easy way to update all the gems
installed in a project unless specific version information was provided in the
&lt;code&gt;Gemfile&lt;&#x2F;code&gt;. None of my projects have had a version directly specified in the
&lt;code&gt;Gemfile&lt;&#x2F;code&gt; with the exception of Rails. Each project also has solid test
coverage (though I must admit it&#x27;s usually not complete).&lt;&#x2F;p&gt;
&lt;p&gt;For each project I went through and ran &lt;code&gt;bundle update&lt;&#x2F;code&gt; and kept track of the
results. I did not keep track of unique gems so the four Rails projects
probably had a lot of duplicate gems each one more or less likely to have
different versions of different gems installed depending on when I started the
project.&lt;&#x2F;p&gt;
&lt;p&gt;Across all of the different projects I had 2214 gems installed. Of those 813
had updates. My initial plan was to go through the updates and see how many of
those updates were security or bug fixes, how many were added features, or
performance improvements, but I wasn&#x27;t counting on the shear number of gems
that my projects were depending on.&lt;&#x2F;p&gt;
&lt;p&gt;The big question for myself after I updated the Gems was how much will this be
now? Running through the thousands of tests in all of the projects I had
exactly 7 tests that were now failing and they were all due too projects that
removed or renamed a piece of functionality that I was making use of. In one
case I had to extend the core Hash method to replace the functionality. All in
all it took me about a quarter of an hour to fix all the tests after updating
my Gems.&lt;&#x2F;p&gt;
&lt;p&gt;Since I didn&#x27;t actually go through all of the gems I don&#x27;t know for sure that
my projects are in anyway more secure, faster, or more stable but I can&#x27;t
imagine they&#x27;re in a worse state. If you have test coverage on your projects
you should try and update the gems and see for yourself.&lt;&#x2F;p&gt;
</content>
	</entry>
	<entry xml:lang="en">
		<title>Auditing Heroku SSH Keys</title>
		<published>2012-11-26T10:18:00-05:00</published>
		<updated>2012-11-26T10:18:00-05:00</updated>
		<link href="https://stelfox.net/blog/2012/auditing-heroku-ssh-keys/" type="text/html"/>
		<id>https://stelfox.net/blog/2012/auditing-heroku-ssh-keys/</id>
		<content type="html">&lt;p&gt;A good friend of mine recently left the organization I work for and the task of
resetting our passwords and auditing credentials fell on me. Since we use
Heroku for our development platform I needed to not only reset the credentials
for the web portion (which conveniently also handles resetting the API key) but
also revoke any SSH keys he may have added to access it.&lt;&#x2F;p&gt;
&lt;span id=&quot;continue-reading&quot;&gt;&lt;&#x2F;span&gt;
&lt;p&gt;Sadly Heroku does not seem to provide any web interface that I could find for
examining what keys were associated with the account. Searching for this
information also didn&#x27;t turn up very valuable results; most people were looking
to add keys or resolve issues with missing keys rather than revoking them. I
suspect not many people think of SSH keys when it comes time to revoke access
which is a dire mistake.&lt;&#x2F;p&gt;
&lt;p&gt;I took to the command line to solve my issue as I knew you could list and add
keys that way, so it was a minor leap of logic to assume they could revoke keys
as well. I ran &lt;code&gt;heroku help keys&lt;&#x2F;code&gt; to get the syntax for the commands and was
pleasantly surprised to see an additional option listed in there:&lt;&#x2F;p&gt;
&lt;pre&gt;&lt;code&gt;keys:clear       #  remove all authentication keys from the current user
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;As a now two person web-shop it&#x27;s not a terrible amount of work to add our keys
back in and looking through there were already some keys in there that should
have been revoked long ago. One command and our applications were safe from
mischief, though I know my former associate wouldn&#x27;t abuse that privilege
beyond perhaps pointing out the security flaw I&#x27;d allowed.&lt;&#x2F;p&gt;
</content>
	</entry>
	<entry xml:lang="en">
		<title>CarrierWave, S3 and Filenames</title>
		<published>2012-08-09T20:00:57+00:00</published>
		<updated>2012-08-09T20:00:57+00:00</updated>
		<link href="https://stelfox.net/blog/2012/carrierwave-s3-and-filenames/" type="text/html"/>
		<id>https://stelfox.net/blog/2012/carrierwave-s3-and-filenames/</id>
		<content type="html">&lt;p&gt;This is going to be a real quick post. I&#x27;m using the &lt;code&gt;carrier_wave&lt;&#x2F;code&gt; gem with
&lt;code&gt;fog&lt;&#x2F;code&gt; for one of my projects and found that when a file is stored on S3 the
&lt;code&gt;identifier&lt;&#x2F;code&gt;, and &lt;code&gt;filename&lt;&#x2F;code&gt; methods return nil. I got around this issue in two
separate ways neither of which I&#x27;m particularly happy about.&lt;&#x2F;p&gt;
&lt;span id=&quot;continue-reading&quot;&gt;&lt;&#x2F;span&gt;
&lt;p&gt;Outside of the uploader, you can use the File utility and the URL of the object
to get the base filename like so:&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;ruby&quot; class=&quot;language-ruby &quot;&gt;&lt;code class=&quot;language-ruby&quot; data-lang=&quot;ruby&quot;&gt;File.basename(Model.asset.url)
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;If you try and do this within the uploader itself like this:&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;ruby&quot; class=&quot;language-ruby &quot;&gt;&lt;code class=&quot;language-ruby&quot; data-lang=&quot;ruby&quot;&gt;File.basename(self.url)
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;It will work, but not when creating additional versions such as thumbnails as
the file hasn&#x27;t actually been created yet so a URL can&#x27;t be built and you&#x27;ll
get an error trying to perform &lt;code&gt;File.basename(nil)&lt;&#x2F;code&gt;. You&#x27;d need to go back up
to the model and get the normal version&#x27;s URL like so:&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;ruby&quot; class=&quot;language-ruby &quot;&gt;&lt;code class=&quot;language-ruby&quot; data-lang=&quot;ruby&quot;&gt;File.basename(self.model.asset.url)
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;Now if you&#x27;re trying to get the file name to build part of the store_dir,
you&#x27;ve just created an infinite loop! Ruby will be happy to tell you that the
stack level too deep (&lt;code&gt;SystemStackError&lt;&#x2F;code&gt;). So ultimately how did I end up
getting it into my store_dir?&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;ruby&quot; class=&quot;language-ruby &quot;&gt;&lt;code class=&quot;language-ruby&quot; data-lang=&quot;ruby&quot;&gt;self.model.attributes[&quot;asset&quot;]
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;The file name gets stored raw directly in the database, and thus you can pull
it out by accessing the value directly without going through the accessor that
get overridden by CarrierWave. I&#x27;m pretty sure this is a bug, and will report
it with example code and a test (as is appropriate for any bug report &lt;em&gt;hint&lt;&#x2F;em&gt;)
as soon as my dead line has passed.&lt;&#x2F;p&gt;
</content>
	</entry>
	<entry xml:lang="en">
		<title>Security Through Obesity</title>
		<published>2012-08-08T15:06:56+00:00</published>
		<updated>2012-08-08T15:06:56+00:00</updated>
		<link href="https://stelfox.net/blog/2012/security-through-obesity/" type="text/html"/>
		<id>https://stelfox.net/blog/2012/security-through-obesity/</id>
		<content type="html">&lt;p&gt;Jeremy Spilman recently &lt;a href=&quot;http:&#x2F;&#x2F;www.opine.me&#x2F;a-better-way-to-store-password-hashes&#x2F;&quot;&gt;proposed changes&lt;&#x2F;a&gt; to how user&#x27;s hashes are stored
in website&#x27;s and companies databases. This post was originally going to look at
some of the issues involved in the scheme he envisioned, however, he rather
quickly posted a &lt;a href=&quot;http:&#x2F;&#x2F;www.opine.me&#x2F;all-your-hashes-arent-belong-to-us&#x2F;&quot;&gt;followup article&lt;&#x2F;a&gt; with a well thought out solution that
countered all of the issues that other people and myself were able to come up
with. I&#x27;d strongly recommend reading both if you haven&#x27;t done so. Instead of
announcing flaws, I&#x27;m turning this into a post with a simple functional
implementation of the described scheme in Ruby using DataMapper.&lt;&#x2F;p&gt;
&lt;span id=&quot;continue-reading&quot;&gt;&lt;&#x2F;span&gt;
&lt;p&gt;At first I&#x27;d like to point out that this is one of those few examples where a
form of security through obscurity is actually increasing not only the
perceived security but the cost to attack a system as well.&lt;&#x2F;p&gt;
&lt;p&gt;Please note this code is a minimal, functional, example and should not be used
in production. It is missing a lot of things that I personally would add before
attempting to use this but that is an exercise for the reader. It is licensed
under the MIT license. I&#x27;ll walk through the code briefly afterwards going over
some bits.&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;ruby&quot; class=&quot;language-ruby &quot;&gt;&lt;code class=&quot;language-ruby&quot; data-lang=&quot;ruby&quot;&gt;# encoding: utf-8

require &#x27;rubygems&#x27;           # You only need this if you use bundler
require &#x27;dm-core&#x27;
require &#x27;dm-migrations&#x27;
require &#x27;dm-sqlite-adapter&#x27;
require &#x27;dm-validations&#x27;
require &#x27;scrypt&#x27;

DataMapper.setup(:default, &#x27;sqlite:hash.db&#x27;)

class User
  include DataMapper::Resource 

  property :id,             Serial
  property :username,       String, :required =&gt; true, :unique =&gt; true
  property :crypt_hash,     String, :required =&gt; true, :length =&gt; 64
  property :salt,           String, :required =&gt; true, :length =&gt; 25

  def check_password(plaintext_password)
    encrypted_hash = scrypt_helper(plaintext_password, self.salt)
    hash_obj = SiteHash.first(:crypt_hash =&gt; encrypted_hash)

    if hash_obj.nil?
      puts &#x27;Invalid password&#x27;
      return false
    end

    verification_hash = scrypt_helper(plaintext_password, hash_obj.salt)

    if self.crypt_hash == verification_hash
      return true
    else
      puts &#x27;WARNING: Found matching hash, but verification failed.&#x27;
      return false
    end
  end

  def password=(plaintext_password)
    generate_salt

    encrypted_password = SiteHash.new
    encrypted_password.crypt_hash = scrypt_helper(plaintext_password, self.salt)
    encrypted_password.save

    self.crypt_hash = scrypt_helper(plaintext_password, encrypted_password.salt)
  end

  private

  def generate_salt
    self.salt = SCrypt::Engine.generate_salt(:max_time =&gt; 1.0)
  end

  def scrypt_helper(plaintext_password, salt)
    SCrypt::Engine.scrypt(plaintext_password, salt, SCrypt::Engine.autodetect_cost(salt), 32).unpack(&#x27;H*&#x27;).first
  end
end

class SiteHash
  include DataMapper::Resource

  property :id,             Serial
  property :crypt_hash,     String,   :required =&gt; true, :length =&gt; 64
  property :salt,           String,   :required =&gt; true, :length =&gt; 25

  def initialize(*args)
    super
    generate_salt
  end

  private

  def generate_salt
    self.salt = SCrypt::Engine.generate_salt(:max_time =&gt; 1.0)
  end
end

DataMapper.finalize
DataMapper.auto_upgrade!
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;I tried to keep this as a simple minimum implementation without playing golf.
Strictly speaking the validations on the data_mapper models aren&#x27;t necessary
and could have been removed, in this case, however, the length fields do
actually indicate a bit more of what you might expect to see in the database,
while the requires are just good habits living on.&lt;&#x2F;p&gt;
&lt;p&gt;Both of the two models are required to have both a salt and a hash, the name
&#x27;crypt_hash&#x27; was chosen do too a conflict with one of data_mapper&#x27;s reserved
words &#x27;hash&#x27;, the same goes for the model name, however, that class comes from
elsewhere. Raw scrypt&#x27;d hashes are 256 bits long or 64 hex characters long,
while the salts are 64 bits (16 hex characters) plus some meta-data totaling 25
hex characters in this example.&lt;&#x2F;p&gt;
&lt;p&gt;Salts are hashes are computed by the &#x27;scrypt&#x27; gem. In this example I&#x27;ve bumped
up the max time option to create a hash from the default of 0.2 seconds up to 1
second. This is one of those things that I could have left out as the default
is fine for an example, but it also couldn&#x27;t hurt slightly increasing it in
case someone did copy-paste this into production.&lt;&#x2F;p&gt;
&lt;p&gt;The one thing that I&#x27;d like to point out is a couple of &#x27;puts&#x27; statements I
dropped in the check_password method on the User model. The first one simply
announces an invalid password. A lot of these could indicate a brute force
attack. The second one is more serious, it indicates that there is either a bug
in the code, a hash collision has occurred, or an attacker has been able to
drop in hash of their choosing into the site_hashes table, but haven&#x27;t updated
the verification hash on the user model yet. I&#x27;d strongly recommend reading
through both of Jeremy&#x27;s posts if you want to understand how this threat works
and specifically the second post to see how the verification hash protects what
it does.&lt;&#x2F;p&gt;
&lt;p&gt;So how would you use this code? Well you&#x27;d want to create a user with a
password and then check if their password is valid or not later on like so:&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;ruby&quot; class=&quot;language-ruby &quot;&gt;&lt;code class=&quot;language-ruby&quot; data-lang=&quot;ruby&quot;&gt;User.create(:username =&gt; &#x27;admin&#x27;, :password =&gt; &#x27;admin&#x27;)
User.first(:username =&gt; &#x27;admin&#x27;).check_password(&#x27;admin&#x27;)
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;One of the key ways this separation increases the security of real users hashes
is by having a large number of fake hashes in the hash table that the attackers
will have to crack at the same time. As a bonus I&#x27;ve written a module to handle
just that for the code I&#x27;ve already provided. Once again this is licensed under
the MIT license and should not be considered production ready.&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;ruby&quot; class=&quot;language-ruby &quot;&gt;&lt;code class=&quot;language-ruby&quot; data-lang=&quot;ruby&quot;&gt;# This is the code above, you can also include everything below
# this in the same file if you&#x27;re into that sort of thing
require &#x27;user_hash_example&#x27;

module HashFaker
  def self.fast_hash
    SiteHash.create(:crypt_hash =&gt; get_bytes(32))
  end

  def self.hash
    SiteHash.create(:crypt_hash =&gt; scrypt_helper(get_bytes(24), generate_salt))
  end

  def self.generate_hashes(count = 5000, fast = false)
    count.times { fast ? fast_hash : hash }
  end

  private

  def self.generate_salt
    SCrypt::Engine.generate_salt(:max_time =&gt; 1.0)
  end

  def self.get_bytes(num)
    OpenSSL::Random.random_bytes(num).unpack(&#x27;H*&#x27;).first
  end

  def self.scrypt_helper(plaintext_password, salt)
    SCrypt::Engine.scrypt(plaintext_password, salt, SCrypt::Engine.autodetect_cost(salt), 32).unpack(&#x27;H*&#x27;).first
  end
end
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
</content>
	</entry>
	<entry xml:lang="en">
		<title>Adding a Table Prefix to DataMapper Tables</title>
		<published>2012-08-07T14:09:33+00:00</published>
		<updated>2012-08-07T14:09:33+00:00</updated>
		<link href="https://stelfox.net/blog/2012/adding-a-table-prefix-to-datamapper-tables/" type="text/html"/>
		<id>https://stelfox.net/blog/2012/adding-a-table-prefix-to-datamapper-tables/</id>
		<content type="html">&lt;p&gt;So I recently encountered a situation where I needed to define a prefix on the
tables used by the &amp;quot;data_mapper&amp;quot; gem. When I went searching I found quite a bit
of information about similar projects in Python, and PHP named DataMapper but
nothing about the ruby &amp;quot;data_mapper&amp;quot;. The search continued eventually ending in
my reading through the source of the data_mapper gem only to find that there
was no feature for simply defining a prefix.&lt;&#x2F;p&gt;
&lt;span id=&quot;continue-reading&quot;&gt;&lt;&#x2F;span&gt;
&lt;p&gt;Reading through the source though did allow me to find any easy way to
implement such functionality. The following snippet is a minimalist data_mapper
initialization and setup of one model with a table prefix of &lt;code&gt;source_&lt;&#x2F;code&gt; (chosen
at random and of no significance).&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;ruby&quot; class=&quot;language-ruby &quot;&gt;&lt;code class=&quot;language-ruby&quot; data-lang=&quot;ruby&quot;&gt;# encoding: utf-8

# (1)
require &quot;dm-core&quot;
require &quot;dm-migrations&quot;

# (2)
module PrefixNamingConvention
  def self.call(model_name)
    # (3)
    prefix = &quot;source_&quot;
    # (4)
    table_name = DataMapper::NamingConventions::Resource::UnderscoredAndPluralized.call(model_name)

    &quot;#{prefix}#{table_name}&quot;
  end
end

# (5)
DataMapper::Logger.new($stdout, :debug)

# (6)
DataMapper.setup(:default, &quot;sqlite:example.db&quot;)
DataMapper.repository(:default).adapter.resource_naming_convention = PrefixNamingConvention

# (7)
class Person
  include DataMapper::Resource

  property :id, Serial
  property :first_name, String
  property :last_name, String
  property :email, String
end

# (8)
DataMapper.finalize
DataMapper.auto_upgrade!
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;So here are some notes on what&#x27;s going on in this snippet. Each area that I
will be discussing has been annotated with a number like &amp;quot;# (1)&amp;quot; to make it
easier to find a section you have questions about.&lt;&#x2F;p&gt;
&lt;ol&gt;
&lt;li&gt;Since this is an example I&#x27;m only including the bare minimum data mapper
gems to accomplish the task. If you&#x27;re using bundler you may need to also
require &amp;quot;rubygems&amp;quot; to get this too work.&lt;&#x2F;li&gt;
&lt;li&gt;This is where the real work happens, DataMapper uses external modules that
receive the &amp;quot;call&amp;quot; method to handle the conversion of class names to table
names. By default DataMapper uses the module
&amp;quot;DataMapper::NamingConventions::Resource::UnderscoredAndPluralized&amp;quot;, which
I&#x27;ll use later to maintain the same names.&lt;&#x2F;li&gt;
&lt;li&gt;This is where I&#x27;m defining the table prefix. This could be defined in a
global, call another method or class, whatever your heart desires to get a
string that will be used as a prefix.&lt;&#x2F;li&gt;
&lt;li&gt;Here I&#x27;m getting what DataMapper would have named the table if I wasn&#x27;t
interferring&lt;&#x2F;li&gt;
&lt;li&gt;I&#x27;m logging to standard out so that I can see the queries called to verify
that DataMapper is creating tables with the names that I want. This is used
later on in this post to demonstrate this solution working, however, it
could be left out without affecting anything.&lt;&#x2F;li&gt;
&lt;li&gt;Initial setup of a sqlite database, and then the good stuff. Once a database
has been setup with a specific adapter you can change the naming convention
DataMapper will use to generate table names. This is accomplished by passing
the module constant name through the repositories adapter and too
&amp;quot;resource_naming_convention&amp;quot; as demonstrated in the code.&lt;&#x2F;li&gt;
&lt;li&gt;Here I&#x27;m defining an example model of no importance. This is purely for
demonstration, normally DataMapper would name this model &amp;quot;people&amp;quot;.&lt;&#x2F;li&gt;
&lt;li&gt;Inform DataMapper we&#x27;re done setting it up and to run the migrations to
create the model defined.&lt;&#x2F;li&gt;
&lt;&#x2F;ol&gt;
&lt;p&gt;When you run this ruby file (assuming you have the &amp;quot;data_mapper&amp;quot; and
&amp;quot;dm-sqlite-adapter&amp;quot; gem installed) you&#x27;ll see output very similar too this:&lt;&#x2F;p&gt;
&lt;pre&gt;&lt;code&gt;~ (0.001402) PRAGMA table_info(&quot;source_people&quot;)
~ (0.000089) SELECT sqlite_version(*)
~ (0.077840) CREATE TABLE &quot;source_people&quot; (&quot;id&quot; INTEGER NOT NULL PRIMARY KEY AUTOINCREMENT, &quot;first_name&quot; VARCHAR(50), &quot;last_name&quot; VARCHAR(50), &quot;email&quot; VARCHAR(50))
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;Notice the third line? Specifically the name of the table? It&#x27;s named exactly
as it would have been except now it has a prefix of &amp;quot;source_&amp;quot;.&lt;&#x2F;p&gt;
&lt;p&gt;Hope this saves someone else some trouble. Cheers!&lt;&#x2F;p&gt;
</content>
	</entry>
	<entry xml:lang="en">
		<title>Thoughts on IPv6 Security and Mitigation</title>
		<published>2012-07-22T03:07:42+00:00</published>
		<updated>2012-07-22T03:07:42+00:00</updated>
		<link href="https://stelfox.net/blog/2012/thoughts-on-ipv6-security-and-mitigation/" type="text/html"/>
		<id>https://stelfox.net/blog/2012/thoughts-on-ipv6-security-and-mitigation/</id>
		<content type="html">&lt;p&gt;I setup IPv6 on my home network with an OpenWRT router and Hurricane Electric
and now I suddenly have an opinion on the state of IPv6 security. This is
something that I&#x27;ve been meaning to do for some time and have been mulling over
in the back of my mind. I&#x27;ll go over the details from start to finish of
setting up hurricane electric on the router in another post as the information
to do so is very scattered and disjointed. It does appear to be very well
documented on the OpenWRT wiki but I found that they leave out some very
important steps, so stay tuned for that.&lt;&#x2F;p&gt;
&lt;span id=&quot;continue-reading&quot;&gt;&lt;&#x2F;span&gt;
&lt;p&gt;Lets start with the loss of NAT. NAT was never intended to be a security
measure and lots of people will argue with me for saying that it was. However,
the truth of the matter is that any machine that is behind a NAT is not
directly addressable from the internet without someone on the inside
intentionally poking holes (even if that someone is a bad-guy).&lt;&#x2F;p&gt;
&lt;p&gt;Anyone technically knowledgeable enough can usually use fingerprints of how a
machine responds to different types of traffic both normal and unusual to
identify what operating system, version, and specific details about the
services and potential vulnerabilities of that machine. This information is
invaluable to attackers.&lt;&#x2F;p&gt;
&lt;p&gt;One of IPv6&#x27;s biggest selling points (and one that I quite enjoy, don&#x27;t
misunderstand this post) is that every device is addressable. This can
potentially allow attackers to learn more about what they&#x27;re attacking.&lt;&#x2F;p&gt;
&lt;p&gt;So how do you counter this? Well firewalls can help quite a bit in thwarting OS
fingerprinting, but even the strongest firewall won&#x27;t completely prevent this.
Another, more protective layer that IPv6 gives you for free is it&#x27;s sheer size.&lt;&#x2F;p&gt;
&lt;p&gt;Doing a pure enumeration of a single home subnet remotely (that is you are not
on the link IPv6 local link) would take millennia by some estimations, as
opposed the IPv4 address space which could be done at home in a matter of
weeks. One house vs the world. That is the scale we are now working at in IPv6.&lt;&#x2F;p&gt;
&lt;p&gt;The vast scale, however, while enough to defend against random scans, will not
prevent your address showing up in server logs that you connect to. A single
IPv6 address can be scanned just as easily as an IPv4 address.&lt;&#x2F;p&gt;
&lt;p&gt;What&#x27;s more is that once an attacker has a presence on a subnet they can
enumerate every single machine on that network in a matter of seconds to
minutes. Since most home users are infected through drive by trojans, found in
emails, and websites that the user chooses to go to, and attackers are already
used to not having direct access to a machine from the internet, means the
slowness and difficulty of the raw scans just simply won&#x27;t be an issue.&lt;&#x2F;p&gt;
&lt;p&gt;Due too the ease of enumerating local networks and that they probably contain
more vulnerable machines.... I&#x27;ll leave that extrapolation as an exercise to my
readers. I do predict that infrastructure will become a larger target due to
this, just to collect their logs to attack home users.&lt;&#x2F;p&gt;
&lt;p&gt;The next thing that I want to bring up is IPv6, by default, uses the
interface&#x27;s MAC address to generate the last 12 characters &#x2F; 48 bits of the
IPv6 address. The rest of the local address consists of an identifier
indicating that the address was generated using a MAC address and was not
randomly generated.&lt;&#x2F;p&gt;
&lt;p&gt;So what does this actually tell us? Well if the OS and OS version can help up
specify attacks, why not the brand and possibly the model of the MAC address?
What attack vectors are waiting in the firmware of our ethernet and wireless
cards? Firmware that almost never gets updated, and is known to have bugs and
quirks?&lt;&#x2F;p&gt;
&lt;p&gt;That one is actually an easy one, RFC 3041 defines &amp;quot;Privacy Addresses&amp;quot;. These
are completely random local addresses that get generated once a day. Logs
become increasingly useless on the server end, there is a lot of decoys on
networks, and we&#x27;re no longer exposing as much information to potential
hostiles.&lt;&#x2F;p&gt;
&lt;p&gt;I use Fedora 17 and it took me a while to figure out how to enable privacy
addresses the &amp;quot;Red Hat&amp;quot; way. You can easily do it generally on any Linux system
with sysctl. Just add the following to your &lt;code&gt;&#x2F;etc&#x2F;sysctl.conf&lt;&#x2F;code&gt; and reload
sysctl:&lt;&#x2F;p&gt;
&lt;pre&gt;&lt;code&gt;net.ipv6.conf.all.use_tempaddr = 2
net.ipv6.conf.default.use_tempaddr = 2
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;But that isn&#x27;t the &amp;quot;Red Hat&amp;quot; way. Red Hat manages it&#x27;s interfaces and network
configuration through various interface configuration files living in
&lt;code&gt;&#x2F;etc&#x2F;sysconfig&#x2F;network-scripts&lt;&#x2F;code&gt;. For any IPv6 enabled interface you can turn
on privacy addresses with the following line for example in
&lt;code&gt;&#x2F;etc&#x2F;sysconfig&#x2F;network-scripts&#x2F;ifcfg-eth0&lt;&#x2F;code&gt;:&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;sh&quot; class=&quot;language-sh &quot;&gt;&lt;code class=&quot;language-sh&quot; data-lang=&quot;sh&quot;&gt;IPV6_PRIVACY=&quot;rfc3041&quot;
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;After restarting your network interfaces they will additionally have privacy
addresses that will be change automatically. The interfaces still have the MAC
based addresses as well but they will not longer be the default and thus will
not show up in remote server logs.&lt;&#x2F;p&gt;
&lt;p&gt;Now, what would be a solid and strong step forward was a way to have a local
machine register it&#x27;s privacy address with a local IDS&#x2F;IPS with an expiration,
and to automatically trigger the IDS&#x2F;IPS whenever a new connection is made to
an expired privacy address. It would almost be like a free honey pot on your
own network.&lt;&#x2F;p&gt;
</content>
	</entry>
	<entry xml:lang="en">
		<title>Ruby&#x27;s XMLRPC::Client and SSL</title>
		<published>2012-02-14T18:08:51+00:00</published>
		<updated>2012-02-14T18:08:51+00:00</updated>
		<link href="https://stelfox.net/blog/2012/rubys-xmlrpc-client-and-ssl/" type="text/html"/>
		<id>https://stelfox.net/blog/2012/rubys-xmlrpc-client-and-ssl/</id>
		<content type="html">&lt;p&gt;For the past few days I&#x27;ve been working on a Ruby project that needed to
interact with a remote XMLRPC API. This isn&#x27;t particularly unusual but it was
the first time from within a Ruby application. Luckily enough Ruby has a built
in XMLRPC client that handles a lot of the messy bits.&lt;&#x2F;p&gt;
&lt;span id=&quot;continue-reading&quot;&gt;&lt;&#x2F;span&gt;
&lt;p&gt;The XMLRPC::Client class itself seems fairly simple. There are only a handful
of methods, five of which are for opening a new connection in a few different
ways, and at least two ways to open each type of connection.&lt;&#x2F;p&gt;
&lt;p&gt;As a starting point this was a simplified chunk of code that I was using to
connect to the remote API:&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;ruby&quot; class=&quot;language-ruby &quot;&gt;&lt;code class=&quot;language-ruby&quot; data-lang=&quot;ruby&quot;&gt;require &#x27;xmlrpc&#x2F;client&#x27;

class APIConnection
  def initialize(username, password, host)
    # Build the arguments for the XMLRPC::Client object
    conn_args = {
      :user =&gt; username,
      :password =&gt; password,
      :host =&gt; host,
      :use_ssl =&gt; true,
      :path =&gt; &quot;&#x2F;api&quot;
    }

    @connection = XMLRPC::Client.new_from_hash(conn_args)
  end

  def version
    @connection.call(&quot;version&quot;)
  end
end
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;The problem I ran into was when connecting to a server using HTTPS. I knew that
this certificate was good however I continued to get the message:&lt;&#x2F;p&gt;
&lt;pre&gt;&lt;code&gt;warning: peer certificate won&#x27;t be verified in this SSL session
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;Ruby has taken the approach of by default not including any trusted certificate
authorities which I greatly appreciate especially considering that in 2010 and
2011 12 certificate authorities were known to have been hacked including major
ones such as VeriSign, and &lt;a href=&quot;https:&#x2F;&#x2F;web.archive.org&#x2F;web&#x2F;20120418062943&#x2F;http:&#x2F;&#x2F;www.symantec.com:80&#x2F;connect&#x2F;blogs&#x2F;diginotar-ssl-breach-update&quot;&gt;DigiNotar&lt;&#x2F;a&gt;. Some of which were &lt;a href=&quot;http:&#x2F;&#x2F;nakedsecurity.sophos.com&#x2F;2011&#x2F;08&#x2F;29&#x2F;falsely-issued-google-ssl-certificate-in-the-wild-for-more-than-5-weeks&#x2F;&quot;&gt;proven&lt;&#x2F;a&gt; to
have issued false certificates.&lt;&#x2F;p&gt;
&lt;p&gt;Since &lt;code&gt;XMLRPC::Client&lt;&#x2F;code&gt; doesn&#x27;t expose it&#x27;s SSL trust settings through it&#x27;s
methods I went on a bit of a journey through Google to find an answer. What I
found was overly disturbing, a lot of people don&#x27;t seem to understand what SSL
is actually for. The solutions I found from the most egregious to least:&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;Disabling OpenSSL certificate checking globally with &lt;code&gt;OpenSSL::SSL::VERIFY_NONE&lt;&#x2F;code&gt;&lt;&#x2F;li&gt;
&lt;li&gt;Overriding the &lt;code&gt;Net::HTTP&lt;&#x2F;code&gt; certificate checking&lt;&#x2F;li&gt;
&lt;li&gt;Disabling OpenSSL certificate checking locally by extending &lt;code&gt;XMLRPC::Client&lt;&#x2F;code&gt;
and over-riding how it was establishing connections&lt;&#x2F;li&gt;
&lt;li&gt;Using an SSL stripping proxy&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;I couldn&#x27;t find a solution out there that didn&#x27;t the security conscious voice
in my head scream in despair. I asked on &lt;a href=&quot;http:&#x2F;&#x2F;stackoverflow.com&#x2F;questions&#x2F;9199660&#x2F;why-is-ruby-unable-to-verify-an-ssl-certificate&quot;&gt;StackOverflow&lt;&#x2F;a&gt; for a good
solution. When I asked I didn&#x27;t have a good grasp on how Ruby was handling SSL
certificates at all. The thorough answer from &lt;a href=&quot;http:&#x2F;&#x2F;stackoverflow.com&#x2F;a&#x2F;9238221&#x2F;95114&quot;&gt;emboss&lt;&#x2F;a&gt; didn&#x27;t quite answer
my question but it gave me more than enough to really hunt down what I wanted.&lt;&#x2F;p&gt;
&lt;p&gt;First stop, I needed the certificates that I&#x27;ll be using to verify the
connection. Every single certificate authority that issues certificates for
public websites makes the public portion of their certificates available and
this is what we need to verify the connection. To find out which ones you
specifically need you can go to the API server&#x27;s address and look at it&#x27;s
certificate information by clicking on the site&#x27;s lock icon. Every browser is a
little different so you&#x27;ll have to find this out on your own. With Chrome (and
perhaps others) you can download each of the certificates in the chain that
you&#x27;ll need to verify the server&#x27;s certificate.&lt;&#x2F;p&gt;
&lt;p&gt;The server I was connecting to was using a RapidSSL certificate, who has been
verified by GeoTrust. You want to grab their certificates base64 encoded in PEM
format. Stick them all in a &lt;code&gt;ca.crt&lt;&#x2F;code&gt; file.&lt;&#x2F;p&gt;
&lt;p&gt;How do we get &lt;code&gt;XMLRPC::Client&lt;&#x2F;code&gt; to actually use that information without hacking
it all to pieces? &lt;code&gt;Net::HTTP&lt;&#x2F;code&gt; has a few methods that allow you to set the
appropriate connection settings and &lt;code&gt;XMLRPC::Client&lt;&#x2F;code&gt; uses &lt;code&gt;Net::HTTP&lt;&#x2F;code&gt;. If
&lt;code&gt;XMLRPC::Client&lt;&#x2F;code&gt; allowed to you specify this directly somehow I would&#x27;ve been a
lot happier.&lt;&#x2F;p&gt;
&lt;p&gt;Here&#x27;s that code snippet again, this time forcing certificate verification with
the &lt;code&gt;ca.crt&lt;&#x2F;code&gt; file. This code assumes that the &lt;code&gt;ca.crt&lt;&#x2F;code&gt; file lives in the same
directory as the connection script:&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;ruby&quot; class=&quot;language-ruby &quot;&gt;&lt;code class=&quot;language-ruby&quot; data-lang=&quot;ruby&quot;&gt;require &#x27;xmlrpc&#x2F;client&#x27;

class APIConnection
  def initialize(username, password, host)
    # Build the arguments for the XMLRPC::Client object
    conn_args = {
      :user =&gt; username,
      :password =&gt; password,
      :host =&gt; host,
      :use_ssl =&gt; true,
      :path =&gt; &#x27;&#x2F;api&#x27;
    }

    @connection = XMLRPC::Client.new_from_hash(conn_args)

    @connection.instance_variable_get(&#x27;@http&#x27;).verify_mode = OpenSSL::SSL::VERIFY_PEER
    @connection.instance_variable_get(&#x27;@http&#x27;).ca_file = File.join(File.dirname(__FILE__), &#x27;ca.crt&#x27;)
  end

  def version
    @connection.call(&#x27;version&#x27;)
  end
end
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;Those last two lines in the initialize method first dive into the connection
we&#x27;ve already setup (but before it&#x27;s been called), grab the of Net::HTTP and
tells it to force peer verification and to use the certificate file we created
before. No more warning, and we&#x27;re actually safe.&lt;&#x2F;p&gt;
</content>
	</entry>
	<entry xml:lang="en">
		<title>Exploration of an ACN Iris 3000</title>
		<published>2011-05-01T15:48:08+00:00</published>
		<updated>2011-05-01T15:48:08+00:00</updated>
		<link href="https://stelfox.net/blog/2011/exploration-of-a-acn-iris-3000/" type="text/html"/>
		<id>https://stelfox.net/blog/2011/exploration-of-a-acn-iris-3000/</id>
		<content type="html">&lt;p&gt;So I found a dirt cheap video SIP phone (ACN Iris 3000) at a local HAM fest.
After looking around I found the vendor has locked in the phone with their
specific service with an iron grip and had gone out of business. I guess I
should expect that kind of anti-competitive behavior from a business that
Donald Trump has a vested interest in.&lt;&#x2F;p&gt;
&lt;span id=&quot;continue-reading&quot;&gt;&lt;&#x2F;span&gt;
&lt;p&gt;I&#x27;ve come across one post on a forum that seems to have been crawled and copied
out every where. The poster had cracked it and got it working with an Asterisk
server which is what my ultimate goal for this phone is, however they claim to
have done it by getting root through telnet. The problem being that port 23
(telnet) is not open so this was a dead end.&lt;&#x2F;p&gt;
&lt;p&gt;This is a running document of how I&#x27;m doing it, you&#x27;ll notice that I&#x27;m writing
this as I go.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;reconnaissance&quot;&gt;Reconnaissance&lt;&#x2F;h2&gt;
&lt;p&gt;First thing&#x27;s first a little run down of what I&#x27;ve found. I can change the
network address and the way it&#x27;s handling networking (either bridged or NAT). I
can not get into the Administrators menu which is where all the juicy bits seem
to be. I&#x27;ve found that the factory reset code is 7517517, though that doesn&#x27;t
get me anything beyond cleaning up it&#x27;s last known phone number.&lt;&#x2F;p&gt;
&lt;p&gt;Through a very thorough &lt;code&gt;nmap&lt;&#x2F;code&gt; scan I&#x27;ve found that ports TCP 21, 79, 113, 513,
514, 554, 5060, 7022, and 8080 are all open. There doesn&#x27;t appear to be any UDP
ports available which actually surprised me, since SIP over UDP is pretty
common.&lt;&#x2F;p&gt;
&lt;p&gt;7022 and 8080 both immediately caught my eye. 7022 looks like someone moved SSH
(port 22) to a non-standard port, and 8080 is a very common alternate port for
HTTP. Connecting to 7022 via telnet confirmed my suspicions of SSH. I received
this prompt:&lt;&#x2F;p&gt;
&lt;pre&gt;&lt;code&gt;Connected to 10.0.0.85.
Escape character is &#x27;^]&#x27;.
SSH-2.0-dropbear_0.45
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;Bingo. SSH it is, and an old version of dropbear at that. Unfortunately as the
one poster I found said the password was neither blank nor &#x27;root&#x27;. I suspect
that they had an older firmware revision and these &#x27;bugs&#x27; were ironed out in a
later revision. That&#x27;s OK though it&#x27;ll just take a bit more work.&lt;&#x2F;p&gt;
&lt;p&gt;As for port 8080 it is definitely running a web configuration interface. All it
asks for is a password (which we don&#x27;t have). The extension for the login page
(esp) makes me suspect that the Iris device is running a copy of AppWebServer
or something similar and using embedded javascript as the server side
processing. For now that doesn&#x27;t provide much but it could be very useful later
on.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;attack&quot;&gt;Attack&lt;&#x2F;h2&gt;
&lt;p&gt;So while looking for an exploit for DropBear 0.45 I started up a SSH dictionary
attack and encountered by first real problem. The screen started blinking while
running three or more threads trying to break in, at first I thought it was
kind of funny but then it turned off completely.&lt;&#x2F;p&gt;
&lt;p&gt;Turns out the adapter I have for it is only rated for pushing out 500mA and the
phone itself takes up to 1500mA, apparently I hit that limit and browned-out
the phone. It still seems to work but if I want to take this route I&#x27;ll need a
more robust power supply. Looking around I found a 1500mA supply and after
checking the boards for damage I gave it a shot and everything seems to be
working OK.&lt;&#x2F;p&gt;
&lt;p&gt;Unfortunately I wasn&#x27;t able to find any viable exploits for that particular
DropBear version as the vulnerabilities that had been found were either DoS
vulnerabilities or were only useful with valid credentials.&lt;&#x2F;p&gt;
&lt;p&gt;The basic dictionary attack failed and I started up a more comprehensive one. I
could easily start brute forcing this but it would take a very long time,
especially if the company realized that a weak password wasn&#x27;t cutting it.&lt;&#x2F;p&gt;
</content>
	</entry>
	<entry xml:lang="en">
		<title>Linux N Issues &amp; KDE Multi-Monitor Woes</title>
		<published>2011-02-25T14:37:39+00:00</published>
		<updated>2011-02-25T14:37:39+00:00</updated>
		<link href="https://stelfox.net/blog/2011/linux-n-issues-kde-multi-monitor-woes/" type="text/html"/>
		<id>https://stelfox.net/blog/2011/linux-n-issues-kde-multi-monitor-woes/</id>
		<content type="html">&lt;p&gt;So I recently did a fresh install of Fedora 14 with KDE installed (not the KDE
spin mind you) on my ThinkPad. I&#x27;m pleasantly surprised with hows it&#x27;s working
everything seems to be working out the box very stably. I used it without issue
for a solid month and a half without a single issue.&lt;&#x2F;p&gt;
&lt;span id=&quot;continue-reading&quot;&gt;&lt;&#x2F;span&gt;
&lt;p&gt;Earlier this week I started having issues with my wireless card on some
networks, but not at all of them. The most prominent one being my home network.
I&#x27;ve had issues with my access point dropping connections before on a wide
array of machines and not actually dropping it (IE: my laptop would see it as
connected but the AP wouldn&#x27;t exchange traffic with it). So when I started
seeing this behavior I expected that issue to have cropped up again.&lt;&#x2F;p&gt;
&lt;p&gt;The actual behavior that I was witnessing was this:&lt;&#x2F;p&gt;
&lt;ol&gt;
&lt;li&gt;Connect to wireless network&lt;&#x2F;li&gt;
&lt;li&gt;Use the connection for 5-10 seconds&lt;&#x2F;li&gt;
&lt;li&gt;Pages would start timing out even though the connection was still &#x27;active&#x27;&lt;&#x2F;li&gt;
&lt;&#x2F;ol&gt;
&lt;p&gt;Disconnecting and reconnecting to the wireless would start the situation all
over again, which quickly became frustrating but I didn&#x27;t have time to mess
with it so I just plugged into an ethernet port and went about my business.&lt;&#x2F;p&gt;
&lt;p&gt;The next day I received my the &lt;code&gt;logwatch&lt;&#x2F;code&gt; from my laptop (Yes, my laptop sends
it&#x27;s logs to my email) and it mentioned more than 20,000 new entries of an
error I&#x27;ve never seen before:&lt;&#x2F;p&gt;
&lt;pre&gt;&lt;code&gt;iwlagn 0000:03:00.0: BA scd_flow 0 does not match txq_id 10
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;After poking around a bit online I found that the issue is with a recent kernel
update (I&#x27;m currently running 2.6.35.11-83) changing the behavior of some
sanity checks to wireless connections that support &#x27;N&#x27;. Turns out my wireless
card and all the networks I was having issues with support &#x27;N&#x27;. Good to know.&lt;&#x2F;p&gt;
&lt;p&gt;It was easy enough to solve that issue, the kernel module just needed an option
passed to it that I believe just disables &#x27;N&#x27;. This is all well and good but I
haven&#x27;t had to manually pass options to a kernel module for a couple of
releases now (I think the last was Fedora 10). Since then the file
&lt;code&gt;&#x2F;etc&#x2F;modules.conf&lt;&#x2F;code&gt; has been deprecated in favor of placing files in
&lt;code&gt;&#x2F;etc&#x2F;modules.d&#x2F;&lt;&#x2F;code&gt;. There are some files that come stock in there but none are
passing parameters to modules and the naming scheme doesn&#x27;t seem to conform to
anything.&lt;&#x2F;p&gt;
&lt;p&gt;I was unsure if there was something specific I had to name the file or if it
needed to be in one of the existing files. I ended up creating the
file &lt;code&gt;&#x2F;etc&#x2F;modprobe.d&#x2F;iwlagn.conf&lt;&#x2F;code&gt; and putting the following in it:&lt;&#x2F;p&gt;
&lt;pre&gt;&lt;code&gt;options iwlagn 11n_disable=1
# This one might be needed instead
#options iwlagn 11n_disable50=1
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;After I rebooted the problem vanished like it was never there. If you notice
there is a second option in there that is commented out. I found some people
where the first option didn&#x27;t work but replacing it with that second option
did, so if one doesn&#x27;t work for you try the second option.&lt;&#x2F;p&gt;
&lt;p&gt;The second issue that I encountered was just this morning. I&#x27;m working at a
remote site today that I&#x27;m not at very often, and am using my laptop as my
desktop workhorse. Usually when I&#x27;m at a remote site, I steal an office that
isn&#x27;t in use and claim it as my own, and today was no different. There was a
screen in this office that had its power plugged it but it&#x27;s VGA cable was just
sitting there. I figured &#x27;why not?&#x27; so I plugged it in and KDE happily
announced that it detected a new display and offered to take me to the settings
interface that would allow me to configure it.&lt;&#x2F;p&gt;
&lt;p&gt;&amp;quot;Awesome!&amp;quot; I thought, multiple desktops has always been one of those things I
had to tweak and search around for and it looks like KDE is making some serious
strides in their support for it. When I turned it on I wasn&#x27;t really paying
attention to the settings and my laptop display ended up on the wrong side of
the screen and my primary desktop on the LCD. Not exactly what I wanted but
it&#x27;s cool that it was that easy to setup.&lt;&#x2F;p&gt;
&lt;p&gt;I went back into the configuration options switched things around, but no
matter what I did, the &#x27;primary desktop&#x27; was always on the external monitor.
What&#x27;s more is that there isn&#x27;t any option for selecting the primary display!
That &lt;em&gt;used to&lt;&#x2F;em&gt; be there...&lt;&#x2F;p&gt;
&lt;p&gt;I hunted around before getting frustrated and searching around online. Sure
enough other people were annoyed by this regression but the solution was very
easy (though it appears you have to do it every time).&lt;&#x2F;p&gt;
&lt;p&gt;To set the primary monitor to my laptop screen (LCDS1) I just opened a shell
and put this in:&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;sh&quot; class=&quot;language-sh &quot;&gt;&lt;code class=&quot;language-sh&quot; data-lang=&quot;sh&quot;&gt;xrandr --output LCDS1 --primary
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;Poof! Everything is all set and I&#x27;m happy once again. I hope that the KDE
developers put back the primary display selection in the settings but for now
it&#x27;s easy enough. Hopefully this will help other people on the net.&lt;&#x2F;p&gt;
</content>
	</entry>
	<entry xml:lang="en">
		<title>The Home Network and NAT as a Security Layer</title>
		<published>2011-02-17T18:15:02+00:00</published>
		<updated>2011-02-17T18:15:02+00:00</updated>
		<link href="https://stelfox.net/blog/2011/the-home-network-and-nat-as-a-security-layer/" type="text/html"/>
		<id>https://stelfox.net/blog/2011/the-home-network-and-nat-as-a-security-layer/</id>
		<content type="html">&lt;p&gt;One of the hot-topics for IPv6 (which I have been thinking about a lot lately)
is NAT. I normally wouldn&#x27;t go into detail about specifics that are obvious to
people in my field but for the sake of this post I will. NAT or Network Address
Translation, is a way for a large number of computers to share a single public
IP address.&lt;&#x2F;p&gt;
&lt;span id=&quot;continue-reading&quot;&gt;&lt;&#x2F;span&gt;
&lt;p&gt;The router that is handling the NAT will keep track of connections coming in
and out of it and re-write the destination IP to an internal address to keep
the traffic flowing.&lt;&#x2F;p&gt;
&lt;p&gt;NAT was necessary with IPv4 because IPv4 only had 4,294,967,296 addresses on
the internet and quite a few those were unroutable or reserved. With a world
population of 7 billion only half the population of the planet could have a
single device online at a time. IPv6 solves this issue by increasing the number
of public routable address to 3.4x10^38. That means that each person alive
could have 4.9x10^28 addresses online at any given time.&lt;&#x2F;p&gt;
&lt;p&gt;So what does this mean for NAT? Clearly we don&#x27;t need it any more right? There
is no way I&#x27;ll ever use 4.9x10^28 addresses. I&#x27;m willing to bet Google doesn&#x27;t
own that many machines. Well this is where the debate starts. NAT was never
designed to be used as a security tool and it has even had some security
ramifications because of it.&lt;&#x2F;p&gt;
&lt;p&gt;The weakness of NAT is also it&#x27;s primary strength. What do I mean by that? You
can&#x27;t attack a computer if you can&#x27;t talk to it. In my opinion, this alone has
protected innumerable regular home users from all kinds of terrible things
online. A standard COTS router that comes with most internet connections will
stop port-scans and automated attack tools at the door.&lt;&#x2F;p&gt;
&lt;p&gt;Sounds good right? So why would people be opposed to it? Simple. It complicates
things. There are a limited number of simultaneous connections that can go
through a single NAT device. This hard limit of 65,536 connections (in reality
this number is an order of magnitude less - 32,768) isn&#x27;t changed between IPv4
and IPv6 and there really isn&#x27;t a good reason to change it. Sounds like a lot
but trust me it gets used up quickly.&lt;&#x2F;p&gt;
&lt;p&gt;There is also identity reasons, behind a NAT could be 100 people or 1 and to
the rest of the world it will all look the same. If someone breaks into a home
network there isn&#x27;t any way to differentiate that cracker from a normal user to
the outside world. This privacy also gives home users plausible deniability for
anything that happens on their network.&lt;&#x2F;p&gt;
&lt;p&gt;But those arguments against have very little to do with security. So what is
all this hype about NAT being a form of security through obscurity? The
argument I come across whenever I ask naysayers about NAT, is that if a user
gets infected then the network can still be enumerated behind the NAT as if all
the computers were on the Internet. This argument has one fatal flaw. It is
depending on a user to get infected. A firewall has this exact same
&amp;quot;vulnerability&amp;quot; so would they argue that a firewall is not a layer of security?
I thought not.&lt;&#x2F;p&gt;
&lt;p&gt;NAT has it&#x27;s problems, but claiming it is not a security layer is just plain
wrong. IPv6 is here to stay and we should really start looking at the security
implications of everything involving it. New security models need to be created
and lots of of research needs to be done in this area still. In the mean time I
suspect a lot of malware and viruses will start making use of IPv6 and how
relatively unknown it really is.&lt;&#x2F;p&gt;
</content>
	</entry>
</feed>

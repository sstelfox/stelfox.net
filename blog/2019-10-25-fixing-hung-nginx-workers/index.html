<!doctype html><html lang=en><head><meta charset=UTF-8><meta http-equiv=X-UA-Compatible content="ie=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=author content><meta name=description content="While cleaning up some tech debt, a curious issue cropped up. Nginx was running in an alpine container as a front end load balancer. It had a dynamic config that got periodically updated by a sidecar, and had filebeat shipping logs out to a central collector but otherwise was just a very simple Nginx config.
Every now and then the container would crash, it would automatically recover fast enough no alarms were lost and the clients would just resend their requests."><meta name=keywords content="blog,programming,linux,systems,personal,rust,philosophy,linux,nginx,diagnostics"><meta name=robots content="noodp"><meta name=theme-color content><link rel=canonical href=https://stelfox.net/blog/2019-10-25-fixing-hung-nginx-workers/><title>Fixing Hung Nginx Workers :: Sam Stelfox
</title><link rel=stylesheet href=/main.949191c1dcc9c4a887997048b240354e47152016d821198f89448496ba42e491.css integrity="sha256-lJGRwdzJxKiHmXBIskA1TkcVIBbYIRmPiUSElrpC5JE="><link rel=apple-touch-icon sizes=180x180 href=/apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/favicon-16x16.png><link rel=manifest href=/site.webmanifest><link rel=mask-icon href=/safari-pinned-tab.svg color><link rel="shortcut icon" href=/favicon.ico><meta name=msapplication-TileColor content><meta itemprop=name content="Fixing Hung Nginx Workers"><meta itemprop=description content="While cleaning up some tech debt, a curious issue cropped up. Nginx was running in an alpine container as a front end load balancer. It had a dynamic config that got periodically updated by a sidecar, and had filebeat shipping logs out to a central collector but otherwise was just a very simple Nginx config.
Every now and then the container would crash, it would automatically recover fast enough no alarms were lost and the clients would just resend their requests."><meta itemprop=datePublished content="2019-10-25T11:26:31-05:00"><meta itemprop=dateModified content="2019-10-25T11:26:31-05:00"><meta itemprop=wordCount content="759"><meta itemprop=keywords content="Linux,Nginx,Diagnostics"><meta name=twitter:card content="summary"><meta name=twitter:title content="Fixing Hung Nginx Workers"><meta name=twitter:description content="While cleaning up some tech debt, a curious issue cropped up. Nginx was running in an alpine container as a front end load balancer. It had a dynamic config that got periodically updated by a sidecar, and had filebeat shipping logs out to a central collector but otherwise was just a very simple Nginx config.
Every now and then the container would crash, it would automatically recover fast enough no alarms were lost and the clients would just resend their requests."><meta property="article:published_time" content="2019-10-25 11:26:31 -0500 -0500"></head><body><div class=container><header class=header><span class=header__inner><a href=/ style=text-decoration:none><div class=logo><span class=logo__mark>></span>
<span class=logo__text>./Sam_Stelfox.sh</span>
<span class=logo__cursor></span></div></a><span class=header__right><nav class=menu><ul class=menu__inner><li><a href=/about/>About</a></li><li><a href=/blog/>Blog Posts</a></li><li><a href=/notes/>Various Notes</a></li></ul></nav><span class=menu-trigger><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M0 0h24v24H0z" fill="none"/><path d="M3 18h18v-2H3v2zm0-5h18v-2H3v2zm0-7v2h18V6H3z"/></svg></span></span></span></header><div class=content><main class=post><div class=post-info></p></div><article><h2 class=post-title><a href=https://stelfox.net/blog/2019-10-25-fixing-hung-nginx-workers/>Fixing Hung Nginx Workers</a></h2><div class=post-content><p>While cleaning up some tech debt, a curious issue cropped up. Nginx was running
in an alpine container as a front end load balancer. It had a dynamic config
that got periodically updated by a sidecar, and had filebeat shipping logs out
to a central collector but otherwise was just a very simple Nginx config.</p><p>Every now and then the container would crash, it would automatically recover
fast enough no alarms were lost and the clients would just resend their
requests. No data was losts, everything failed gracefully, but it&rsquo;s still a
pretty crazy thing to leave happening.</p><p>There was nothing in the log besides some client errors and configuration
reloading notifications. The external metrics showed the container throwing
some kind of memory party before finally burning out and crashing. It was a
pretty steady trend upwards indicative of some kind of a memory leak. The graph
below illustrates this. Each of those drops is either the container crashing or
me manually restarting it to test behavior. Can you figure out when I fixed the
issue?</p><img alt="Graph showing Nginx memory usage. The graph goes from chaotic to\nsteady" src=/images/nginx_memory_consumption.png><p>To figure out what was going on I had to get an invitation that container&rsquo;s
party.</p><p>I don&rsquo;t think there is great tooling for inspecting individual processes inside
of containers. As much as we&rsquo;d like to think of containers as isolated
applications, its not uncommon for them to run <a href=https://github.com/krallin/tini>a minimal init container</a>,
or be composed of separate different processes themselves that handle different
things (Nginx has a master process and spawns off many worker processes). We
still need the data on those processes.</p><p>The common solution to getting this data seems to be to &ldquo;just exec into the
container&rdquo;. In general <code>exec</code> should be restricted. If you find yourself
running exec in a container, you&rsquo;re missing tooling or metrics that should
avoid that (which is the case here).</p><p>Denying <code>exec</code> is probably a longer discussion but the short of it is: If you
can <code>exec</code> into the container, you can extract its secrets, configuration, and
access any services that it&rsquo;s allowed to (likely escalating your personal
privileges within the cluster). This is without considering the ramifications
of <code>exec</code>&lsquo;ing into a privileged container.</p><p>In any event, not all container clusters solution allow external execution
(looking at you AWS ECS, its the one good thing I have to say about you).</p><p>I chose to modify the container to log the data I needed instead. I setup a
wrapper that started and backgrounded the following script:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span><span style=color:#75715e>#!/bin/sh
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>while</span> <span style=color:#f92672>[</span> <span style=color:#ae81ff>1</span> <span style=color:#f92672>]</span>; <span style=color:#66d9ef>do</span>
</span></span><span style=display:flex><span>  <span style=color:#75715e># This generates a JSON output that can be pulled via the ECS logs of the top</span>
</span></span><span style=display:flex><span>  <span style=color:#75715e># memory consuming processes using what we have available in alpine.</span>
</span></span><span style=display:flex><span>  ps -o pid,time,rss,vsz,args |
</span></span><span style=display:flex><span>    tail -n +2 |
</span></span><span style=display:flex><span>    awk <span style=color:#e6db74>&#39;memstat: { print &#34;{\&#34;pid\&#34;:&#34; $1 &#34;,\&#34;time\&#34;:\&#34;&#34; $2 &#34;\&#34;,\&#34;rss\&#34;:&#34; $3 &#34;,\&#34;vsz\&#34;:&#34;
</span></span></span><span style=display:flex><span><span style=color:#e6db74>$4 &#34;,\&#34;cmd\&#34;:\&#34;&#34; substr($0, index($0,$5)) &#34;\&#34;}&#34; }&#39;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>  sleep <span style=color:#ae81ff>300</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>done</span>
</span></span></code></pre></div><p>Yes, I&rsquo;m abusing <code>awk</code> to generate JSON I can parse later.</p><p>Each message has a prefix of <code>memstat:</code> so I can easily filter and extract that
data from the live log stream. After a couple of hours (the time it generally
took for the party to get going) the issue was pretty obvious. Inside the
container there was a growing number of processes with the name <code>nginx: worker process is shutting down</code>. Some Googling turned up <a href=https://forum.nginx.org/read.php?2,262403,262403>a post in the Nginx mailing
list</a> from a while ago.</p><p>The responses claimed that it was the result of a third party module, but this
is stock Nginx (version 1.15.8 at the time) which was at least three years
older than the post. It did indicate a clue though: config reloading.</p><p>I don&rsquo;t know why Nginx believed connections were still open, we have request
and connect timeouts both in this config and upstream. Some of these processes
were sticking around for hours (I never actually saw one exit after getting
into this hung state). Every config reload was dropping a zombie worker or two
into the container, permanently consuming ~30Mb of RAM. When it hit the
configured threshold, the party gets stopped.</p><p>An option was added in the <a href=http://nginx.org/en/docs/ngx_core_module.html#worker_shutdown_timeout>Nginx core module</a> to handle situations where
the worker wouldn&rsquo;t close, but this definitely seems like a bug of some kind.
There is a newer version of Nginx (1.17.5 at the time of the writing) that
could very well have fixed this issue.</p><p>Adding <code>worker_shutdown_timeout 60s;</code> to the main Nginx config solved the issue
(60 seconds matches our request and connect timeouts so nothing valid should
last longer than that). Sure enough Nginx went back to stable, and predictably
low memory usage.</p></div></article><hr><div class=post-info><p><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-tag meta-icon"><path d="M20.59 13.41l-7.17 7.17a2 2 0 01-2.83.0L2 12V2h10l8.59 8.59a2 2 0 010 2.82z"/><line x1="7" y1="7" x2="7" y2="7"/></svg>
<span class=tag><a href=https://stelfox.net/tags/linux/>linux</a></span>
<span class=tag><a href=https://stelfox.net/tags/nginx/>nginx</a></span>
<span class=tag><a href=https://stelfox.net/tags/diagnostics/>diagnostics</a></span></p></div></main></div><footer class=footer></footer></div><script type=text/javascript src=/bundle.min.85fad2de4f13fec3bcb3b3cb10430cdb44a7b4a9749b32938241a5c6e77718df7624f1002b880521fdc26e24ec1077fda214bf1cb36ee3045510760d09638cae.js integrity="sha512-hfrS3k8T/sO8s7PLEEMM20SntKl0mzKTgkGlxud3GN92JPEAK4gFIf3CbiTsEHf9ohS/HLNu4wRVEHYNCWOMrg=="></script></body></html>
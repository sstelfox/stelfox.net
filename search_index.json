[{"cid":"80ebb92beeca375fae31700cc363ed1c5e97902e","content":"\nWhile using a tool that unexpectedly was running part of its build using the docker daemon on Linux. I need to quickly come up with a workaround. Most Linux distributions have natively moved away from Docker in favor of the more secure and community maintained \"podman\" project.\n\nThe specific error I was seeing was:\n\n```text\nrequests.exceptions.ConnectionError: ('Connection aborted.', FileNotFoundError(2, 'No such file or directory'))\n```\n\nConnection aborted and file not found together... You generally only see that combination with unix sockets... Which is what tipped me off that this tool was doing something unusual. After a quick use of strace I saw it was trying to access the docker daemon's unix control socket. There is luckily an environment variable that most docker client implementations respect `DOCKER_HOST`, and a podman socket compatibility layer that lets you keep that sweet rootless permission model:\n\n```bash\nsystemctl --user start podman.socket\nexport DOCKER_HOST=http+unix:///run/user/$(id -u)/podman/podman.sock\n```\n\nSome tools are also shelling out to the `docker` executable behind the scenes as well. The creators of podman maintained CLI compatibility with the docker client which very nicely allows us to use a shell alias to replace the last bit of docker. I keep the following alias in user's my shell config:\n\n```bash\nalias docker=podman\n```\n\nIt's not a 100% solution but it handles most cases and the remaining ones tends to be bugs in the software (such as automatically modifying the URL when not the default with an HTTP prefix \"to be helpful\").\n","created_at":1683772862,"fuzzy_word_count":300,"path":"/blog/2023/05/podman-socket-compatibility/","published_at":1683772862,"reading_time":2,"tags":["docker","linux","podman"],"title":"Podman Socket Compatibility for Docker Tools","type":"blog","updated_at":1683772862,"weight":0,"word_count":243},{"cid":"f3715ef0cab4053f4e1e2398019c030952c08386","content":"\nTracing is a fantastic Rust library that I've found immensely useful. At first glance, the distinctions and roles of Subscribers, Layers, Filters, and Writers seem clear and well-documented. But when dealing with less common use cases, understanding their interactions and handling trait-based errors can become challenging.\n\nBased off the nomenclature alone I would guess multiple \"Subscribers\" would be needed for outputting the same events to different outputs for the various events being traced. The names are unfortunately a bit misleading. What are actually needed are \"Layers\". The documentation does mention this in the Layers section, but you kind of need to know that's what you're after in the first place to find it. Filters and Writers, on the other hand, are more straightforward. But I kept running into confusion in StackOverflow posts and example code from closed issues. These sources often refer to an outdated API, with filters chained onto builders, creating a tangled mess and leaving me wondering which subscriber has what filter and which output they'll be writing to.\n\nIn my experience, I usually run into this issue when I want to combine tokio-console with an existing subscriber for temporary diagnostics. My go-to workaround has been to simply swap them out, perform the diagnostics, and then swap the production one back in. However, I recently took the time to explore a better solution in a personal project. Now, this is my default tracing configuration:\n\n```rust\nuse std::time::Duration;\n\nuse console_subscriber::ConsoleLayer;\nuse tracing::Level;\nuse tracing_subscriber::{EnvFilter, Layer};\nuse tracing_subscriber::layer::SubscriberExt;\nuse tracing_subscriber::util::SubscriberInitExt;\n\n#[tokio::main]\nasync fn main() {\n    let console_layer = ConsoleLayer::builder()\n        .retention(Duration::from_secs(30))\n        .spawn();\n\n    let (non_blocking_writer, _guard) = tracing_appender::non_blocking(std::io::stderr());\n    let env_filter = EnvFilter::builder()\n        .with_default_directive(Level::INFO.into())\n        .from_env_lossy();\n    let stderr_layer = tracing_subscriber::fmt::layer()\n        .compact()\n        .with_writer(non_blocking_writer)\n        .with_filter(env_filter);\n\n    tracing_subscriber::registry()\n        .with(console_layer)\n        .with(stderr_layer)\n        .init();\n\n    tracing::info!(\"sample program starting up\");\n}\n```\n\nThe dependency section in my `Cargo.toml` file looks like:\n\n```toml\n[dependencies]\nconsole-subscriber = \"^0.1\"\ntokio = \"^1\"\ntracing = \"^0.1\"\ntracing-appender = \"^0.2\"\ntracing-subscriber = \"^0.3\"\n```\n\nI hope this configuration helps you in your Rust adventures! Happy coding!\n","created_at":1681433462,"fuzzy_word_count":400,"path":"/blog/2023/04/chained-tracing-subscribers/","published_at":1681433462,"reading_time":2,"tags":["rust","programming","tracing"],"title":"Combining \"Subscribers\" in Rust's Tracing Library","type":"blog","updated_at":1681433462,"weight":0,"word_count":360},{"cid":"1f9daf183e58a076465d1234bac46e507e737096","content":"\nWhile attempting to automate some filesystem creation that involved LVM I kept running into an issue occasionally with some holding open the logical volumes. I would attempt to disable the volume using the following command:\n\n```console\n$ lvchange -an system/storage\nLogical volume system/storage contains a filesystem in use.\n```\n\nAll of the mounts for the filesystems that were on the volume were unmounted, so it must have been a process. The trick to finding this out is to query all the processes mount files to find out what is holding this open. In my case since I'm searching for the system volume I'll need to filter out systemd. The following the command will find the offending processes:\n\n```console\n$ grep system /proc/*/mounts | grep -vE '(cgroup|systemd)'\n/proc/338/mounts:/dev/mapper/system-storage /mnt/storage_target xfs rw,noatime,attr2,inode64,logbufs=8,logbsize=32k,noquota 0 0\n/proc/421/mounts:/dev/mapper/system-storage /mnt/storage_target xfs rw,noatime,attr2,inode64,logbufs=8,logbsize=32k,noquota 0 0\n```\n\nFrom here we need to identify the offending processes using the PID identifiers from the previous output:\n\n```console\n$ ps -q 338,421 -o comm=\nsystemd-udevd\nsystemd-logind\n```\n\nDisabling, shutting down, or kill the offending processes allow the LVM to be disabled and properly removed. Of course it was systemd...\n","created_at":1582502942,"fuzzy_word_count":200,"path":"/blog/2020/02/logical-volume-in-use/","published_at":1582502942,"reading_time":1,"tags":["linux","lvm"],"title":"Logical Volume in Use","type":"blog","updated_at":1582502942,"weight":0,"word_count":190},{"cid":"d595c469e59bc4f9fd1e371d5b02df728aeffc5f","content":"\nIt's been a hot second since I've dived into the lands of initramfs and as is the way of things, it has gotten a tad more complicated. The simple way that used to work wonders (and is still required) to start with, was to identify if the file is compressed:\n\n```console\n$ file /boot/initramfs-current.img\n/boot/initramfs-current.img: ASCII cpio archive (SVR4 with no CRC)\n```\n\nIn this case the file appears to be entirely uncompressed which is convenient and likely exactly what you'll experience for reasons I'm about to get to. This could indicate that there is a type of compression in place (such as gzip or the like) in which case your initramfs has likely not been generated by a modern version of Dracut and this post isn't right for you.\n\nNext let's extract the contents into a new temporary directory:\n\n```console\n$ mkdir init_tmp\n$ cd init_tmp\n$ cpio -mvid \u003c /boot/initramfs-current.img\n.\nearly_cpio\nkernel\nkernel/x86\nkernel/x86/microcode\nkernel/x86/microcode/AuthenticAMD.bin\n62 blocks\n```\n\nThis is where things got weird and I got caught up. No /init? No tools for dealing with LVM, filesystems, device scanning? No core system directories like /dev, /sys, or /proc? What is going on here. We can also quickly see the size doesn't match what we extracted:\n\n```console\n$ du --max-depth=1 -h\n32K     ./kernel\n36K     .\n$ ls -lh /boot/initramfs-current.img\n-rwxr-xr-x 1 root root 13M Feb 18 15:13 /boot/initramfs-current.img\n```\n\nTurns out this is an early optimization by Dracut to get updated microcode to the processor before we pass control over to any userspace programs. It's actually pretty slick and I'll have to figure out how this works in some future post. To get to the real initramfs we simply need to skip the first one using dd.\n\nPreviously when we extracted the CPIO archive it told us how large it was (The 62 blocks at the end). We simply need to skip those then we should be able to decode the inner archive.\n\nSide note for clarity with the following commands, I switched back to my home directory and emptied out the extracted contents of the init_tmp directory we created earlier as that's where we want to put the extracted inner initramfs contents.\n\n```console\n$ dd if=/boot/initramfs-current.img bs=512 skip=62 of=inner-initramfs-current.img\n25995+1 records in\n25995+1 records out\n13309841 bytes (13 MB, 13 MiB) copied, 0.0848529 s, 157 MB/s\n```\n\nOnce again we need to identify if compression is present:\n\n```console\n$ file inner-initramfs-current.img\ninner-initramfs-current.img: LZ4 compressed data (v0.1-v0.9)\n```\n\nFrom here we can decompress it and extract the inner archive much like before using the appropriate utility (lz4cat does the trick here, your compression may vary).\n\n```console\n$ cd init_tmp\n$ lz4cat ../inner-initramfs-current.img | cpio -mvid\n.\nbin\nbin/bash\nbin/cat\nbin/chown\nbin/chroot\n...\u003ccontents remove for brevity\u003e\nvar\nvar/lock\nvar/run\nvar/tmp\n54581 blocks\n```\n\nBam! That is a lot more like it. I'm really curious how the kernel boot process works with that early microcode but this got me where I needed to be and I hope this helps someone else out.\n\nAdditional note: I missed this but apparently Dracut ships with a utility called skipcpio which you can pipe one of these initramfs files through to skip the extra dd step. Had a friend point that out after I wrote this up...\n","created_at":1582069322,"fuzzy_word_count":600,"path":"/blog/2020/02/extracting-dracut-initramfs/","published_at":1582069322,"reading_time":3,"tags":["dracut","linux"],"title":"Extracting Dracut Built initramfs","type":"blog","updated_at":1582069322,"weight":0,"word_count":569},{"cid":"3f81d05e5913f2d53d88b37dd782a0604b635f22","content":"\nI've recently been playing around with Blender (following [this tutorial series](https://www.youtube.com/playlist?list=PLjEaoINr3zgEq0u2MzVgAaHEBt--xLB6U)). In Part 4 of the Level 1 series, the host Andrew Price is teaching about loop selects which very simply is holding down Alt while clicking on a vertex. The issue wasn't working for me though I found quite a few other users experiencing the issue.\n\nThe most common fix was when people had three button mouse emulation enabled (a common setting people turn on when using laptops or Macs). I checked this and it wasn't my issue.\n\nI'm running the Cinnamon desktop environment on Fedora (though I'm sure this would universally apply to all Cinnamon instances), which has a \"convenience\" feature that allows you to hold down a modifier key inside a window to move or resize it. By default this is Alt.\n\nTo change this open up the System Settings app, navigate to the Windows section, choose the Behavior tab and change the setting of \"Special key to move and resize windows\" to Disabled`. Problem solved loop select is working like a charm.\n","created_at":1574890922,"fuzzy_word_count":200,"path":"/blog/2019/11/blender-loop-select-in-cinnamon/","published_at":1574890922,"reading_time":1,"tags":["blender","cinnamon","linux","tips"],"title":"Blender Loop Select in Cinnamon","type":"blog","updated_at":1574890922,"weight":0,"word_count":176},{"cid":"cb32d9f13185140d6f90e3d6cc293ca5b93cedcb","content":"\nWhile cleaning up some tech debt, a curious issue cropped up. Nginx was running in an alpine container as a front end load balancer. It had a dynamic config that got periodically updated by a sidecar, and had filebeat shipping logs out to a central collector but otherwise was just a very simple Nginx config.\n\nEvery now and then the container would crash, it would automatically recover fast enough no alarms were lost and the clients would just resend their requests. No data was lost, everything failed gracefully, but it's still a pretty crazy thing to leave happening.\n\nThere was nothing in the log besides some client errors and configuration reloading notifications. The external metrics showed the container throwing some kind of memory party before finally burning out and crashing. It was a pretty steady trend upwards indicative of some kind of a memory leak. The graph below illustrates this. Each of those drops is either the container crashing or me manually restarting it to test behavior. Can you figure out when I fixed the issue?\n\n![Graph showing nginx memory usage. The graph goes from chaotic to steady](/images/nginx_memory_consumption.png)\n\nTo figure out what was going on I had to get an invitation that container's party.\n\nI don't think there is great tooling for inspecting individual processes inside of containers. As much as we'd like to think of containers as isolated applications, its not uncommon for them to run [a minimal init container](https://github.com/krallin/tini), or be composed of separate different processes themselves that handle different things (Nginx has a master process and spawns off many worker processes). We still need the data on those processes.\n\nThe common solution to getting this data seems to be to \"just exec into the container\". In general exec should be restricted. If you find yourself running exec in a container, you're missing tooling or metrics that should avoid that (which is the case here).\n\nDenying exec is probably a longer discussion but the short of it is: If you can exec into the container, you can extract its secrets, configuration, and access any services that it's allowed to (likely escalating your personal privileges within the cluster). This is without considering the ramifications of exec'ing into a privileged container.\n\nIn any event, not all container clusters solution allow external execution (looking at you AWS ECS, its the one good thing I have to say about you).\n\nI chose to modify the container to log the data I needed instead. I setup a wrapper that started and backgrounded the following script:\n\n```bash\n#!/bin/sh\n\nwhile [ 1 ]; do\n  # This generates a JSON output that can be pulled via the ECS logs of the top\n  # memory consuming processes using what we have available in alpine.\n  ps -o pid,time,rss,vsz,args |\n    tail -n +2 |\n    awk 'memstat: { print \"{\\\"pid\\\":\" $1 \",\\\"time\\\":\\\"\" $2 \"\\\",\\\"rss\\\":\" $3 \",\\\"vsz\\\":\"\n$4 \",\\\"cmd\\\":\\\"\" substr($0, index($0,$5)) \"\\\"}\" }'\n\n  sleep 300\ndone\n```\n\nI'm slightly abusing awk to generate JSON I can parse later, but it works and it doesn't need to be fancy.\n\nEach message has a prefix of \"memstat:\" so I can easily filter and extract that data from the live log stream. After a couple of hours (the time it generally took for the party to get going) the issue was pretty obvious. Inside the container there was a growing number of processes with the name \"nginx: worker process is shutting down\". Some Googling turned up [a post in the Nginx mailing list](https://forum.nginx.org/read.php?2,262403,262403) from a while ago.\n\nThe responses claimed that it was the result of a third party module, but this is stock Nginx (version 1.15.8 at the time) which was at least three years older than the post. It did indicate a clue though: config reloading.\n\nI don't know why Nginx believed connections were still open, we have request and connect timeouts both in this config and upstream. Some of these processes were sticking around for hours (I never actually saw one exit after getting into this hung state). Every config reload was dropping a zombie worker or two into the container, permanently consuming ~30Mb of RAM. When it hit the configured threshold, the party gets stopped.\n\nAn option was added in the [Nginx core module](http://nginx.org/en/docs/ngx_core_module.html#worker_shutdown_timeout) to handle situations where the worker wouldn't close, but this definitely seems like a bug of some kind. There is a newer version of Nginx (1.17.5 at the time of the writing) that could very well have fixed this issue.\n\nAdding \"worker_shutdown_timeout 60s;\" to the main Nginx config solved the issue (60 seconds matches our request and connect timeouts so nothing valid should last longer than that). Sure enough Nginx went back to stable, and predictably low memory usage.\n","created_at":1572020791,"fuzzy_word_count":800,"path":"/blog/2019/10/fixing-hung-nginx-workers/","published_at":1572020791,"reading_time":4,"tags":["linux","nginx","operations","diagnostics"],"title":"Fixing Hung Nginx Workers","type":"blog","updated_at":1572020791,"weight":0,"word_count":781},{"cid":"163c6432cb151b3ada11f13c7772be44ab5294ad","content":"\nI recently began trying out [Cinnamon](https://github.com/linuxmint/Cinnamon) as my desktop environment and I've been thoroughly enjoying it. The only issue I was having was occasionally a page's form input fields would have a dark background while still having dark text making it impossible to read, and very difficult to write.\n\nIt wasn't happening everywhere, and I couldn't track down what about a website would cause the issue. Most prominently for me was when this showed up in AWS's interface.\n\n![Example of Dark Input in AWS Security Groups](/images/dark_firefox_inputs.png)\n\nSearching around apparently this was an issue with dark GTK themes and has been a [known issue for about 18 years](https://bugzilla.mozilla.org/show_bug.cgi?id=70315) and was re-reported again [about 3 years ago](https://bugzilla.mozilla.org/show_bug.cgi?id=1283086). There have been a really bad workaround that people have suggested including [extensions](https://github.com/DmitriK/darkContrast#text-contrast-for-dark-themes), [custom user CSS](https://stackoverflow.com/questions/19911090/firefox-how-to-see-text-in-input-fields-with-black-background-set-in-preference), and [using desktop shortcuts with environment variables](https://medium.com/@lsm/fix-firefox-dark-text-input-on-ubuntu-18-when-using-gnome-dark-themes-98f253f8ed7f).\n\nNone of these work well, will be inconsistent based on how you open Firefox, or might cause other issues with different websites. Generally I'm not a fan of using random extensions for security, stability, and performance reasons. Ultimately Mozilla hasn't fixed this issue, but there is a workable override at this point.\n\nTo solve the issue we do need to override the GTK theme with a light one. Generally people recommend \"Adwaita\" for the light theme that fixes it and it seems to work well for me. You can check and make sure that the theme is available by checking the \"/usr/share/themes\" directory on your system.\n\nWhen you've confirmed the presence of the theme, open up Firefox and go to the URL \"about:config\". This override value doesn't exist by default so you'll need to right click anywhere in that page and go to \"New -\u003e String\" in that context menu. For the preference name enter \"widget.content.gtk-theme-override\" and for the value enter \"Adwaita\". After refreshing the effected pages the background color issue should be resolved.\n\nIf you're making use of a \"user.js\" to enforce configuration options you can add the following line to it address the issue:\n\n```javascript\nuser_pref(\"widget.content.gtk-theme-override\", \"Adwaita\");\n```\n\nA little side note in case someone comes looking for other options. There was another \"about:config\" option that some Stack Overflow answers recommended changing, \"widget.content.allow-gtk-dark-theme\". The default is \"false\" and they're recommending changing it to... \"false\". It's not the answer to this\nproblem.\n\nHope this helps someone else out. Cheers!\n","created_at":1555170802,"fuzzy_word_count":400,"path":"/blog/2019/04/fixing-dark-input-boxes-in-firefox/","published_at":1555170802,"reading_time":2,"tags":["linux","firefox","tips"],"title":"Fixing Dark Input Boxes in Firefox","type":"blog","updated_at":1555170802,"weight":0,"word_count":379},{"cid":"168b237a8bc63e2d816ac7029bce92fd80c08564","content":"\nOnce upon a time there was a single AWS account. In this AWS account was\nseveral regions but a single VPC. To make sure expansions into other regions\nwas possible this VPC chose to use the largest private subnet which just so\nhappened to also be the default (`10.0.0.0/8`).\n\nAnother AWS account enter the picture and while they were single they came to\nthe same conclusion and followed the best practices and defaults to their\nheart's content. Normally this wouldn't be a problem for either of them, but\nthey found each other and tied the knot and were happily together for the rest\nof time...\n\nBut in this story there is a darkness looming. Communication was not everything\neither of them desired. There were secret things that couldn't be said in\npublic forums of the internet but they both desperately wanted share. There was\na solution... But it involved dark magicks.\n\nI found myself in a situation where two AWS VPCs needed to communicate\nsensitive data between the two, but they were using overlapping IP address\nspaces. There was a lot of room available in both, but even some individual IPs\noverlapped and renumbering would prove problematic and time consuming.\nEventually these two VPCs were intended to be merged anyway, but business\nrequirements needed a basic level of communication sooner.\n\nThe solution I came up with may be useful for others in a pinch; Two layers of\n1:1 NAT were employed allowing each to communicate with what each side seemed\nto believe were unique IPs. To do this we need to have a usable IP address that\nwe can map into without potentially wrecking havoc on access to random sites on\nthe internet.\n\nI was lucky in that all the hosts that need to talk to each other had addresses\non both sides below `10.7.0.x`. This is more addresses than are available to\nthe `192.168.0.0/16` private address space but covers only about 25% of the\n`172.16.0.0/16` space. If you're in a worse situation where hosts are properly\nscattered all over the `10.0.0.0/8` address you can still use this technique\nbut it will require a bit more manual configuration mapping allocating either\n/24 to route or in the most extreme case individual host addresses.\n\nBefore we go any further, I definitely consider this technique to be a band-aid\nfor the issue. For longer term connectivity some form of migration should be\nplanned and executed on. This makes a GREAT and stable band-aid though.\n\nIf you'd like to follow along you'll need two VPCs, each with two EC2 instances\nto work as the tunnel hosts and likely two more to be test hosts to make use of\nthe tunnels.\n\nThis part is easy, we'll use CentOS 7 hosts as a base. You'll need to\nadditionally install the following software:\n\n* iptables-services\n* libreswan\n* tcpdump (optional but invaluable to diagnose issues)\n\nIf you're not on AWS you'll also want to make sure that NetworkManager and\nfirewalld are both ***removed from the system***. They will break the\nconfigurations you put in place if left to their own machinations. If you\nremove NetworkManager remember to enable the network service. For good measure\nhere is a minimal DHCP config you can use to configure `eth0` on your system:\n\n```\n# /etc/sysconfig/network-scripts/ifcfg-eth0\n\nDEVICE=\"eth0\"\nNM_CONTROLLED=\"no\"\nONBOOT=\"yes\"\nTYPE=\"Ethernet\"\n\nBOOTPROTO=\"dhcp\"\nIPV4_FAILURE_FATAL=\"yes\"\n```\n\nLet's also start with a minimal IPTables ruleset. This is pretty close to the\ndefaults, but it's good to be sure that we're all on the same page:\n\n```\n# /etc/sysconfig/iptables\n\n*nat\n:PREROUTING ACCEPT [0:0]\n:INPUT ACCEPT [0:0]\n:OUTPUT ACCEPT [0:0]\n:POSTROUTING ACCEPT [0:0]\n\n# NAT rules will be added here\n\nCOMMIT\n\n*filter\n:INPUT DROP [0:0]\n:FORWARD DROP [0:0]\n:OUTPUT ACCEPT [0:0]\n\n-A INPUT -m conntrack --ctstate ESTABLISHED,RELATED -j ACCEPT\n-A INPUT -p icmp -j ACCEPT\n-A INPUT -i lo -j ACCEPT\n\n-A INPUT -m tcp -p tcp --dport 22 -j ACCEPT\n\n# Filter rules will be added here\n\n-A INPUT -j REJECT --reject-with icmp-host-prohibited\n-A FORWARD -j REJECT --reject-with icmp-host-prohibited\n\nCOMMIT\n```\n\nPlace the contents of that file in `/etc/sysconfig/iptables` as the header\nindicates. The differences from the default are mostly in that we have also\ndefined the `nat` table and switched the default action on the `INPUT` and\n`FORWARD` chains to drop. Both the default and this one will reject the traffic\nanyways so this doesn't actually change the behavior of the firewall.\n\nDefining the `nat` table doesn't change any behavior either, but I'll be\nreferencing it later on in the post and you should add the rules where\nindicated by the comment. If you get confused by any of my instructions around\nadding the firewall rules, there is a complete ruleset at the end of the post\nyou can reference directly.\n\nFinally let's make sure the firewall is enabled and running:\n\n```\nsystemctl enable iptables.service\nsystemctl start iptables.service\n```\n\n## Basic Connectivity\n\nFrom this point on it is going to become important to distinguish the two\nnetworks I'll be bridging. This method is very symmetric (all the firewalls and\nconfigs should effectively be the same on the two tunnel instances) but there\nare a few places where the remote IP and local IPs need to be referenced. Going\nforward I'm going to refer to the two networks as `east` and `west` but these\nare arbitrary labels.\n\nYou'll need to collect the public IP from the AWS console for your tunnel hosts\nin both the `east` and `west`. For me I'm going to use `5.5.5.5` for the `west`\nIP and `7.7.7.7` for the `east` IP. If you see these in the configs you'll want\nto replace them with the appropriate values for your networks. If you expect\nthis to last a long time or will be a business critical tunnel I highly\nrecommend using an Elastic IP on each of these hosts.\n\nYou'll need to setup a dedicated security group for each of the tunnel hosts.\nTo avoid bouncing back and forth between these the security groups as we\nprogress through the guide I'm going to put all the rules we're going to need\nin the following table. These are inbound rules only and can be hardened a bit\n(but I'll get to that later), let's focus on getting this up and running first.\n\n|           Type          |   Protocol   | Port Range |        Source        |         Description        |\n|:-----------------------:|:------------:|:----------:|:--------------------:|:--------------------------:|\n| SSH                     | TCP          | 22         | 0.0.0.0/0            | SSH Access                 |\n| Custom Protocol         | ESP (50)     | All        | {other public IP}/32 | IPSec Encapsulated Packets |\n| Custom UDP Rule         | UDP          | 500        | {other public IP}/32 | IPSec Key Management       |\n| Custom ICMP Rule - IPv4 | Echo Request | N/A        | 0.0.0.0/0            | Connectivity Checking      |\n| All TCP                 | TCP          | 0-65535    | 10.0.0.0/8           | Internal TCP Traffic       |\n| All UDP                 | UDP          | 0-65535    | 10.0.0.0/8           | Internal UDP Traffic       |\n\nYou'll want to replace `{other public IP}` with the public IP of the tunnel\nhost in the opposite network. For example if this is the security group for the\n`west` tunnel host, you'd be allowing the traffic from `7.7.7.7`.\n\nIf you're doing this in another environment you may also need `UDP/4500` from\nthe other public IP when NAT traversal is required. AWS EC2 instances are NAT'd\n[but we can work around that][1] and will include that later on.\n\nWith the security groups in place, and the local firewalls configured make sure\neach host can ping each other. If they can great! If not, double check all the\nIPs, security group rules, and iptables rules all match what I have documented\nhere.\n\n## The IPSec Tunnel\n\nThis tunnel provides strong authentication and encryption for all the traffic\nthat will be exchanged between the two networks. We've already installed the\nrequired packages we just need to configure the various pieces to get it\nrunning.\n\nFirst let's handle the firewall. In the `/etc/sysconfig/iptables` file we\nstandardized on earlier we need to add a couple of rules to each tunnel host\nfor the IPSec traffic. Add these just after the note for adding filter rules\nand before the `REJECT` rules:\n\n```\n-A INPUT -p esp -j ACCEPT\n-A INPUT -m udp -p udp --sport 500 --dport 500 -j ACCEPT\n#-A INPUT -m udp -p udp --sport 4500 --dport 4500 -j ACCEPT\n```\n\nThis will allow tunneled packets and key exchange through the firewall. If\nyou're not on AWS when setting this up you may need to uncomment that third\nrule for NAT traversal packets.\n\nThese rules are pretty unrestricted, but we have already narrowed down who will\nbe able to connect using the security group for these machines. By leaving a\nmore refined specification out of our definition here our IPTables rules can\nremain symmetric on both hosts making automated management through a devops\ntool simpler.\n\nNext up there are some specific sysctl settings that need to be adjusted for\nthe tunneled packets to not be rejected by the kernel. The reason behind the\nsysctl settings is pretty well [documented on LibreSwan's FAQ][4] if you're\ncurious for why they're needed.\n\nYou'll want to append the following to `/etc/sysctl.conf` on both tunnel hosts:\n\n```\nnet.ipv4.conf.default.accept_redirects = 0\nnet.ipv4.conf.default.send_redirects = 0\nnet.ipv4.conf.default.rp_filter = 0\nnet.ipv4.conf.all.accept_redirects = 0\nnet.ipv4.conf.all.send_redirects = 0\nnet.ipv4.conf.all.rp_filter = 0\n\n# Annoyingly, this seems to ignore the defaults set above. This should be\n# interface that libreswan will be receiving the IPSec connections on\nnet.ipv4.conf.eth0.rp_filter = 0\n```\n\nWith that in place run `sysctl -p` to apply the new settings and `systemctl\nrestart iptables.service` to update the firewall rules.\n\nWe'll quickly do a global IPSec config to make sure we're on the same page.\nReplace the contents of `/etc/ipsec.conf` on each tunnel host with the\nfollowing:\n\n```\n# /etc/ipsec.conf\n\nconfig setup\n  protostack=netkey\n\ninclude /etc/ipsec.d/*.conf\n```\n\nIPSec has a couple of ways of handling authentication. The most secure is\nasymmetric encryption using RSA keys which requires each host to have a private\nkey and knowledge of the other host's public key. To these keys on each tunnel\nhost run the following commands:\n\n```\nsudo ipsec initnss\nsudo ipsec newhostkey --output /etc/ipsec.secrets\n```\n\nSimilar to our `west` and `east` analogy, IPSec has the concept of `left` and\n`right` hosts on either side of a tunnel. We're going to map them the same way\nbut need to get the public keys of each host first so they can verify each\nother.\n\nOn our `west` host, which will be our `left` host for the IPSec config retrieve\nthe public key with the following two commands. The output will be one very\nlong line that begins with `leftrsasigkey=` record this entire output.\n\n```\nCKAID=\"$(sudo ipsec showhostkey --list | head -n 1 | awk '{ print $NF }')\"\nsudo ipsec showhostkey --left --ckaid ${CKAID} | tail -n 1\n```\n\nA bit of an explanation of those two commands. The first one extracts the\nunique key identifier for the first key present (there shouldn't be any\nothers), while the second gets the actual public key for that identifier. We'll\nneed to repeat the process on our `east` host slightly modified which will be\nour `right` host:\n\n```\nCKAID=\"$(sudo ipsec showhostkey --list | head -n 1 | awk '{ print $NF }')\"\nsudo ipsec showhostkey --right --ckaid ${CKAID} | tail -n 1\n```\n\nThis will also output another long line that this time will begin with\n`rightrsasigkey=` which you should also record. On both hosts you'll want to\nplace the following IPSec tunnel config at `/etc/ipsec.d/vpc-link-tunnel.conf`:\n\n```\nconn vpc-link-tunnel\n  auto=start\n  pfs=yes\n  type=transport\n\n  leftid=@west_tunnel_server\n  rightid=@east_tunnel_server\n  left={west external ip}\n  right={east external ip}\n\n  authby=rsasig\n  leftrsasigkey={left/west sig key}\n  rightrsasigkey={right/east sig key}\n```\n\nBe sure to replace the `{west external ip}` with the external IP address of our\n`west` server and likewise the `{east external ip}` with the external IP\naddress of our `east` server. Be sure to replace the last two lines with the\noutput of the two keys we got from our `west` and `east` tunnel hosts.\n\nThat's it for the IPSec configuration, let's start the daemon up and verify\nthat it's working on both tunnel servers:\n\n```\nsudo systemctl enable ipsec.service\nsudo systemctl start ipsec.service\n```\n\nLet's check the IPSec status to make sure it's happy:\n\n```\nsudo ipsec status\n```\n\nThere will be quite a bit of output but what you're looking for is a line that\nlooks like this:\n\n```\n000 Total IPsec connections: loaded 1, active 1\n```\n\nIf the loaded count is 0, double check the presence and file names as well as\nthe global config. If you've properly loaded the config but it isn't coming up\nas active, review the contents of `/var/log/secure` for any IPSec error\nmessages. If there is an authentication error, most likely the public keys got\ncopied incorrectly. Make sure that both keys exist in both configs and match\nthe outputs from the key extraction commands earlier on.\n\nIf there are connection issues there are quite a few other bits that could have\ngone wrong. Review the firewalls, security groups, and IPSec configs to make\nsure the addresses are correct and the protocols are allowed through.\n\nOnce the details have been worked out and the tunnel is up, all the traffic\nbetween the two hosts should now be encrypted. This can be verified using\n`tcpdump` and sending a couple pings at the other host. When IPSec is flowing\nthe traffic will look something along the lines of:\n\n```\n21:51:02.807688 IP 10.0.1.156 \u003e 7.7.7.7: ESP(spi=0x171f19e9,seq=0xe), length 116\n```\n\nMake sure this is working, everything beyond this depends on the IPSec tunnel\nup and running correctly.\n\n## The GRE Tunnel\n\nThe GRE overlay isn't required for this to work and does add 24 bytes of\noverhead to each packet but it provides us some benefits.\n\nThe first and probably most important is that each end will have a fixed\nprivate IP address as it's routing target. If the GRE tunnel is down for any\nreason the tunnel host won't attempt to send any forwarded traffic to a public\nIP address. This provides a layer of security against other misconfigurations.\n\nSince all of the traffic will only be routed to the tunnel endpoint if the GRE\ntunnel is up and will always travel over the GRE tunnel we can simplify our\nfirewall policy around enforcement of encrypted traffic. If we guarantee all\nGRE traffic is encrypted over the IPSec tunnel, all traffic using the GRE\ntunnel will be encrypted with only a single universal firewall rule.\n\nOne final benefit with the firewall is that we get a separate interface we can\nuse to identify the direction traffic is traveling through our tunnels without\nworrying about the details of IP addresses (which will be changing in unusual\nways later on).\n\nIf these benefits don't justify the 24 byte per packet overhead to you, you're\nwelcome to skip this section but you'll need to figure out the changes to the\nfirewall rules and routing tables on your own later on.\n\nLet's start the setup with a safety net. We need to allow the GRE traffic\nthrough the firewall on the tunnel hosts, but we want to make sure that we only\npass if it has been properly encrypted with the IPSec. We can use the iptables\n`policy` module. Add the following rules to the filter section of each of our\nfirewalls:\n\n```\n-A INPUT -m policy --dir in --pol ipsec --proto esp -p gre -j ACCEPT\n-A OUTPUT -m policy --dir out --pol ipsec --proto esp -p gre -j ACCEPT\n-A OUTPUT -p gre -j DROP\n```\n\nThese three lines are all that is required to enforce that all of our traffic\nbeing routed between the two networks will always be encrypted if they have any\nhope of making it.\n\nRestart the firewall so the change can take effect:\n\n```\nsudo systemctl restart iptables.service\n```\n\nConfiguring a GRE tunnel on a CentOS box is very simple on each host create a\nnew file `/etc/sysconfig/network-scripts/ifcfg-tun0` with the following\ncontents:\n\n```\n# /etc/sysconfig/network-scripts/ifcfg-tun0\n\nDEVICE=tun0\nBOOTPROTO=none\nONBOOT=yes\nTYPE=GRE\n\nMY_INNER_IPADDR=10.255.254.1\n#MY_OUTER_IPADDR={current side external IP}\n\nPEER_INNER_IPADDR=10.255.254.2\nPEER_OUTER_IPADDR={opposing side external IP}\n\n# Not needed since we only have one tunnel. Can be any 32 bit numerical value\n#KEY=12345678\nEOF\n```\n\nFor completeness I've included `MY_OUTER_IPADDR` and `KEY` commented out as\nthey may be useful for other GRE tunnels but not necessary for this one. For\nthe `west` server `{current side external IP}` should be replaced by the `west`\ntunnel server's external IP and `{opposing side external IP}` with the east\ntunnel server's external IP. Reverse the settings on the east tunnel server.\n\nOn each tunnel host bring the tunnel up:\n\n```\nsudo ifup tun0\n```\n\nThe state of the tunnel can be checked with `sudo ip addr show tun0` which will\nhave an output similar to the following:\n\n```\n5: tun0@NONE: \u003cPOINTOPOINT,NOARP,UP,LOWER_UP\u003e mtu 8977 qdisc noqueue state UNKNOWN group default qlen 1000\n    link/gre 0.0.0.0 peer 7.7.7.7\n    inet 10.255.254.1 peer 10.255.254.2/32 scope global tun0\n       valid_lft forever preferred_lft forever\n```\n\nYou're specifically looking for the `UP` and `LOWER_UP` flags. You can double\ncheck the tunnel is functioning by pinging `10.255.254.1` and `10.255.254.2`\nfrom the `east` and `west` tunnel host respectively.\n\nWe now have a private encrypted layer 2 tunnel between the two VPC tunnel\nhosts, next up is to get other traffic in the VPC passing across the tunnel.\n\n## Tunnel Host Routing and Rewriting\n\nUp to this point everything has been setting up pretty standard tunnels between\nLinux hosts. This is where the magic needs to start happening. Each network\nneeds to see the other network with a different IP space. I've already\ndiscussed that I'll be using `172.16.0.0/12` as our mapping network.\n\nSince we're going to start forwarding traffic between networks we need to\nenable it in the kernel. On both tunnel hosts the following line needs to be\nadded to `/etc/sysctl.conf` and `sysctl -p` run again to apply the change:\n\n```\nnet.ipv4.ip_forward = 1\n```\n\nWe need to ensure each tunnel server routes our mapping network to the other\none. This should be added / removed based on the status of our GRE tunnel so\nwe'll add it as a static route in `/etc/sysconfig/network-scripts/route-tun0`.\n\nFor our `west` tunnel server the contents of the file should be:\n\n```\n172.16.0.0/12 via 10.255.254.2\n```\n\nFor our east tunnel server the contents of the file should be:\n\n```\n172.16.0.0/12 via 10.255.254.1\n```\n\nRestart the network (again dealing with a minor disruption) and check the\nrouting table with the following commands:\n\n```\nsudo systemctl restart network.service\nsudo ip -4 route\n```\n\nYou should see the new route present in the routing table, but now we have a\nproblem. If the firewalls allowed us to forward traffic right now, any traffic\neither tunnel host received with a destination of `172.16.0.0/12` would ping\npong back and forth across the tunnel until it's TTL expired. This would end up\nbeing a nasty traffic amplification issue if we allowed it.\n\nHandling this requires us to rewrite the packet destination received from the\ntunnel to the VPC's network before the kernel can make a routing decision on\nit and thus we use our first firewall incantation in the `nat` table. On each\ntunnel host add the following rule:\n\n```\n-A PREROUTING -i tun0 -d 172.16.0.0/12 -j NETMAP --to 10.0.0.0/12\n```\n\nSide note: It's not documented very well but when the `NETMAP` is used in the\n`PREROUTING` chain it only effects the destination network. When used in the\n`POSTROUTING` chain it only effects the source address (which we'll make use of\nlater).\n\nWhile we're updating our firewall we should also allow our forwarded traffic.\nThe following two rules need to be added to the `filter` section of each tunnel\nhost:\n\n```\n-A FORWARD -i eth0 -o tun0 -s 10.0.0.0/12 -d 172.16.0.0/12 -j ACCEPT\n-A FORWARD -i tun0 -o eth0 -s 172.16.0.0/12 -d 10.0.0.0/12 -j ACCEPT\n```\n\nYou may notice that I'm specifying `10.0.0.0/12` instead of `10.0.0.0/8`. This\nis a limitation I mentioned at the beginning of this article which worked in my\ninstance. You can't uniquely map a larger network into a smaller network. If\nyour hosts are more scattered this is where you'll need to start duplicating\nrules and using smaller subnet masks for targeted groups of hosts. There will\nbe other rules coming up shortly you'll need to update as well.\n\nAs part of this our rules won't forward traffic coming from our tunnel hosts\nsubnet of `10.255.254.0/30` as it is way outside of `10.0.0.0/12`. Simply\nallowing this subnet won't allow us to receive the responses to any traffic\nleaving our tunnel hosts for the opposite network as the source address will\nappear local to the VPC. We can reserve two more addresses within the range of\n`172.16.0.0/12` to work as our tunnel endpoints. This isn't strictly necessary\nif you really need the two addresses but they make diagnostics significantly\nsimpler.\n\nWe can map our two addresses appropriately using the fixed 1:1 NAT mapping in\nthe kernel by adding the following rules in the `nat` section of each tunnel\nhosts firewall:\n\n```\n-A PREROUTING -i tun0 -d 172.31.254.1 -j DNAT --to-destination 10.255.254.1\n-A PREROUTING -i tun0 -d 172.31.254.2 -j DNAT --to-destination 10.255.254.2\n\n-A POSTROUTING -o tun0 -d 10.255.254.1 -j SNAT --to-source 172.31.254.1\n-A POSTROUTING -o tun0 -d 10.255.254.2 -j SNAT --to-source 172.31.254.2\n```\n\nOnly half of these rules apply to each tunnel host, but it doesn't hurt having\nboth sets on both hosts and it keeps us symmetrical. You should be able to ping\neach of the tunnel hosts equivalent `172.31.254.0/30` address at this point (if\nyou restart the firewall).\n\nRight now if a client host added a route pointing at either of the tunnel host\nfor the mapped network it would make it out the opposite tunnel host's `eth0`\ninterface but it would still have a `10.0.0.0/12` source address and the packet\nwould never return to the tunnel host, much less the host on the other network.\n\nThis is a bit tricky as we only want to rewrite the source address (requiring a\n`POSTROUTING` rule) but only want it to effect mapped traffic addresses coming\nin from a normal VPC network, and `POSTROUTING` can't match on source\ninterface. We want to handle this rewriting before any other changes have\noccurred which requires us to do the source address rewriting happen on the\nsource tunnel host.\n\nTo handle this we can use a combination of traffic markers and our handy\n`NETMAP` target. On both of the tunnel hosts add the following two rules to the\n`nat` section:\n\n```\n-A PREROUTING -i tun0 -d 172.16.0.0/12 -s 10.0.0.0/12 -j MARK --set-mark 0x01\n-A POSTROUTING -o tun0 -m mark --mark 0x01 -s 10.0.0.0/12 -j NETMAP --to 172.16.0.0/12\n```\n\nLet's restart the firewall again to make sure all the rules have been properly\napplied:\n\n```\nsudo systemctl restart iptables.service\n```\n\nThat's the last of the changes we need to make to the tunnel hosts, now the\nother hosts need to learn how to send their traffic to the other side...\n\n## VPC Routing\n\nHosts inside a VPC will directly send traffic to any other host within it's\ndefined network. For networks beyond their VPC subnet (such as our\n`172.16.0.0/12`) network will send their traffic to their default gateway which\nis the VPC router. These routers are configurable within the AWS web console by\ngoing to the `VPC` section, finding the relevant VPC you're using and clicking\non the link to your `Main Route Table`.\n\nUnder the `Routes` sub-tab on the selected Route Table, click on the `Edit\nroutes` button. Add `172.16.0.0/12` as a destination to the routes. Click on\nthe `Target` drop down, choose `Instance` and find your VPC tunnel host in the\nlist. Click `Save Routes` and allow a minute or two for the route to update.\n\nThere is a sneaky potential issue here. If you've gone and done some deep\ncustomization to your VPC, you may have created and specified additional route\ntables for specific subnets. You'll want to evaluate each of the potential\nroute tables and add the same route to each one.\n\nThere is one final thing generally stopping our traffic from flowing freely. By\ndefault every single EC2 instance drops any traffic that reaches an EC2\ninstance with a source or destination address that doesn't match the IP that\nhas been assigned to that instance. This is generally a very useful protection,\nbut we'll be shooting out packets with source addresses in the `172.16.0.0/12`\nrange so need to disable this protection on each of tunnel hosts.\n\nFind your tunnel host in the list of your EC2 instances. Right click on the\ninstance, go to the `Networking` sub-menu, and choose `Change Source/Dest.\nCheck`. It will pop up a confirmation, confirm it by clicking on `Yes,\nDisable`.\n\nNow the only thing preventing hosts in each VPC from talking to each other is\ntheir respective inbound security groups but the traffic should flow freely.\nWe're effectively done and everything should be happy.\n\nIt may not be immediately obvious but you will have to do some math to convert\nthe IP addresses of the remote subnet into the mapping network. Specifically\nyou'll need to replace the first octet (`10`) with the mapping network's first\noctet (`172`), then add `16` to the second octet. If the resulting second octet\nis greater than `31` it won't be able to traverse the network. The remaining\ntwo octets are left unchanged.\n\nSome examples of what this translation looks like:\n\n* `10.0.0.2` becomes `172.16.0.2`\n* `10.4.1.80` becomes `172.20.1.80`\n* `10.30.56.100` becomes `172.46.56.100` and is unroutable\n\nNo matter which side of the tunnel you're on the other side's addresses will\nalways be mapped this way.\n\n## Hardening\n\nWe have some fairly wide open firewall rules for passing traffic on the\ntunneling hosts themselves and in the security groups on them. These can\ncertainly be tightened further and I'll even cover some situations in a bit\nabout when you might want to do that. As it stands right now the internal\nprivate IP addresses of the tunnel host's and clients haven't matter beyond\nwhether or not they were in the routable range.\n\nIf you use ephemeral containers or autoscaling IP addresses are going to change\nfrequently. To harden the rules on the tunnel hosts themselves would need to be\nupdated whenever these addresses change which removes a lot of benefits. Since\nwe're already using a dedicated security group for our tunnel hosts, we can\ninstead have other security groups reference it directly.\n\nTo allow traffic from the opposite VPC side, allow the relevant port's traffic\nfrom the tunnel host's security group and bam problem solved. This is still\nsomewhat course granularity of firewalling as you are effectively granting the\nentire other VPC access to that service port. In a lot of cases that will be\nenough and additional network controls such as inter-service authentication\nwill be sufficient to mitigating additional issues.\n\nIf you do need finer granularity you can start by limiting traffic on the VPC\ntunnel's inbound security group from the opposite side. If that is not fine\ngrained enough you can eventually resort to firewall rules in the `FORWARD`\nchain itself.\n\nThere is one additional benefit of putting the rules in the forward chain if\nyour addresses are sufficiently static to deploy rules through it. With\nsecurity groups alone, the traffic will traverse the tunnel before being\ndropped by the opposing side's security group. Likely your service will retry\nthe connection. These little bits of traffic do add up and will take time.\n\nIf you instead firewall with reject packets as they enter the tunnel, a service\nwill get immediate feedback the traffic won't flow. No additional bandwidth is\nwasted and the latency will be very small. You can also log these packets with\na `LOG` target before rejecting them so you can audit and diagnose traffic that\ndoesn't make it through the tunnel.\n\nFor those reasons I do prefer to firewall at the tunnel hosts themselves for\nsufficiently static services.\n\n## Troubleshooting\n\nI've tried to include basic diagnostics for each piece that we've built up but\nif you're still having issues getting traffic flowing here is a checklist to\nlook over that might help diagnose the source of the issue:\n\n* Restart the tunnel host's network\n* Verify the tunnel host's firewalls match the final reference firewall below\n* Restart the tunnel host's firewalls\n* Make sure each tunnel host can ping the other one\n* Ensure `libreswan` service is up and running (`/var/log/secure` will have\n  any errors it encounters if the tunnel isn't coming up\n* Verify the GRE tunnel is up by pinging the other end's tunnel IP\n* Check the routing table on both tunnel hosts\n* Ensure source and destination hosts are within the `10.0.0.0/12` range\n* Make sure the source / destination checking is disabled on the tunnel host's\n  EC2 instances\n* Check to make sure the VPC routing tables include `172.16.0.0/12` pointing at\n  the tunnel hosts in both networks.\n* Check the relevant security groups to make sure all other traffic is allowed\n  to/from the tunnel hosts in each security group\n\nIf all else fails sniff the traffic on the interfaces you expect for the\npackets in each place to make sure they're going where you expect. Usually this\nmakes it pretty clear to me whether packets are even getting to the tunnel\nhosts and which interface they either stop or aren't being manipulated at.\n\n## Conclusion\n\nThis post was quite a wild ride for me to write up and is probably my longest\npost to date. If you've made it this far I'm incredibly flattered. I hope this\nhelps other people out there and I would especially love to hear from anyone\nthat makes use of this information or finds an issue with anything in the post.\n\nEither [send me an email][2] or [open an issue][3] for my website's public\nrepository. Cheers!\n\n## Reference Firewall\n\nIf you had issues following along with incrementally building up our firewall\n(I'm sorry!) the final firewall you should end up with (comments removed)\nshould like the following:\n\n```\n# /etc/sysconfig/iptables\n\n*nat\n:PREROUTING ACCEPT [0:0]\n:INPUT ACCEPT [0:0]\n:OUTPUT ACCEPT [0:0]\n:POSTROUTING ACCEPT [0:0]\n\n-A PREROUTING -i tun0 -d 172.31.254.1 -j DNAT --to-destination 10.255.254.1\n-A PREROUTING -i tun0 -d 172.31.254.2 -j DNAT --to-destination 10.255.254.2\n\n-A POSTROUTING -o tun0 -d 10.255.254.1 -j SNAT --to-source 172.31.254.1\n-A POSTROUTING -o tun0 -d 10.255.254.2 -j SNAT --to-source 172.31.254.2\n\n-A PREROUTING -i tun0 -d 172.16.0.0/12 -j NETMAP --to 10.0.0.0/12\n-A PREROUTING -i tun0 -d 172.16.0.0/12 -s 10.0.0.0/12 -j MARK --set-mark 0x01\n-A POSTROUTING -o tun0 -m mark --mark 0x01 -s 10.0.0.0/12 -j NETMAP --to 172.16.0.0/12\n\nCOMMIT\n\n*filter\n:INPUT DROP [0:0]\n:FORWARD DROP [0:0]\n:OUTPUT ACCEPT [0:0]\n\n-A INPUT -m conntrack --ctstate ESTABLISHED,RELATED -j ACCEPT\n-A INPUT -p icmp -j ACCEPT\n-A INPUT -i lo -j ACCEPT\n\n-A INPUT -m tcp -p tcp --dport 22 -j ACCEPT\n\n-A INPUT -p esp -j ACCEPT\n-A INPUT -m udp -p udp --sport 500 --dport 500 -j ACCEPT\n\n-A INPUT -m policy --dir in --pol ipsec --proto esp -p gre -j ACCEPT\n-A OUTPUT -m policy --dir out --pol ipsec --proto esp -p gre -j ACCEPT\n-A OUTPUT -p gre -j DROP\n\n-A FORWARD -i eth0 -o tun0 -s 10.0.0.0/12 -d 172.16.0.0/12 -j ACCEPT\n-A FORWARD -i tun0 -o eth0 -s 172.16.0.0/12 -d 10.0.0.0/12 -j ACCEPT\n\n-A INPUT -j REJECT --reject-with icmp-host-prohibited\n-A FORWARD -j REJECT --reject-with icmp-host-prohibited\n\nCOMMIT\n```\n\n[1]: {{\u003c relref \"2019-03-17-aws-elastic-ip-details.md\" \u003e}}\n[2]: mailto:sam@stelfox.net\n[3]: https://github.com/sstelfox/stelfox.net/issues/new\n[4]: https://libreswan.org/wiki/FAQ\n","created_at":1553728290,"fuzzy_word_count":5100,"path":"/blog/2019/03/merging-overlapping-subnets/","published_at":1553728290,"reading_time":24,"tags":["aws","ipsec","linux","networking"],"title":"Merging Overlapping Subnets","type":"blog","updated_at":1553728290,"weight":0,"word_count":5052},{"cid":"45223b7061ca7eece276004807e83eb215448f9b","content":"\nOne of the Cisco Catalyst 3750 I had to work on recently had it's flash\ncompletely wiped. When this happens you can only flash the filesystem using the\nXMODEM serial console. This is a fairly well documented process on Windows. On\nLinux most of the documented ways involve switching between multiple utilities\nand can be tricky. I wanted to documented how I did this and possibly help\nother in the same situation.\n\nI'm doing this on Fedora, but the only thing specific to Fedora is installing\nthe packages which can be done with the following command (and are pretty\nstandard names):\n\n```console\ndnf install lrzsz screen -y\n```\n\nFrom an external perspective the switch was rapidly blinking it's `SYST` light\nwith all of the other lights off. I connected up a console cable, the serial\nport showed up as `/dev/ttyUSB0`. We'll connect to the serial port using screen\nwith the following command:\n\n```console\nsudo screen /dev/ttyUSB0 9600\n```\n\nPower cycling the switch gets you the boot messages that show us the issue:\n\n```console\nUsing driver version 1 for media type 1\nBase ethernet MAC Address: xx:xx:xx:xx:xx:xx\nXmodem file system is available.\nThe password-recovery mechanism is enabled.\nInitializing Flash...\nmifs[2]: 0 files, 1 directories\nmifs[2]: Total bytes     :    3870720\nmifs[2]: Bytes used      :       1024\nmifs[2]: Bytes available :    3869696\nmifs[2]: mifs fsck took 0 seconds.\nmifs[3]: 0 files, 1 directories\nmifs[3]: Total bytes     :   27998208\nmifs[3]: Bytes used      :       1024\nmifs[3]: Bytes available :   27997184\nmifs[3]: mifs fsck took 1 seconds.\n...done Initializing Flash.\ndone.\nLoading \"flash:/c3750-ipservicesk9-mz-15.0-2_SE10a.bin\"...flash:/c3750-ipservicesk9-mz-15.0-2_SE10a.bin: no such file or directory\n\nError loading \"flash:/c3750-ipservicesk9-mz-15.0-2_SE10a.bin\"\n\nInterrupt within 5 seconds to abort boot process.\nBoot process failed...\n\nThe system is unable to boot automatically.  The BOOT\nenvironment variable needs to be set to a bootable\nimage.\n\n\nswitch:\n```\n\nWe can confirm this isn't just a badly named file quickly:\n\n```console\nswitch: dir flash:/\nDirectory of flash://\n\n\n32513024 bytes available (1024 bytes used)\n\nswitch:\n```\n\nIf you you get the following error:\n\n```console\nswitch: dir flash:\nunable to stat flash:/: invalid argument\n```\n\nThe flash hardware hasn't been enabled yet. We need to initialize it with the\n`flash_init` command which will allow us access to the flash again:\n\n```console\nswitch: flash_init\nInitializing Flash...\nflashfs[0]: 0 files, 1 directories\nflashfs[0]: 0 orphaned files, 0 orphaned directories\nflashfs[0]: Total bytes: 32514048\nflashfs[0]: Bytes used: 1024\nflashfs[0]: Bytes available: 32513024\nflashfs[0]: flashfs fsck took 6 seconds.\n...done Initializing Flash.\n\nswitch: dir flash:/\nDirectory of flash://\n\n\n32513024 bytes available (1024 bytes used)\n\nswitch:\n```\n\nDefinitely nothing present. At this point we need to use XMODEM to get the\nfirmware file. You'll need to use your support contract to get the latest\nfirmware for your model of switch. I made sure this file was in my current\ndirectory (saves some time in a bit). For me this file was named\n`c3750-ipservicesk9-mz.150-2.SE10a.bin` you'll want to replace this with the\nappropriate filename you pull yourself. Right, carrying on.\n\nXMODEM is a very slow process especially when we use the default speed of 9600\nbaud. We can speed this process up by 12x by adjusting the baud rate. You can\nskip this bit though if you're willing to wait several hours for the file to\ntransfer.\n\nTo speed up the baud rate execute the following command, be aware that your\nterminal is expected to immediately become responsive and this is fine:\n\n```console\nswitch: set BAUD 115200\n```\n\nKill the screen session by pressing `Ctrl-a` quickly following by a lone `k`.\nIt will prompt you to confirm exiting the session, do so with `y` and start a\nnew session with the higher baud rate:\n\n```console\n$ sudo screen /dev/ttyUSB0 115200\n```\n\nPress return and you'll get back to the switch prompt. This next bit is tricky\nas there is about a ten second timeout between starting the copy command and\nneeding to being the transfer. If you don't quite make it I've got a tip after\nthe command below:\n\n```console\nswitch: copy xmodem:/ flash:/c3750-ipservicesk9-mz.150-2.SE10a.bin\n```\n\nQuick press `Ctrl-a` followed by `:`. In the prompt that shows up type in the\nfollowing:\n\n```console\nexec !! sx -b -X c3750-ipservicesk9-mz.150-2.SE10a.bin\n```\n\nIf you get an error, it reports as cancelled, it times out just try again. This\ntime though after pressing `Ctrl-a` and `:` you can press your up arrow to\nbring up the last run command. Assuming you typed it in correctly this should\ngive you enough time to start the transfer. It will show you the progress of\nthe transfer which depending on size will take between twenty and thirty\nminutes (or hours if you didn't update the baud rate).\n\nSite note: One of the benefits of performing the `sx` command through screen is\nthat if anything happens to your terminal the transfer will continue. This was\nparticularly important for me at the time as I found myself SSH'd into a laptop\nfar away from myself over an unstable and shared cell connection running these\ncommands.\n\nWhen it's complete you're going to want to ensure the `BOOT` variable properly\nmatches the filename you just transferred:\n\n```console\nset BOOT flash:/c3750-ipservicesk9-mz.150-2.SE10a.bin\n```\n\nWe're going to reset the baud rate back to normal (skip this if you didn't\nupdate your baud rate) which will cause you to loose the connection again:\n\n```console\nunset BAUD\n```\n\nDisconnect as we did before and reconnect at the 9600 rate. One final command\nshould start the switch and get you back into the good graces of the Cisco CLI:\n\n```console\nswitch: boot\n```\n\nOne final tip, if you want to access the boot menu of the switch I had a hell\nof a time trying to figure out how to actually send the break command to\ninterrupt the boot menu. None of the recommended combinations of `Ctrl`, `Alt`,\n`Shift`, `b`, `Break`, and `Scroll Lock`. You can instead press the `Mode`\nbutton on the front of the switch to emulate this break key.\n\nCheers friends. Hope this helps another CLI adventurer.\n","created_at":1553484390,"fuzzy_word_count":1100,"path":"/blog/2019/03/reflashing-cisco-catalyst-with-xmodem/","published_at":1553484390,"reading_time":5,"tags":["cisco","tips"],"title":"Reflashing Cisco Catalyst With XMODEM","type":"blog","updated_at":1553484390,"weight":0,"word_count":1030},{"cid":"7a90daf7b7c59c4a40b22df0b1596358b0ed93a6","content":"\nSometimes it becomes important to understand how your cloud provider implements\ncertain networking details. While working through an issue in AWS I needed to\nunderstand how they handle public IP addressing. While this issue for me was\nspecific to an Elastic IP all of their public addresses are handled this way\nand may bite you even without them. The problems specifically crop up when a\nhosted piece of software does NAT traversal detection and changes it's behavior\nbased on the result.\n\nAmazon attaches public IP address to EC2 instances (and likely other of their\nhosted services) through 1:1 NAT mapping of external to internal addresses. All\ntraffic will reach your instance without issue, but your system's native\nnetworking stack will (by default) not become aware of what that public IP is.\n\nIn most cases this isn't an issue, the traffic ends up at your instance and is\nproperly changed to the internal address so your system responds without any\nadditional configuration work.\n\nThe first issue is tunneled traffic. I [experienced this][1] in my last post\nwith IPSec. The tunneled traffic is not modified by going through a 1:1 NAT and\nyour host will not recognize the external address inside the tunnel, which in\nmost cases will result in silently dropped packets (the \"oh this isn't for me\"\nsyndrome).\n\nThere are ways to get your system to respond to tunneled packets with other IP\naddresses such as treating it like a high-availability shared virtual server IP\naddress but I have yet to find a clean way to deal with responding to the\naddress. IPSec had a way to deal with this built in directly and I'd refer you\nback to [my other post][1] to read about that.\n\nUsually sniffing the tunneled traffic is enough to diagnose these issues.\nWhat is a bit more pernicious, is when a server changes its behavior when it\ndetects a NAT in place. This requires deeper understanding of the protocol in\nuse and in some cases the specific server implementation. Once again this post\nis ultimately about IPSec (and specifically libreswan).\n\nWhile you don't have to make any changes to get IPSec working with AWS public\nIPs it will detect the NAT and start encapsulating the traffic. The\nencapsulation will require an additional port being opened on your security\ngroup firewall but more importantly, it will add variable latency and\nadditional processing overhead to each packet. Not everything is sensitive to\nthis but you'll be needlessly adding overhead to your network traffic is you\ndon't address it. In my case (AWS VPC to AWS VPC) there aren't any meaningful\nNATs in place that require this encapsulation to function appropriately.\n\nIf you encounter this and are using libreswan on AWS you only need to add the\nfollowing two lines to your tunnel's config disable this behavior:\n\n```\nencapsulation=no\nnat-keepalive=no\n```\n\nI hope this helps someone else diagnose weird behavior when working with public\nIP addresses on AWS.\n\n[1]: {{\u003c relref \"2019-03-14-fighting-with-ipsec.md\" \u003e}}\n","created_at":1552857990,"fuzzy_word_count":500,"path":"/blog/2019/03/aws-elastic-ip-details/","published_at":1552857990,"reading_time":3,"tags":["aws","networking"],"title":"AWS Elastic IP Details","type":"blog","updated_at":1552857990,"weight":0,"word_count":487},{"cid":"980d6c3f72f7e2dc998948e6dd074e345283d2bb","content":"\nIPSec is a well known and well understood protocol that is pretty easy to get\nsetup and going... Most of the time. While setting up an IPSec tunnel to an AWS\nhost I came across a new and unique experience that didn't seem to have an\neasily searchable solution.\n\nI had two CentOS 7 EC2 instances, each set up with their own Elastic IP in a\ndefault VPC. I installed and configured libreswan with the following config:\n\n```\nconn setup\n  protostack=netkey\n\nconn site-tunnel\n  auto=start\n  pfs=yes\n\n  leftid=@left_tunnel_server_name\n  rightid=@right_tunnel_server_name\n  left=%defaultroute\n  right=\u003cremote-ip\u003e\n\n  authby=rsasig\n  leftrsasigkey=\u003cleft-key\u003e\n  rightrsasigkey=\u003cright-key\u003e\n```\n\nThe IPSec link came up without issue but pings were timing out without a\nresponse. When sniffing the incoming packets, I was appropriately seeing the\nICMP echo requests coming in encapsulated in the tunnel and being re-injected.\nThere was never a response being generated.\n\nThe traffic capture was also pretty clear as to why. The encapsulated packet\nhad the elastic IP as the destination which the EC2 instance doesn't\n*ACTUALLY* have and wasn't aware... So it didn't attempt to respond.\n\nI went down a bit of a rabbit hole attempting to get the instance to recognize\nthat address with limited success. Adding the IP to an additional loopback\ninterface... IPTables DNAT rules... Some of them worked but they were dirty\nhacks that would have been left until something blew up...\n\nThe solution was simple and even improved the overall performance of the\nlink. By default libreswan operates in `tunnel` mode which sends along the\ndestination information as well (and can be used for routing network across).\nFor the use I almost always use these links for, `transport` is more\nappropriate and has less protocol overhead.\n\nBy adding the following line to the connection configuration on both sides the\nproblem vanished:\n\n```\n  type=transport\n```\n\nWith any luck the next person that comes across this issue will find this post\nand their life will be a tad bit easier.\n","created_at":1552613190,"fuzzy_word_count":400,"path":"/blog/2019/03/fighting-ipsec-on-aws/","published_at":1552613190,"reading_time":2,"tags":["ipsec","linux","networking"],"title":"Fighting IPSec on AWS","type":"blog","updated_at":1552613190,"weight":0,"word_count":314},{"cid":"9399f76dc354201d0449dfb3a05b6d48dd3dd7ec","content":"\nGit was built and developed with the intention of being a distributed reversion\ncontrol system. Most people now use it with one or another central repository\neven when working on large teams which is perfectly fine if that model works\nfor you and your team.\n\nIt can be useful to quickly work with others on private repositories without\nrequiring them to get on your platform of choice, or for sensitive repositories\nkeep the repository entirely under your control. Occasionally platforms like\nGitHub have service outages, and while you won't have access to any of your\nintegrations a private repository can quickly allow your team to keep\ncollaborating.\n\nIf you're using Linux or Mac OS X, setting up a local repo that you can push to\nis trivial. You simply create a place for it, initialize it as a git repo, then\npush to it like so:\n\n```\nmkdir -p ~/repos/private_repo.git\ncd ~/repos/private_repo.git\ngit init --bare\n```\n\nIn the directory that contains your current repository add the new repo\ndestination as an origin and push the contents to it:\n\n```\ngit remote add local ~/repos/private_repo.git\ngit push local --all\n```\n\nHaving another copy of your repository locally doesn't do you much good. To\npush your repository to another system, the only requirements is an account on\nan SSH server that has the git binary installed.\n\nLog in to your SSH server and setup the repository like before:\n\n```\nssh user@my-remote-system.example.tld\nmkdir ~/repos/private_repo.git\ncd ~/repos/private_repo.git\ngit init --bare\n```\n\nIn the local copy of your current repository you add an origin just like\nbefore, but use a slightly different syntax to indicate its on a remote system\ninstead:\n\n```\ngit remote add remote user@my-remote-system.example.tld:~/repos/private_repo.git\ngit push remote --all\n```\n\nTo clone from it you would use the same syntax as you did for the origin:\n\n```\ngit clone user@my-remote-system.example.tld:~/repos/private_repo.git\n```\n\nIf multiple users are allowed on the SSH hosts and you want to allow them\naccess to the repository, you'll need to place the directory in a place all\nusers can access and handle permissions on.\n\nYou can have as many origins as you'd like and push / pull from them\nindependently.\n","created_at":1545418410,"fuzzy_word_count":400,"path":"/blog/2018/12/hosting-your-own-private-git-repo/","published_at":1545418410,"reading_time":2,"tags":["linux","tips"],"title":"Hosting Your Own Private Git Repo","type":"blog","updated_at":1545418410,"weight":0,"word_count":345},{"cid":"62d299c69b4a88a73e7feb434f5862e60a9319c7","content":"\nI recently had cause to use OpenVPN on the standard HTTPS port to protect my\ntraffic. This was done as a compromise with administrators who didn't want to\nchange their egress filtering, but wanted to allow me to continue doing my\nnormal work.\n\nI already run several webservers, including this one, and didn't want to give\nup exclusive access to the precious TCP port 443. The recommended way to deal\nwith this is to make use of the `port-share` option built into OpenVPN. This\nleft me with two choices, run this on an existing server sharing the port with\nexisting websites, or setup a dedicated server just for this instance of\nOpenVPN.\n\nI couldn't find any other posts that took a look at how this port sharing\neffects the performance of the HTTPS server so I felt like doing a quick\nanalysis for other curious parties.\n\nI setup a fresh Nginx server with Let's Encrypt certificates that mimics my\nproduction setup and used `ab` to bench the service for 30 seconds. The mean\nmeasured rate was 288.70 +/- 14.39 requests per second. Mean request\nfullfillment took 3.47 +/- 0.17 ms.\n\nAfter enabling `port-share` on OpenVPN I reran the exact same test. The result\nwas 139.76 +/- 67.68 requests per second. Mean measured request fullfillment\n7.44 +/- 4.16 ms.\n\nThat is a 51% peak request handling reduction, each request has an additional\n4-8ms of latency, and an almost 40x increase of jitter. That is a massive\nrelative impact but the vast majority of the websites I run need won't be\nterribly impacted by that additional latency.\n\nI ultimately ended up setting up a seperate server for OpenVPN as I didn't want\nto mess with known working systems.\n","created_at":1541365742,"fuzzy_word_count":300,"path":"/blog/2018/11/performance-impact-of-openvpn-port-sharing/","published_at":1541365742,"reading_time":2,"tags":["linux","openvpn","performance","security"],"title":"Performance Impact of OpenVPN Port Sharing","type":"blog","updated_at":1541365742,"weight":0,"word_count":286},{"cid":"37b288ce2d331652640c7bc4e0529908138332cd","content":"\nDNS-over-TLS is a relatively new privacy enhancing protocol that encrypts all\nof your DNS requests to a trusted server. In an age when airports, and coffee\nshops are outsourcing 'free wifi' to corporate entities that are likely\nharvesting as much data as they can this is a nice addition. I largely use VPNs\nwhen connected to these access points which provides at least as good\nprotection as DNS-over-TLS which has caused me to largely overlook this\ndevelopment.\n\nWhen I found out that Android 9 natively supports this through the 'Private\nDNS' feature I got significantly more interested. I've always wanted to dictate\nwhat DNS server my phone used, not for privacy reasons but rather to provide\nanother layer of security when not connected to my VPN. When you're in control\nof your DNS server you can blacklist known malicious domains and as an added\nbonus the worst of the ad and tracking networks.\n\nTurns out DNS-over-TLS is incredibly simple. It makes no changes to normal DNS\npackets, it just wraps the TCP queries in a TLS tunnel with a preconfigured\nserver address for certificate validation.\n\nI use unbound for my DNS server of choice which natively supports exposing a\nTLS protected port, but with a very limited config. My preferred choice of\ndealing with this is instead to use Nginx to proxy locally to unbound, allowing\nme to ensure the TLS config matches current best practices and limits access to\nthe servers TLS private key to a single service.\n\nThe downside of using Nginx is that you lose the ability to log which internet\naddresses are making requests (as the stream access_log directive doesn't seem\nto be working for the packaged Nginx server). Since this is an unauthenticated\nservice I'm slightly concerned about anonymous use and abuse but will address\nthat if it ever becomes an issue.\n\nFirst off we need to ensure we have a working local resolver on our host. I'll\nuse unbound here as its my preference but this will work just as well with Bind\nor any other DNS resolver that supports TCP queries. If you want to use another\nserver, just skip past the unbound config to the Nginx section.\n\n## Unbound\n\n```\ndnf install unbound -y\n```\n\nI clear out some of the garbage default files that ship with the package, and\nperform the automated control key creation:\n\n```\nrm -f /etc/unbound/{conf.d,keys.d,local.d}/*\nunbound-control-setup\n```\n\nI then drop in my config to `/etc/unbound/unbound.conf`. I've included comments\nin the config around options that aren't immediately obvious from their names.\n\n```\n# /etc/unbound/unbound.conf\n\nserver:\n  verbosity: 1\n\n  interface-automatic: no\n  num-threads: 4\n\n  # Disable statistics collection\n  statistics-interval: 0\n  statistics-cumulative: no\n\n  # Reserve this many ports per-thread to prevent conflicts\n  outgoing-range: 4096\n\n  # Restrict the ports being used to match a tighter SELinux policy\n  outgoing-port-avoid: 0-32767\n  outgoing-port-permit: 32768-60999\n\n  # Per-thread limits for active TCP connections\n  outgoing-num-tcp: 100\n  incoming-num-tcp: 100\n\n  # Bind to port with SO_REUSEPORT so queries can be distributed over threads\n  so-reuseport: yes\n\n  # Maximum UDP response size, this can prevent some fragmentation issues\n  max-udp-size: 3072\n\n  # For our TCP clients, allow us to still use UDP to make upstream requests.\n  # All of the TLS requests will come in over TLS even if they'd happily be\n  # served in a single UDP packet.\n  udp-upstream-without-downstream: yes\n\n  access-control: 127.0.0.1 allow\n  access-control: ::1 allow\n  access-control: 0.0.0.0/0 deny\n\n  chroot: \"\"\n  directory: \"/etc/unbound\"\n  username: \"unbound\"\n\n  use-syslog: yes\n  log-time-ascii: yes\n\n  pidfile: \"/var/run/unbound/unbound.pid\"\n\n  hide-identity: yes\n  hide-version: yes\n  hide-trustanchor: yes\n\n  # Perform various hardening on queries\n  harden-short-bufsize: yes\n  harden-large-queries: yes\n  harden-glue: yes\n  harden-dnssec-stripped: yes\n  harden-below-nxdomain: yes\n  harden-referral-path: yes\n  harden-algo-downgrade: yes\n\n  # Perform some upstream privacy protection (and in some cases performance\n  # improvements) on our queries\n  qname-minimisation: yes\n\n  # Aggressive NSEC uses the DNSSEC NSEC chain to synthesize NXDOMAIN and other\n  # denials, using information from previous NXDOMAINs answers.\n  aggressive-nsec: yes\n\n  # Use 0x20-encoded random bits in the query to foil spoof attempts. This\n  # feature is an experimental implementation of draft dns-0x20.\n  use-caps-for-id: yes\n\n  # Enforce privacy of these addresses. Strips them away from answers to\n  # protect against potential rebind attacks at the expense of potential DNSSEC\n  # validation failures. Since we're directly serving clients this shouldn't be\n  # an issue.\n  private-address: 10.0.0.0/8\n  private-address: 172.16.0.0/12\n  private-address: 192.168.0.0/16\n  private-address: 169.254.0.0/16\n  private-address: fd00::/8\n  private-address: fe80::/10\n  private-address: ::ffff:0:0/96\n\n  # In the event this server gets unwanted replies, it will clear the cache as\n  # a safety measure to flush potential poisonings out of it\n  unwanted-reply-threshold: 10000\n\n  # Don't allow this server to... query this server...\n  do-not-query-localhost: yes\n\n  # Don't prefetch cache entries that are about to expire\n  prefetch: no\n\n  # Round-robin returned RRSET queries\n  rrset-roundrobin: yes\n\n  minimal-responses: yes\n\n  module-config: \"validator iterator\"\n\n  # Perform tests automatically to make sure this resolver is ready for root\n  # key rollovers\n  root-key-sentinel: yes\n\n  val-clean-additional: yes\n  val-log-level: 2\n\n  # Serve expired responses from cache, with TTL 0 in the response,\n  # and then attempt to fetch the data afresh.\n  serve-expired: yes\n\n  # Enable pulling updated anchor trusts automatically\n  auto-trust-anchor-file: \"/var/lib/unbound/root.key\"\n\n  # Trust anchor signaling sends a RFC8145 key tag query after priming.\n  trust-anchor-signaling: yes\n\n  # Instruct the auto-trust-anchor-file probing to avoid changes to anchors\n  # using 30 days hold down\n  add-holddown: 2592000\n  del-holddown: 2592000\n\nremote-control:\n  control-enable: yes\n  control-interface: ::1\n\n  server-key-file: \"/etc/unbound/unbound_server.key\"\n  server-cert-file: \"/etc/unbound/unbound_server.pem\"\n\n  control-key-file: \"/etc/unbound/unbound_control.key\"\n  control-cert-file: \"/etc/unbound/unbound_control.pem\"\n\nauth-zone:\n  name: \".\"\n  for-downstream: no\n  for-upstream: yes\n  fallback-enabled: yes\n  master: b.root-servers.net\n  master: c.root-servers.net\n  master: e.root-servers.net\n  master: f.root-servers.net\n  master: g.root-servers.net\n  master: k.root-servers.net\n```\n\nI always need to stress changes like these should be understood before being\nblindly used. Make sure you start and enable the service:\n\n```\nsystemctl start unbound.service\nsystemctl enable unbound.service\n```\n\nPerform various queries against the service to ensure it's working as intended:\n\n```\ndig +tcp google.com @::1\ndig +tcp reddit.com @::1\ndig +tcp stelfox.net @::1\n```\n\nIf you get reasonable responses from those queries then we have a working\nresolver. The next step is to add our TLS layer with Nginx.\n\n## Nginx\n\nWith a resolver setup the next bit is to setup Nginx to proxy requests to the\nresolver. You need to have a trusted TLS certificate and key for your server's\ndomain name. Let's Encrypt works great for this and I use both an ECDSA\ncert/key pair and an RSA cert/key pair but you can use one or the other if that\nis your preference. I'll cover requesting these certificates in a future post,\nin the mean time there are plenty of existing Let's Encrypt tutorials.\n\nThis can be a bit tricky as anyone reading this might have an Nginx config in\nplace already. It'll be up to you to merge this config with yours. To assist\nwith this merging I've kept the Nginx config as minimal while still being\ncompletely functional as possible and avoided using includes to increase the\nclarity of the config.\n\nOn Fedora 28 you'll need both the `nginx` and the `nginx-mod-stream` package.\nAfter installing them both you'll want to place the following config to\n`/etc/nginx/nginx.conf`.\n\n```\n# /etc/nginx/nginx.conf\n\nerror_log /var/log/nginx/error.log warn;\nuser nginx;\nworker_processes auto;\n\nevents {\n  worker_connections 1024;\n}\n\n# Needs to be loaded before the http or stream blocks are used\nload_module \"/usr/lib64/nginx/modules/ngx_stream_module.so\";\n\n# This is where your normal http block would live\n#http {\n#}\n\nstream {\n  upstream dns_tcp_servers {\n    server [::1]:53;\n  }\n\n  server {\n    listen 853 ssl;\n    proxy_pass dns_tcp_servers;\n\n    ssl_certificate /etc/nginx/nginx.ec.crt;\n    ssl_certificate_key /etc/nginx/nginx.ec.key;\n\n    ssl_certificate /etc/nginx/nginx.rsa.crt;\n    ssl_certificate_key /etc/nginx/nginx.rsa.key;\n\n    ssl_handshake_timeout 30s;\n    ssl_session_cache shared:DNSTCP:50m;\n    ssl_session_tickets off;\n    ssl_session_timeout 1h;\n\n    ssl_protocols TLSv1.2;\n    ssl_ciphers 'ECDHE-ECDSA-AES256-GCM-SHA384:ECDHE-RSA-AES256-GCM-SHA384:ECDHE-ECDSA-CHACHA20-POLY1305:ECDHE-RSA-CHACHA20-POLY1305:ECDHE-ECDSA-AES128-GCM-SHA256:ECDHE-RSA-AES128-GCM-SHA256:ECDHE-ECDSA-AES256-SHA384:ECDHE-RSA-AES256-SHA384:ECDHE-ECDSA-AES128-SHA256:ECDHE-RSA-AES128-SHA256';\n    ssl_prefer_server_ciphers on;\n  }\n}\n```\n\nThe SSL configuration is largely what Mozilla currently recommends for a\n'Modern' config. Any client that supports DNS-over-TLS is new enough that\nthey'll have access to modern queries. You'll need to put your key and\ncertificates in the appropriate locations for that config.\n\nWith that in place start up the server:\n\n```\nsystemctl start nginx.service\nsystemctl enable nginx.service\n```\n\nYou'll need to allow `953/tcp` through your firewall and will likely need to\nmake changes to your SELinux policy to allow Nginx to listen on that port.\n\nI haven't found a good way to directly test the resolver using command line\nclients but by configuring my Android phone towards my server and temporarily\nadding the following to my unbound server config I can that queries being made\nand responded to appropriately:\n\n```\nlog-queries: yes\nlog-replies: yes\n```\n\nThe next steps are on you. Configure unbound with your domain blacklists of\nchoice and voila additional privacy and security for those times when VPNs are\nto power hungry.\n","created_at":1541192942,"fuzzy_word_count":1400,"path":"/blog/2018/11/run-your-own-dns-over-tls-server/","published_at":1541192942,"reading_time":7,"tags":["linux","nginx","security"],"title":"Run Your Own DNS-over-TLS Server","type":"blog","updated_at":1541192942,"weight":0,"word_count":1370},{"cid":"96419ef9a4d2cf43339215fca64639727fb57fe9","content":"\nWhile working on a replacement webserver, I encountered some odd behavior which\ntook a bit to track down to CloudFlare. This isn't a bug or an issue with\nCloudFlare, it was just unexpected.\n\nThe server was configured to respond to `www.example.tld` as well as\n`example.tld`, to both encrypted and unencrypted connections. Any requests to\nthe `www.` domain get redirected to `https://example.tld`. The config was\nroughly:\n\n```\nserver {\n  listen 80;\n  listen [::]:80;\n\n  listen 443 ssl http2;\n  listen [::]:443 ssl http2;\n\n  server_name www.example.tld;\n\n  # Valid / Basic SSL settings (omitted for brevity)\n\n  return 301 https://example.tld$request_uri;\n}\n```\n\nThis worked fine. To match this config, I configured the unencrypted root\ndomain to redirect traffic from http to https with a config like the following:\n\n```\nserver {\n  listen 80;\n  listen [::]:80;\n\n  server_name example.tld;\n  return 301 https://example.tld$request_uri;\n}\n\nserver {\n  listen 443 ssl http2;\n  listen [::]:443 ssl http2;\n\n  server_name example.tld;\n\n  # Valid / Basic SSL settings (omitted for brevity)\n\n  # Normal site config\n}\n```\n\nWhen I attempted to go to the site, it performed the https redirect, but\ncontinued to try to redirect the browser. CloudFlare was hitting my upstream on\nthe unencrypted port and always getting the redirect message.\n\nI wanted to leave this in place so clients would have the same experience when\nI bypassed CloudFlare's HTTP CDN proxy. The way I solved this was to simply\nturn on 'Full (strict)' SSL setting to ensure CloudFlare also connected to my\nupstream on HTTPS, and relied on the 'Always Use HTTPS' setting when CloudFlare\nwas active to maintain the same behavior. If you don't have a valid SSL\ncertificate (such as a self signed certificate) you'll have to use 'Full SSL'\ninstead.\n\nFunnily enough, this is apparently [common enough][1] of an issue to be\nmentioned directly in the Help for the SSL configuration within CloudFlare.\n\n[1]: https://support.cloudflare.com/hc/en-us/articles/115000219871\n","created_at":1540182969,"fuzzy_word_count":300,"path":"/blog/2018/10/weird-cloudflare-behavior/","published_at":1540182969,"reading_time":2,"tags":["nginx","linux","cloudflare"],"title":"Weird CloudFlare Behavior","type":"blog","updated_at":1540182969,"weight":0,"word_count":298},{"cid":"40ffa1f5c26760b4743e5d83a125a606a4cc161c","content":"\nThis last Thursday I had the privilege of giving a talk at our local Linux User\nGroup about diagnosing firewall issues on Linux entitled \"It's Never the\nFirewall: Diagnosing Linux Firewall Issues\". I really enjoyed giving the talk,\nhowever, I left a few questions unanswered. While I may do a more extensive\npost on everything that I went through in the talk (I have been lax on writing\ncontent for this blog), this post is more to answer the outstanding questions\nand of course [to make my slides available][1].\n\nAs far as I remember the only question I wasn't able to answer was about file\ndescriptors related to TCP connections. It isn't exactly a firewall issue but\nexhaustion of file descriptors is one of the issues I've seen blamed on the\nfirewall (which in turn was relevant to the talk).\n\nEstablished TCP connections consume file descriptors on Linux systems. Each\nuser session is restricted by a limit defined by the system administrator (or\nmore likely using the distribution's defaults). Some services consume a large\nnumber of file descriptors for their basic operations before any network\nconnections are involved ([Redis][2] and [Elasticsearch][3] being two common\nservices that both recommend increasing this limit).\n\nWhen file descriptors are exhausted there are two common behaviors of\napplications depending on how they're written. The first and more common\nbehavior will an immediately terminated connection, which to the user will look\nquite similar to a iptables REJECT response. The less common behavior is\nwaiting until a file descriptor to become available before accepting the\nconnection, which will result in a hanging incomplete connection much like an\niptables DROP target.\n\nAn important caveat here is that I'm talking about user perception. If you look\nat the packets being exchanged, it's clear that this isn't the case.\n\nA quick way to diagnose if this is an issue is to compare the current number of\nopen file descriptors by a user to their limit. In my experience most of these\ncommands are available to unprivileged users which is also handy if there is a\ncomplicated process for using or executing commands with elevated privileges.\n\nFirst let's check what the total number of open file descriptors are on the\nsystem:\n\n```\n[user@sample-host] ~ $ lsof | wc -l\n185779\n```\n\nIf it is under 10,000 there is a very good chance this is not the problem\nyou're looking for. If you see a large number like the output above, it is\nworth investigating more. The host that I sampled that from currently has no\nfile descriptor issues.\n\nFirst you'll want to identify the main process of the application that is\nhaving issues. Make sure you note the user that the application is running as.\nIdentify the current configured limits for that process (replace \u003cPID\u003e with the\npid you just found):\n\n```\n[user@sample-host] ~ $ grep 'open files' /proc/\u003cPID\u003e/limits\nMax open files            1024                 4096                 files\n```\n\nThis tells the process is currently operating with a soft limit of 1,024 file\ndescriptors and a hard limit of 4,096. This is where the diagnostics can get a\nlittle fuzzy. File descriptors are limited by user *session* not by process or\nuser which is what we can directly query on. To get a rough idea of where we're\nat lets query those two specifics.\n\n```\n[user@sample-host] ~ $ lsof -p \u003cPID\u003e | wc -l\n5\n[user@sample-host] ~ $ lsof -u \u003cUSER\u003e | wc -l\n721\n```\n\nThis particular example is enough for us to clearly see file descriptors are\nnot an issue. The current number of file descriptors open for the user (721) is\nbelow the process's soft limit of 1,024. If the number of file descriptors is\ncloser to our limits you'll have to get an exact count of what is going on.\nUnfortunately I don't know an easy one liner to check this.\n\nTo get an exact count you need to enumerate all processes with the same session\nidentifier (SID) as your target process, query them all for the current file\ndescriptor consumption, and add them up into a total. As a rule of thumb I\nrecommend leaving about a 25% overhead for your service during peak load which\ncan be set in your system limits file for that service's user.\n\n[1]: /files/it_is_never_the_firewall.pdf\n[2]: https://redis.io/topics/clients\n[3]: https://www.elastic.co/guide/en/elasticsearch/reference/current/file-descriptors.html\n","created_at":1539545769,"fuzzy_word_count":700,"path":"/blog/2018/10/its-never-the-firewall/","published_at":1539545769,"reading_time":4,"tags":["firewall","iptables","linux","security"],"title":"It's Never the Firewall","type":"blog","updated_at":1539545769,"weight":0,"word_count":699},{"cid":"c84b0468add18e38b9729da72c8bc63039ec68c1","content":"\nRoute53 doesn't allow multiple definitions of the same name/type pair of DNS\nentries which is quite a headache. This is the first time I've had a conflict\nof a TXT record in Route53 at the base, specifically both Google's site\nverification, and SPF records both want to live at the root of the domain. The\nsite verification record needs to stay around as Google periodically\nre-verifies the domain.\n\nTo get this to work you need to quote both the Google verification string and\nthe SPF record, but you also have to ensure that there is a newline in the\nfield. The contents of the entry should look something like this:\n\n```\n\"google-site-verification=\u003cauth string\u003e\"\n\"v=spf1 include:_spf.google.com ~all\"\n```\n\nYou can use [Google's MX Toolbox][1] to verify the SPF record.\n\n[1]: https://toolbox.googleapps.com/apps/checkmx/check\n","created_at":1528997769,"fuzzy_word_count":200,"path":"/blog/2018/06/spf-and-google-site-verification-in-route-53/","published_at":1528997769,"reading_time":1,"tags":["aws","dns","tips"],"title":"SPF and Google Site Verification in Route 53","type":"blog","updated_at":1528997769,"weight":0,"word_count":125},{"cid":"8ee3600704ce0800ee160fc201bbbd44ed2979c5","content":"\nI have the privilege of working full remote. To stay connected with our other\nremote workers and the main office we keep a live video conference going all\nthe time. It's pretty convenient and definitely allows me to continue to feel\nconnected with the company.\n\nDuring one of our stand ups, a coworker mentioned that they'd like to see how\nthings looked over time. I have three 1440p monitors and largely leave the\nvideo conference on my right most window, which makes recording (an audio free)\ntimelapse pretty easy with FFmpeg.\n\nThere are a couple of stack overflow posts about this but I wanted to use VP9\n(really I wanted to use AV1, but it isn't well supported yet). This adds a bit\nof a trick as VP9 wants to analyze a bit of the video and will break if there\naren't enough frames so some additional options are needed to get it to work.\n\nThese are the settings that worked well for me:\n\n```\nffmpeg -f x11grab -framerate 1 \\\n  -thread_queue_size 2048 -probesize 10M -analyzeduration 10M \\\n  -s 2560,1440 -i :1.0+5120,0 \\\n  -vf settb=\\(1/30\\),setpts=N/TB/30 -r 30 \\\n  -s 1280,720 -vcodec vp9 -crf 24 video_conference_$(date +%Y-%m-%d).mkv\n```\n\nI organized the parameters so they could easily be explained on a line by line\nbasis. The first line captures the X11 render output (sorry wayland users,\nyou'll have to look elsewhere) with a framerate of 1/second.\n\nThe second line holds the parameters that tune the analyze phase for the low\nbandwidth and low framerate, these values are byte size not duration (10M is\n10Mb not 10 minutes).\n\nThat third line will likely need to be customized to your display setup. I\nwanted to capture the size of one of my monitors (-s 2560x1440) which is pretty\nstraight forward. Unusually, my X display is ':1' (visible by printing the\n$DISPLAY environment variable) instead of the normal ':0'. Finally, x11grab\ntreats multi-monitor displays as one buffer, so to get my right most monitor I\nhave to add an X offset of the combined widths of my other two monitors (that's\nthe 5120,0).\n\nThis fourth line is kind of black magic, but ultimately it's adding some\nmetadata to the stream saying that the output will be 30fps, and don't complain\nthat the original framerate was 1fps.\n\nThe last line controls the output, I wanted to resize the video down to 720p\nwith a reasonable encode quality, using the VP9 codec I mentioned before, and\nstoring it with a date stamped file name.\n\nWhen your done recording just Ctrl-C out of it (and give it a few seconds, it\nmay seem like it hung while writing out the data).\n\nIf I was going to make a recommendation, you may want to speed this up to 60fps\ninstead of 30 as a full workday will be a tediously long 8 minute video to\nwatch.\n","created_at":1528497459,"fuzzy_word_count":500,"path":"/blog/2018/06/timelapse-of-a-linux-desktop/","published_at":1528497459,"reading_time":3,"tags":["linux","tips"],"title":"Timelapse of a Linux Desktop","type":"blog","updated_at":1528497459,"weight":0,"word_count":474},{"cid":"c80eccad28c21b64a231e3741c7343f388eb09fb","content":"\nI recently moved to an area with Google Fiber and jumped on the chance to have a cheap and fast connection, and I didn't need to sell my soul to certain other companies. I already owned a [Ubiquiti EdgeRouter PoE 5](https://www.ubnt.com/edgemax/edgerouter-poe/) which has been battle tested at easily routing a gigabit worth of small packets.\n\nWhen setting up my service, the representative I talked to told me I was able to use my own router, but I would still need to get a Google Fiber Network Box. I confirmed this with the staff who handed me the network box, informing me I just had to \"put it into bridge mode\". Turns out there is no bridge mode, and no you don't need the Fiber Box.\n\nAfter much Googling and confirming those two points, I came across [other](http://comptechrt.blogspot.com/2015/10/google-fiber-with-3rd-party.html) [people](https://www.stevejenkins.com/blog/2015/11/replace-your-google-fiber-network-box-with-a-ubiquiti-edgerouter-lite/) have been in this situation [before](https://productforums.google.com/forum/#!msg/fiber/AbNh8ij72Mw/l0quYBiiCJ8J) and have written up their own very good documentation on very similar setups. A lot of the information is at least a year old so it was worth going over carefully, and didn't go quite as far as I'd like.\n\nOne thing I will say ahead of time. I'm not adventuring into the TV service as I don't have it. From the messages I read, if you want this a cheap Gigabit switch between your router and the fiber jack that you can additionally plug the TV box into will do the trick (but you want to avoid powering the jack with PoE if you go this route).\n\nThere was some discussion from an official Google representative in one of the many threads that made a passing mention they would switch to untagged traffic so I wanted to verify what was happening. For this I [became the wire]({{\u003c ref \"2018-05-13-quick-and-silent-gigabit-packet-interception\" \u003e}}) and sniffed the traffic directly. The settings on the wire matched the contents of a deleted post from a Google representative early on in [this thread](https://productforums.google.com/forum/#!msg/fiber/AbNh8ij72Mw/l0quYBiiCJ8J) reproduced here for future reference:\n\n\u003e Here's the gory details if you really want to use your own router:\n\u003e\n\u003e 1. Traffic in/out of the fiberjack is vlan tagged with vlan2.\n\u003e 2. DHCP traffic should have 802.1p bit = 2\n\u003e 3. IGMP traffic should have 802.1p bit = 6\n\u003e 4. All other internet traffic 802.1p bit = 3\n\u003e\n\u003e You can send data without the 802.1p bits but your performance will get throttled to something like 10Mbit.\n\u003e\n\u003e NOTE:  This data is subject to change. We are planning on changing the data in/out of the fiberjack to be untagged, which will then make it really easy for you to connect your own router.\n\u003e\n\u003e A word of warning, most consumer routers don't have hardware forwarding (that is my feeble understanding) so you might not be happy with the performance on your network, and which will also probably affect tv service quality.\n\nOne other piece of information that I gleaned from the traffic capture is that the IPv6 prefix length of /56. If you want to reproduce this, look into the DHCPv6 messages for a pair of solicit / request (actually a response but that is how it is labeled).\n\nI had some initial concern about VLAN tagging as I suspected it may force software handling of all routed packets. The EdgeRouter PoE has hardware offloading for VLAN tagging available so this is not an issue for this router. For anyone using this as a reference for another router, you'll want to make sure yours supports this.\n\n## Basic Connectivity\n\nThe first task was to get basic IPv4 connectivity going. When I started the configuration my router was on version 1.9.1.1, which I upgraded to 1.10.1 *after* I finished all of the configuration. I didn't have any issues with the software, everything seemed very stable.\n\nThe basic connectivity just requires tagging outbound traffic with VLAN 2, to get the full speed the packets additionally need to have the correct 802.1q QoS tags. The one bonus we'll add that is totally unecessary is powering the jack with PoE since we're already going to messing with that interface.\n\n*Sidenote: If you want to play with egress values, changing after the fact requires editing of the [config on disk and a full restart]({{\u003c relref \"#changing-the-egress-qos-value\" \u003e}}). It's kind of a pain to go through a bunch of different options.*\n\nThe EdgeRouter PoE doesn't seem to be able to adjust QoS settings based on the protocol (I could be wrong I didn't look into this too deeply, maybe the advanced traffic control stuff?), but since I don't have the TV service, I don't have the IGMP traffic to worry about. That just leaves DHCPv4 packets.\n\nI confirmed that DHCPv4 will get an address when tagged with a QoS value of 3 (which is what the bulk traffic should be tagged with). Later on, I go into detail [about an issue]({{\u003c relref \"#initial-connectivity-time\" \u003e}}) that I think could be potentially related to this but I haven't done additional testing.\n\nI tried several mapping values for different types of traffic and settled on mapping everything to a QoS value of 3 (option 3 below). The mappings that I tried are:\n\n1. 0:2\n2. 0:3\n3. 0:3 1:3 2:3 3:3 4:3 5:3 6:3 7:3\n4. 0:3 1:3 2:3 3:3 4:4 5:5 6:6 7:7\n5. 0:6\n\nTo configure the router, connect the fiber jack to eth0 on the router and make sure the fiber jack doesn't have its external power connected. SSH into your router (this will depend on your current settings, or do a full reset and use the default static IP setting) and go into configure mode:\n\n```console\n$ configure\n```\n\nThere are a lot of changes that need to be made, the one thing you'll want to double check is the name of your firewall rules which should have already been setup. By default they get named \"WAN_IN\", \"WANv6_IN\", \"WAN_LOCAL\", and \"WANv6_LOCAL\" and I didn't need to make any changes to them to get things working. Because there are so many changes from the default WAN settings, we're going to start from a clean slate:\n\n```console\n# delete interfaces ethernet eth0\n```\n\nNow we need to setup the physical connection details again and we'll add the PoE configuration to power the jack. Please note that these will not go into effect until we commit them (which we're going to wait to do until later).\n\n```console\n# edit interfaces ethernet eth0\n# set description \"Google Fiber Jack\"\n# set duplex auto\n# set speed auto\n# set poe output 48v\n# exit\n```\n\nWe're now going to want to create the VLAN interface on eth0, which is done through the \"vif\" sub-commands off the interface. While not necessary yet, we'll also place the IPv6 firewalls in place, even if you don't intend to using IPv6 having these in place can prevent accidents if it ever gets enabled.\n\n```console\n# edit interfaces ethernet eth0 vif 2\n# set description Internet\n# set address dhcp\n# set egress-qos \"0:3 1:3 2:3 3:3 4:3 5:3 6:3 7:3\"\n# set firewall in name WAN_IN\n# set firewall in ipv6-name WANv6_IN\n# set firewall local name WAN_LOCAL\n# set firewall local name WANv6_IN\n# exit\n```\n\nTo ensure the routing performance is as good as it can be, we want to ensure the relevant hardware offload settings are configured:\n\n```console\n# edit system offload\n# set ipv4 forwarding enable\n# set ipv4 vlan enable\n# exit\n```\n\nAt this point we need to commit the change to allow the VLAN interface to be created. Routing won't work yet, but this will power the jack and get it booting. To get basic routing working the outbound interface for NAT will also need to be updated and recommitted. You'll want to double check your NAT rule number matches mine (5010).\n\n```console\n# commit\n# set service nat rule 5010 outbound-interface eth0.2\n# commit\n```\n\nNow you'll have to wait a while for everything to come up, which can be up to five minutes but your IPv4 connectivity should be all set.\n\nI also setup IPv6 separately using DHCPv6-PD, giving a separate /64 network to each of my internal networks. I'm going to leave that for another post though.\n\n## Changing the egress-qos Value\n\nIf you want to change the \"egress-qos\" value after the VLAN interface comes up you'll be presented with the following error message:\n\n```console\nadmin@ubnt-router:~$ configure\n[edit]\nadmin@ubnt-router# set interfaces ethernet eth0 vif 2 egress-qos \"0:2\"\n[edit]\nadmin@ubnt-router# commit\negress-qos can only be set at vlan creation for 2\n\nCommit failed\n[edit]\n```\n\nTo actually change this value, you need to switch to root, and use \"vi\" to edit the \"/config/config.boot\" file and reboot the router itself.\n\n## Initial Connectivity Time\n\nSeveral time during the course of testing this I assumed I had broken my connectivity while testing. I was actually experiencing surprisingly long delays getting connectivity after changes. Part of this is that the PoE to the fiber jack isn't maintained while the router is rebooting, forcing the jack to boot as well, but a large portion of that is the IPv4 connectivity itself.\n\nThe timings I measured during a reboot were:\n\n* 1 minute 18 seconds to first ping of internal LAN interface\n* An additional 35 seconds before the SSH port was open\n* 19 more seconds before logins were allowed\n* A further 33 seconds before the router saw the link from the fiber jack come up\n* 22 seconds until IPv6 connectivity was established (internal hosts now have IPv6 internet)\n* An additional 3 minutes 46 seconds before the eth0.2 interface gets an IPv4 address and IPv4 connectivity is available.\n\nTotal boot time: 5 minutes 53 seconds. That is pretty crazy and it's almost all waiting for an IPv4 address on the link. Either something is really slow on Google's end, or this delay is because the DHCP traffic is tagged with an incorrect QoS value (I haven't tested it).\n\nEither way, reboots once setup are pretty rare and I can wait the almost six minutes without internet when it happens.\n\n## MSS Clamping\n\n[Steve Jenkins](https://www.stevejenkins.com/blog/2015/11/replace-your-google-fiber-network-box-with-a-ubiquiti-edgerouter-lite/)'s post has the most complete documentation on setting up the EdgeRouter (and makes his configs [available on GitHub](https://github.com/stevejenkins/UBNT-EdgeRouter-Example-Configs)), but I was a tad confused about him using MSS clamping, which I've left out of my config.\n\nMSS clamping is used to restrict MTU sizes through TCP headers, and is very useful when tunneling traffic or wrapping in an authentication mechanism such as PPPoE, MPLS, or GRE. During the crafting of packets, hosts won't be aware of the tunnel which will have its own MTU and packet overhead and thus can hit some performance snags.\n\nWith this config, we are at most adding 4 bytes to each packet as a VLAN tag but won't encounter a tunnel that will break a packet up into multiple tunneled packets when a packet is to large and rebuild it at the other exactly the same. Any fragmentation that occurs in transit will be visible to the MSS detection algorithms and handled correctly without the overhead or limitation of MSS clamping. If anything I'd expect it to slow down the connection slightly for large packets.\n\nYou can [find the complete config I built up in this post here](./edgerouter-poe-google-fiber-example.conf).\n\n## Additional References\n\n* https://community.ubnt.com/t5/EdgeRouter/Comcast-IPv6-issues-when-hwnat-enabled-on-ER-X/m-p/1850112#U1850112\n* https://community.ubnt.com/t5/EdgeRouter/Updated-Google-Fiber-EdgeRouter-Lite-PoE-IPv4-amp-IPv6-config/td-p/1790440\n* https://community.ubnt.com/t5/EdgeRouter/IPv6-DHCPv6-and-DHCPv6-PD/td-p/1115361\n* https://kazoo.ga/dhcpv6-pd-for-native-ipv6/\n* https://community.ubnt.com/t5/EdgeRouter/The-generation-of-etc-radvd-conf-is-missing-my-configuration/td-p/1355531\n* https://community.ubnt.com/t5/EdgeRouter/ERlite-1-5-upnp2-secure-mode/td-p/923222\n* https://flyovercountry.org/2014/02/google-fiber-gigabit-speeds-your-router-part-1-vlans/\n* http://itnutt.com/how-to-bypass-google-fibers-network-box/\n* https://netswat.com/blog/google-fiber-ubiquitis-edgerouter/\n* https://netswat.com/blog/wp-content/uploads/2016/07/Edge-Setup-Interfacesv3.txt\n* https://netswat.com/blog/wp-content/uploads/2016/07/EdgeRouter-TVScript6.txt\n\n","created_at":1527381669,"fuzzy_word_count":1900,"path":"/blog/2018/05/edgerouter-poe-for-google-fiber/","published_at":1527381669,"reading_time":9,"tags":["network","ubiquiti"],"title":"Setting Up EdgeRouter PoE on Google Fiber","type":"blog","updated_at":1527381669,"weight":0,"word_count":1868},{"cid":"92cd4384a2651793897c509ebc50092c9a5eff6a","content":"\nNormally handling HTTP responses in Ruby is rather straight forward. There is a native library in Ruby that handles HTTP requests which parses the responses into a neat data structure that you can then operate on. What if you want to work on stored HTTP responses outside of a connection though? This was the situation I found myself in and thanks to a series of unusual decisions in the Ruby core library I found myself left out in the cold.\n\nFor reference this is in the latest stable Ruby as of this writing (2.5.1).\n\nLet's start with a very small HTTP response stored in a variable for us to test on:\n\n```ruby\nraw_http_body = \u003c\u003c-BODY.rstrip\nJust the body...\nBODY\n\nraw_http_response = \u003c\u003c-RESP.rstrip\nHTTP/1.1 200 Ok\\r\nConnection: close\\r\nContent-Length: #{raw_http_body.bytesize}\\r\n\\r\n#{raw_http_body}\nRESP\n```\n\nThe above is a little bit weird but is a minimum reasonable HTTP response. All lines are approprietly terminated with both a carriage return (explicit) and\na newline (implicit in how the strings are defined). The \"Content-Length\"\nheader is the exact number of bytes present in the body (thus the two \"#rstrip\"\ncalls). The \"Date\" header was omitted due to this line in [RFC7231](https://tools.ietf.org/html/rfc7231#section-7.1.1.2)\n\n\u003e An origin server MUST NOT send a Date header field if it does not have a clock capable of providing a reasonable approximation of the current instance in Coordinated Universal Time.\n\n...Which the content of this static site does not have.\n\nWith our minimal response out of the way how do we go about parsing it? The\n[Ruby 2.5.1 stdlib documentation](https://ruby-doc.org/stdlib-2.5.1/libdoc/net/http/rdoc/Net/HTTPResponse.html) doesn't specify how it can be created by end users which usually means it isn't intended for use by users of the language directly and digging through the Ruby source, you'll see this is precisely the case. Which means ***Ruby does not have a HTTP response parser available in it's standard library***. This is pretty frustrating, but maybe it can be worked around.\n\nHow does the \"Net::HTTP\" library make use of it? Even if the methods aren't listed for public documentation they're still public APIs on the class and should be able to be used without monkey patching right? The response is setup in [the connect method of Net::HTTP](https://github.com/ruby/ruby/blob/v2_5_1/lib/net/http.rb#L958) and it comes down to a few relevant lines that can be summarized as:\n\n1. Open a socket to the web server\n2. Write the formatted request to the socket\n3. Pass the socket to \"HTTPResponse#read_new\"\n\nSo we need a socket like object containing our response, which we can do with \"StringIO\" and pass it to the appropriate method. Let's see what happens:\n\n```ruby\nrequire 'net/http'\nrequire 'stringio'\n\nresp_io = StringIO.new(raw_http_response)\nresponse = Net::HTTPResponse.read_new(resp_io)\n```\n\nWe get a raised exception:\n\n```text\nNet::HTTPBadResponse: wrong status line: \"HTTP/1.1 200 Ok\\r\\n\"\n```\n\nThat is definitely a valid status line, so what is going on here? Back to Ruby's source code... \"Net::HTTPResponse#read_new\" starts off by calling \"Net::HTTPResponse#read_status_line\" which uses this regex for extracting and checking the validity of the status line:\n\n```ruby\n/\\AHTTP(?:\\/(\\d+\\.\\d+))?\\s+(\\d\\d\\d)(?:\\s+(.*))?\\z/in\n```\n\nI had never seen the \"/n\" modifier for Ruby's regular expressions and it seems to be completely undocumented. This turned out to be a red herring as it simply sets `Regexp::NOENCODING` (had to dig into the [spec/ruby/core/regexp/options_spec.rb](https://github.com/ruby/ruby/blob/3527c05a8f4e189772cdac17f166bd9626c24661/spec/ruby/core/regexp/options_spec.rb) file to figure that one out).\n\nSo why isn't that regular expression matching? Spoiler: It's the newline (the carriage return is fine). That is a violation of the HTTP spec, but it is working normally for Ruby's HTTP requests so what gives? Apparently we have to go deeper...\n\nIt's getting the header string by calling `#readline` which on standard [IO](http://ruby-doc.org/core-2.5.1/IO.html#method-i-readline) objects returns the newline (The `IO` class if the base for `StringIO`, and `Socket` objects in addition to many others). In [Ruby 2.4](https://blog.bigbinary.com/2017/03/07/io-readlines-now-accepts-chomp-flag-as-an-argument.html) and later there is a chomp flag that changes this behavior but it isn't being used in this case, and it would take the carriage return with it if it was.\n\nSo... We must not be operating on an actual `IO` subclass... And sure enough, `Net::HTTP#connect` after getting the raw socket wraps it in a `Net::BufferedIO` object which is another internal hidden class. You can see the definition [of it here](https://github.com/ruby/ruby/blob/v2_5_1/lib/net/protocol.rb#L81) and here is its `#readline` method:\n\n```ruby\ndef readline\n  readuntil(\"\\n\").chop\nend\n```\n\nYep, for some reason this one private internal API has decided to complicate a Ruby standard API convention and strip off the trailing carriage return and new line. Wrapping our `StringIO` object in a `BufferedIO` object does solve this problem but there is no reason for these complications...\n\n```ruby\nresp_io = StringIO.new(raw_http_response)\nbuf_io = Net::BufferedIO.new(resp_io)\nresponse = Net::HTTPResponse.read_new(buf_io)\n```\n\nOr does it?\n\n```ruby\nresponse.body\n# NoMethodError: undefined method `closed?' for nil:NilClass\n```\n\nWe need to pull one more trick from the `Net::HTTP#transport_request` to get the body. The first line actually returns the body, but we want to treat this like a normal `HTTPResponse` so we want to make sure the `#body` method works:\n\n```ruby\nresponse.reading_body(buf_io, true) { yield res if block_given? }\nresponse.body\n```\n\nThere are a couple of differences still from a normal response body. The only one of particular note to me is that normally the response get it's `#uri` data from the request. This isn't available with the response alone but can be set pretty easily:\n\n```ruby\nrequire 'uri'\nresponse.uri = URI.parse('http://example.tld')\n```\n\nAltogether this is what it looks like:\n\n```ruby\nrequire 'net/http'\nrequire 'stringio'\nrequire 'uri'\n\nraw_http_body = \u003c\u003c-BODY.rstrip\nJust the body...\nBODY\n\nraw_http_response = \u003c\u003c-RESP.rstrip\nHTTP/1.1 200 Ok\\r\nConnection: close\\r\nContent-Length: #{raw_http_body.bytesize}\\r\n\\r\n#{raw_http_body}\nRESP\n\nresp_io = StringIO.new(raw_http_response)\nbuf_io = Net::BufferedIO.new(resp_io)\n\nresponse = Net::HTTPResponse.read_new(buf_io)\nresponse.reading_body(buf_io, true) { yield res if block_given? }\nresponse.uri = URI.parse('http://example.tld')\n```\n\nYou now have a valid `Net::HTTPResponse` object\n","created_at":1527083599,"fuzzy_word_count":1e3,"path":"/blog/2018/05/parsing-http-responses-in-ruby/","published_at":1527083599,"reading_time":5,"tags":["ruby","tips"],"title":"Parsing HTTP Responses in Ruby","type":"blog","updated_at":1527083599,"weight":0,"word_count":967},{"cid":"8bf5b9e17b2b4c73cd73d2d0f3f84e434dc07e1f","content":"\nI regularly find myself inspecting traffic on Linux systems. Usually I'm already on the client or server when doing this (such as when diagnosing weird low level app behavior, or unknown, or unusual traffic). It has been a while since I've needed to silently be the wire between two black boxes.\n\nWhile verifying link level information about bypassing my Google Fiber Network Box I needed to be that wire again. Before I connected any wires to anything I needed to be sure I wouldn't accidentally leak traffic as I wasn't sure what would impact the link.\n\nYou'll need a Linux computer with two gigabit ethernet ports. My last two laptops haven't had any built in ethernet ports, but USB gigabit adapters are cheap and I already had a bunch.\n\nI went through and disabled the services that would configure network interfaces (NetworkManager and ModemManager) as well as all of the networking services on my system, and confirming your firewalls are all going to allow the traffic through.\n\nI double checked no packets were sent out up on link up by listening on each respective interface, plugging in to a powered but otherwise disconnected switch, and bringing the interface up.\n\nOnce I'd verified everything in one root terminal I ran:\n\n```console\n$ ip link set eth0 promisc on multicast off arp off\n$ ip link set eth1 promisc on multicast off arp off\n\n$ ip link add name intercept0 type bridge\n$ ip link set intercept0 promisc on multicast off arp off up\n\n$ ip link set eth0 master intercept0\n$ ip link set eth1 master intercept0\n```\n\nIn another terminal either as root, or as a user in the \"wireshark\" group begin recording the traffic of interest:\n\n```console\n$ tshark -i intercept0 -w recording.pcap\n```\n\nAt this point, connect the cables between the two boxes of interest. When ready bring the links up:\n\n```console\n$ ip link set eth0 up\n$ ip link set eth1 up\n```\n\nWelcome to being the wire.\n","created_at":1526194509,"fuzzy_word_count":400,"path":"/blog/2018/05/quick-and-silent-gigabit-packet-interception/","published_at":1526194509,"reading_time":2,"tags":["linux","networking"],"title":"Quick and Silent Gigabit Packet Interception","type":"blog","updated_at":1526194509,"weight":0,"word_count":337},{"cid":"b5b49da24689c1a0065972e8fc156efe5aa34d8b","content":"\n# Converting OpenLDAP Schemas to LDIF\n\nI've been writing software to work against an OpenLDAP instance, with a highly customized schema. The operators of the existing system only had the schema files and searching around found several elaborate ways to convert the files which I tried with mixed success. After doing the research to figure this out, it became clear I could probably have used \"slapcat\" and have dumped the active schema directly to LDIF.\n\nAs a sample of how I converted these, I'll use the \"rfc2307bis.schema\" file which didn't seem to come with a matching LDIF file in the source distribution. You'll need to identify the dependencies of the schema, which I've tended to just do with trial and error. If a dependency is missing you'll receive an error like the following:\n\n```console\n5ab6f5a6 /etc/openldap/schema/cosine.schema: line 1084 objectclass: ObjectClass not found: \"person\"\n```\n\nYou can identify the requisite schema file by grep'ing for the missing object in the other schema files and adding it to the config. The \"rfc2307.schema\" file depends on the \"core.schema\" and \"cosine.schema\" files. With this in mind you can use the following script to convert the LDIF file:\n\n```console\n$ SCHEMA_CONV_DIR=\"$(mktemp -d)\"\n\n$ cat \u003c\u003c EOF \u003e ${SCHEMA_CONV_DIR}/convert.conf\ninclude /etc/openldap/schema/core.schema\ninclude /etc/openldap/schema/cosine.schema\ninclude /etc/openldap/schema/rfc2307bis.schema\nEOF\n\n$ slapcat -f ${SCHEMA_CONV_DIR}/convert.conf -F ${SCHEMA_CONV_DIR} -n 0 \\\n  -s \"cn={2}rfc2307bis,cn=schema,cn=config\" | sed -re 's/\\{[0-9]+\\}//' \\\n  -e '/^structuralObjectClass: /d' -e '/^entryUUID: /d' -e '/^creatorsName: /d' \\\n  -e '/^createTimestamp: /d' -e '/^entryCSN: /d' -e '/^modifiersName: /d' \\\n  -e '/^modifyTimestamp: /d' -e '/^$/d' \u003e /etc/openldap/schema/rfc2307bis.ldif\n\n$ rm -rf ${SCHEMA_CONV_DIR}\n```\n\nOne important thing to note is the schema identifier in the slapcat command \"cn={2}rfc2307bis,cn=schema,cn=config\". The \"{2}\" there will be the line number from the \"convert.conf\" file counting from 0 and will likely be different for the schemas you're converting, and the name will be defined by the contents of the schema file.\n\nYou'll also want to pay attention to the file names and make sure the inputs and outputs match your expectations.\n","created_at":1521944422,"fuzzy_word_count":400,"path":"/blog/2018/03/converting-openldap-schemas-to-ldif/","published_at":1521944422,"reading_time":2,"tags":["openldap","linux","tips"],"title":"Converting OpenLDAP Schemas to LDIF","type":"blog","updated_at":1521944422,"weight":0,"word_count":336},{"cid":"fff12f9ac7015eb4dca785250f423e26faa07ef6","content":"\nWhile setting up and OpenLDAP server I found my distribution shipped with a couple of schema files, but no equivalent LDIF files. I found ways to convert the file using \"slapcat\" and \"slaptest\" and the files were valid on their own.\n\nI was specifically trying to bootstrap an OpenLDAP server, with it's schema, from scratch for a CI/CD system to test against. To accomplish this I was making use of the \"include\" directive in a configuration LDIF file and saw some very odd behavior.\n\nWhen I included the LDIF schema files that shipped with the software, I could include multiple of them back to back like so:\n\n```ldif\ninclude: file:///etc/openldap/schema/core.ldif\ninclude: file:///etc/openldap/schema/cosine.ldif\ninclude: file:///etc/openldap/schema/inetorgperson.ldif\n```\n\nWhen I included my converted file, it would successfully apply it, then just stop... I assumed at first that there was a syntax error, but there was no error. Everything contained in the included LDIF was present in the DIT. It had simply stopped.\n\nAfter many frustrating attempts, and hours Googling, the result came down to... There was an empty line at the end of the file. I couldn't find any documentation about this behavior and identified it through dumb luck. For anyone that comes across this I hope this solves your issue as well.\n","created_at":1521944422,"fuzzy_word_count":300,"path":"/blog/2018/03/including-ldif-files-in-openldap/","published_at":1521944422,"reading_time":1,"tags":["openldap","operations","linux","tips"],"title":"Including LDIF Files in OpenLDAP","type":"blog","updated_at":1521944422,"weight":0,"word_count":207},{"cid":"a5afbf2438c954b926254fb5df26547dd43b4ec4","content":"\n*Note: If you've come here looking to build a root filesystem for 32 bit ARM devices I suspect everything but the build tuple will be the same. The issues that need to be worked around largely packaging and profile issues that should all be the same.*\n\nI got a hold of a Zynq 7100 development board, and while I've played with some embedded ARM microcontrollers such as the STM32F3 series and more basic RISC style microcontrollers like Atmel's SAMD10 and Atmega lines, I've never played with FPGA development before so I considered this an interesting learning opportunity.\n\nTo do development of the FPGA and generally use the board at all you have to shell out $2,995 for a non-transferable license of a proprietary piece of software called Vivado to develop on the FPGA. For a hobby project just exploring the board this isn't going to fly. There is a 30-day evaluation version though and there are guides to getting it to work in Linux.\n\nFor this post I'm going to gloss over this part and get to the meat of the largest issue I had while attempting to bootstrap the Linux portion of this development board. Xilinx maintains its own very rough hacked together distribution called [PetaLinux](http://www.wiki.xilinx.com/PetaLinux) which is just a very poorly designed wrapper around [Yocto Linux](https://www.yoctoproject.org/).\n\nUnfortunately I haven't been fully able to remove PetaLinux from my build, I still need to use it to integrate the board specifics with the Linux kernel and in turn compile the Linux kernel, u-boot, and handle the configuration to point at a root filesystem living on the SD card. PetaLinux's incredibly limited documentation can at least get you this far. This post covers building that root filesystem and guides around some of the problems the Gentoo cross process doesn't cover.\n\nI want a target that is a bit more inclusive than most embedded Linux root filesystems (think IoT devices). This device is less constrained than devices like most OpenWRT capable devices (we're not limited to 16MB of space). Let's quickly define some criteria that will determine the successful build of a root filesystem:\n\n* It will have all the utilities necessary to support interactive logins\n* It will have a working file editor\n* It will have a valid native compiler for itself\n* It will have a working package manager to allow it to extend itself\n\nFrom this set of goals we will both be able to re-compile everything natively on the board if we so choose, and get access to the vast majority of packaged software from the Gentoo repositories as well as easily perform project development directly.\n\nTo get started you will have to have a working Gentoo install to start the cross compilation from. I've personally had issues with the hardened, SELinux, and no-multilib profile variants. If you encounter issues I strongly recommend trying the standard system profile on your build host. I'm also sure there is probably some way to get the portage cross tooling working in other distributions, but I'll leave that as an exercise to the reader.\n\n## Tooling\n\nTo get started we're going to want to setup an overlay specifically for our cross development. This will allow customization of the profile for the device later on. This is largely for use beyond this guide and is a good practice to separate changes from your system and the target board.\n\nThese commands are pretty straight forward and do need to be run as root:\n\n```console\n$ mkdir -p /usr/local/overlay/portage-crossdev/{profiles,metadata}\n$ echo 'crossdev' \u003e /usr/local/overlay/portage-crossdev/profiles/repo_name\n$ echo 'masters = gentoo' \u003e /usr/local/overlay/portage-crossdev/metadata/layout.conf\n$ chown -R portage:portage /usr/local/overlay/portage-crossdev\n\n$ cat \u003c\u003c EOF \u003e /usr/local/overlay/portage-crossdev/metadata/layout.conf\nmasters = gentoo\nthin-manifests = true\nEOF\n\n$ mkdir -p /etc/portage/repos.conf\n$ cat \u003c\u003c EOF \u003e /etc/portage/repos.conf/crossdev.conf\n[crossdev]\nlocation = /usr/local/overlay/portage-crossdev\npriority = 10\nmasters = gentoo\nauto-sync = no\nEOF\n```\n\nWith our overlay setup we now need to install the cross development tools (once again as root):\n\n```console\n$ emerge sys-devel/crossdev\n```\n\nThe next step is to build our initial tool chain. Having worked with many tool chains before this is absolutely the easiest time I've ever had setting one up. One command will get you all the way to a C/C++ compiler, linker, bintools, and a standard library. The specific tool chain target tuple is for a glibc based tool chain, on a Xilinx variant of an arm processor (or arm-xilinx-linux-gnueabi).\n\n```console\n$ crossdev --stable -s4 -t arm-xilinx-linux-gnueabi\n```\n\nThis will result in a very bare bones root filesystem in \"/usr/arm-xilinx-linux-gnueabi\". This doesn't really have anything of value yet.\n\n## Base System\n\nWe need to configure portage and then a profile for our build. First off the portage configuration, this exists in \"/etc/arm-xilinx-linux-gnueabi/etc/portage/make.conf\" and varies slightly from the default.\n\n```bash\nCHOST='arm-xilinx-linux-gnueabi'\nCBUILD='x86_64-pc-linux-gnu'\nARCH='arm'\n\nHOSTCC='x86_64-pc-linux-gnu-gcc'\n\nCFLAGS='-O2 -pipe -fomit-frame-pointer'\nCXXFLAGS=\"${CFLAGS}\"\n\nROOT=\"/usr/${CHOST}/\"\n\nACCEPT_KEYWORDS='arm'\n\nUSE=\"${ARCH}\"\n\nFEATURES='sandbox noman noinfo nodoc'\n\n# Be sure we dont overwrite pkgs from another repo..\nPKGDIR=\"${ROOT}packages/\"\nPORTAGE_TMPDIR=\"${ROOT}tmp/\"\n\nELIBC='glibc'\n\nPKG_CONFIG_PATH=\"${ROOT}usr/lib/pkgconfig/\"\n\nPYTHON_TARGETS='python2_7'\n```\n\nIf you pay attention compared to the defaults there are a few changes I've explicitly made:\n\n1. Do not build packages, we simply don't need them\n2. Allow PAM to be included\n3. Reject the testing arm packages (~arm keyword)\n4. Re-enable the file collision protections between packages\n5. Explicitly define our valid python target at 2.7 only\n\nThe first three align with my original stated goals, building will be allowed and preferred on the device so we won't need to host any packages. We want a standard interactive login and ideally we want a stable system (as much as possible). The last one is personal preference as in my build (after this guide is over) I'll be using software that doesn't work on Python 3 variants.\n\nThe fourth change is something I want to draw specific attention to. This is disabled by default because the stock ARM profile is inherently broken. It attempts to force both a complete busybox system in addition to the standard Gentoo base. The faux busybox binaries directly conflict and you'll end up in a weird mixed state that isn't good. This is true of libraries as well which will result in some core libraries failing to compile (\"dev-libs/gmp\" was the first one that failed on me).\n\nTo both fix that issue and allow us to have a clean build, I needed to build a custom Gentoo profile for targeting this device. This minimal profile will work cleanly for our target.\n\n```console\n$ rm /usr/arm-xilinx-linux-gnueabi/etc/portage/make.profile\n$ mkdir /usr/arm-xilinx-linux-gnueabi/etc/portage/make.profile\n$ echo 5 \u003e /usr/arm-xilinx-linux-gnueabi/etc/portage/make.profile/eabi\n$ cat \u003c\u003c EOF \u003e /usr/arm-xilinx-linux-gnueabi/etc/portage/make.profile/parent\n/usr/portage/profiles/base\n/usr/portage/profiles/arch/arm/armv7a\nEOF\n\n$ mkdir -p /usr/arm-xilinx-linux-gnueabi/etc/portage/package.accept_keywords\n$ cat \u003c\u003c EOF \u003e /usr/arm-xilinx-linux-gnueabi/etc/portage/package.accept_keywords/system\nsys-apps/coreutils ~arm\nsys-apps/sandbox ~arm\nEOF\n```\n\nThere is one final workaround we're going to need to put in place before we can begin compiling our system. Currently \"sys-apps/portage\" and \"dev-python/pyxattr\" are incorrectly packaged and will use the system library paths rather than those in the chroot.\n\nThis prevents both of them from being compiled with native extensions and incorrectly places the files inside the chroot (They're in the 64 bit path on a 32 bit target). This is fixable once we have the root filesystem on the device but in the meantime we need to set some use flags to avoid the issue:\n\n```console\n$ mkdir -p /usr/arm-xilinx-linux-gnueabi/etc/portage/package.use\n$ echo 'sys-apps/portage -native-extensions -xattr' \u003e /usr/arm-xilinx-linux-gnueabi/etc/portage/package.use/portage\n```\n\nWith that in place sit back grab a cup of your favorite warm beverage and watch the system compile (seriously this is going to take a hot minute):\n\n```console\n$ arm-xilinx-linux-gnueabi-emerge --update --newuse --deep @system\n```\n\nThere is one final gotcha with the root filesystem, the commands to date will not create several important directories. These can be created with the following command:\n\n```console\n$ mkdir -p /usr/arm-xilinx-linux-gnueabi/{dev,home,proc,root,sys}\n```\n\nAt this point you should have a mostly complete root filesystem and may want to start diverging from this guide (but pay attention to the kernel modules section, and the rebuild section). There are a couple of things that won't currently work, specifically:\n\n* Authentication\n* Serial Console\n* Networking\n\n## Authentication \u0026 Serial Usage\n\nFirst authentication, we need to provide root with a password and PAM needs its configuration files to function. We can't use the native tools (without qemu binary emulation) to change the password so the fastest way to give root a password is to pre-generate a password hash and drop it directly into the relevant files. If you'd like to keep going you can use the hash below for the super secure password 'root' (I don't recommend it):\n\n```console\n$ sed -i 's`root:[^:]*:`root:$6$ufWqa3MP$CfFwj0M7tW15gUYBRVms3GG2FJTRMAhlpkwV7Bp4aro6mGFHmotMjHoePNoTd1Gf9fgzh/jJM3rvJgkGgSjz31:`' /usr/arm-xilinx-linux-gnueabi/etc/shadow\n```\n\nAnd the requisite PAM configuration files:\n\n```console\n$ arm-xilinx-linux-gnueabi-emerge sys-auth/pambase\n```\n\nThe serial console is going to be more device specific and is a bit tricky to figure out. To find this out on my board I created a service that collected the names of all devices under \"/dev\", logged them to a file and found mine to be \"ttyPS0\" (also one I've never seen before).\n\nThe following command will replace the default serial configuration with one for this device (you may also have to change the baud rate it's running at):\n\n```console\n$ sed -i 's`^s0:.*$`ps0:12345:respawn:/sbin/agetty -L 115200 ttyPS0 vt100`' /usr/arm-xilinx-linux-gnueabi/etc/inittab\n```\n\n## Networking\n\n```console\n$ arm-xilinx-linux-gnueabi-emerge sys-apps/net-tools net-misc/netifrc \\\n  net-misc/dhcpcd net-misc/iputils sys-apps/iproute2\n```\n\nTo get it to come up by default, since we can't use the \"rc\" tools natively yet we can cheat. This assumes your kernel is configured to use the legacy network names (which are more consistent and predictable :-/). This will setup eth0 to come up automatically and use DHCP to grab an address on the network:\n\n```console\n$ cat \u003c\u003c 'EOF' \u003e /usr/arm-xilinx-linux-gnueabi/etc/conf.d/net\n# /etc/conf.d/net\nmodules=\"iproute2\"\n\n# Default DHCP config for interfaces\ndhcp=\"release nonis nontp\"\n\nconfig_eth0=\"dhcp\"\nEOF\n\n$ echo 'arm-board' \u003e /usr/arm-xilinx-linux-gnueabi/etc/hostname\n$ echo 'hostname=\"arm-board.localhost\"' \u003e /usr/arm-xilinx-linux-gnueabi/etc/conf.d/hostname\n\n$ cat \u003c\u003c 'EOF' \u003e /usr/arm-xilinx-linux-gnueabi/etc/hosts\n# /etc/hosts\n\n127.0.0.1 localhost4 localhost\n::1       localhost6 localhost\nEOF\n\n$ cd /usr/arm-xilinx-linux-gnueabi/etc/init.d/\n$ ln -s net.lo net.eth0\n\n$ cd /usr/arm-xilinx-linux-gnueabi/etc/runlevels/default/\n$ ln -s /etc/init.d/net.eth0 net.eth0\n$ rm -f netmount\n```\n\n## Kernel Modules\n\nPart of the kernel build that has to happen still in the PetaLinux environment are kernel modules. One of the build artifacts is the root filesystem PetaLinux thinks you're going to use. These contain very important kernel modules which need to be extracted.\n\nInside the root of your PetaLinux project after a build you should find a file \"images/linux/rootfs.tar.gz\" which will have a directory inside it \"./lib/modules\". The contents need to be transferred to your root filesystem. If you transfer that to the system you're building the board root on you can extract all of the appropriate files using the following command:\n\n```console\n$ tar -xf rootfs.tar.gz -C /usr/arm-xilinx-linux-gnueabi/ ./lib/modules\n```\n\nYou can verify they are present by confirming a directory that looks along the lines of \"4.9.0-xilinx-v2017.2\" exists in \"/usr/arm-xilinx-linux-gnueabi/lib/modules/\".\n\n## SSH Server\n\nIf you would additionally like an SSH server running (that supports root login) there is a bit of a trick. User privilege separation requires a dedicated user and group named `sshd` for this to work.\n\nThe OpenSSH ebuild doesn't create this user and I'm not entirely sure what does. For now we can solve this issue by creating the user and group manually.\n\n```console\n$ arm-xilinx-linux-gnueabi-emerge net-misc/openssh\n\n$ echo 'sshd:x:22:22:added by portage for openssh:/var/empty:/sbin/nologin' \u003e\u003e /usr/arm-xilinx-linux-gnueabi/etc/passwd\n$ echo 'sshd:*:0:0:::::' \u003e\u003e /usr/arm-xilinx-linux-gnueabi/etc/shadow\n$ echo 'sshd:x:22:' \u003e\u003e /usr/arm-xilinx-linux-gnueabi/etc/group\n\n$ cat \u003c\u003c 'EOF' \u003e /usr/arm-xilinx-linux-gnueabi/etc/ssh/sshd_config\n# /etc/ssh/sshd_config\n\nHostKeyAlgorithms ssh-ed25519,ecdsa-sha2-nistp521,ssh-rsa\n\nClientAliveInterval 10\n\nUseDNS no\n\nAllowTcpForwarding no\nUsePAM yes\n\nPasswordAuthentication yes\nPermitRootLogin yes\nEOF\n\n$ cd /usr/arm-xilinx-linux-gnueabi/etc/runlevels/default/\n$ ln -s /etc/init.d/sshd sshd\n```\n\n## Additional Tools\n\nThis is a pretty solid foundation for any root Linux system. Everything at this point is going to preferential and determined by your project requirements. A few things you may want to include:\n\n* VIM\n* NTPd or Chronyd for time keeping\n* A syslog server (I recommend syslog-ng) and in turn logrotate\n* Network performance testing tools (such as iperf3)\n\nFrom the above list I wanted both VIM and iperf3 and thus ran the following:\n\n```console\n$ echo 'net-misc/iperf ~arm' \u003e /usr/arm-xilinx-linux-gnueabi/etc/portage/package.accept_keywords/network_utils\n$ echo 'app-editors/vim minimal' \u003e /usr/arm-xilinx-linux-gnueabi/etc/portage/package.use/vim\n$ arm-xilinx-linux-gnueabi-emerge app-editors/vim net-misc/iperf\n```\n\n## Rebuilding on the System\n\nOnce all the packages you want for your base system are installed, the root may be in an inconsistent state. It's a good idea to run a sync, global use update, a preserved rebuild, and dependency clean on the board before continuing:\n\n```console\n$ emerge --sync\n$ arm-xilinx-linux-gnueabi-emerge --update --newuse --deep @world\n$ arm-xilinx-linux-gnueabi-emerge @preserved-rebuild\n$ arm-xilinx-linux-gnueabi-emerge --depclean\n```\n\nWe now need to get the root filesystem on a live board and rebuilding cleaning up some of the mismatch package flags and irregularities introduced by the cross compilation process. At a minimum want to fix the incorrectly built portage package so everything is usable normally.\n\nBefore transferring this it's a good idea to preemptively adjust the make config to no longer be a cross environment, and remove the special case for portage. This can be done with the following command:\n\n```console\n$ cat \u003c\u003c 'EOF' \u003e /usr/arm-xilinx-linux-gnueabi/etc/portage/make.conf\nARCH='arm'\nCFLAGS='-O2 -pipe'\nCXXFLAGS=\"${CFLAGS}\"\nCHOST='arm-xilinx-linux-gnueabi'\n\nACCEPT_KEYWORDS='arm'\nFEATURES='sandbox noman noinfo nodoc'\nUSE=\"${ARCH} pam\"\n\nELIBC='glibc'\n\nL10N='en'\nLINGUAS='en'\n\nPYTHON_TARGETS='python2_7'\nEOF\n\n$ rm -f /usr/arm-xilinx-linux-gnueabi/etc/portage/package.use/portage\n```\n\nWe now need to package up our root filesystem:\n\n```console\n$ tar -cJf ~/xilinx_root_non_native.txz -C /usr/arm-xilinx-linux-gnueabi .\n```\n\nThis next bit requires the proper settings in PetaLinux and a completed build (you'll need your own BOOT.BIN, image.ub, and system.dtb files). After inserting an appropriately size SD card (You're going to want 4 or 8Gb more likely than not). For me the device showed up as mmcblk0 on my machine. Confirm yours before following the next steps:\n\n```console\n$ dd if=/dev/zero bs=1M count=1 oflag=sync of=/dev/mmcblk0\n\n$ parted --script -a optimal /dev/mmcblk0 -- mklabel msdos\n$ parted --script -a optimal /dev/mmcblk0 -- mkpart primary fat32 100 600\n$ parted --script -a optimal /dev/mmcblk0 -- mkpart primary ext4 600 -1\n\n$ dd if=/dev/zero bs=1M count=1 oflag=sync of=/dev/mmcblk0p1\n$ dd if=/dev/zero bs=1M count=1 oflag=sync of=/dev/mmcblk0p2\n\n$ mkfs.vfat -n BOOT -F 32 /dev/mmcblk0p1\n$ mkfs.ext4 -L rootfs /dev/mmcblk0p2\n\n$ mkdir -p /mnt/boot\n$ mount /dev/mmcblk0p1 /mnt/boot\n\n$ mkdir -p /mnt/root\n$ mount /dev/mmcblk0p2 /mnt/root\n```\n\nYou'll need to copy \"BOOT.BIN\", \"image.ub\", and \"system.dtb\" to \"/mnt/boot\" and extract the root filesystem into the root directory (compressed version still lives at \"~/xilinx_root_non_native.txz\").\n\n```console\n$ tar -xf ~/xilinx_root_non_native.txz -C /mnt/root\n```\n\nEnsure the writes complete and cleanly unmount the filesystems:\n\n```console\n$ sync\n$ umount /mnt/boot /mnt/root\n```\n\nStick the microSD card into the board and let it boot up. If you're following this guide you should be able to get to a login screen and be able to login with root / root.\n\nThe device should be on the network and you should be able to SSH to the device. For my board at the very least I haven't gotten the hardware clock working correctly so it needs to be set manually upon every boot. Before we can compile quite a few of the packages the date needs to be roughly correct. You can reference the build host's time using the following command:\n\n```console\n$ date +%s\n```\n\nAnd set it on the board using the following command (replacing VALUE with the value returned above):\n\n```console\n$ date --set=\"@VALUE\"\n```\n\nWe now need to sync the system's packages and fix portage. This is where we have to work around the issue of portage being incorrectly installed by prefixing any use of the portage python module with a '64 bit' path:\n\n```console\n$ PYTHONPATH='/usr/lib64/python2.7/site-packages' env-update\n\n$ cat \u003c\u003c 'EOF' \u003e /etc/locale.gen\nen_US ISO-8859-1\nen_US.UTF-8 UTF-8\nEOF\n$ locale-gen\n\n$ PYTHONPATH='/usr/lib64/python2.7/site-packages' eselect locale set \"$(eselect locale list | grep 'en_US.utf8' | awk '{ print $1 }' | grep -oE '[0-9]+')\"\n$ PYTHONPATH='/usr/lib64/python2.7/site-packages' env-update\n\n$ . /etc/profile\n\n$ PYTHONPATH='/usr/lib64/python2.7/site-packages' emerge --sync\n$ PYTHONPATH='/usr/lib64/python2.7/site-packages' emerge --oneshot sys-apps/portage\n```\n\nThere is a circular dependency that has to be broken during this update:\n\n```console\n$ USE=\"dev-util/pkgconfig internal-glib\" emerge dev-util/pkgconfig\n```\n\nWith the circular update broken we can update everything (this will recompile pkgconfig again)\n\n```console\n$ emerge --update --newuse --deep @world\n```\n\nThe last will recompile quite a few packages (though not all). I recommend shutting the system down, removing the SD card and making a clean backup of the root by performing the following commands once the drive is back in your machine (this assumes the same device name as before):\n\n```console\n$ mount /dev/mmcblk0p2 /mnt/root\n$ rm -rf /mnt/root/root/.bash_history /mnt/root/etc/ssh/ssh_host* /usr/portage/*\n$ tar -cJf ~/xilinx_root_native.txz -C /mnt/root .\n```\n\nYou now have a solid base to perform development on and a good backup in case you mess up. I hope this helps someone else out there.\n","created_at":1513637362,"fuzzy_word_count":2900,"path":"/blog/2017/12/cross-compiling-gentoo-for-xilinx-boards/","published_at":1513637362,"reading_time":14,"tags":["arm","embedded","linux","tips"],"title":"Cross-Compiling Gentoo for Xilinx Boards","type":"blog","updated_at":1513637362,"weight":0,"word_count":2864},{"cid":"295d258ec3d9f84f42c291c6ff50c0b4a52a6ae1","content":"\n# Converting CPIO Files to Tarballs\n\nI needed to convert a directory full of CPIO files to tar balls. This quick script did the trick for me but didn't preserve the user / group. Running it as root will preserve the ownership information but that wasn't important for my immediate use case.\n\n```bash\n#!/bin/bash\n\nSRC_DIR=$(pwd)\nfor i in *.cpio; do\n  CPIO_TMP_DIR=\"$(mktemp -d /tmp/cpioconv.XXXX)\"\n  (cd ${CPIO_TMP_DIR} \u0026\u0026 cpio -idm \u003c \"${SRC_DIR}/${i}\" \u0026\u0026 tar -cf ${SRC_DIR}/${i%%.cpio}.tar .)\n  rm -rf ${CPIO_TMP_DIR}\ndone\n```\n","created_at":1512445616,"fuzzy_word_count":100,"path":"/blog/2017/12/converting-cpio-files-to-tarballs/","published_at":1512445616,"reading_time":1,"tags":["linux","tips"],"title":"Converting CPIO Files to Tarballs","type":"blog","updated_at":1512445616,"weight":0,"word_count":80},{"cid":"0eb821e747176754ce4b819583657c5f64155cab","content":"\n# Unusable Secret Key\n\nI use a Yubikey NEO to store subkeys used for signing and authentication. I started experiencing a weird issue with it. It coincided with me rebuilding my system so diagnosing it ended up being harder than normal. The behavior I experienced allowed me to use the key to authenticate (SSH'ing worked fine) but any attempt to sign new data resulted in an 'Unusuable secret key' error. For git this resulted in the following message:\n\n```text\ngpg: skipped \"Sam Stelfox \u003csstelfox@bedroomprogrammers.net\u003e\": Unusable secret\nkey\ngpg: signing failed: Unusable secret key\nerror: gpg failed to sign the data\nfatal: failed to write commit object\n```\n\nThere is a second Yubikey I use on occasion that contains my companies software signing key. When reviewing my available secret keys, it seemed like GPG was listing those private keys as available when they weren't. That was a red herring and unrelated (though still likely a bug). After resetting my GPG config as well as the agent, and re-importing my key the private keys were not be listed at all.\n\nUltimate the issue was that all of my subkeys were expired and only became visible when I used the following command:\n\n```console\n$ gpg2 --list-options show-unusable-subkeys --list-keys\n/home/sstelfox/.gnupg/pubring.kbx\n---------------------------------\npub   rsa4096/0x30856D4EA0FFBA8F 2016-04-26 [C] [expires: 2019-12-01]\n      Key fingerprint = DC75 C8B8 7434 4360 FB30  3FC9 3085 6D4E A0FF BA8F\nuid                   [ unknown] Sam Stelfox \u003csstelfox@bedroomprogrammers.net\u003e\nuid                   [ unknown] Sam Stelfox \u003csam@stelfox.net\u003e\nuid                   [ unknown] Sam Stelfox \u003csam@pwnieexpress.com\u003e\nuid                   [ unknown] [jpeg image of size 2803]\nsub   rsa2048/0x5E2FD479ABDD395A 2016-04-26 [S] [expired: 2017-12-01]\nsub   rsa2048/0xD87BD950C29A0FA2 2016-04-26 [E] [expired: 2017-12-01]\nsub   rsa2048/0x4B218A4FB733A150 2016-04-26 [A] [expired: 2017-12-01]\n```\n","created_at":1512405481,"fuzzy_word_count":300,"path":"/blog/2017/12/unusable-secret-key/","published_at":1512405481,"reading_time":2,"tags":["gpg","security","tips"],"title":"Unusable Secret Key","type":"blog","updated_at":1512405481,"weight":0,"word_count":280},{"cid":"0c6787082cae88662bae266826824d1a33e4d558","content":"\n# XFCE Failed to Connect to Socket\n\nWhile trying to build up a minimal Gentoo graphical environment I kept running into an error every time I logged into XFCE from lightdm (I didn't try starting up XFCE any other way). There are tons of blog posts that relate to systemd, ubuntu, or crouton but none related to Gentoo.\n\nThe first error message that pops up is:\n\n```text\nUnable to contact settings server\n\nFailed to connect to socket /tmp/dbus-xxxxxxxxx: Connection refused\n```\n\nOnce you click through there was a second error message, but I believe it was due to the previous error and not actually an issue:\n\n```text\nUnable to load a failsafe session\n\nUnable to determine failsafe session name. Possible causes: xfconfd isn't\nrunning (D-Bus setup problem), environment variable $XDG_CONFIG_DIRS is set\nincorrectly (must include \"/etc\"), or xfce4-session is installed incorrectly.\n```\n\nWhere the x's are replaced with a random string. My issue ultimately was dbus not being compiled with the 'X' use flag. Adding that flag to \"sys-apps/dbus\" and re-emerging with the new flags got me to a clean desktop. Great success.\n","created_at":1511785389,"fuzzy_word_count":200,"path":"/blog/2017/11/xfce-failed-to-connect-to-socket/","published_at":1511785389,"reading_time":1,"tags":["gentoo","linux","xfce"],"title":"XFCE Failed to Connect to Socket","type":"blog","updated_at":1511785389,"weight":0,"word_count":180},{"cid":"7315cccb787257ccbeeee374d2fad748e179bc49","content":"\n# Unable to Enter LUKS Passphrase\n\nWhile setting up a Gentoo install with a full disk encryption, I continuously got to a point where the passphrase would show up on boot but I was unable to enter the passphrase. The behavior of the keyboard was also odd, it would toggle it's numlock light every couple of button presses.\n\nUltimately the issue was with me not having XHCI support compiled into my kernel and having my keyboard plugged into a USB 3.0 or 3.1 port. After enabling and recompiling my kernel the issue immediately cleared up.\n","created_at":1511750991,"fuzzy_word_count":100,"path":"/blog/2017/11/unable-to-enter-luks-passphrase/","published_at":1511750991,"reading_time":1,"tags":["gentoo","linux"],"title":"Unable to Enter LUKS Passphrase","type":"blog","updated_at":1511750991,"weight":0,"word_count":89},{"cid":"7ee8750bbe8bbd1264f570bc64e13b713d66c4f3","content":"\n# Downgrading Glibc in Gentoo\n\nWhile refining some automated setup scripts at some point I upgraded to a testing/unstable version of glibc. When I attempted to get the box back on to the stable version I hit a solid protection mechanism built into the portage scripts that prevents downgrading glibc. Attempts will give you the following error message:\n\n```console\n * Sanity check to keep you from breaking your system:\n *  Downgrading glibc is not supported and a sure way to destruction\n * ERROR: sys-libs/glibc-2.25-r9::gentoo failed (pretend phase):\n *   aborting to save your system\n```\n\nIt is correct in that this is a very dangerous thing to do. For my use case this was a non-production system that I was using to test various configurations and hardening procedures so didn't particularly care if the packages all became corrupt. I was also confident that my method would be relatively safe (fully bootstrapping the system).\n\nTo disable this check (you should seriously understand what you're doing and/or not care about the system you perform this on if you continue) you need to edit the appropriate eclass file which exists at \"/usr/portage/eclass/toolchain-glibc.eclass\". Locate the following line and disable it by prefixing it with a hash sign (#).\n\n```console\ndie \"aborting to save your system\"\n```\n\nFor the version I was editing it was line 507.\n\nTo do this downgrade safely I performed the following steps (this will take a while as it fully bootstraps your system):\n\n* Review all use flags and accept keywords to ensure you'll be in the state you want to\n* Sync the repository metadata (\"emerge --sync\")\n* Disable the glibc safety check documented above\n* Run the following commands to re-bootstrap the system and undo the safety check modification by re-syncing:\n\n```console\n$ /usr/portage/scripts/bootstrap.sh\n$ emerge --emptytree --with-bdeps=y @world\n$ emerge --depclean\n$ emerge --sync\n```\n\nReboot to ensure all running programs are on the correct versions and the system should be back in a happy state.\n","created_at":1510748865,"fuzzy_word_count":400,"path":"/blog/2017/11/downgrading-glibc-in-gentoo/","published_at":1510748865,"reading_time":2,"tags":["linux","gentoo","tips"],"title":"Downgrading Glibc in Gentoo","type":"blog","updated_at":1510748865,"weight":0,"word_count":322},{"cid":"cbc21695f689b6d5f647b9007f842ef005ef3ee4","content":"\n# Gentoo Fstab Failure\n\nI use Gentoo with OpenRC quite a bit both for my personal servers and as a compilation test bed for new software since I can control the dependency versions very tightly.  I have a set of scripts I've been using for quite some time that handle setting up a hardened, fairly minimal install.\n\nI recently encountered a weird issue with them that resulted in an esoteric error that prevented my host from fully booting and leaving the root filesystem read-only. There also didn't seem to be much reliable information on the problem so I'm documenting it here in hopes it may help someone else.\n\nIn addition to the read-only filesystem, none of the system services would start up leaving me with the kernel's default hostname of \"(none)\". This includes networking so I had to diagnose this directly via the console. I was able to login as a user but unable to use sudo to switch to a new user (though I could still use it to execute commands with elevated privileges).\n\nI was able to verify all the OpenRC services had not started with by executing:\n\n```console\n$ sudo rc-status\n```\n\nI attempted to start the SSH daemon (though any networked service should behave the same) using the following command:\n\n```console\n$ sudo service sshd start\n```\n\nThis was the first time any error had directly presented itself to my screen (and since my filesystems were read-only, and my logger was stopped I had no on disk logs to go off from). The error that was repeated over my screen was the following:\n\n```console\n * Checking local filesystems  .../sbin/fsck.xfs: XFS file system.\nfsck.fat 4.0 (2016-05-06)\nopen: No such file or directory\n\n * Filesystems couldn't be fixed\n                                         [ !! ]\n * ERROR: fsck failed to start\n * ERROR: cannot start root as fsck would not start\n```\n\nThis was quite confusing and more than a little concerning as XFS doesn't use fsck, instead it uses xfs_repair. There was also the \"No such file or directory\" error which also didn't seem to make sense.\n\nUltimately, after reviewing the fsck init script I found the culprit. There was a bad line in my /etc/fstab file pointing at a device that didn't exist. I was able to remount my root filesystem read-write and edit my fstab to fix the device name in my fstab file:\n\n```console\n$ sudo mount -o remount,rw /\n$ sudoedit /etc/fstab\n```\n\nTracing things back, this was ultimately caused by the Gentoo installation medium detecting it's only disk as /dev/sda while the kernel I use exposed it as /dev/vda. Since I was already using a GPT filesystem I was able to adjust my scripts so they reference this particular partition by UUID instead of directly by device name.\n","created_at":1509549154,"fuzzy_word_count":500,"path":"/blog/2017/11/gentoo-fstab-failure/","published_at":1509549154,"reading_time":3,"tags":["gentoo","linux","tips"],"title":"Gentoo Fstab Failure","type":"blog","updated_at":1509549154,"weight":0,"word_count":462},{"cid":"71c05d6e3a042982c62b12dc3b30c58a62458fde","content":"\n# Cron Daemon\n\nCron is a pretty standard utility and there isn't much to it. I generally use `cronie` as my cron daemon with the associated `anacron` helper for systems that aren't always on such as laptops and desktops. Cron runs tasks periodically, and anacron helps ensure that a missed task will get run if it was off or power-cycled when it would have otherwise run.\n\n## File Format\n\nThe configuration format differs slightly between crontabs, regular cron files, and anacron entries. At the beginning of all the files environment variables can be set using key/value pairs to tweak the settings of followed by entries for that file one to a line. The first five portions of the cron and crontab entry format consist of numbers, steps, ranges, lists, or the wildcard (*) character.\n\nThe fields are in order minutes (can be 0-59), hours (0-23), date (1-31), month (1-12), and day of week (0-6, 0 being Sunday, and 6 being Saturday). The wildcard character matches all values for the field. A hyphen can be used to specify a range of numbers (such as 0-5 for the first six hours of a day). A comma can be used to specify several numbers or ranges (0,20-23 for minutes would be at the top of the hour and each of the four minutes between 20 and 23). The last one is step values which specifies every Nth value starting at the lower end of the range (*/5 is every five minutes, 3-23/7 is every seventh hour starting at 3 which would be 3, 10 and 17).\n\nFor cron files the next field is the username. The username is omitted from crontabs as that is implicitly implied by the user that created it. And lastly is the command that should be run. The following is a sample cron config that you should understand before trying...\n\n```cron\n# /etc/cron.d/sample\n\nSHELL=/bin/bash\nPATH=/sbin:/bin:/usr/sbin:/usr/bin\nMAILTO=root\nHOME=/\n\n# On the 23rd minute after midnight the next time there is a Friday the 13th...\n# execute a fork bomb... spoooookkyy alerts\n23 0 13 * 5 root /bin/bash -c ':(){ :|: \u0026 };:'\n```\n\nAnacron has its own variation of the file format the config of which is available at /etc/anacrontab. The variables at the header are generally the same The format is period in days, the delay in minutes for anacron to execute the job (can be useful to spread the jobs around), a unique job identifier name for logging purposes, and finally a script or command to be run. A sample might look like:\n\n```cron\n# /etc/anacrontab\n\nSHELL=/bin/sh\nPATH=/sbin:/bin:/usr/sbin:/usr/bin\nMAILTO=root\n\n# the maximal random delay added to the base delay of the jobs\nRANDOM_DELAY=45\n# the jobs will be started during the following hours only\nSTART_HOURS_RANGE=3-22\n\n# Period   Delay  Job ID        Command\n1          5      cron.daily    nice run-parts /etc/cron.daily\n7          25     cron.weekly   nice run-parts /etc/cron.weekly\n@monthly   45     cron.monthly  nice run-parts /etc/cron.monthly\n```\n\n## Restricting Access\n\nTasks executed through cron will be executed with the privileges of the user who created the crontab but can be done while they're not present. It is generally a good idea to create an allow-list of users to prevent abuse of resources on a system if its a multi-user system. In practice this tends to be pretty easy to manage as most interactive users tend to not need scheduled tasks.\n\nThe primary means of restricting access is by adding entries to /etc/cron.allow, and /etc/cron.deny. If the \"allow\" file exists the \"deny\" file will be ignored. If neither exist then all users will be able to define cron entries. The files themselves consist of one username per line with no leading or trailing whitespace.\n\nStarting out with an empty allow list still allows for root to run cron tasks but prevents any user on that system from executing them until added. This can be done with the following command:\n\n```\ntouch /etc/cron.allow\n```\n\nYou can confirm the restriction from any user account by running the following command (you'll see the same error):\n\n```console\n$ crontab -l\nYou (testuser) are not allowed to use this program (crontab)\nSee crontab(1) for more information\n```\n\nThe allow and deny files don't support specifying groups (which would generally make access management to these significantly easier). This can be worked around using the pam_listfile module but doesn't provide as much context when a user is prevented to run a cron job. A user who is not permitted to run cron jobs through PAM will have their configured jobs silently fail (the system log will reflect the failure).\n\nAdd the following line to /etc/pam.d/crond session section:\n\n```pam\nsession    required   pam_listfile.so item=group sense=allow file=/etc/cron.groups.allow onerr=fail\n```\n\nCreate a file /etc/cron.groups.allow with the following contents:\n\n```txt\ncrontab\n```\n\nThis assumes the group crontab already exists on your system. To allow a user to run cron entries they now just need to be added to the crontab group. If they are not, they'll still be able to edit, view, and clear their crontab entries but when they attempt to execute, lines like the following will show up in the system's log:\n\n```syslog\n2017-10-26T22:52:01.000000+00:00 collapsed-autumn-sound crond[706]: pam_listfile(crond:session): Refused user testuser for service crond\n2017-10-26T22:52:01.000000+00:00 collapsed-autumn-sound crond[702]: (testuser) PAM ERROR (Authentication failure)\n2017-10-26T22:52:01.000000+00:00 collapsed-autumn-sound crond[702]: (testuser) FAILED to open PAM security session (Authentication failure)\n```\n\n## Ensure Tasks Don't Overlap\n\nOne trick I've picked up to ensure that a periodically running long command doesn't overlap with itself (for example a backup script, or scrubbing a large ZFS pool).\n\nThis can be done with the flock utility, it creates an empty file with a lock on it that is released when your command or script is finished executing. If it can't obtain the lock, it simply won't run your script and return a bad exit code. Be sure to use a unique lock file for each task.\n\nAn example of how to do this would look in a user's crontab would look like the following:\n\n```cron\n0 0 * * * /usr/bin/flock --nonblock /tmp/backup.lock $HOME/scripts/full_backup.sh\n```\n\nYou can have flock wait for any number of seconds by replacing `--nonblock` with `--wait 300` (300 being five minutes).\n","created_at":1509053742,"fuzzy_word_count":1e3,"path":"/notes/cron/","published_at":1509053742,"reading_time":5,"tags":["linux"],"title":"Cron Daemon","type":"notes","updated_at":1509053742,"weight":0,"word_count":992},{"cid":"42a6b6d16382128b974e8b32b36a6abe6bdffcac","content":"\nSyslog-NG is a fast, reliable, and secure syslog daemon that can do advanced\nprocessing and log centralization while maintaining a sane configuration file\nsyntax. I've recently come to vastly prefer it over my previous long term\nfavorite [Rsyslog][3].\n\nIt's important to note that when modifying the logs statements, they will be\nprocessed in order. This means log statements that finalize a message will\nnever make it past that statement. This finalization behavior can be a great\ntool for optimizing the processing path of logs but can result in unexpected\nbehavior if you don't pay attention when re-ordering the statements.\n\n```\n# /etc/syslog-ng/syslog-ng.conf\n\n@version: 3.7\n@module system-source\n\noptions {\n  # IP addresses are more reliable descriptors and doesn't require a network\n  # connection for consistent logging\n  use_dns(no);\n\n  # Output log stats every 12 hours, and include details about individual\n  # connections and log files.\n  stats_freq(43200);\n  stats_level(1);\n\n  # Use a more standard timestamp, but keep the precision requested for\n  # RFC5424 TIME-SECFRAC\n  ts_format(iso);\n  frac_digits(6);\n};\n\nsource local {\n  system();\n  internal();\n};\n\ndestination bootFile { file(/var/log/boot.log); };\ndestination cronFile { file(/var/log/cron); };\ndestination mailFile { file(/var/log/maillog); };\ndestination messageFile { file(/var/log/messages); };\ndestination secureFile { file(/var/log/secure); };\ndestination spoolFile { file(/var/log/spooler); };\ndestination syslogFile { file(/var/log/syslog); };\n\nfilter authpriv { facility(authpriv); };\nfilter boot { facility(local7); };\nfilter cron { facility(cron); };\nfilter kern { facility(kern); };\nfilter mail { facility(mail); };\nfilter messages { level(info) and not (facility(mail, authpriv, cron)); };\nfilter spool { facility(uucp) or (facility(news) and level(crit)); };\nfilter syslog { facility(syslog); };\n\nlog { source(local); filter(authpriv); destination(secureFile); };\nlog { source(local); filter(boot); destination(bootFile); };\nlog { source(local); filter(cron); destination(cronFile); };\nlog { source(local); filter(mail); destination(mailFile); };\nlog { source(local); filter(messages); destination(messageFile); };\nlog { source(local); filter(spool); destination(spoolFile); };\nlog { source(local); filter(syslog); destination(syslogFile); };\n```\n\nThe base config can also be downloaded [from me][4].\n\n## Using a Log File as a Source\n\nIt is not uncommon for a daemon to write out its own log files rather than\nusing syslog. These files should usually be aggregated to a central log server\nas well for future analysis. One example (if you don't already have it sending\nto syslog) is auditd's log. The following source can be used to read that log\nfile:\n\n```\nsource auditd {\n  file(\n    /var/log/audit/audit.log\n\n    follow_freq(1)\n\n    default_facility(local6)\n    default_priority(info)\n\n    flags(no_parse, sanitize_utf8)\n\n    program_override(auditd)\n  );\n};\n```\n\nThis configuration will poll the file once a second for any changes (you may\nwant to drop this for programs that don't log as often). I recommend setting a\nprogram, facility, and priority as the defaults are not ideal for daemon logs.\nKnow what the flags mean and set them according to the file you're pulling in.\n\n## Receiving Secure Logs from Other Hosts\n\nSystem logs hold valuable diagnostic and security related events, but may\naccidentally contain additional sensitive information. Whenever possible, log\nsenders should be authenticated and should be using an encrypted transport\nmechanism. This is very easy to setup and tune with Syslog-ng.\n\nYou should create a [certificate authority][1] dedicated to authenticating\nclients to your logging infrastructure. The server cert does not have to be\nsigned by the same certificate as your clients, but your clients must be able\nto verify the validity of the server using their CA. Clients should not use a\npublic CA for authentication as any attacker could purchase a valid certificate\nand poison your log data.\n\nPlace the server certificate at `/etc/syslog-ng/server.crt` and it's associated\nprivate key (no passphrase supported) at `/etc/syslog-ng/server.key`. Ensure\nboth have sane restrictive permissions such as `0600`. All certificates and\nkeys should be in the PEM format.\n\nCreate two directories `/etc/syslog-ng/ca.d` and `/etc/syslog-ng/crl.d`. Place\nthe CA certificate in the `ca.d` directory and switch to that directory.\nSyslog-ng uses the hashed directory format so a symlink needs to be created\nwith it's hash. The following one liner will create the appropriate hash link\nfor the certificate authority file `syslog_clients.crt`:\n\n```\nln -s syslog_clients.crt $(openssl x509 -noout -hash -in syslog_clients.crt).0\n```\n\nIf you have a CRL associated with the client certificate (doesn't hurt to\npre-generate an empty one and configure it now) place it in the `crl.d`\ndirectory. This also needs a hash identifier but the name is slightly\ndifferent. This assumes a CRL file name of `syslog_clients.crl`, and that\nyou're in the `crl.d` directory:\n\nTODO: Check and see if this works, `x509` may need to be replaced with `crl`.\nUltimately the hash identifier needs to be the hash of the issuing CA so it may\nsimply need to be reused from the last step.\n\n```\nln -s syslog_clients.crl $(openssl x509 -noout -hash -in syslog_clients.crl).r0\n```\n\nYou'll also want to generate custom a custom `dhparam` file using the following\ncommand:\n\n```\nopenssl dhparam -out /etc/syslog-ng/dhparam.pem 2048\nchmod 0600 /etc/syslog-ng/dhparam.pem\n```\n\nFor the security and safety of your log server, I highly recommend restricting\nthe cipher list to only those supported by TLS 1.2 or later. You can find which\nof these your log server supports by running the following command on it:\n\n```\nopenssl ciphers -v | grep TLSv1.2 | awk '{ print $1 }' | tr '\\n' ':' | sed -e 's/:$//'\n```\n\nThe output of the above will be used in the `ciphers` parameter to `tls` in the\nconfiguration below.\n\nSimilarly with ECDH curves, view what is supported on your system (and remove\nthe NIST curves just in case they really are untrustworthy):\n\n```\nopenssl ecparam -list_curves | sed 's/://g' | awk '{ print $1 }' | grep -v prime | tr '\\n' ':'\n```\n\nThis list will be used as the contents of the `ecdh_curve_list` parameter to\n`tls` in the configuration below.\n\n```\nsource tlsListener {\n  max_connections(100)\n\n  syslog(\n    trasport(tls)\n    tls(\n      ca_dir(/etc/syslog-ng/ca.d)\n      crl_dir(/etc/syslog-ng/crl.d)\n\n      dhparam_file(/etc/syslog-ng/dhparam.pem)\n\n      cert_file(/etc/syslog-ng/server.crt)\n      key_file(/etc/syslog-ng/server.key)\n\n      ciphers(ECDHE-RSA-AES256-GCM-SHA384:ECDHE-ECDSA-AES256-GCM-SHA384:ECDHE-RSA-AES256-SHA384:ECDHE-ECDSA-AES256-SHA384:ECDHE-RSA-AES128-GCM-SHA256:ECDHE-ECDSA-AES128-GCM-SHA256:ECDHE-RSA-AES128-SHA256:ECDHE-ECDSA-AES128-SHA256:AES256-GCM-SHA384:AES256-SHA256:AES128-GCM-SHA256:AES128-SHA256:DHE-DSS-AES256-GCM-SHA384:DHE-RSA-AES256-GCM-SHA384:DHE-RSA-AES256-SHA256:DHE-DSS-AES256-SHA256:DHE-DSS-AES128-GCM-SHA256:DHE-RSA-AES128-GCM-SHA256:DHE-RSA-AES128-SHA256:DHE-DSS-AES128-SHA256)\n      ecdh_curve_list(secp256k1:secp384r1:secp521r1)\n      peer_verify(require_trusted)\n      ssl_options(no_sslv2, no_sslv3, no_tlsv1)\n    )\n\n    so_rcvbuf(1MiB)\n    so_sndbuf(1MiB)\n  );\n\n  tags(secure)\n};\n\ndestination networkLogs {\n  file(\n    /var/log/remote/${HOST}/${YEAR}-${MONTH}-${DAY}_sys.log\n\n    create_dirs(yes)\n  )\n};\n\nlog { source(tlsListener); destination(networkLogs) };\n```\n\nThere are a few parameters that should be tuned to your specific log host.\nThose being the `max_connections`, `so_rcvbuf` and `so_sndbuf`.\n\nThe `max_connections` option is easy, it should be set based on the number of\nlog clients you expect to have. There is memory overhead for each of these\nslots so setting it very high to ignore it isn't ideal, it's still a good idea\nto give yourself ~30% overhead to accommodate future clients and reconnection\nsurges when the server may not be aware clients disconnected.\n\nThe buffer settings should be set based on the peak number of messages any\nindividual client may produce. This is initially hard to guess, but can be easy\nto measure if you use the graphite plugin to monitor incoming log volumes.\nDetails on tuning this is provided in the syslog-ng Open Source Edition\nAdministrator Guide in section 6.5.1, but roughly it seems that 1MiB of buffer\nis roughly able to support 1k msg/s.\n\nAdjustments to `so_rcvbuf` may require changing the system's\n`net.core.rmem_max` sysctl parameter, and `so_sndbuf` may require changing the\nsystem's `net.core.wmem_max` sysctl parameter. `so_sndbuf` is much less\nimportant here than on the clients so it may also be omitted in the interest of\nsaving memory.\n\nThe other two considerations for this are largely administrator preference. The\nfirst, I prefer to tag my logs that come in through this mechanism as 'secure'\nsince we received them authenticated, and encrypted. It is very unlikely they\nwere tampered with in transit. This is used to differentiate with logs that\ncame in through the UDP network mechanism from clients that don't support\nsending their logs over TLS. If you prefer to save the few bytes with every\nmessage you can remove the `tags` configuration or adjust it to your liking.\n\nThe other is the path for per-host logs (defined in the destination). I would\nconsider carefully what the most likely use case of the logs are going to be\n(when are you going to want to access them?). There are generally two schools\nof thought for these types of file, host-centric or date-centric.\n\nIf you are likely going to be using your raw log files to investigate an issue\non an individual host (performance issue, limited security breach, etc) then a\nhost centric view is going to be most efficient for you. If instead you want to\nsee patterns or similarities between many hosts at once during a certain time\nwindow (large security breach, common changes such as package upgrades between\nseveral hosts, etc), it'll likely be easier to work with your logs based on the\ndate. I prefer the host centric approach as I additionally push my logs into\n`graylog`, but limit the time window that `graylog` keeps relying on these on\ndisk files for long term archiving.\n\nYou'll need to allow TCP port 6514 inbound on your host's firewall from your\nlog clients (I usually allow the whole network since this is a heavily\nauthenticated service).\n\n## Receiving Insecure Logs from Other Hosts\n\nNote all log generators support sending their logs over the network using TLS\nwith client certificates. Most switches and routers only support sending their\nlog information using UDP. We get no authentication or privacy guarantees so\nthey may be spoofed fairly easily, reviewing these when looking for hostile\nactions should be viewed in this light. The logs themselves are still\npotentially invaluable information for diagnostic and incident response and\nthus should still be collected.\n\nDue to the potential for abuse of this I highly recommend you create a filter\nthat whitelist IPs allowed to send to this destination. (TODO: Give an example\nof this)\n\n```\nsource udpListener {\n  syslog(\n    transport(udp)\n    tags(insecure)\n  )\n};\n\nlog { source(udpListener); destination(networkLogs) };\n```\n\nGenerally the devices that I have sending UDP only logs to my server have a\nfairly low volume, if you do have high incoming volumes, you'll likely want to\nincrease the `so_rcvbuf` option.\n\nI reuse the same destination target for this as used in the secure log\nreception, if you aren't using that be sure to pull the `networkLogs`\ndestination from there.\n\nThe log statements between to the two can be combined to simplify the reasoning\naround what will end up in the final logs. Filtering of specific sources and be\nhandled with channels within the log context if so desired.\n\nYou'll need to open UDP port 514 inbound from your insecure log client to allow\nthe logs through.\n\n### Secure Flag Spoofing\n\nWith the current configuration it is possible for an insecure client to set the\nsecure flag before syslog-ng receives it. Ideally this source would remove the\ntag if it is set. A rewrite rule like the following can be used to remove the\ntag:\n\n```\nrewrite secureRemoval {\n  clear_tag(secure);\n};\n```\n\nThe problem with the above is rewrites can't be applied directly to sources,\nthey have to be part of a log statement. There are advanced uses involving\nnested logs that may provide this but I haven't looked into it. The alternative\nis to specify a filter like the following and use it in places where I only\ncare about the secure logs (which is currently my preferred choice):\n\n```\nfilter secureOnly { tags(secure) and not tags(insecure); };\n```\n\nIt may also be useful to separately log these attempts to pollute the logs\nusing something like the following:\n\n```\ndestination spoofed { file(/var/log/network/spoofed_secure.log); };\nfilter spoofedSecure { tags(secure, insecure); };\nlog { source(tlsListener); filter(spoofedSecure); destination(spoofed); flags(final); }\n```\n\n## Sending Logs Securely\n\nTo match our secure reception, we need to be able to securely send logs to our\ncentral log host. Similar to the server portion of this you'll need to generate\na client certificate \u0026 key pair, as well as place the server's issuing cert (or\nchain of certs) into `/etc/syslog-ng/ca.d` and perform the same hashing step on\neach individual certificate as was done in the server portion.\n\nIf the server certificate is publicly verifiable, you can instead point\n`ca_dir` at the system directory (which on my test system is `/etc/ssl/certs`).\nEnsure this directory uses the hashing format by looking for tell-tale symlinks\nwith names similar to `ce5e74ef.0`.\n\n```\ndestination tlsLogServer {\n  syslog(\n    10.0.0.10\n\n    disk_buffer(\n      disk_buf_size(256MiB)\n      mem_buf_size(16MiB)\n      reliable(no)\n    )\n\n    transport(tls)\n    tls(\n      ca_dir(/etc/syslog-ng/ca.d/)\n\n      cert_file(/etc/syslog-ng/client.crt)\n      key_file(/etc/syslog-ng/client.key)\n\n      ciphers(ECDHE-RSA-AES256-GCM-SHA384:ECDHE-ECDSA-AES256-GCM-SHA384:ECDHE-RSA-AES256-SHA384:ECDHE-ECDSA-AES256-SHA384:ECDHE-RSA-AES128-GCM-SHA256:ECDHE-ECDSA-AES128-GCM-SHA256:ECDHE-RSA-AES128-SHA256:ECDHE-ECDSA-AES128-SHA256:AES256-GCM-SHA384:AES256-SHA256:AES128-GCM-SHA256:AES128-SHA256:DHE-DSS-AES256-GCM-SHA384:DHE-RSA-AES256-GCM-SHA384:DHE-RSA-AES256-SHA256:DHE-DSS-AES256-SHA256:DHE-DSS-AES128-GCM-SHA256:DHE-RSA-AES128-GCM-SHA256:DHE-RSA-AES128-SHA256:DHE-DSS-AES128-SHA256)\n      ecdh_curve_list(secp256k1:secp384r1:secp521r1)\n      peer_verify(require_trusted)\n      ssl_options(no_sslv2, no_sslv3, no_tlsv1)\n    );\n  );\n};\n\nlog { source(local); destination(tlsLogServer); };\n```\n\nYou should replace `10.0.0.10` with the IP or hostname of your log host.\n\nAs with the server side there is some level of tuning required for your\nspecific setup. One that is missing but may be relevant is the `so_sndbuf`\noption which might need tuning from the defaults, but due to the buffer on the\nsystem is less valuable here than on the receiver.\n\nThe disk / memory buffer has two modes of operation which should be considered\nbased on your deployment. When the `reliable` option is disabled, and the\nconnection to the log server is backed up or lost, messages will begin by first\nqueuing to memory and when that fills up will begin queuing to disk.\n\nThe documentation on the reliable mode was a bit confusing but if I read it\ncorrectly, when lost it will buffer to both memory and disk, when the memory\nfills up it will continue buffering to disk only which seems to largely make\nthe memory buffer redundant.\n\nBoth modes guarantee message delivery in the event there is only a connection\nissue, but only the reliable mode guarantees message delivery in the event\nsyslog-ng reloads, restarts, or crashes at the expense of a large performance\nhit. For the systems I operate that performance penalty for a slim chance of\nmessage loss is not worth the trade off, but environments under certain\ncompliance requirements may not have the luxury of allowing that trade off.\n\nYou'll need to allow TCP port 6514 outbound in your host's firewall to the log\nserver to ensure your logs arrive.\n\nWhile not required, I highly recommend enabling mark messages (in another\nsection in this document) for all outbound network based destinations to be\nused as a health indicator for the connection and host.\n\n## Mark Messages\n\nWhen sending logs to a remote system it can be incredibly valuable to know\nwhether the sending host has not sent anything because it or its logging daemon\nare down or if there simply has not been any log generated on the machine.\n\nThe destinations themselves can take a couple of mark options to send along a\nmark notice whenever there has been no destination traffic. The following is an\nexample of sending a mark after 15 minutes of inactivity to a destination:\n\n```\nmark_freq(900);\nmark_mode(dst_idle);\n```\n\nThere are more mark modes which can be specified and are detailed in chapter 9\nof the syslog-ng OSE admin guide (though `dst_idle`, `none`, and `periodical`\nare the only ones I find of value).\n\n## Testing Sources, Filters, and Destinations\n\nBefore relying on a complex set of rules you should test them extensively.\nNormally this would be rather tricky, relying on legitimate messages to come\nin. This task is easier if the host you're testing is a log server, but that\nlimits your testing to simply that source.\n\nSince syslog-ng allows a source to be a combination of individual sources, we\ncan turn individual sources into a log server and test directly against them\nusing the logger utility. This should be done one source at a time (though by\nsetting different ports you could potentially test them all at once).\n\nIf I wanted to test the following source:\n\n```\nsource local {\n  system();\n  internal();\n};\n```\n\nI would add a syslog block like so:\n\n```\nsource local {\n  system();\n  internal();\n\n  syslog(\n    ip(127.0.0.1)\n    port(5014)\n    transport(udp)\n    tags(testCase)\n  );\n};\n```\n\nAfter a restart, you can use the logger utility to send messages like so:\n\n```\nlogger --udp --server 127.0.0.1 --port 5014 --tag syslogTest --id=$$ --priority local1.info \"Test message\"\n```\n\nA few important details from this, when filtering on program name, you'll want\nto modify the `--tag` parameter. The message is the last quoted portion, the\nfacility is the portion before the period in the priority, and the severity the\nportion after the dot.\n\nIf you perform filtering based on IP you'll need to modify the specific IP to\nmatch against 127.0.0.1.\n\nEstablish your test and check all of your filters for both positive and\nnegative matches, and ensure all configured destinations are outputting the\nexpected results. Once testing is complete, be sure to remove the extra source.\n\nI like to additionally test, on an actual system what messages I might be\nmissing from sources. Adding the following rules will log all messages that\nhaven't matched any filters to `/var/log/fallback.log`. I recommend adding all\nof your sources to the log entry.\n\n```\ndestination fallback {\n  file(\n    /var/log/fallback.log\n\n    template(\"${ISODATE} ${SOURCE}-\u003e${FACILITY}.${LEVEL} ${PROGRAM} ${TAG} ${MSGID} ${MSG}\\n\")\n    template_escape(no)\n  );\n};\nlog { source(local); destination(fallback); flags(fallback); };\n```\n\n## Audit Log Filter\n\nIf you have [auditd][2] configured to send it's messages to syslog via it's\ndispatcher it is useful to filter those out into their own file so the audit\nreporting tools can still operate on them. The following example is intended to\nbe used with the configuration documented on my [auditd][2] page as well as the\nlistener defined in the secure log reception section in on this page.\n\nPay attention to the location of where these blocks are placed as they include\na `final` flag to prevent polluting the normal logs from the hosts.\n\n```\ndestination networkAuditFile {\n  file (\n    /var/log/remote/${HOST}/${YEAR}-${MONTH}-${DAY}_audit.log\n\n    # We need to strip out the headers so the utilities can understand the\n    # format\n    template(\"${MSG}\\n\")\n    template_escape(no)\n  );\n};\n\n# Info filter is required as audispd will include status reports about it's\n# operations.\nfilter auditLogs {\n  program(audispd) and facility(local6) and level(info);\n};\n\n# The filter `secureOnly` is mentioned in the insecure logs from other hosts\n# section and can be removed is you don't receive insecure logs.\nlog {\n  source(tlsListener);\n  destination(networkAuditFile);\n\n  filter(auditLogs);\n  filter(secureOnly);\n\n  flags(final);\n}\n```\n\nWith this in place you can make use of the audit tools as normal, while on the\nlog host, by `cat`'ing in appropriate file(s) into the tool. For an example:\n\n```\ncat /var/log/remote/testhost.example.tld/2017/10/19_audit.log | aureport --interpret --failed --executable\n```\n\n## Value Parsing Concern\n\nThe default value when using a parser on the incoming log data is to simply\ndrop a message. I *NEVER* want to lose a message because of a bug in the\nparser. At the same time if a data source is expecting a type and it doesn't\nget it, that could cause harm to the data source. Maybe the log message does go\nthrough to the internal() messaging source and it simply won't show up at the\ndestination (test this). Alternatively I can try and send it anyway by setting\nthe following global option:\n\n```\non-error(fallback-to-string);\n```\n\n## Log Rotation\n\nSyslog-ng doesn't handle log rotation on its own, but [logrotate][5] will\nhandle this quite well.\n\n## Future Work\n\nThere is a package named `logcheck` that seems interesting for extracting\nanomalous events from logs. It may be useful directly or at least as a\nreference for anomalous events. Syslog-ng is capable of some pretty powerful\nprocessing, which when combined with the SMTP endpoint, may be sufficient on\nits own (there are also more powerful tools such as Graylog as well).\n\nThere are use flags for enable SMTP destinations, JSON handling, and AMQP\ndestinations. They may be useful and should be looked into to see what kind of\nvalue I can get out of them.\n\nI may want to modify the group() and user() variable to drop permissions when I\ncan... Reduce privileges wherever possible.\n\nVery good chance that the process accounting logs are something worth\ninvestigating, but that is beyond the scope of this page.\n\nFor logs coming in from TLS authenticated clients we can inject the\ncertificate's CN by modifying that destinations template to include\n`${.TLS.X509_CN}`. I'm not sure if that is worth while but could add extra\nvalidity information.\n\n[1]: {{\u003c ref \"./certificate_authority.md\" \u003e}}\n[2]: {{\u003c ref \"./auditd.md\" \u003e}}\n[3]: {{\u003c ref \"./rsyslog.md\" \u003e}}\n[4]: /note_files/syslog-ng/syslog-ng.conf\n[5]: {{\u003c ref \"./logrotate.md\" \u003e}}\n","created_at":1508910962,"fuzzy_word_count":3300,"path":"/notes/syslog-ng/","published_at":1508910962,"reading_time":16,"tags":["linux","security"],"title":"Syslog-NG","type":"notes","updated_at":1540945455,"weight":0,"word_count":3204},{"cid":"8fdd4c11a2c4b12377b19ae721711a0778b1112c","content":"\n[CFSSL][1] is a toolkit of utilities for TLS PKI infrastructures and supports\nmore functionality than I've personally needed. It is a fast and convenient way\nto setup and manage a multi-layer internal certificate authority.\n\nI've used it to generate an internal root CA, with sub-CAs for internal only\nserver certificates, and separate CAs for each domain of client certificates\n(such as VPN, log, mail, and LDAP servers). This allows the root CA to be\nprotected more stringently than specific domains.\n\nCFSSL supports hardware backed private keys (HSMs), including the [Yubikey\nNEO][2] as long as you don't need \u003e 1 signature/sec (which I definitely don't).\nYou could also set it up with a Red October server for private key management,\nwhich could in turn be HSM backed.\n\nI haven't settled on software or processes to reliably handle my PKI\ninfrastructure.\n\n## Installation\n\nThese notes assume you have a working [golang][3] installation. To install\ncfssl and the other associated utilities run the following command:\n\n```\ngo get -u github.com/cloudflare/cfssl/cmd/...\n```\n\nIf you have any errors, ensure your golang installation is sane. Some\ndistributions do not have support for all of the available ciphers\n(specifically the eliptic curve variants in Red Hat based distributions) which\nmay impact your ability to install and use the tool.\n\n## Root CA Setup\n\nFor the sake of organization I use a fairly simple directory structure which\ncan be created with the following command:\n\n```\nmkdir -p ca/{source,json,output}\ncd ca\n```\n\nAll the configuration is done through JSON files. The config for a root CA may\nlook something like the following (which I placed in `ca/source/root.json`):\n\n```json\n{\n  \"CN\": \"Stelfox Root Certificate Authority\",\n  \"key\": {\n    \"algo\": \"ecdsa\",\n    \"size\": 521\n  }\n}\n```\n\nIf you want to use RSA as your root key, you'd replace the `algo` field with\n`rsa` and the `size` field with an appropriately sized key (I'd recommend 3072\nor 4096 but you should match it to your security standards.\n\nTo generate the cert/key pair you can use the following command:\n\n```\ncfssl genkey -initca source/root.json \u003e json/root.json\n```\n\nYou can turn the resulting files into a set of PEM encoded files more familiar\nto day to operation:\n\n```\ncat json/root.json | cfssljson -bare output/root\n```\n\nIn the output directory you'll find `root.csr`, `root-key.pem`, and `root.pem`\nwhich is the effective CSR used to generate the cert, the private key, and the\ncert itself respectively. You can view the contents of the certificate using\nthe following command:\n\n```\nopenssl x509 -in output/root.pem -text -noout\n```\n\nWe'll then want to create a `config.json` file to define who, and how we sign\nfuture certificates.\n\n```json\n{\n  \"signing\": {\n    \"profiles\": {\n      \"subca\": {\n        \"ca_constraint\": {\n          \"is_ca\": true,\n          \"max_path_len\": 0\n        },\n        \"expiry\": \"8760h\",\n        \"usages\": [\"cert sign\", \"crl sign\"]\n      }\n    }\n  }\n}\n```\n\n## Generating a Sub-CA\n\nWe'll generate a CA to handle internal server authentication (my servers to my\nservers). I created the following file in `sources/servers.json`:\n\n```json\n{\n  \"CN\": \"Stelfox Server Certificate Authority\",\n  \"key\": {\n    \"algo\": \"ecdsa\",\n    \"size\": 521\n  }\n}\n```\n\nAnd generate the servers CA with the following command:\n\n```\ncfssl gencert -ca file:output/root.pem -ca-key file:output/root-key.pem \\\n  -config config.json -profile subca source/servers.json \u003e json/servers.json\n```\n\nWhich can in turn be turned into normal certificates as before with the\nfollowing command:\n\n```\ncat json/servers.json | cfssljson -bare output/servers\n```\n\nYou'll also want to generate a certificate for your first server using this.\nBefore we can we'll want to add another profile to our config. This is up to\nyou to merge into your config:\n\n```json\n\"server\": {\n  \"expiry\": \"8760h\",\n  \"usages\": [\"signing\", \"key encipherment\", \"server auth\", \"client auth\"]\n}\n```\n\n## Creating a Server Certificate\n\nCreate a certificate configuration for the server (this one for\ntesthost.stelfox.net with a couple of CNAMEs) store in `source/testhost.json`.\nIf you use the hosts array, be sure to include the CN name as well if it needs\nto be validated as well.\n\n```json\n{\n  \"CN\": \"testhost.stelfox.net\",\n  \"key\": {\n    \"algo\": \"ecdsa\",\n    \"size\": 521\n  },\n  \"hosts\": [\n    \"testhost.stelfox.net\",\n    \"tst01.dev.sfa.stelfox.net\",\n    \"fud01.dev.sfa.stelfox.net\"\n  ],\n  \"names\": [\n    {\n      \"C\": \"US\",\n      \"ST\": \"No State\",\n      \"L\": \"No Town\",\n      \"O\": \"Stelfox Personal Systems\",\n      \"OU\": \"Research \u0026 Development\"\n    }\n  ]\n}\n```\n\nAnd generate the key and certificate:\n\n```\ncfssl gencert -ca file:output/servers.pem -ca-key file:output/servers-key.pem \\\n  -config config.json -profile server source/testhost.json \u003e json/testhost.json\ncat json/testhost.json | cfssljson -bare output/testhost\n```\n\n## Distributing Trust\n\nI generally recommend distributing the CA certificate for the specific service\nto the specific service and not make it a generally trusted the system as a\nwhole. This is especially true for this program as without a HSM there is no\nprotection over any of the private keys.\n\n[1]: https://github.com/cloudflare/cfssl\n[2]: {{\u003c ref \"./yubikey.md\" \u003e}}\n[3]: https://golang.org/\n","created_at":1508884762,"fuzzy_word_count":800,"path":"/notes/cfssl/","published_at":1508884762,"reading_time":4,"tags":["certificates","security"],"title":"CFSSL","type":"notes","updated_at":1540945455,"weight":0,"word_count":776},{"cid":"30d6cc07de316e0526eb38917afea8bef0ffa402","content":"\n# Investigating LVM From Dracut\n\nIn my [my last post]({{\u003c ref \"2017-10-24-visible-yet-missing-logical-volumes\" \u003e}}), I covered finding logical volumes that were missing from LVM from within a live CD (which is effectively a whole standard environment). Working with dracut is quite a bit more limited.\n\nTurns out that the commands I'm normally used to for operating and inspecting LVM volumes can all be accessed as a second parameter to the \"lvm\" tool like so:\n\n```console\n$ lvm vgscan\n$ lvm pvscan\n$ lvm lvscan\n```\n\nFor my particular issue, it led me to notice that block device of my root filesystem was missing due to a missing kernel driver...\n","created_at":1508859907,"fuzzy_word_count":200,"path":"/blog/2017/10/investigating-lvm-from-dracut/","published_at":1508859907,"reading_time":1,"tags":["linux","lvm","tips"],"title":"Investigating LVM From Dracut","type":"blog","updated_at":1508859907,"weight":0,"word_count":101},{"cid":"64c1ce9b6726dc0a664ff47bb91b7352b9a8a38b","content":"\n# Visible Yet Missing Logical Volumes\n\nWhile working on an automated install script for an embedded board, I hit an issue with the logical volumes never showing up in /dev/mapper, and in turn unable to be mounted. This left me in the dracut emergency shell (after about three minutes), with little to go on beyond the following error:\n\n```console\n[187.508531] dracut Warning: Could not boot.\n[187.510560] dracut Warning: /dev/disk/by-uuid/5681-902D does not exist\n[187.512534] dracut Warning: /dev/mapper/system-root does not exit\n[187.513990] dracut Warning: /dev/system/root does not exist\n```\n\nAfter booting into a live CD I checked to make sure the volume group showed up under pvscan like so:\n\n```console\n# pvscan\n  PV /dev/mmcblk0p3   VG system     lvm2 [19.50 GiB / 0    free]\n  Total: 1 [19.50 GiB] / in use: 1 [19.50 GiB] / in no VG: 0 [0   ]\n```\n\nAnd the logical volumes were also showing up with lvscan:\n\n```console\n# lvscan\n  inactive          '/dev/system/root' [18.45 GiB] inherit\n  inactive          '/dev/system/swap' [1.05 GiB] inherit\n```\n\nNotice that they are both marked inactive? That's our issue. To fix it we can mark all logical volumes under our volume group as active (replace \"system\" with your volume group name):\n\n```console\n$ vgchange --activate y system\n```\n\nThis didn't fix the LVM volumes showing up at boot but it did allow me to get back into my root filesystem as a chroot so I could investigate the issue which I've finished documenting in [another post]({{\u003c ref \"/blog/2017-10-24-investigating-lvm-from-dracut\" \u003e}}).\n","created_at":1508857092,"fuzzy_word_count":300,"path":"/blog/2017/10/visible-yet-missing-logical-volumes/","published_at":1508857092,"reading_time":2,"tags":["linux","lvm","tips"],"title":"Visible Yet Missing Logical Volumes","type":"blog","updated_at":1508857092,"weight":0,"word_count":236},{"cid":"62848d5b0c24ea0124e498abbbff09f046810f0e","content":"\nOver the years I've found myself using many different naming schemes for\nservers under my control. I came across a [naming convention][1] that finally\nfeels correct. That blog post is quite well written and will let it stand on\nits own. In the event it ever disappears the important bits (and those where\nI've personalized it) are included here.\n\nThere are ultimately two or three DNS names that each host receives. The first\nis a permanent unique identifier for the host. The blog post offers the\n[mnemonic encoding][2] project as a way to generate the hostnames. I've built\n[my own generator][3] that accomplishes the same effect with a vastly larger\npotential space (not that I need it). It has the added bonus of generating\ninteresting names that are easy to talk about. Some samples (none real):\n\n```\nbrave-noon-funeral.stelfox.net\nobscured-dawn-bush.stelfox.net\nstriking-evening-moss.stelfox.net\n```\n\nThe second name is a CNAME intended to be redirectable to new instances or\nservers if an old one dies or otherwise needs to be decommissioned. This is\ntied directly to the function of the machine, while letting the machine behind\nthe name be replaceable.\n\nThis name is a combination of a short purpose code, a serial number (indicating\nthe specific instances of the purpose), what environment it runs in\n(production, staging, etc), a location code, and your domain. The blog post\nalso includes a country code but I've chose to leave that out.\n\nThe last name(s) are convenience names intended to expose services to users.\nThese domains are along the lines of `webmail.stelfox.net` and can point at\nmultiple instances of purpose names or unique instance names.\n\nSerials are unique to the purpose and site and are zero padded integers (01,\n02). Serials are only unique within their local location code. Environment\nshould not be taken into account when calculating the serial.\n\n## List of Purposes\n\nThese three letter purpose codes are kind of arbitrary. For consistency this\ndocuments the purpose codes I've generated for specific uses.\n\n* app - Application server (non-web)\n* sql - Database server\n* ftp - SFTP server\n* mta - Mail server\n* dns - Name server\n* cfg - Configuration management server (Puppet, Ansible, etc.)\n* mon - Monitoring server (Nagios, Sensu, etc.)\n* prx - Proxy / Load balancer (software)\n* ssh - SSH jump / Bastion host\n* sto - Storage server\n* vcs - Version control software (Git)\n* vmm - Virtual machine manager\n* web - Web server\n* cch - Cache server (Redis, Memcached, etc.)\n* vpn - VPN server\n\n## List of Environments\n\nSimilar to the purpose codes, these are the chosen three letter codes for\ndifferent environments.\n\n* dev - Development\n* tst - Testing\n* stg - Staging\n* prd - Production\n\n## Location Codes\n\nLocation codes in the original blog post are broken into two segments the local\ncode based on the [United Nations Code for Trade and Transport Locations][4]\n(UN/LOCODE) which is more specific than IATA airport codes and the country\ncode. For my uses I leave the country code out.\n\n[1]: http://mnx.io/blog/a-proper-server-naming-scheme/\n[2]: http://web.archive.org/web/20090918202746/http://tothink.com/mnemonic/wordlist.html\n[3]: https://github.com/sstelfox/dotfiles/blob/master/bin/server_name_generator\n[4]: http://www.unece.org/cefact/locode/service/location.html\n","created_at":1508543942,"fuzzy_word_count":500,"path":"/notes/server-naming-convention/","published_at":1508543942,"reading_time":3,"tags":null,"title":"Server Naming Convention","type":"notes","updated_at":1508773842,"weight":0,"word_count":481},{"cid":"5fb107ff96adfdbf144d063dd0241a76912f01ec","content":"\n# Vultr Deny All Firewall\n\nWhile setting up new instances on Vultr for testing, I wanted to initially ensure that no traffic beyond my own could touch the instances. After adding a matching rule for SSH to my IPv4 address, a default rule shows up that drops any unspecified traffic. Switching to the IPv6 I wanted to add a drop all rule (as I wouldn't be using IPv6 until the system was up).\n\nThe interface only allows \"accept\" rules to be created and additionally you'll be greeted with this message while trying to figure out what to do:\n\n\u003e This firewall ruleset will not be active until at least one rule is added.\n\nThe creative solution I came up with was to add an SSH rule with a custom IP of \"::1/128\". The loopback IPv6 address... The drop all rule showed up and unless there is some bug in Vultr's firewalling nothing should be able to reach these instances over IPv6.\n\nIf you're interested in using Vultr, I'd appreciate it if you consider using my [affiliate link](https://www.vultr.com/?ref=7199712) to sign up.\n","created_at":1508534316,"fuzzy_word_count":200,"path":"/blog/2017/10/vultr-deny-all-firewall/","published_at":1508534316,"reading_time":1,"tags":["security","tips"],"title":"Vultr Deny All Firewall","type":"blog","updated_at":1508534316,"weight":0,"word_count":174},{"cid":"77d82e019f655d49fd6b3e1dd5516057fdeacf1d","content":"\nI keep a copy of my mutt config both [here on the site][1] as well as in my\n[public dotfiles][2]. Eventually I'll likely document my reasoning, preferences\nand the tradeoffs made in that config (and change my mind on most in the\nprocess).\n\n## Vim\n\nSince I use vim as my editor I also added the following line to my vim\nconfiguration file to autowrap my lines at 72 characters, but only for mutt\ncomposed messsages.\n\n```\nau BufRead /tmp/mutt-* set tw=72\n```\n\n[1]: /note_files/mutt/muttrc\n[2]: https://github.com/sstelfox/dotfiles\n","created_at":1508518260,"fuzzy_word_count":100,"path":"/notes/mutt/","published_at":1508518260,"reading_time":1,"tags":["cli","linux"],"title":"Mutt","type":"notes","updated_at":1508518645,"weight":0,"word_count":79},{"cid":"50b9dba6523e490a2482372556ff54bba4936019","content":"\niptables (and ip6tables) is a simple application for interacting directly with\nthe Linux kernel's netfilter firewall. nftables is attempting to replace it.\nnftables has better performance but is incredibly complex and difficult to use.\nUntil the tooling and usability around nftables is addressed iptables will\nlikely remain the reigning champion.\n\nIt is important that your systems have effective ingress and egress rules.\nDepending on your host OS, you'll find your rule set stored in different\nlocations but the following list are where I've seen them before:\n\n* /etc/conf.d/iptables, /etc/conf.d/ip6tables\n* /etc/sysconfig/iptables, /etc/conf.d/ip6tables\n* /var/lib/iptables/rules-save, /var/lib/ip6tables/rules-save\n\nI have a base rule set that I riff off of for [iptables][1], and\n[ip6tables][2]. It can generally be hardened a tad depending on the local\nservices you have available (such as a DNS resolver, local NTP servers, or an\nHTTP(S) proxy).\n\n## Logging From Rules\n\nIf you're using my [auditd][3] rules, you'll already have a detailed log of any\nconnection to or from the system so this may be of little use. Sometimes it's\nvaluable to log attempts that don't successfully make it through the firewall\n(such as failed outbound network attempts).  These are also valuable if your\nsystem has too many connections for the auditd rules to be effective, or you're\nnot using my rule set.\n\nThe final step is actually making iptables log the information you want. This\nis as simple as appending `--log-prefix \"iptables: \"` to whatever LOG targets\nyou have configured. Like so:\n\nThe following rule will log any new SSH connection attempts on the default port\nfor any traffic that successfully makes it to the rule:\n\n```\n-A INPUT -m state --state NEW -m tcp -p tcp --dport 22 -j LOG --log-prefix \"sshConn: \"\n```\n\n## Additional Gentoo Config\n\nGentoo's defaults have an annoying habit to override the rules every time the\nservice is shut down. Before enabling the services I additionally drop the\nconfigs around service control in place at [/etc/conf.d/iptables][4] and\n[/etc/conf.d/ip6tables][5].\n\nThis expects the iptables rules to be at `/var/lib/iptables/rules-save` and\n`/var/lib/ip6tables/rules-save`.\n\n## Blocking ISPs/AS Numbers\n\nI've created [a script][6] that generates rules to block the subnets associates\nwith given AS numbers. AS numbers can be found on [Arin's website][7] and [Team\nCymru][8] keeps a list of other places that keep this information.\n\nIt fetches all network via whois and generates appropriate iptables rules.\nMaybe you have to adjust your whois-Query since I only ran tests against the\nripe db.\n\nThis script could be made significantly more efficient, and safer to update by\nswitching to ipsets, but I haven't gotten around to it.\n\n[1]: /note_files/iptables/iptables.rules\n[2]: /note_files/iptables/ip6tables.rules\n[3]: {{\u003c ref \"./auditd.md\" \u003e}}\n[4]: /note_files/iptables/gentoo_iptables_conf\n[5]: /note_files/iptables/gentoo_ip6tables_conf\n[6]: /note_files/iptables/block_as.sh\n[7]: ftp://ftp.arin.net/info/asn.txt\n[8]: http://www.team-cymru.org/Services/ip-to-asn.html\n","created_at":1508516042,"fuzzy_word_count":500,"path":"/notes/iptables/","published_at":1508516042,"reading_time":2,"tags":["linux","security"],"title":"IPTables","type":"notes","updated_at":1540945455,"weight":0,"word_count":415},{"cid":"fea0aa58cd8b06ca0fa7a38a6367548d7b17b024","content":"\n# Security Principles\n\nWhile reviewing current security hardening practices put out by several organizations and attempting to filter the good recommendations from the outdated legislated requirements, I came across one of the best descriptions of basic security principles. You can find it in the [NIST Guide to General Server Security](http://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-123.pdf) (published in 2008).\n\nI've replicated section 2.4 from the linked document (I have removed the footnotes, but it is otherwise unchanged) in its entirety here for safe keeping and to hopefully help expose this to more people.\n\nWhen addressing server security issues, it is an excellent idea to keep in mind the following general information security principles\n\n- **Simplicity:** Security mechanisms (and information systems in general) should be as simple as possible. Complexity is at the root of many security issues.\n- **Fail-Safe:** If a failure occurs, the system should fail in a secure manner, i.e., security controls and settings remain in effect and are enforced. It is usually better to lose functionality rather than security.\n- **Complete Mediation:** Rather than providing direct access to information, mediators that enforce access policy should be employed. Common examples of mediators include file system permissions, proxies, firewalls, and mail gateways.\n- **Open Design:** System security should not depend on the secrecy of the implementation or its components.\n- **Separation of Privilege:** Functions, to the degree possible, should be separate and provide as much granularity as possible. The concept can apply to both systems and operators and users. In the case of systems, functions such as read, edit, write, and execute should be separate. In the case of system operators and users, roles should be as separate as possible. For example, if resources allow, the role of system administrator should be separate from that of the database administrator.\n- **Least Privilege:** This principle dictates that each task, process, or user is granted the minimum rights required to perform its job. By applying this principle consistently, if a task, process, or user is compromised, the scope of damage is constrained to the limited resources available to the compromised entity.\n- **Psychological Acceptability:** Users should understand the necessity of security. This can be provided through training and education. In addition, the security mechanisms in place should present users with sensible options that give them the usability they require on a daily basis. If users find the security mechanisms too cumbersome, they may devise ways to work around or compromise them. The objective is not to weaken security so it is understandable and acceptable, but to train and educate users and to design security mechanisms and policies that are usable and effective.\n- **Least Common Mechanism:** When providing a feature for the system, it is best to have a single process or service gain some function without granting that same function to other parts of the system. The ability for the Web server process to access a back-end database, for instance, should not also enable other applications on the system to access the back-end database.\n- **Defense-in-Depth:** Organizations should understand that a single security mechanism is generally insufficient. Security mechanisms (defenses) need to be layered so that compromise of a single security mechanism is insufficient to compromise a host or network. No \"silver bullet\" exists for information system security.\n- **Work Factor:** Organizations should understand what it would take to break the system or network’s security features. The amount of work necessary for an attacker to break the system or network should exceed the value that the attacker would gain from a successful compromise.\n- **Compromise Recording:** Records and logs should be maintained so that if a compromise does occur, evidence of the attack is available to the organization. This information can assist in securing the network and host after the compromise and aid in identifying the methods and exploits used by the attacker. This information can be used to better secure the host or network in the future. In addition, these records and logs can assist organizations in identifying and prosecuting attackers.\n","created_at":1508390352,"fuzzy_word_count":700,"path":"/blog/2017/10/security-principles/","published_at":1508390352,"reading_time":4,"tags":["security"],"title":"Security Principles","type":"blog","updated_at":1508390352,"weight":0,"word_count":652},{"cid":"a286a4c33247f51526b8d532f1b90c186d3549ea","content":"\n# Linux Audit Rule Paths\n\nI encountered a little bit of confusion while rewriting my [auditd rules]({{\u003c ref \"/notes/auditd\" \u003e}}) which some Googling did not help me solve.\n\nWhen monitoring a file or directory there are two forms the rules can take. They are effectively equivalent in their functionality. The simpler form is the following format:\n\n```audit-conf\n-w /etc/shadow -p wa\n-w /boot -p wa\n-w /etc/dont_readme -p r\n```\n\nThese three rules are all behaving slightly differently. The first will audit any writes or attribute changes to the shadow file. The second, being a directory, will be recursively watched for writes and attribute changes (including the directory itself). The last line will create an audit record whenever the file is read.\n\nThe second form is more consistent with the syscall rule types but more verbose. Examples are given in the man page for converting between the two forms. Before I show you an example of those rules, this is the part of the man page that ultimately confused me:\n\n\u003e If you place a watch on a file, its the same as using the -F path option on a syscall rule.\n\nSimple enough, this is what the first line of the first example would look like in this style:\n\n```audit-conf\n-a always,exit -F path=/etc/shadow -F perm=wa\n```\n\nBefore changing my rules over to this form I checked a couple to ensure they were behaving correctly then migrated the rest. After applying the new rules I found quite a few audit records I'd expect missing. If I had kept reading (the next line from the last man page) I would have found this:\n\n\u003e If you place a watch on a directory, its the same as using the -F dir option on a syscall rule.\n\nSo the correct way to represent the second line in the first example would be like this:\n\n```audit-conf\n-a always,exit -F dir=/boot -F perm=wa\n```\n\nThe long and short of it, is that the watch flag (-w) has different behaviors depending on what it is pointing at. Having only read the first sentence, I assumed the \"path=\" argument would behave the same as a watch. Attempts to Google whether or not \"path=\" was recursive didn't turn up anything. Now that I've read the man page while writing up this point it is very clearly stated that it is not.\n\nIf anyone else comes across this, \"path=\" is not recursive, \"dir=\" is. Be careful when translating your rules.\n","created_at":1508210420,"fuzzy_word_count":400,"path":"/blog/2017/10/linux-audit-rules/","published_at":1508210420,"reading_time":2,"tags":["linux","tips"],"title":"Linux Audit Rule Paths","type":"blog","updated_at":1508210420,"weight":0,"word_count":393},{"cid":"4e2fa4b020d7b6102d60e04677452369aceff968","content":"\n# Vulnerable Smart Cards\n\nIn addition to the [WiFi vulnerability]({{\u003c ref \"./2017-10-16-a-krack-in-the-defenses.md\" \u003e}}) a much more limited vulnerability was [announced](https://crocs.fi.muni.cz/public/papers/rsa_ccs17) around private GPG keys that were generated using [Infineon's RSA Library version v1.02.013](https://www.commoncriteriaportal.org/files/epfiles/0782V2a_pdf.pdf).\n\nThe vulnerability lies in shortcuts taken to speed up the key generation using the library. The performance increase makes the private key vulnerable to factorization attacks using an extension to [Coppersmith's attack](https://en.wikipedia.org/wiki/Coppersmith%27s_attack).\n\nIt has been [confirmed that YubiKey 4s](https://www.yubico.com/2017/10/infineon-rsa-key-generation-issue/) are effected as are many nations national ID cards. Earlier versions of YubiKey were not affected (including my preference the Neo). This brings up a controversial decision [made by Yubico](https://www.yubico.com/2016/05/secure-hardware-vs-open-source/) a little over a year ago to switch from tested open source and widely inspected libraries, to closed source versions.\n\nThis led some very [respected individuals](https://plus.google.com/+KonstantinRyabitsev/posts/fjgYKfPMF13) to accuse Yubico that they were relying on [security through obscurity](https://en.wikipedia.org/wiki/Security_through_obscurity) as core to their security and I have to agree. Would a FOSS implementation guaranteed this would have been spotted sooner? No, absolutely not.\n\nIf this implementation was open source the fix could be assessed to ensure it was complete. This very public vulnerability would have also driven passionate security researchers to look and test other parts of the code, after all exposing one bug statistically indicates the presence of many more.\n\nOne amusing anecdote from the Yubico blog post I've already linked to is this snippet:\n\n\u003e Yubico has developed the firmware from the ground up. These devices are loaded by Yubico and cannot be updated.\n\nIt seems they didn't roll their own crypto library, which is good but also belies the first part of this sentence. The latter part is concerning. If they can't be updated all YubiKey 4s out there can not and should never ever be used for key generation and there is nothing consumers that have already purchased the devices can do about their existing faulty devices without purchasing a new one or with any luck have Yubico replace them with a non-vulnerable version as they did with [CVE-2015-3298](https://developers.yubico.com/ykneo-openpgp/SecurityAdvisory%202015-04-14.html).\n\nWhile this doesn't directly effect me, I did distinctly notice a mention that this effects TPM chips used in many laptops, and the keys used by Windows BitLocker. If your laptop uses an affected chip, your full disk encryption could be broken by a determined individual over the course of a year or a well financed attacker in under a month.\n\nIf you have a GPG key, or the public portion of any RSA key that may be affected you can test it using a [convenient online analyzer](https://keychest.net/roca).\n\n***Update:*** Yubico is providing mitigation recommendations and optional [YubiKey replacements](https://www.yubico.com/keycheck). There are also reports rolling in that GitHub is taking the proactive step of disabling all keys that have been found to be weak according to the ROCA tests (Well done GitHub!).\n","created_at":1508171158,"fuzzy_word_count":500,"path":"/blog/2017/10/vulnerable-smart-cards/","published_at":1508171158,"reading_time":3,"tags":["news","security"],"title":"Vulnerable Smart Cards","type":"blog","updated_at":1508171158,"weight":0,"word_count":455},{"cid":"c278ce8ee16897656e77a177fee031280e676e46","content":"\n# A KRACK In the Defenses\n\nAn advisory from US CERT has been circulating for the last week about a protocol level flaw in WPA \u0026 WPA2. The advisory itself was:\n\n\u003e US-CERT has become aware of several key management vulnerabilities in the 4-way handshake of the Wi-Fi Protected Access II (WPA2) security protocol. The impact of exploiting these vulnerabilities includes decryption, packet replay, TCP connection hijacking, HTTP content injection, and others. Note that as protocol-level issues, most or all correct implementations of the standard will be affected. The CERT/CC and the reporting researcher KU Leuven, will be publicly disclosing these vulnerabilities on 16 October 2017.\n\nDetails of the vulnerability have been [released today](https://www.krackattacks.com/), and paint a pretty horrifying picture. Ultimately this is an issue with a mechanism for dealing with lost packets during the initial 4-way key negotiation and the client's behavior when they receive one of these packets after they're session is already established.\n\nAlmost all WiFi devices out there are vulnerable to this attack and patches should be applied as soon as they are available. Some very quick and important notes I'd like to make available for people:\n\n* This is an active against clients\n* WPA \u0026 WPA2 both personal and enterprise are effected\n* The WiFi association key (your network's passphrase) is safe, this attack breaks a per-client session key.\n* Assume all your traffic is being sent in plain text and can be manipulated by an attacker until you have patched.\n\nIf you have a VPN available to your client devices, having it active will protect your traffic from this attack.\n","created_at":1508156623,"fuzzy_word_count":300,"path":"/blog/2017/10/a-krack-in-the-defenses/","published_at":1508156623,"reading_time":2,"tags":["security"],"title":"A KRACK In the Defenses","type":"blog","updated_at":1508156623,"weight":0,"word_count":256},{"cid":"77fb7300bd1638174d0466d0c6778c30b6a7ea85","content":"\nThis is a very basic operation on a Linux system, but I have to interact with\nit so rarely since I run either [chronyd][1] or [ntpd][2] on all of machines.\nOccasionally, I find a device that needs a helping hand.\n\nOn a device with a known good time (or approximate enough):\n\n```sh\ndate +%s\n```\n\nThis will get you the current unix timestamp. On the target system needing\nupdating as root (replacing `\u003cunix timestamp\u003e` with the results from the last\ncommand):\n\n```\ndate --set=\"@\u003cunix timestamp\u003e\"\nhwclock --systohc\n```\n\nEnsure both clocks are updated:\n\n```\ndate\nhwclock --show\n```\n\n[1]: {{\u003c ref \"./chronyd.md\" \u003e}}\n[2]: {{\u003c ref \"./ntpd.md\" \u003e}}\n","created_at":1507912223,"fuzzy_word_count":100,"path":"/notes/time/","published_at":1507912223,"reading_time":1,"tags":["linux"],"title":"Time","type":"notes","updated_at":1540945455,"weight":0,"word_count":92},{"cid":"3e67e9bdcc7857051a1aec93741494cc7f975dee","content":"\n# The Case of an Empty Executable\n\nI recently came across a [short article](http://trillian.mit.edu/~jc/humor/ATT_Copyright_true.html) written about a decade ago. It was a curious thing already as it was hosted in a user's home directory off a web server with the standard `~\u003cusername\u003e` showing up in the URL. The important part that caught my eye was this:\n\n\u003e The \"true\" program does nothing; it merely exits with a zero exit status. This can be done with an empty file that's marked executable, and that's what it was in the earliest unix system libraries.\n\nBeing a curious sort, and presented with an old mystery, I had to try out this little tidbit of information:\n\n```console\n$ echo -n \u003e test\n$ chmod +x test\n$ ./test\n$ echo $?\n0\n```\n\nSure enough, an empty file can be successfully run. It obviously doesn't have a known binary header, so it won't be interpreted as a valid executable on it's own. Even scripts rely on the shebang header (\"#!\") to be considered valid by the kernel, so something else has to be executing this. We can confirm the kernel isn't recognizing this by abusing \"strace\" into calling an explicit execve on the file:\n\n```console\n$ strace ./test\nexecve(\"./test\", [\"./test\"], 0x7ffe74e0a720 /* 70 vars */) = -1 ENOEXEC (Exec format error)\nfstat(2, {st_mode=S_IFCHR|0620, st_rdev=makedev(136, 3), ...}) = 0\nwrite(2, \"strace: exec: Exec format error\\n\", 32strace: exec: Exec format error\n) = 32\ngetpid()                                = 24047\nexit_group(1)                           = ?\n+++ exited with 1 +++\n```\n\nThis is exactly what I was expecting and the [Kernel source](https://github.com/torvalds/linux/blob/c2315c187fa0d3ab363fdebe22718170b40473e3/fs/binfmt_script.c#L24) reflects exactly what I'd expect. I'll leave it up to you test the results on an empty but otherwise valid shell script. With the kernel cleared of any odd behavior I was left with only one suspect. A little shell by the name of bash.\n\nBash is a coy devil with several different mechanisms built-in to execute a program. Likely one of these culprits are being used behind the scenes when we run a program. A quick trip the [man page](https://www.gnu.org/software/bash/manual/bash.txt) and a late night cup of coffee narrowed down my search to the following functions:\n\n* command\n* eval\n* exec\n\nI decided it was time to talk to each of them one by one and see what was up. Putting them under the bright light I was surprised that they all were telling the same story:\n\n```console\n$ command ./test\n$ echo $?\n0\n$ eval ./test\n$ echo $?\n0\n$ exec ./test\n-bash: /home/playground/test: Success\n```\n\nHow sinister! Someone had gotten to them first, I have to go higher into their organization. This calls for... The Source. I quickly traverse into the builtin directory and identify the commonality \"parse_and_execute\". This is where it gets a little fuzzy as bash is a rather complicated code base and I didn't want to spend to much time on this in the middle of the night.\n\nAfter parsing the file, it does seem to treat it as a script (as expected). There are two possibilities here and I didn't trace down which was true. Either \"parse_and_execute\" is simply returning with a success or it is sending the contents to \"execute_command_internal\", which in turn defaults to a successfully return value.\n\nThe motive remains unclear, but no harm seems to be getting done so I'm going to call this one case closed. It'd be interesting to see how other shells behave with empty files.\n","created_at":1507864141,"fuzzy_word_count":600,"path":"/blog/2017/10/the-case-of-an-empty-executable/","published_at":1507864141,"reading_time":3,"tags":["linux"],"title":"The Case of an Empty Executable","type":"blog","updated_at":1507864141,"weight":0,"word_count":577},{"cid":"7c73901348cbe5a422f78d0e464cbef7bf988c8f","content":"\nAuditd collects any configured syscall execution with critical security\nmetadata associated with the event. This can help enrich other security tools\nsuch as [AIDE][1] to determine what user and process are responsible for the\nchange.\n\nFor reliable operation the rules should be carefully tuned to your system.\nTracking every write to disk will generate an unreasonable amount of events and\ndepending on the configuration of the kernel's audit subsystem, may trigger a\nkernel panic.\n\nAuditd will also identify the IP address a remote user is connecting in allow\ncross system tracing of events.\n\n## Recommended Events to Audit\n\n* All logins to the system\n* Writes \u0026 attribute changes on sensitive files that are largely static\n  (think /etc)\n* Writes \u0026 attribute changes to critical system binaries\n* Write to the kernel / initramfs\n* Any program executed with root privileges\n* Any program execution that has the suid bit set\n* Changes to SELinux policies\n* Time / Date changes (both to the system and sensitive files)\n* Truncation of log files\n* Access to the audit and audit reporting tools and logs\n* Creation, modification, and deletion of special files (think mknod)\n* Mount and unmount operations\n* User \u0026 Group creation and removal\n* Read/write access to private key material\n* Changes to the hostname\n* Any failure to access a file\n* Changes to power state (such as shutdown)\n* Administrative access to user home directories\n* Executions out of temporary directories (Only important if this is possible,\n  but never hurts)\n\n## Log Integrity\n\nFor these logs to be meaningful they need to be shipped off a system. Auditd\nitself can receive audit events over the network and comes with a utility for\npushing them over the network. Without Kerberos though these events are sent\nwithout authentication or encryption and could be tampered with or spoofed by a\ncrafty attacker.\n\nSome log aggregators are able to read (tail) files and consume the lines as\nevents (such as [RSyslog][2]) and do so over a SSL or otherwise encrypted\ntunnel.\n\nThe final option for getting logs off the system, is to specify a custom\ndispatcher using the `dispatcher` configuration option in\n`/etc/audit/auditd.conf`.  This needs to be an executable on the system that\nwill take audit records through STDIN. A custom dispatcher could POST\nindividual events to an API, log the events directly to syslog, or anything\nelse that can be coded. This will be run with root permissions, so the ability\nfor it to drop permissions is highly desirable.\n\n## Configuration\n\nI have a sample [/etc/audit/auditd.conf][3] and [/etc/libaudit.conf][4]\navailable as well as a matching set of [/etc/audit/audit.rules][5] that are\ntuned for my environments. I consider them a good starting point for other\npeople wanting detailed auditing logs.\n\n## Audit Dispatcher\n\nBy default auditd has two mechanisms for sending audit events to an\nadministrator for review. The first and common one is to log directly to disk.\nThe other is to pass the events to a dispatch program. This is a very powerful\nsecond option as the other program will receive a binary stream of all events\nfrom auditd and can do anything with them.\n\nAuditd comes with a dispatcher with the ability to send the messages directly\nto syslog, to a remote auditd instance, or to a unix socket. I normally use the\nlog file option rather than the dispatcher, and use the ability to read in\nfiles as a log source inside my syslog daemon of choice to get those messages\nto a central log server. Sending them directly to syslog is significantly more\nefficient (push vs poll).\n\nIf you use another dispatcher other than the one that comes with auditd, be\naware that it will be started up with root privileges.\n\nUnfortunately it seems like the dispatcher that comes with auditd 2.6.4 is\nbroken. With all plugins disabled (to eliminate them as a source of an issue),\nI was still receiving the following message in syslog:\n\n```\nDispatcher protocol mismatch, exiting\n```\n\nWhich of course, hasn't seemed to have been encountered by anyone else online.\nDigging through the source code, it seems `audispd` hasn't been updated for an\nupdate to the protocol built into auditd. I'll have to look into either fixing\nthe source or writing my own dispatcher...\n\nI wasn't able to find anything about fixes that may be related so I'm unsure\nwhen it was fixed. I updated to 2.7.1 and it seemed to resolve that issue.\n\nTo get the information going to syslog I made two relatively small changes to\nthe existing configurations. You really only need to set 'active' to yes in\n`/etc/audisp/plugins.d/syslog.conf`. I made minor tweaks to the configuration\nto better support my logging environment (I keep `local6` reserved for auditd\nrecords). These two files include my relevant changes:\n\n* [/etc/audisp/audispd.conf][6]\n* [/etc/audisp/plugins.d/syslog.conf][7]\n\nIf you reliably have your audit records going to syslog, you may want to\nconsider removing or reducing the amount of audit logs stored. For a system\nthat generates significant numbers of logs, the syslog route can be quite a bit\nmore efficient, and support significantly more messages (this will only work\nwith disk queues removed).\n\n## More References\n\n* https://access.redhat.com/documentation/en-US/Red_Hat_Enterprise_Linux/6/html/Security_Guide/chap-system_auditing.html\n* http://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-123.pdf\n* https://security.stackexchange.com/questions/4629/simple-example-auditd-configuration\n* https://linux-audit.com/tuning-auditd-high-performance-linux-auditing/\n\n[1]: {{\u003c ref \"./aide.md\" \u003e}}\n[2]: {{\u003c ref \"./rsyslog.md\" \u003e}}\n[3]: /note_files/auditd/auditd.conf\n[4]: /note_files/auditd/libaudit.conf\n[5]: /note_files/auditd/audit.rules\n[6]: /note_files/auditd/audispd.conf\n[7]: /note_files/auditd/audispd_syslog.conf\n","created_at":1507836801,"fuzzy_word_count":900,"path":"/notes/auditd/","published_at":1507836801,"reading_time":4,"tags":["linux","security"],"title":"Auditd","type":"notes","updated_at":1540945455,"weight":0,"word_count":833},{"cid":"a16ef489bf4cf224c49495144f13d3d17d7cdf3d","content":"\nAIDE (Advanced Intrusion Detection Environment) is a file and directory\nintegrity checker that compares the current hashes, permissions, and attributes\nof files directories against a known database built from the system.\n\nThis can be run periodically to detect manipulation of critical system files,\nthough a motivated attacker with an appropriate level of permissions could\nmodify this database or disable the check as long as the system is able to\nwrite to it.\n\nFor this to be effective this process needs to be added to the systems update\nprocess and the database should be backed up immediately after being updated.\nWith good backups in place this can help identify what changes were made by an\nattacker after a breach which can be invaluable in knowing both the impact and\nintent of a security incident.\n\n## Important Caveat\n\nThere does seems to be an issue that causes core dumps like so:\n\n```\nFloating point exception (core dumped)\n```\n\nI initially suspected UTF-8 characters in the path name or to many files. Both\nI was able to disprove. I can enable all checks on an effected directory except\nfor any of the hashes (I tested all the ones available to me individually).\n\nThe issue itself could be with the specific compiled version (community/aide\n0.16-1 on arch linux) I was testing with.\n\nAfter `strace`'ing down the files it was having issues with I simply excluded\nthe files that were triggering the issue from checksum validation. I couldn't\nfind anything unusual about the files themselves though... No extended\nattributes, normal permissions, readable... Maybe a bug in `libmhash`?\n\n## Secure Usage\n\nWhile this utility provides no preventative defense measures, it is extremely\nuseful in the detection of malicious behavior on the host. To perform this\ndetection, a cron job should periodically have AIDE analyze the files it is\nmonitoring and report the findings. A central logging system should be setup to\nconsume these reports.\n\nFor these reports to be effective, the reference database needs to be trusted\nand well maintained. To accomplish this I recommend hosting a read-only NFS\nvolume mounted at a known location to store the current database.\n\nThis database will likely need to be updated after every system update as\ncritical binaries and configuration files may have been updated. If an\nautomation system like [puppet][1] is in use, automatic modification of\nconfiguration files or installed packages may also trigger this.\n\n## Initial Setup\n\nThe defaults are reasonably sane so you can continue with the default\nconfiguration that ships with several distributions, but there are several\nthings it will miss. Arch Linux's default doesn't catch:\n\n* SELinux and extended attributes (though SELinux doesn't apply)\n* Permissions and security hearings under /dev\n* Any of the content or security attributes for most files under /etc\n* /lib64 (though if it's a symlink it's likely covered)\n* /srv (though it's not commonly used)\n\nI have one [that avoids][3] these issues. I recommend reviewing it,\nunderstanding it, and adjusting it for your environment before installing it to\n`/etc/aide/aide.conf`. Be sure to restrict access to the file appropriately:\n\n```sh\nchmod 0600 /etc/aide/aide.conf\nchown root:root /etc/aide/aide.conf\n```\n\nYou need to initialize the database as a baseline for the system, which can be\ndone with the following command:\n\n```sh\naide --init\n```\n\nDepending on the amount of packages installed on your system and the speed of\nyour disks this can take quite some time (on a fairly minimal production system\nfor me this took about two minutes). You then need to copy the created database\ninto the reference location (these locations are dependent on [my\n  configuration][3]):\n\n```sh\nmkdir -p /var/lib/aide/reference\ncp /var/lib/aide/aide-$(hostname -f).db.new.gz /var/lib/aide/reference/aide-$(hostname -f).db.gz\n```\n\nYou can confirm the system is working with:\n\n```sh\naide --check\n```\n\nIf any monitored file or directory's attributes changed they will be reported\nto both STDOUT and written to `/var/log/aide/aide.log`. This file is always\ncompletely rewritten after every run, if you want to keep historical reports,\n[logrotate][2] can be used to move these files aside.\n\nI do recommend reviewing the configuration file both to ensure all critical\nsystem directories are covered on your systems and to be aware of what is\nmonitored and to what extent. Pay special attention to the package manager\nfiles of your system as they may not be included in the default configuration.\n\nIf you make any changes it is wise to validate the configuration afterwards by\nrunning:\n\n```sh\naide --config-check\n```\n\nIt will produce no output and exit with a zero status if the configuration is\nvalid.\n\nI have a modified configuration file covering my general use case\n[available][3], a slightly tweaked [cron job][4] ([original here][5]), and\nslightly [modified script][6] to check AIDE integrity remotely using SSH\n(original in the `contrib` directory of the AIDE source). If you use any of\nthese you will likely need to adjust paths to match the configuration.\n\n[1]: {{\u003c ref \"./puppet.md\" \u003e}}\n[2]: {{\u003c ref \"./logrotate.md\" \u003e}}\n[3]: /note_files/aide/aide.conf\n[4]: /note_files/aide/aide.cron\n[5]: http://sources.gentoo.org/cgi-bin/viewvc.cgi/gentoo-x86/app-forensics/aide/files/aide.cron\n[6]: /note_files/aide/sshaide.sh\n","created_at":1507688385,"fuzzy_word_count":800,"path":"/notes/aide/","published_at":1507688385,"reading_time":4,"tags":["linux","security"],"title":"AIDE","type":"notes","updated_at":1540945455,"weight":0,"word_count":788},{"cid":"767cc96a3770171bce9325a80fadd8ade03fb7ae","content":"\n## NEO\n\n```\ndnf install ykpers -y\nykpersonalize -m82\n```\n\nUnplug and replug it back in and it should be usable as a smartcard.\n\n## NFC / HTTP Auth\n\n```\ndnf install ykpers -y\nykpersonalize -n https://api.stelfox.net/sessions/yknfc?t=\n```\n\nThis will hit the API with a URL like:\n\n```\nhttps://api.stelfox.net/session/yknfc?t=ccccccuddclhrkuvurcufviveulljleihvreukifegjh\n```\n\nThe API can then return a token that for accessing additional functionality.\n\n## Resetting\n\nThis will wipe all keys, user, and admin pins on the card.\n\nThis requires scdaemon and gpg-agent to be working and able to connect to the\nsmartcard. It needs to be plugged into the computer and requires GPG version\n2.0.22 or later. On yubikeys prior to the YubiKey4 check the version and\nconfirm it's version 1.0.6 or later using the following command:\n\n```\ngpg-connect-agent --hex \"scd apdu 00 f1 00 00\" /bye\n```\n\nYou'll get back a line that looks like:\n\n```\nD[0000]  01 00 06 90 00\n```\n\nIndicating version 1.0.6. To reset the applet create a file with the following\ncontents:\n\n```\n/hex\nscd serialno\nscd apdu 00 20 00 81 08 40 40 40 40 40 40 40 40\nscd apdu 00 20 00 81 08 40 40 40 40 40 40 40 40\nscd apdu 00 20 00 81 08 40 40 40 40 40 40 40 40\nscd apdu 00 20 00 81 08 40 40 40 40 40 40 40 40\nscd apdu 00 20 00 83 08 40 40 40 40 40 40 40 40\nscd apdu 00 20 00 83 08 40 40 40 40 40 40 40 40\nscd apdu 00 20 00 83 08 40 40 40 40 40 40 40 40\nscd apdu 00 20 00 83 08 40 40 40 40 40 40 40 40\nscd apdu 00 e6 00 00\nscd apdu 00 44 00 00\n/echo Card has been successfully reset.\n/bye\n```\n\nAnd cat it into `gpg-connect-agent` like so:\n\n```\ncat FILE | gpg-connect-agent\n```\n\n## Using as a RNG source\n\nThis may allow me to use the yubikey's hardware RNG to generate entropy on my\nhost:\n\nhttps://github.com/infincia/TokenTools\n\n## Vulnerability\n\nA vulnerability was published around YubiKey NEOs use as smartcards and\n[Yubico's][1] response is top notch. I recommend following the steps in\nchecking on your key to see if you're affected.\n\n[1]: https://developers.yubico.com/ykneo-openpgp/SecurityAdvisory%202015-04-14.html\n","created_at":1507595312,"fuzzy_word_count":400,"path":"/notes/yubikey/","published_at":1507595312,"reading_time":2,"tags":["linux","security"],"title":"Yubikey","type":"notes","updated_at":1508887487,"weight":0,"word_count":349},{"cid":"b6bc2952856c9d87f804dcd32bfff784bc05cf4b","content":"\nI followed the TAILS setup guide to get a secure offline environment running to\nperform this generation task. The steps I took are documented in the tails\ndocument.\n\n## Initial Key Creation\n\nFor simplicity I wanted to clear out the GnuPG configuration that starts out in\nplace. Makes things a lot nicer later on.\n\n```console\nrm -rf ~/.gnupg/*\n```\n\nI pulled in the .gnupg/gpg.conf from my dotfiles by hand.\n\nAnd begin the key generation process\n\n```console\ngpg --expert --gen-key\n```\n\nChoose '8' which is RSA (set your own capabilities). Disable all the\ncapabilities except for Certify and press 'q' to continue. Use a 4096 bit key.\nSet the expiration to 2 years. The certificate can be resigned and republished\nwith a later expiration date.\n\nSet the personal attributes appropriately, a passphrase and let the key\ngeneration happen. Once done, sub-keys need to be generated. We need to edit\nthe existing key to create more keys.\n\n```console\ngpg --expert --edit-key \u003cemail used in key\u003e\n```\n\nAdd any additional email addresses you might need to the key.\n\n```console\ngpg\u003e adduid\ngpg\u003e uid 1\ngpg\u003e primary\n```\n\nGenerate the sub keys:\n\n```console\ngpg\u003e addkey\n```\n\nThe above will require the master key's private key. Create another '8' type\nkey with only the 'Sign' capability and 2048 bits valid for 6 months Do this\nagain for the encryption and authentication keys. A higher bit usage may be\nuseful, some smartcards only support 2048, if higher bits are supported 3072 or\n4096 may be better.\n\nI cropped and scale an image of myself down to 120x144 (Quality at 50% still\naccurately reflected my likeness quite well and came in at 2.8Kb). The image\nhas to be under 4Kb but the smaller the better. I ended by ensuring the file\nwas stripped of metadata and minimized as much as I could with `jpegoptim -s`.\nIt has to be transferred to the secure TAILS environment and then added to the\ncertificate while in the editkey mode we're currently in at this point in the\ntutorial.\n\nTurns out the size can be up to 240x288...\n\n```console\ngpg\u003e addphoto\n```\n\nAnd follow the prompts. When everything is good save the changes made:\n\n```console\ngpg\u003e save\n```\n\nNow I need to setup a backup for all the contents in case they're lost or\ndisplaced. We need to backup the keyrings, a raw copy of the master private\nkey, a revocation certificate just in case, then the public key.  Backup and\nexport ~/.gnupg/secring.gpg and ~/.gnupg/pubring.gpg\n\n```console\nmkdir ~/gpg_backups\ncp ~/.gnupg/{sec,pub}ring.gpg ~/gpg_backups/\ngpg -a --export-secret-key sstelfox@bedroomprogrammers.net \u003e ~/gpg_backups/secret_key.gpg\ngpg -a --export sstelfox@bedroomprogrammers.net \u003e ~/gpg_backups/publickey.gpg\ngpg -a --gen-revoke sstelfox@bedroomprogrammers.net \u003e ~/gpg_backups/revocation_cert.gpg\n```\n\nFor the revocation key choose 'Key has been compromised' and an empty\ndescription, as that is the intended usage for this particular CRL.\n\nWe also want a `paperkey` backup file of the private key.\n\n```console\ngpg --export-secret-key sstelfox@bedroomprogrammers.net | paperkey \u003e ~/gpg_backups/paperkey.bak\n```\n\nIt is expected that the paperkey.bak file is printed out on a piece of\nacid-free paper and put someplace very very safe. In the worst possible\nscenario, this needs to be hand typed back into a text file. Once done\nrestoration can be done by pulling in your public key and the typed file like\nthe following:\n\n```console\npaperkey --pubring ~/gpg_backups/publickey.gpg \\\n  --secrets ~/gpg_backups/paperkey.bak --output ~/recovered_secret.gpg\n```\n\nWe need to then export just the subkeys for day to day usage. This is a\nnon-obvious practice.\n\nTake out the subkeys:\n\n```console\ngpg -a --export-secret-subkeys sstelfox@bedroomprogrammers.net \\\n  \u003e ~/gpg_backups/subkey_secrets.gpg\n```\n\nDelete the secret keys:\n\n```console\ngpg --delete-secret-keys sstelfox@bedroomprogrammers.net\n```\n\nYou'll need to double confirm the deletion. We now have all the public keys we\nwant and no secret keys... We need to now import back in just the subkeys.\n\n```console\ngpg --import ~/gpg_backups/subkey_secrets.gpg\n```\n\nYou should see just the subkeys in the the secret ring:\n\n```console\ngpg -K\n```\n\nWe can then export just the 'laptop' keys.\n\n```console\ngpg -a --export-secret-keys sstelfox@bedroomprogrammers.net \u003e ~/gpg_backup/laptop_keys_secret.gpg\ngpg -a --export sstelfox@bedroomprogrammers.net \u003e ~/gpg_backup/laptop_keys_public.gpg\n```\n\nThese two files need are what will be transferred to the laptop and other\nmachines that they are needed. To import the two files:\n\n```console\ngpg --import laptop_keys_public.gpg\ngpg --import laptop_keys_secret.gpg\n```\n\nLaptop keyrings aren't needed with the smartcard assuming the public key\nportions are published at the URL set on the smartcard.\n\n## Normal Smartcards\n\nThe secret keys are best suited to be stored on a smartcard (such as a YubiKey,\nthough that needs extra configuration). This assumes that pcscd and libccid are\ninstalled on the system being used. TAILS is already setup for it.\n\nThe recommended reader \u0026 tokens for this kind of use are OpenPGP Smartcard V2\n(with breakout) combined with a Gemalto USB Shell Token V2.\n\nBy default these keys have a user \u0026 admin key set respectively to 123456 and\n12345678. If the user pin is mistyped 3 times the card is blocked until the\nadmin pin is provided. If the admin pin is provided incorrectly three times the\ncard will be destroyed. We need to change these from the default like so:\n\n```\ngpg --card-edit\n\u003e admin\n\u003e passwd\n```\n\nChange the pin then the admin pin (option 1, then 3 respectively). While in we\nshould set the url metadata field to provide the location where your public key\ncan be downloaded. In my case `https://stelfox.net/publickey.gpg` this can be\nset using the following command:\n\n```\n\u003e url\n```\n\nAlso a few other bits of metadata:\n\n```\n\u003e name\n\u003e lang\n```\n\nOne thing that may be worth considering is the 'Signature PIN' value being set\nto 'not forced'. As far as I can tell this is only used to tell the gpg-agent\nwhether it's allowed to cache the PIN for performing signatures or not.\n\nIt will reduce the security of your card if a PIN isn't required for every\nsignature performed but it make it quite a bit easier to use as part of a\nnormal workflow. Hostiles won't be able to get the key material but they will\nbe able to sign data on your behalf that will be very difficult to prove didn't\ncome from you.\n\nThe recommendation is to always force the requirement of a PIN. I believe with\na five minute timeout the risk is acceptable and can make lots of fast changes\ninside a git repository managable. The pin will be required again regardless of\ntime if the card is removed and readded.\n\nNOTE: There seems to be a better option. Using the `yubikey-manager` package in\nFedora (present in at least Fedora 29) we can enforce the requirement of\ntouching the pad whenever one of the keys is used. Enter pin once for the\ntimeout windows then touch for every action. This has the benefit of being a\nmuch easier workflow while preventing an attacker from using the key without a\nphysical presence. This can be done with the following commands:\n\n```console\nykman openpgp touch sig on\nykman openpgp touch aut on\nykman openpgp touch enc on\n```\n\nThe above requires having a YubiKey 4 or later.\n\nExit out and open up the gpg --edit-key view again. We need to add the subkeys.\nFirst we need to switch to the private key view:\n\n```console\n\u003e toggle\n```\n\nFor the three keys (numbered 1-3) you want to transfer them using the following\ncommands:\n\n```console\n\u003e key 1\n\u003e keytocard\n\u003e key 1\n```\n\nThis selects the keys individually, copies them, then deselects them. It needs\nto be done for each of them. End this with a 'save' command and you should be\nleft with just stubs of the keys in the secret keyring.\n\nOn new machines that need to have the stubs added we can perform the following\nstatus:\n\n```console\ngpg --card-edit\n\u003e fetch\n\u003e quit\n```\n\nYou should be able to view the stubs and their presence on the card with:\n\n```console\ngpg -K\ngpg --card-status\n```\n\nShould be able to test that the card is working by encrypting a message and\nthen decrypting it with smartcard.\n\n```console\ncat \u003c\u003c EOF \u003e message.txt\nJust a secret test message...\nEOF\n\ngpg -esar sstelfox@bedroomprogrammers.net message.txt\n```\n\nIt should ask you for your pin before continuing. Decrypting can be done using\nthe following:\n\n```console\ngpg -d message.txt.asc\n```\n\nThat confirms that the signing key \u0026 encryption key are both working. The\nauthentication key is for using the GPG agent as an SSH agent. To test this one\nwe need to run the gpg-agent with it's SSH compatibility layer like so:\n\n```console\ngpg-agent --enable-ssh-support\nsource ~/.gpg-agent-info\n```\n\nTest to make sure a card is showing up:\n\n```console\nssh-add -l\n```\n\nTo get it in an appropriate format for authorized_keys file:\n\n```\nssh-add -L\n```\n\nFor use on other linux systems the scdaemon binary is required which may be in\na different package...\n\n* Additional notes: https://wiki.fsfe.org/TechDocs/CardHowtos/CardWithSubkeysUsingBackups\n\nWe need to check the GPG configuration (gpg.conf) against that link as we may\nneed to sign and encrypt with an additional alternate key (hidden-encrypt-to\nand default-recipient entries).\n\nOn gentoo I needed to change the default use flags for app-crypt/gnupg with the\nfollowing:\n\n```\n# /etc/portage/package.use/gnupg\n\napp-crypt/gnupg smartcard usb\n```\n\n## Final Tasks\n\nGet the key ID of the primary key from `gpg -k` and push it to the common\npublic key server:\n\n```\ngpg --send-keys 0xBEBEF280BCE92620\n```\n\nAlso export the file for uploading to my website:\n\n```\ngpg --armor --export 0xBEBEF280BCE92620 \u003e publickey.gpg\n```\n\nThe only time the master key should be required to come out to play:\n\n* You need your main key (e.g. to sign another PGP key)\n* You have to replace your card and want to reuse the subkeys\n* Revoking subkeys that have been compromised\n* Your card was lost or stolen and you need to revoke the subkeys\n\nTAILS should be booted backup, if a new key needs to get signed it needs to be\nimported from a file. Then signed with `gpg --sign-key \u003cKey ID\u003e`.\n\n## Key Signing Party\n\nGet a fingerprint summary file and print out copies for people present using\nthe following command:\n\n```\ngpg --fingerprint sstelfox@bedroomprogrammers.net \u003e key_for_partying.txt\n```\n\nPull keys by their IDs (0x12345678 for the example), compare the contents of\nthe key to what you expect, sign the key, push it to the keyserver, and email\nthe owner a copy of their signature.\n\n```\ngpg --recv-keys 0x12345678\ngpg --list-keys 0x12345678\ngpg --sign-key 0x12345678\n\ngpg --send-keys --keyserver keyserver.ubuntu.com 0x12345678\ngpg --send-keys --keyserver hkps.pool.sks-keyservers.net 0x12345678\n\ngpg --armor --export 0x12345678 --output 0x12345678.signed-by.your-id.asc\n```\n\nEmail the 0x12345678.signed-by.your-id.asc file to one of the emails listed in\nthe key. When someone sends you a signature just import it like so:\n\n```\ngpg --import your-id.signed-by.0x12345678.asc\n```\n\nYou can see your signatures with:\n\n```\ngpg --list-sigs \u003cyour-id\u003e\n```\n\nMore note URLs:\n\n* https://help.ubuntu.com/community/GnuPrivacyGuardHowto\n\n## Git Notes\n\nGit needs to be configured with which key to use for signatures. First you need\nto find your key ID, this can be done using the following command:\n\n```\ngpg --list-secret-keys\n```\n\nThe key ID will look something like: `8EE30EAB`. Configure the global signing\nkey using the following command, replacing the sample key ID with yours:\n\n```\ngit config --global user.signingkey 8EE30EAB\n```\n\nEnsure that they are doing this automatically:\n\n```\ngit config --global commit.gpgsign true\n```\n\nThis will ask you for your pin or password on your GPG key everytime you commit\nwhich can be remedied by using a GPG agent. This will reduce your security\ngenerally though.\n\n## Key Transitions\n\nFor key transitions where your key hasn't been compromised, a transition\nstatement needs to be published by both the old and new key. I have a [sample\ntransition statement][1] that can be filled in with your respective\ninformation. Variables in message are embedded with {var} commands are {{cmd}}.\n\nThe variables in the template are firstName, fullName, email, oldKeyId,\nnewKeyId, pubKeyUrl, and pubStmntUrl. The pubKeyUrl and pubStmntUrl variables\nshould be valid paths to files on an HTTPS protected webserver.\n\nOnce you've filled out the transition statement you need to ensure both keys\nare cross signed and the signatures are published.\n\n```\ngpg --local-user $oldKeyId --sign-key $newKeyId\ngpg --local-user $newKeyId --sign-key $oldKeyId\ngpg --send-keys $oldKeyId $newKeyId\n```\n\nThe file itself still needs to be signed by both keys which can be done with\nthe following command:\n\n```\ngpg --local-user $oldKeyId --local-user $newKeyId --clearsign $keyTransitionFile\n```\n\nIf both keys don't exist on the same machine you'll need to refer to the\nsection titled 'Multiple Clearsigned Signatures' for how to handle the\nsituation.\n\nI recommend you publish the key transition with a name that includes the date,\nlike `key-transition-2017-08-17.txt`.\n\n## Expiration / Renewal of Subkeys\n\nTODO: I need to cover certificate key transitions and signed key transition\nstatements.\n\n### Option 1: Generate a new signing / encryption key\n\nPros:\n\n* Most secure\n* Some level of forward secrecy (over large time scales)\n* Helps protect against unknown key compromises\n\nCons:\n\n* Only one keypair stored per smartcard (decrypting old files means restoring a\n  backup of the old key)\n* More complicated\n* Requires users to refresh their keys about you\n\nSteps:\n\n1. Generate new keys\n2. Generate CRLs for old keys\n3. Load keys on to smartcard\n4. Push new keys and CRLs to keyservers\n\n### Option 2: Extend Expiration\n\nPros:\n\n* Simple\n* Fast\n\n```\ngpg --edit-key 0x12345678\ngpg\u003e key 1\ngpg\u003e expire\n...\ngpg\u003e key 1\ngpg\u003e key 2\ngpg\u003e expire\n...\ngpg\u003e save\n\ngpg --send-keys 0x12345678\n```\n\n## Maintenance\n\nTo ensure we get updated keys and revocation announcements the keys in your\nkeyring should be periodically refreshed with the public key server. This can\nbe done all at once using the following command:\n\n```\ngpg2 --refresh-keys\n```\n\nLikewise after modifying your own key it should be pushed to the public key\nservers for consumption and availability of others.\n\n```\nPRIMARY_KEY_EMAIL=sstelfox@bedroomprogrammers.net\ngpg2 --send-key $(gpg2 -k $PRIMARY_EMAIL | grep pub | awk '{ print $2 }' | cut -d '/' -f 2)\n```\n\nThere is a social network disclosure that occurs when all keys are refreshed at\nonce. The description of this disclosure is:\n\n\u003e We assume there probably exists at least one subset of public keys in this\n\u003e keyring that identifies it, i.e. no other individual's keyring contain the\n\u003e same subset of public keys.\n\nA personal thought potentially making this attack easier, it is very likely\nthat one of the keys being refreshed is owner of the keyring itself.\n\nTrusted SSL communications (hkps) eliminates this threat for passive snoopers,\nthe analysis could still be done on the keyserver itself.\n\nTo avoid this individual keys would have to be refreshed independently over\ntime. The requests could still be tied to the keyring machine's IP address to\ncorrelate all the refreshed keys to build back up the contents of the keyring.\nAvoiding this would require use of individual Tor circuits to mask and\ndistribute key refreshes.\n\nA project has been built up to handle this issue. The following are relevant:\n\n* https://code.openhub.net/file?fid=BbMaEKchr9cDAOVs8ozX5mJ40g8\u0026cid=RfbvTf3fwdw\u0026fp=405976\u0026mp\u0026projSelected=true#L0\n* https://github.com/EtiennePerot/parcimonie.sh/blob/master/parcimonie.sh\n\n## Diagnostics\n\n### No Usuable Subkey\n\nOne one of my devices I was persistently getting an odd error message whenever\nI tried to sign something. The error message was:\n\n```\n$ gpg2 -esar sam@stelfox.net sample-file\ngpg: no default secret key: Unusable secret key\ngpg: tor-setup-script.sh: sign+encrypt failed: Unusable secret key\n```\n\nTurns out I hadn't updated my public keys on that device and `gpg2 --card-edit`\nfollowed by a `fetch` didn't actually update my certificate. I needed to\nrefresh the key from the key server to get my new signatures and public key and\nretrust it.\n\n```\ngpg2 --refresh-keys 0x30856D4EA0FFBA8F\ngpg2 --edit-key 0x30856D4EA0FFBA8F\ntrust\n```\n\n### Card Not Visible to User (May be seen by root)\n\nI installed `pcsc-tools` to get `pcsc_scan` but I don't believe that\ncontributed to the fix. Ultimately I needed to start up the `pcscd` service and\nreplug my yubikey.\n\n### Unable to connect to dirmngr (IPC connect call failed)\n\nWhile trying to refresh keys I was getting the following error:\n\n```\ngpg: connecting dirmngr at '/run/user/1000/gnupg/S.dirmngr' failed: IPC connect call failed\n```\n\nI rebooted the machine, and that didn't solve it. Ultimately I believe it was\npermissions / ownership issues on the `~/.gnupg/crls.d` directory but I can't\nbe entirely sure.\n\nI solved this issue by deleting everything in my `.gnupg` directory and\nchecking out the version from my dotfiles again.\n\n## Remote Usage of Smartcard\n\nBasically relies on the extra-socket option for gpg-agent. May be able to\naccomplish this with something like the following command (path may be\nincorrect):\n\n```\nssh -R /run/user/1000/gnupg/S.gpg-agent:/home/sstelfox/.gnupg/S.gpg-agent.remote  -o \"StreamLocalBindUnlink=yes\" remote-host\n```\n\nThis could also live in an `ssh/config` parameter like so:\n\n```\nHost remote\n  RemoteForward /run/user/1000/gnupg/S.gpg-agent:/home/sstelfox/.gnupg/S.gpg-agent.remote\n  StreamLocalBindUnlink yes\n```\n\nThe following command should work but will warn about the agent being in\nrestricted mode.\n\n```\ngpg-connect-agent /bye\n```\n\n## Multiple Clearsigned Signatures\n\nIf multiple keys on different machines need to perform a clearsign on a\ndocument the normal method doesn't support combining these signatures to be\nverified in one pass. This is valuable for allowing multiple independent people\nto attest to the validity of a document and make it easy for people to validate\nthe correctness of it.\n\n### Notes\n\nIf the private keys are on the same machine the `--local-user` flag can simply\nbe specified multiple times. This is likely the most applicable for\nkey-transition statements.\n\n```\ngpg2 --local-user pers1 --local-user pers2 --clearsign content\n```\n\nONLY PARTIALLY IDEAL: Detached signatures can be combined in the same way and\ndon't require a lot of the sed magic used here. A detached signature can be\ngenerated with:\n\n```\ngpg2 --armor --detach-sign content\n```\n\nIf this is done only the dearmor, split, combine, enarmor steps need to be\nperformed.\n\n### Process\n\nFirst we need content to sign:\n\n```\necho 'A very important statement about a very important topic.' \u003e content\n```\n\nEach signatory should receive or generate a copy of the content and verify it.\nThis transport is beyond the scope of this document but could easily be done\nwith a standard `gpg2 -esa -r pers1 -r pers2 content` and emailed then\ndecrypted.\n\nOnce each user has a copy and verified it. They need to perform a normal\nclearsign on the content and send the signed message back to an individual to\nperform the combinatory process.\n\n```\ngpg2 --clearsign content\n```\n\nThis assumes you now have the signed contents file from pers1 and pers2 in the\nfiles `content.pers1.asc` and `content.pers2.asc`. Verify both signatures are\nvalid:\n\n```\ngpg2 --verify content.pers1.asc\ngpg2 --verify content.pers2.asc\n```\n\nWe need to verify the content is still identical in both files (one of them\ncould have changed the content before signing to be tricky). This is kind of a\nformality since the signature won't check out for at least one of them if the\ncontent differs between the two in any way.\n\n```\nsed -n '1,/SIGNATURE/ p' content.pers1.asc | sha1sum\nsed -n '1,/SIGNATURE/ p' content.pers2.asc | sha1sum\n```\n\nWe then need to extract just the signature blocks from each message and turn\nthem into the raw gpg2 packets:\n\n```\nsed -n '/SIGNATURE/,$ p' content.pers1.asc | gpg2 --dearmor | gpgsplit --no-split \u003e pers1.sig\nsed -n '/SIGNATURE/,$ p' content.pers2.asc | gpg2 --dearmor | gpgsplit --no-split \u003e pers2.sig\n```\n\nWe then combine them into one enarmored signature (order doesn't matter):\n\n```\ncat pers1.sig pers2.sig | gpg2 --enarmor | sed -n '5,$ p' | grep -v -- ----- \u003e combo.sig\n```\n\nAnd append the signatures back onto the content:\n\n```\n(sed -n '1,/SIGNATURE/ p' content.pers1.asc; echo; cat combo.sig; \\\n  echo '-----END PGP SIGNATURE-----') \u003e content.combo.asc\n```\n\nVerify the signatures are still good:\n\n```\ngpg2 --verify content.combo.asc\n```\n\n[1]: /note_files/gnupg/key_transition_template.txt\n","created_at":1507592134,"fuzzy_word_count":3100,"path":"/notes/gpg-process-notes/","published_at":1507592134,"reading_time":15,"tags":["gpg","linux","security"],"title":"GPG Process Notes","type":"notes","updated_at":1720581397,"weight":0,"word_count":3078},{"cid":"87a83e3276085c713fa2a28c3fda60e2a2c6cafe","content":"\nThis is a weird file... But while installing gentoo on a disk I started getting\na weird error when printing the current partitions on the drive:\n\n```\nWarning: The driver descriptor says the physical block size is 2048 bytes, but Linux says it is 512 bytes.\n```\n\nThe only thing I've been able to find anywhere about this is \"a low-level\ndevice tool (like dd) wrote blocks at the wrong size directly onto the device\".\nWhich apparently can fixed with another dd command with the correct byte size.\n\n```\ndd if=/dev/zero of=/dev/sda bs=2048 count=32 oflag=sync\n```\n\nThat didn't actually seem to do the trick, but recreating the partition table\ndid... It also looks like it was due to me using `bs=1M` when copying the ISO\nover the flash drive.\n","created_at":1507590343,"fuzzy_word_count":200,"path":"/notes/disk-errors/","published_at":1507590343,"reading_time":1,"tags":["linux","tips"],"title":"Disk Errors","type":"notes","updated_at":1507604813,"weight":0,"word_count":123},{"cid":"aa725172705f6268bb0529729d721703a8b0c4dd","content":"\nGRE encapsulates all layer 2 traffic, but does so through an unencrypted\ntunnel. Sensitive traffic should exclusively go through a lower level encrypted\ntunnel like IPSec.\n\n## Firewall\n\nThe following iptables need to be enabled to allow the GRE traffic to and from\nthe system. This should be restricted to / from IP addresses as well.\n\n```\n-A INPUT  -p 47 -j ACCEPT\n-A OUTPUT -p 47 -j ACCEPT\n```\n\n## Manual Setup\n\nI've seen several setups on the internet that make some... Odd choices. When\nmaking one of these tunnels, the inner IP addresses should be an independent\nnetwork from any other on the router. Other setups recommend re-using an\nalready existing IP, or another IP on an existing network which will likely\nleak traffic between the tunnel and the local network.\n\nTunnel keys don't provide very much security, but it is a small layer of\nprotection that is trivial to add.\n\nThis setup is between two hosts. The first host has a private network of\n10.0.0.0/24, a public IP of 1.2.3.4 and will use an inner tunnel IP of\n172.16.10.1. The second host has a private network of 10.16.0.0/24, a public IP\nof 4.3.2.1, and an inner tunnel IP of 172.16.10.2.\n\nI have a suspicion that the local value isn't required but it does restrict\nwhere the tunnel can come from to be valid.\n\n```\n# Host 1\nip tunnel add gre0 mode gre key 0x12345678 csum remote 4.3.2.1 local 1.2.3.4\nip addr add dev gre0 172.16.10.1 peer 172.16.10.2/30\nip link set gre0 up\nip route add 10.16.0.0/24 dev gre0\n```\n\n```\n# Host 2\nip tunnel add gre0 mode gre key 0x12345678 csum remote 1.2.3.4 local 4.3.2.1\nip addr add dev gre0 172.16.10.2 peer 172.16.10.1/30\nip link set gre0 up\nip route add 10.0.0.0/24 dev gre0\n```\n\nTo remove the tunnels you can run this on both machines:\n\n```\nip link set gre0 down\nip tunnel del gre0\n```\n\n## CentOS / Fedora / RHEL Config\n\nA simple point to point tunnel can be established using the standard CentOS\nnetwork config pretty straight forward. I believe the `MY_OUTER_IPADDR` is\noptional (just like the 'local' field in the manual setup).\n\nFor a single host to host connection it seems like it's pretty straight\nforward. You'll need to pick IPs for the inside of the tunnel for both the\nlocal and remote side and place a file on both hosts with a file at\n`/etc/sysconfig/network-scripts/ifcfg-gre0` with contents like the following\n(replacing the addresses with appropriate ones).\n\n```\n# /etc/sysconfig/network-scripts/ifcfg-gre0\n\nDEVICE=gre0\nBOOTPROTO=none\nONBOOT=yes\nTYPE=GRE\n\nMY_INNER_IPADDR=10.98.0.1\n#MY_OUTER_IPADDR=192.168.76.3\n\nPEER_INNER_IPADDR=10.98.0.2\nPEER_OUTER_IPADDR=192.168.56.72\n```\n\n## Other Notes\n\nThere are at least two forms of GRE tunneling, point to point and point to\nmultipoint. I believe there is a multipoint to multipoint as well but haven't\nbeen able to find much about either multipoint anything other than a reference\nthat it appears to use a shared key for tunnel authentication.\n\nYou can specify any remote can connect as long as they have the correct tunnel\nkey using the following command:\n\n```\nip tunnel add gre0 mode gre key 0x12345678 csum remote any\n```\n","created_at":1507587263,"fuzzy_word_count":500,"path":"/notes/gre-tunnel/","published_at":1507587263,"reading_time":3,"tags":["linux","networking"],"title":"GRE Tunnel","type":"notes","updated_at":1507603957,"weight":0,"word_count":492},{"cid":"6290cb485e0fc92eb8693dd21a066de8e282c413","content":"\nAmanda, or the Advanced Maryland Automatic Network Disk Archiver is an open\nsource computer archiving tool that is able to back up data residing on\nmultiple computers on a network.\n\nI am not a huge fan of having xinetd or perl on my system and this is reliant\non both, however, there does not currently seem to be any reasonable open\nsource alternatives that support managing a tape library.\n\nMy notes on setting this up were incomplete and woefully outdated so I removed\nthem. I have included a couple of references to sources of specific information\nI either do or did find useful:\n\n## Cleaning a Tape Library\n\n\u003e Any LTO cleaning tape may be used in any LTO drive\n\n* http://arstechnica.com/civis/viewtopic.php?t=1206963\n* http://www.tandbergdata.com/knowledge-base/index.cfm/are-there-any-guidelines-for-cleaning-the-lto-tape-drive/\n\n## Encryption\n\n* http://linux.die.net/man/8/amcheck\n* https://wiki.zmanda.com/index.php/How_To:Set_up_data_encryption\n* https://serverfault.com/questions/68487/tape-encryption-management-best-practices\n","created_at":1507560623,"fuzzy_word_count":200,"path":"/notes/amanda/","published_at":1507560623,"reading_time":1,"tags":["backups","linux"],"title":"Amanda","type":"notes","updated_at":1507578075,"weight":0,"word_count":124},{"cid":"8059e50797584e76f0a802b7e082e2b75e4db709","content":"\n# Quick Thoughts on Hash Cash\n\nThis is an interesting proof of work concept. The first example I have found of this in the wild is to prevent abuse for [anonymous account registration on an IRC network](https://www.hackint.org/ihashcash).\n\nI reviewed it's source and found that it requests a seed, and payload from a backend PHP script. It assumes that a target collision will happen within 1,000,000 iterations.\n\nThis is broken up into 10 iterations. A pool of four WebWorkers are spawned. Each one iterates through their assigned iterations, appending the iteration value to the salt then calculating the SHA256 value for that iteration. When the iteration is found that matches the payload all workers are stopped and the found value is injected into a hidden form on the field.\n\nThey have it set pretty aggressively at several hours per registration attempt, that does work pretty well as a mechanism against certain types of abuse, though with a poisoned ad campaign you could have plenty of registration attempts from random host all over the internet.\n\n[Actual HashCash](https://en.wikipedia.org/wiki/Hashcash) uses an interesting and well thought out header. Each component needed to validate the work was done is included in the header and part of that is the data being submitted.\n","created_at":1473831419,"fuzzy_word_count":300,"path":"/blog/2016/09/quick-thoughts-on-hashcash/","published_at":1473831419,"reading_time":1,"tags":["cryptocurrency","thoughts"],"title":"Quick Thoughts on Hash Cash","type":"blog","updated_at":1473831419,"weight":0,"word_count":200},{"cid":"dfa68a2f600097289fe7a1fa545961421ae335ee","content":"\n# Better Practices With Sudo\n\nI work with a lot of different linux machines from embedded devices, to cloud servers and open stack hosts. For many of them I'm either the sole administrator or one of three or less with administrative access. Where there are multiple administrative users, we all are generally working as backups to each other. We use sudo whenever we need to execute a task with privileges on any of these machines with no direct root login permitted remotely.\n\nI must confess I have established two habits over time that are against best practices with regard to sudo; Using it to execute a root shell only, and not restricting which commands can be run with sudo.\n\nI'm sure many other administrators commit these sins as well. I've always gotten sudo to the 'good enough' point without ever learning how to configure it properly countless times, which mostly meant leaving the distribution's defaults.\n\nAt face value, executing a shell this way doesn't seem to pose a problem. We use auditd to record administrative changes, and the kernel can track our original login UID and record that in addition to our effective user ID. Permission to use sudo is still restricted to a subset of trusted administrators.\n\nUsing this default configuration is forming bad habits and after working through it it's not particularly hard to make a drastic improvement on the granularity of control.\n\nI'm going to work through the changes I've made slowly building up my final configuration.\n\n***These changes, if made incorrectly or with the wrong paths to binaries may effect your ability to get privileged access to the system. I strongly encourage you to maintain a root shell independent of the shell you are using to test just in case you need to revert a breaking change.***\n\n## Minimal Configuration\n\nRather than looking at what needs to be changed, or removed I prefer to start with a minimal effective configuration.\n\nMost distribution's default sudo configuration pass through environment variables related to locale and a few others. I have left these out since the way I see sudo executed most commonly (\"sudo su -\"), removes any environment variables passed through anyway. If you work on multi-lingual systems or otherwise your administrators make use of multiple system locales, you will want to re-introduce the locale values used.\n\nMy entire starting sudo configuration is the following:\n\n```sudoers\nDefaults env_reset\nDefaults !visiblepw\n\nroot      ALL=(ALL)     ALL\n%wheel    ALL=(root)    ALL\n```\n\nThis is very similar to most distribution's configurations if you ignore the environment variables and comments. The root user and members of the wheel group can all execute anything as sudo as long as the user can authenticate through PAM and the mechanism won't display their password.\n\nThere is also a small restriction in place that ensures members of the wheel group will only be executing commands as the root user. Executing as other user's directly should be a special case and added separately.\n\nUsually distributions also include additional sudo configuration by including all files in \"/etc/sudoers.d\". This configuration isn't going to be terribly long so we may as well KISS it and not allow the inclusion of other files.\n\n## No Need for su\n\nThe first habit I wanted to break was executing \"sudo su -\" instead of \"sudo -s\". Generally when sudo is configured correctly, administrators are supposed to minimize the number of times dropping to a root shell. There are always going to be times when a root shell is necessary.\n\nThe differences between the two methods of executing a root shell are subtle. They are creating to different types of shells. Executing \"sudo su -\" creates a login shell, while \"sudo -s\" doesn't. Both can be subtly changed to provide the other type (Adding the \"-i\" flag to sudo, or removing the \"-\" from su).\n\nA login shell resets your environment, spawns the new user's default shell (in this case root's default shell) and executes the user's profile scripts in addition to the shell's rc files.\n\nBy not using a login shell, administrators can keep their preferred shells while allowing selective bits of their configuration (whitelisted environment variables) through to the new session.\n\nBy removing \"su\" from the process, administrators can enforce permitted root shells just like whitelisting or blacklisting any other binary on the system. The only way to enforce this transition is to blacklist \"su\" directly.\n\nA blacklist is added by creating a command alias that includes the commands to be blacklisted, then adjusting ACLs to make use of them. These need to be defined before they're used. Generally this means all command aliases are at the top of the configuration file. The following command alias will be used for our blacklist. The path to `su` is valid for CentOS 7, other distributions do vary.\n\n```sudoers\nCmnd_Alias BLACKLIST = /bin/su\n```\n\nTo enforce the blacklist the wheel group ACL needs to be adjusted to the following:\n\n```sudoers\n%wheel  ALL=(root)  ALL,!BLACKLIST\n```\n\nNow when you try to execute \"sudo su -\" you'll instead get this warning after authenticating:\n\n```console\nSorry, user \u003cusername\u003e is not allowed to execute '/bin/su -' as root on \u003chostname\u003e.\n```\n\nThis warning will enforce not using the less ideal mechanism.\n\n## Brief Interlude on Blacklists\n\nI'm going to be adding several more things to different forms of blacklists inside sudo. Some of these may be unacceptably inconvenient for some environments. If you find the explained reason insufficient to justify the inconvenience and are willing to accept the risk, remove the offender from the blacklist.\n\nThere is also always a risk that programs allowed through the blacklist have the ability to execute blacklisted applications as root. The blacklist applies only to direct execution through sudo.\n\nPreventing 'commonly used' escalation vectors does make it that much harder on potential attackers and may allow you see an attack in progress through the logs. This should not be considered perfect though. A good example of these vectors is the utility \"awk\". If allowed to be executed through sudo an unrestricted root shell can be acquired with the following command:\n\n```console\n$ sudo awk 'BEGIN {system(\"/bin/sh\")}'\n```\n\n## Editing Files as Root\n\nCommonly when I wanted to edit a particular sensitive configuration file, I would drop to a root shell, then open the file in my preferred editor, possibly saving along the way until I was done. Less commonly I would open my editor directly using \"sudo\" skipping the shell entirely.\n\nThe partially complete saves as part of that workflow, have caused issues though they're temporary. Sudo provides a utility, \"sudoedit\", that covers this use case. It make a copy of the file to be edited into a temporary directory, and allows you to edit and save as you like. When you're done save the file and it will replace the real file with the temporary one you've been editing.\n\nEditing the sudoers file itself should be done using the \"visudo\" command. And can be invoked by:\n\n```console\n$ sudo visudo\n```\n\nIt's a good idea to restrict the list of editors that can be used by visudo (this doesn't affect sudoedit at all) by adding the following line (replace this with your preferred, colon separated list of editors):\n\n```sudoers\nDefaults editor = /bin/vim:/bin/nano\n```\n\n## User Writable Directories\n\nSince the blacklist functionality is based on full paths to binaries, there is a quick way for a user with sudo permissions to bypass the blacklist for a specific program, copy it somewhere else.\n\nWhen an attacker gets into a system and downloads a binary off their site they want to run with privileges. They'll have to put it somewhere they have permission to write to.\n\nThis is less of a threat if you always require authentication to use sudo, trust all your administrators, and are confident their credentials will never be stolen.\n\nA salve to both problems is simply to prevent sudo from executing files in user writable directories, and ensuring it has a sane path to lookup trusted binaries. The following three lines need to be added to the sudoers file:\n\n```sudoers\nCmnd_Alias USER_WRITEABLE = /home/*, /tmp/*, /var/tmp/*\nDefaults ignore_dot\nDefaults secure_path = /sbin:/bin:/usr/sbin:/usr/bin\n```\n\nWe also need to modify our wheel ACL to prevent the execution in the aliases locations. Replace your previous line with the following one:\n\n```sudoers\n%wheel  ALL=(root)  ALL,!BLACKLIST,!USER_WRITEABLE\n```\n\n## Preventing Breakouts\n\nI've already shown that there is a way to abuse individual commands to expose a root shell. There are a few additional common applications that can regularly shell out, advanced text editors, pagers, several unix utilities and any interactive programming shell are easy candidates.\n\nThese utilities likely still need to be available for general administrative purposes, but we don't want them in turn executing other programs. Sudo has a trick up it's sleeve for this, the noexec flag.\n\nThere are two ways to effectively apply this, a whitelist and a blacklist. I encourage you to try the whitelist approach first as it does offer substantially better protection against this potential abuse.\n\nBefore applying this it is useful to know how this works and what it's limitations are. Sudo disables the exec call using \"LD_PRELOAD\" and defining an alternate version of the system call. This is largely effective, but will only work with dynamically linked programs (most coming from a distribution are going to be dynamically linked).\n\n### Whitelisting Programs w/ Exec Privileges\n\nThis is very strict but also very effective. We need to ensure that things we expect and want to be able to execute other programs (like shells) still can. Additionally visudo in turn executes your editor, so it to needs to be able to spawn programs.\n\nBe very sure of the paths in the following change. If you have no shells, or editors that can be executed as root through sudo you may lock yourself out of your system privileges.\n\n```sudoers\nCmnd_Alias SHELLS = /bin/sh, /bin/bash\nCmnd_Alias ALLOWED_EXEC = /usr/sbin/visudo\nDefaults noexec\nDefaults!ALLOWED_EXEC,SHELLS !noexec\n```\n\nAs you use this in your environment you will probably find programs that behave incorrectly and will need to be added to the whitelist. This whitelist (assuming your paths are correct) will at least be enough to allow future modifications of the sudoers file.\n\n### Blacklisting Programs w/ Exec Privileges\n\nThis is a much milder version of the exec restrictions, and won't catch unknown abuses. This will also have the least impact on normal operations to apply and is better than nothing.\n\n```sudoers\nCmnd_Alias EDITORS     = /bin/vim\nCmnd_Alias PAGERS      = /bin/less, /bin/more\nCmnd_Alias BREAKOUT    = /bin/awk, /bin/find\nCmnd_Alias DEVEL_SHELL = /bin/perl, /bin/python, /bin/ruby\n\nDefaults!EDITORS,PAGERS,BREAKOUT,DEVEL_SHELL noexec\n```\n\n## TTYs\n\nEnforcing the use of TTYs generally prevents non-interactive processes from executing anything as sudo either remotely or locally. Examples of this might be from cron, apache, or from a remote Jenkins server. In almost all cases prevention of this type of execution is the ideal behavior.\n\nThere are a [couple](https://unix.stackexchange.com/questions/65774/is-it-okay-to-disable-requiretty) of very [visible](https://bugzilla.redhat.com/show_bug.cgi?id=1020147) search results on this topic that indicate there isn't any security benefit to this, but their are [exceptions](https://superuser.com/questions/180764/sudoers-files-requiretty-flag-security-implications) as well. The argument that seems to have the most merit, is that no special privileges are required to create a PTY. This in turn means an attacking process could spawn the PTY required, and continue it's attack.\n\nThe same argument could be used in favor of the option. An attacker would have learn they need to make this adjustment and actively work around it. As the administrator you know the option is set and should be able to work around it more easily than the attacker.\n\nThe most common form of pain seems to be remotely executing privileged commands through ssh. By providing the SSH command being executed the '-t' flag twice, the client will force a PTY allocation even when there is no local tty. Other more stubborn use cases can be individually exempted.\n\nWhen the user already has a local TTY, the sudoers man page calls out to an additional potential attack vector around TTYs under the 'use_pty' option:\n\n\u003e A malicious program run under sudo could conceivably fork a background process that retains to the user's terminal device after the main program has finished executing. Use of this option will make that impossible.\n\nI haven't been able to find any attacks that exploit this possibility, but I have yet to be impacted by turning that feature on within sudo. Making both changes can be done by adding the following line to the sudoers configuration.\n\n```sudoers\nDefaults requiretty, use_pty\n```\n\n## Notification of Violation\n\nReceiving immediate notification when privilege gain has been attempted can be invaluable to stopping an attacker before they can do any damage. If the linux system has a properly configured MTA forwarding root's email to relevant parties it is recommended to have failure mailed to them directly to take action.\n\n```sudoers\nDefaults mail_badpass, mail_no_perms\nDefaults mailfrom = root\nDefaults mailsub = \"Sudo Policy Violation on %H by %u\"\n```\n\nThe overridden subject provides everything but the command itself (which isn't available through the expanded variables) needed to quickly judge a threat at a glance.\n\n## Auditing Interactive Shells\n\nWith all the protections put in place so far, we still have no visibility or restrictions on what administrators do with the root shells when they use them. These should hopefully be relatively few and far between.\n\nBuilt into sudo is an option to *record* execution of commands. This has proven to be valuable to narrow down things that have gone wrong, or see how something was done before. This may not prove useful as much for an audit tool as a user with root privileges can purge the recordings and logs.\n\nIf auditing is the goal, use of the kernel audit subsystem may be a better choice, but will only give you the command and arguments executed. This shows what was displayed to the privileged shell directly. There will be a future article covering the use of the audit subsystem and centralizing the information in a future post.\n\nIf you didn't go the whitelist exec route, to enable this you will need to pull in the 'SHELLS' command alias from there to make use of this.\n\n```sudoers\nDefaults!SHELLS log_output\n```\n\nOnce this is in place you can get a list of recorded sessions using the command:\n\n```console\n$ sudo sudoreplay -l\nFeb 26 17:56:18 2016 : jdoe : TTY=/dev/pts/7 ; CWD=/home/jdoe ; USER=root ; TSID=000001 ; COMMAND=/bin/bash\n```\n\nTo view an individual session provide \"sudoreplay\" with the TSID value of the session like so:\n\n```console\n$ sudo sudoreplay 000001\n```\n\nRefer to the man page of \"sudoreplay\" for additional tricks such as speeding up playback.\n\n## Final Configuration\n\nSome of the options from above I have combined into a single configuration line. This uses the stricter whitelist policy for exec privileges.\n\n```sudoers\n# /etc/sudoers\n\nCmnd_Alias ALLOWED_EXEC = /usr/sbin/visudo\nCmnd_Alias BLACKLIST = /usr/bin/su\nCmnd_Alias SHELLS = /usr/bin/sh, /usr/bin/bash\nCmnd_Alias USER_WRITEABLE = /home/*, /tmp/*, /var/tmp/*\n\nDefaults env_reset, mail_badpass, mail_no_perms, noexec, requiretty, use_pty\nDefaults !visiblepw\n\nDefaults editor = /usr/bin/vim\nDefaults mailfrom = root\nDefaults mailsub = \"Sudo Policy Violation on %H by %u\"\nDefaults secure_path = /sbin:/bin:/usr/sbin:/usr/bin\n\nDefaults!ALLOWED_EXEC,SHELLS !noexec\nDefaults!SHELLS log_output\n\nroot    ALL=(ALL)   ALL\n%wheel  ALL=(root)  ALL,!BLACKLIST,!USER_WRITEABLE\n```\n","created_at":1456526722,"fuzzy_word_count":2500,"path":"/blog/2016/02/better-practices-with-sudo/","published_at":1456526722,"reading_time":12,"tags":["linux","security","best-practices","sudo"],"title":"Better Practices With Sudo","type":"blog","updated_at":1456526722,"weight":0,"word_count":2459},{"cid":"62dc1d1818c19111e7d2a3e441db0bf75ddbfced","content":"\nI use Rakefiles quite a bit like traditional Makefiles, in that I specify\nimmediate dependencies for an individual task and Rake will execute all of\nthem. If a file or directory is the dependency and it exists, the task that\ncreates it will be skipped. A contrived Rakefile example might look like:\n\n```ruby\nfile 'sample' do |t|\n  puts 'Creating sample directory'\n  Dir.mkdir(t.name)\nend\n\nfile 'sample/population.txt' =\u003e ['sample'] do |t|\n  puts 'Creating sample population file...'\n  # Perhaps download a dataset? Lets just create the file\n  File.write(t.name, \"---\u003e Very important data \u003c---\\n\")\nend\n\ntask :process_population =\u003e ['sample/population.txt'] do\n  puts 'Check out our data!'\n  # Do some processing... whatever you need to...\n  puts File.read('sample/population.txt')\nend\n```\n\nThe first time you run it you'll the following output:\n\n```\n$ rake process_population\nCreating sample directory\nCreating sample population file...\nCheck out our data!\n---\u003e Very important data \u003c---\n```\n\nAnd subsequent runs will skip the creation since they're already present:\n\n```\n$ rake process_population\nCheck out our data!\n---\u003e Very important data \u003c---\n```\n\nThis is fine for statically implementing file contents, but what if you need\nadditional information to generate the file? With a normal rake task you can\nprovide bracketed arguments to access additional information like so:\n\n```\ntask :args_example, :word do |t, args|\n  puts \"The word is: #{args.word}\"\nend\n```\n\nYou'd use it like so:\n\n```\n$ rake args_example[data]\nThe word is: data\n```\n\nThat information isn't made available to the dependent tasks though so we need\nto broaden our scope a little bit. There is another way to provide arguments to\nRake using key value pairs. This has a bonus that was kind of an obvious\nsolution once I found it. Rake provides the values of key/value pairs to a task\nvia environment variables. Another contrived example of how to use this\n(specifically with a file dependency example):\n\n```ruby\nfile 'passed_state' do |t|\n  puts 'Creating state file'\n  File.write(t.name, ENV['state'])\nend\n\ntask :read_state =\u003e ['passed_state'] do\n  puts File.read('passed_state')\nend\n```\n\n```sh\n$ rake read_state state=something\nCreating state file\nsomething\n```\n\nState has been transferred! There is a gotcha, that is handling expiration of\ndata yourself. Passing in state again with a different value you'll see the\nproblem:\n\n```sh\n$ rake read_state state=notsomething\nsomething\n```\n\nIt won't recreate that file again until it's removed which you'll need to\nhandle on your own.\n","created_at":1455828372,"fuzzy_word_count":400,"path":"/blog/2016/02/sharing-context-between-dependent-rake-tasks/","published_at":1455828372,"reading_time":2,"tags":["development","ruby"],"title":"Sharing Context Between Dependent Rake Tasks","type":"blog","updated_at":1455828372,"weight":0,"word_count":398},{"cid":"c7b88c73981737002e617b3d1ea9d3e05a1f04e8","content":"\nI like getting unopinionated feedback on the quality of the code I write.\nSometimes I can get this from other developers but they tend to get annoyed\nbeing asked after every commit whether they consider it an improvement.\n\nThere are a few utilities for Ruby codebases such as [flay][1], [flog][2], and\n[rubocop][3] as well as hosted services such as [Code Climate][4] that can help\nyou identify chunks of code that can use some work.\n\nWhile not directly connected to the quality of the code, I also make use of\n[yard][5] and [simplecov][6] to assess documentation and test coverage of the\ncodebases I work on.\n\nUsing the tools means very little without some reference or understanding\ndoesn't get you very far. For a while I've been using flog and only comparing\nthe numbers against other codebases I control. I finally googled around and\nfound a [blog post][8] by a developer named Jake Scruggs from a while ago\n(2008).\n\nThe blog post includes a rough table for assessing scores on individual methods\nreported from the flog utility. From what I can tell the ranges are still\npretty accurate. I've tweaked the descriptions a bit to fit my mental\nunderstanding a bit but the table is here:\n\n| Method Score |  Description |\n| ------------ | ------------ |\n| 0   - 10     | Awesome      |\n| 10  - 20     | Decent       |\n| 20  - 40     | Might need refactoring |\n| 40  - 60     | Should probably review |\n| 60  - 100    | Danger       |\n| 100 - 200    | Raise the alarm |\n| 200+         | Seriously what are you doing!? |\n\nI wanted to extend this with a second table providing a scale for the overall\nmethod average with a more aggressive scale (an individual couple of methods\ncan be justifiably complex but the overall code base shouldn't be riddled with\nthem) but had a hard time working it out.\n\nI've seen some awesome code bases with a score of 6.4 on average, some bad\nlarger ones with 7.8. Even some mediocre ones around a score of 10.6.\n\nI guess I'll have to think more on it...\n\n[1]: https://github.com/seattlerb/flay\n[2]: https://github.com/seattlerb/flog\n[3]: https://github.com/bbatsov/rubocop\n[4]: https://codeclimate.com/\n[5]: http://yardoc.org/\n[6]: https://github.com/colszowka/simplecov\n[7]: http://blog.codeclimate.com/blog/2013/08/07/deciphering-ruby-code-metrics/\n[8]: http://jakescruggs.blogspot.com/2008/08/whats-good-flog-score.html\n","created_at":1429735630,"fuzzy_word_count":400,"path":"/blog/2015/04/ruby-code-quality-metrics/","published_at":1429735630,"reading_time":2,"tags":["development","ruby"],"title":"Ruby Code Quality Metrics","type":"blog","updated_at":1429735630,"weight":0,"word_count":329},{"cid":"aa312a0527e74df16ca46b13834dd63c41739260","content":"\nEvery now and then I find myself wanting to create a new empty branch in an\nexisting repository. It's useful for things such as [GitHub Pages][1] so you're\nable to keep your content source in the master branch while only keeping the\noutput in the `gh-pages` branch. I've also used it for testing a complete\nrewrite of a code base without the overhead of creating a new repo and copying\naccess permissions.\n\nThis is a pretty straight forward trick to do. You create the branch by\nindicating you want the new branch to be an orphan by passing the '--orphan'\nflag like so:\n\n```sh\ngit checkout --orphan NEW_BRANCH_NAME\n```\n\nThis leaves all the files in place but effectively uncommitted like you just\ninitialized a new repository. Add and commit any files you'd like to keep then\ndelete the rest, everything will still be preserved in the original branches.\n\nWith that done you should be able to easily switch just using a normal\n'checkout' between your normal branches and this new tree.\n\n[1]: https://pages.github.com/\n","created_at":1428972460,"fuzzy_word_count":200,"path":"/blog/2015/04/creating-an-empty-git-branch/","published_at":1428972460,"reading_time":1,"tags":["tips"],"title":"Creating an Empty Git Branch","type":"blog","updated_at":1428972460,"weight":0,"word_count":170},{"cid":"cd598d965cf6e804f7e1a44b8318eccf88e0cb72","content":"\nI need to filter a live log stream for only relevant events and quickly hit an\nissue that I wasn't expecting. The `grep` in my pipe chain was waiting until it\nreceived all the output from the prior command before it began to attempt to\nfilter it.\n\nReading through the grep man page I came across the `--line-buffered` flag\nwhich provides exactly what I needed. I wasn't using the `tail` command but it\nserves really well in this situation to demonstrate the use:\n\n```\ntail -f /var/log/maillog | grep --line-buffered -i error\n```\n\nHope this saves someone a headache in the future!\n","created_at":1424713753,"fuzzy_word_count":100,"path":"/blog/2015/02/unbuffered-pipe-filters/","published_at":1424713753,"reading_time":1,"tags":["linux","tips"],"title":"Unbuffered Pipe Filters","type":"blog","updated_at":1424713753,"weight":0,"word_count":99},{"cid":"428110d6d43ffd07367d97476b2ecda63b2b7bf5","content":"\nWhile running an aide check on one of my servers after updating it, I started\nseeing a large number of very concerning warning messages:\n\n```\n/usr/sbin/prelink: /bin/mailx: at least one of file's dependencies has changed since prelinking\nError on exit of prelink child process\n/usr/sbin/prelink: /bin/rpm: at least one of file's dependencies has changed since prelinking\nError on exit of prelink child process\n/usr/sbin/prelink: /sbin/readahead: at least one of file's dependencies has changed since prelinking\nError on exit of prelink child process\n/usr/sbin/prelink: /lib64/libkrb5.so.3.3: at least one of file's dependencies has changed since prelinking\nError on exit of prelink child process\n/usr/sbin/prelink: /lib64/libgssapi_krb5.so.2.2: at least one of file's dependencies has changed since prelinking\n```\n\nThe list went on with maybe a total of forty packages and libraries. My initial\nreaction was 'Did I get hacked?'. Before running the updates I ran an aide\nverification check which returned no issues and the files that were now\ndisplaying the issue were in the packages that got updated.\n\nWhat was the next worse scenario? The packages had been tampered with and I\njust installed malicious files. This didn't seem likely as the packages are all\nsigned with GPG and an aide check would have caught tampering with my trust\ndatabase, the `gpg` binary, or the aide binary. Still a key could have been\ncompromised.\n\nAfter some Googling I came across people with similar issues, (including one\nannoyingly paywalled Red Hat article on the issue). Several people simply ended\nthe conversation on the assumption the user with the issue had been hacked.\nFinally I [came across one helpful individual][1] with the fix. The binaries\njust need to have their prelink cache updated again. This can be accomplished\nwith the following command on CentOS 6.5 (probably the same on others).\n\n```\n/usr/sbin/prelink -av -mR\n```\n\n*Update:* Ultimately I decided to follow [my own advice][2] (search for\nprelink) and just simply disabled prelinking too prevent it from interferring\nwith aide checks and causing other weird issues. The memory trade-off isn't\nvaluable enough for me.\n\n[1]: http://lists.centos.org/pipermail/centos/2007-December/049222.html\n[2]: {{\u003c ref \"../notes/linux_hardening.md\" \u003e}}\n","created_at":1407874574,"fuzzy_word_count":400,"path":"/blog/2014/08/dependency-prelink-issues/","published_at":1407874574,"reading_time":2,"tags":["linux"],"title":"Dependency Prelink Issues","type":"blog","updated_at":1407874574,"weight":0,"word_count":332},{"cid":"38cd5878fc5469d7d56f21c2a4615c5f13cb1cd4","content":"\nI needed too turn some hexadecimal values into decimal in a bash script and found a real easy way too do it. The following is a very short bash script demonstrating how too turn the hexadecimal string \"deadbeefcafe\" into it's equivalent decimal value of \"244837814094590\".\n\n```bash\n#!/bin/bash\n\nINPUT=\"deadbeefcafe\"\nOUTPUT=$((0x${INPUT}))\n\necho $OUTPUT\n```\n","created_at":1406937024,"fuzzy_word_count":100,"path":"/blog/2014/08/fast-hex-to-decimal-in-bash/","published_at":1406937024,"reading_time":1,"tags":["linux","tips"],"title":"Fast Hex to Decimal in Bash","type":"blog","updated_at":1406937024,"weight":0,"word_count":56},{"cid":"3f5a22100859064a9142bb874af74d0b09ece3eb","content":"\nI'm going to do a more detailed post on emailing from Amazon's infrastructure\nsoon, but in the meantime I wanted to quickly throw out solutions too a couple\nof problems I encountered. These are all specific too Amazon's Route 53, and\nmost are user error (myself).\n\n## SPF Invalid Characters or Format\n\nAfter generating my SPF record, I jumped into Route 53, created a new record\npasted in my record, attempted to save and received the following message:\n\n\u003e The record set could not be saved because:\n\u003e - The Value field contains invalid characters or is in an invalid format.\n\nI was using the SPF record type at a time (see the next section) and assumed\nthat I had messed up the format of my record in some way. I banged my head\nagainst the wall and through RFCs thoroughly before I found the solution...\n\nSolution: Wrap your SPF records in quotation characters.\n\n## No SPF Record Found / Validation Failure\n\nSince I have my DMARC policy in place (I'll cover this in my email follow up),\nI receive daily domain reports from Google whenever something fails validation\nabout my domain. After switching to Route 53 for DNS the `authresult` component\nstarted showing up as fail for SPF.\n\nTesting around a few online SPF validators indicated that none of them were\nable to see my new SPF record, and there had been plenty of time for it too\npropagate.\n\nThe SPF resource record type (RRTYPE 99) is available in Route 53 even though\n[the record type has been deprecated][1]. Not being familiar with this\nparticular decision, I assumed I should be using it *instead* of the TXT record\nI've used for every other domain, and it would be handled correctly or more\nintelligently.\n\nSolution: Either switch the SPF record too a TXT record or, my preference,\nduplicate it into a TXT record so you have both.\n\n## Invalid DKIM record\n\nThis one had me scratching my head for a while. This was my first time\ndeploying DKIM on a domain that I was not running a Bind name server for.\nOpenDKIM is nice enough too generate a Bind record for you which works\nperfectly. It's output looks like the following:\n\n```\ndefault._domainkey.example.tld.   IN      TXT     ( \"v=DKIM1; k=rsa; t=y; s=email; \"\n          \"p=MIHfMA0GCSqGSIb3DQEBAQUAA4HNADCByQKBwQC2Cwpa/+Xhfkzn0QnyQoxRwoJPb+s51dIt9UtFLMlMFuYa/k3GBwZ7UWeyAaQJ3RibSzKV/YwgFuMrzyISrLNSuL2k1bQlQQG8nl23Mu9Mowcb+mV2/3G7roshK6kOLNA0IV2SBl8/0UoNZR/x7c1lzVtVqdj0vW1SsJzgGfbt4LGRvCPyjdg+SLpYtOd/Li4Y1pvHgSRKQRrklpKeJo\"\n          \"nJQ4+lXWqzYtuX9xdNH46ck2HUl56Ob4cy3/gYCJBWrAsCAwEAAQ==\" )  ; ----- DKIM key default for example.tld\n```\n\nCopying and pasting everything between the parentheses in the value field and\npasting them into Route 53 works flawlessly. The catch? This won't be treated\nas a single record, but three individual responses. None of which are complete\nand valid DKIM records.\n\nThis happens because Route 53's value field treats newlines as separate\nrecords.\n\nSolution: Turn it into one long string so it isn't covering multiple lines\nright? Not quite...\n\n## TXTRDATATooLong\n\nCombining the DKIM key into one string like so:\n\n```\n\"v=DKIM1; k=rsa; t=y; s=email; p=MIHfMA0GCSqGSIb3DQEBAQUAA4HNADCByQKBwQC2Cwpa/+Xhfkzn0QnyQoxRwoJPb+s51dIt9UtFLMlMFuYa/k3GBwZ7UWeyAaQJ3RibSzKV/YwgFuMrzyISrLNSuL2k1bQlQQG8nl23Mu9Mowcb+mV2/3G7roshK6kOLNA0IV2SBl8/0UoNZR/x7c1lzVtVqdj0vW1SsJzgGfbt4LGRvCPyjdg+SLpYtOd/Li4Y1pvHgSRKQRrklpKeJonJQ4+lXWqzYtuX9xdNH46ck2HUl56Ob4cy3/gYCJBWrAsCAwEAAQ==\"\n```\n\nAnd attempting to save results in the following error message:\n\n\u003e Invalid Resource Record: FATAL problem: TXTRDATATooLong encountered at ...\u003csnip\u003e\n\nNow we're left in a tricky spot. After some research the reason behind this is\nclear, and makes sense. Though it is another poor usability bug in the way\nAmazon's Route 53 behaves. Individual DNS UDP packets are limited too 255\ncharacters for their response.\n\nToo properly deliver records longer than that DNS servers are supposed to break\nup the response into chunks. Properly implemented clients combine these chunks\ntogether (with no spaces, newlines or other characters added). What this means\nis that the record can be broken up transparently behind the scenes anywhere in\nthe message and the client will put it back together correctly.\n\nThe Route 53 entry form won't handle this for you though, and in hindsight it\nlooks like Bind might not do it for you though I suspected that was more for\nreadability of zone files rather than a technical limitation (and I haven't\ntested whether Bind is intelligent enough too handle just a long string).\n\nSolution: Take the original output of Bind between the parentheses and just\nremove the newline characters, leave the quotation marks and spaces between the\nsections like the following sample and you'll be golden:\n\n```\n\"v=DKIM1; k=rsa; t=y; s=email; \" \"p=MIHfMA0GCSqGSIb3DQEBAQUAA4HNADCByQKBwQC2Cwpa/+Xhfkzn0QnyQoxRwoJPb+s51dIt9UtFLMlMFuYa/k3GBwZ7UWeyAaQJ3RibSzKV/YwgFuMrzyISrLNSuL2k1bQlQQG8nl23Mu9Mowcb+mV2/3G7roshK6kOLNA0IV2SBl8/0UoNZR/x7c1lzVtVqdj0vW1SsJzgGfbt4LGRvCPyjdg+SLpYtOd/Li4Y1pvHgSRKQRrklpKeJo\" \"nJQ4+lXWqzYtuX9xdNH46ck2HUl56Ob4cy3/gYCJBWrAsCAwEAAQ==\"\n```\n\nHope this helps someone else!\n\n[1]: https://tools.ietf.org/html/rfc6686#section-3.1\n","created_at":1406731573,"fuzzy_word_count":700,"path":"/blog/2014/07/spf-and-dkim-records-in-route-53/","published_at":1406731573,"reading_time":4,"tags":["aws","security"],"title":"SPF \u0026 DKIM Records in Route 53","type":"blog","updated_at":1406731573,"weight":0,"word_count":687},{"cid":"9cea3543dc7b1d378d3fa02d4891b0306ea51eb6","content":"\nI've been playing around with my Nexus 5 lately. It was quickly rooted and I\nbegan playing with various ROMs that had been pre-built for the Nexus 5. My\nfirst stop was the CyanogenMod. Since I'd last used CyanogenMod they added a\nbuilt-in framework that provides [transparent text][2] message encryption\ncalled WhisperPush.\n\nWhisperPush is an implementation of [Moxie Marlinspike's][4] highly respected\nTextSecure and I was very excited at the possibility of using it. I immediately\nsigned up for the service.\n\nAfter a day of use I found CyanogenMod far too unstable too use on my primary\ndevice. It locked up multiple times the first day and mobile data simply\nwouldn't work all day. I promptly formatted and flashed my phone, I haven't\nsettled on a new ROM but that's not what this post is about.\n\nIt occurred to me after flashing the phone I was still subscribed to\nWhisperPush. If anyone that texts me was signed up as well. I'd never receive\neven an encrypted blob, it would just silently fail.\n\nSearching around I found there is very little information on it, and no\nofficial way to unregister, especially after you've wiped your device and no\nlonger have your credentials. Ultimately I found a fairly easy solution, just\nre-register and perform the backdoor steps too de-register.\n\nI wiped my phone again and installed a new copy of the CyanogenMod nightly.\nBooted it up and re-enabled WhisperPush. It didn't even note that my number was\nregistered in the past.\n\nI found the solution somewhere in the CyanogenMod forums (though I lost the\nlink, and I'm now too lazy to go find it again). You can unregister by\nperforming the following steps:\n\n1. Connect your computer with ADB too the phone and pair the computer with the\n   phone.\n2. Enable developer options by opening the system settings, choosing 'About\n   phone' and clicking on the 'Build number' about 7 times (it will start\n   counting down).\n3. Open up the developer options found in the root of the system settings menu\n   and enable root access for 'Apps and ADB'.\n4. On the computer use `adb shell` to get a shell on the device.\n5. Switch to root using the `su` command.\n6. Run the following command too view the WhisperPush internal settings:\n\n    ```\n    cat /data/user/0/org.whispersystems.whisperpush/shared_prefs/org.whispersystems.whisperpush_preferences.xml`\n    ```\n\n7. Note down the value for `pref_registered_number` (this should be your phone\n   number with a preceding '+') and `pre_push_password`.\n8. Exit the shell.\n\nFinally too unregister we need too make a DELETE request against the\nWhisperPush API. The classic HTTP swiss army knife `curl` is going to help us\non this front. Run the following command on any linux computer with curl\ninstalled, replacing the registered number and registered password with the\nvalue you recorded earlier.\n\n```bash\ncurl -v -k -X DELETE --basic --user ${pref_registered_number}:${pre_push_password} https://whisperpush.cyanogenmod.org/v1/accounts/gcm\n```\n\nBe sure too include the '+' in your pref_registered_number. You should end up\nwith a status code of 204. The output will look something like the following\n(credentials removed).\n\n```\n* About to connect() to whisperpush.cyanogenmod.org port 443 (#0)\n*   Trying 54.201.5.27...\n* Connected to whisperpush.cyanogenmod.org (54.201.5.27) port 443 (#0)\n* Initializing NSS with certpath: sql:/etc/pki/nssdb\n* skipping SSL peer certificate verification\n* SSL connection using TLS_DHE_RSA_WITH_AES_128_CBC_SHA\n* Server certificate:\n*   subject: OU=Operations,O=\"Cyanogen, Inc.\",E=ops@cyngn.com,C=US,ST=Washington,L=Seattle,CN=whisperpush.cyanogenmod.org\n*   start date: Nov 26 05:39:18 2013 GMT\n*   expire date: Nov 24 05:39:18 2023 GMT\n*   common name: whisperpush.cyanogenmod.org\n*   issuer: E=ops@cyngn.com,CN=Authority,OU=Operations,O=\"Cyanogen, Inc.\",L=Seattle,ST=Washington,C=US\n* Server auth using Basic with user '${pref_registered_number}'\n\u003e DELETE /v1/accounts/gcm HTTP/1.1\n\u003e Authorization: Basic ${encoded credentials}\n\u003e User-Agent: curl/7.29.0\n\u003e Host: whisperpush.cyanogenmod.org\n\u003e Accept: */*\n\u003e \n\u003c HTTP/1.1 204 No Content\n\u003c Server: nginx/1.1.19\n\u003c Date: Wed, 23 Jul 2014 01:45:25 GMT\n\u003c Connection: keep-alive\n\u003c \n* Connection #0 to host whisperpush.cyanogenmod.org left intact\n```\n\nI don't have any way too check that I'm unregistered but it seems too have\nworked. Here is hoping this helps some else out in the future.\n\n[2]: https://whispersystems.org/blog/cyanogen-integration/\n[4]: http://thoughtcrime.org/\n","created_at":1406080499,"fuzzy_word_count":700,"path":"/blog/2014/07/unregistering-from-whisperpush-after-flashing-a-new-rom/","published_at":1406080499,"reading_time":3,"tags":["android","security"],"title":"Unregistering From WhisperPush After Flashing a New ROM","type":"blog","updated_at":1406080499,"weight":0,"word_count":636},{"cid":"51e5b47a0238a044bc0e4cdcc9e7b63e90f87752","content":"\nI recently reflashed my primary router to a newer version of OpenWRT and\nattempted to follow [my own directions][1] written in an earlier blog post to\nadd PXE booting to my local network using the dnsmasq service built in. After\nfollowing my advice I found that the dnsmasq service wasn't starting.\n\nLooking into the `logread` output I finally saw that this was due too a\npermission issue. Combining this with the output of `ps` too identify the user\nthat dnsmasq was running on I was able to both modify my instructions and use\nOpenWRT's own configuration system to perform the configuration instead of\nmodifying the dnsmasq configuration.\n\nFirst was solving the permissions issue. I created a dedicated directory at\n`/var/tftp` and changed the ownership to 'nobody' and 'nogroup' and mode too\n'0755'.\n\nPreviously I used `/var/lib/tftp`, however, the default permissions on the\n`/var/lib` directory is too restrictive and I didn't want to reduce the rest of\nthat directories security posture simply too allow directory traversal.\n\nNext up was getting the TFTP portion of dnsmasq configured and running. Open up\n`/etc/config/dhcp` and under the `dnsmasq` section add the following lines (or\nif these lines already exist adjust the values to match).\n\n```\noption enable_tftp '1'\noption tftp_root '/var/tftp'\noption dhcp_boot 'pxelinux.0'\n```\n\nRun `uci commit dhcp` too commit the changes and finally `/etc/init.d/dnsmasq\nrestart` To apply the changes. You'll want too put the `pxelinux.0` and\nassociated configuration files into the `/var/tftp` directory too complete the\nPXE booting configuration.\n\nI'll probably write a blog post covering my PXE setup and configuration if I\ndon't get distracted by other projects.\n\n[1]: {{\u003c relref \"2013-12-12-using-dnsmasq-as-a-standalone-tftp-server.md\" \u003e}}\n","created_at":1404264405,"fuzzy_word_count":300,"path":"/blog/2014/07/using-openwrts-dnsmasq-as-a-tftp-server/","published_at":1404264405,"reading_time":2,"tags":["linux"],"title":"Using OpenWRT's Dnsmasq as a TFTP Server","type":"blog","updated_at":1404264405,"weight":0,"word_count":264},{"cid":"57b7ec3d64d033bb2f4a282b502de42d2f4a9a3c","content":"\nI randomly started experiencing an issue with one blade in one of my PowerEdge\nC6100 blades. It wouldn't obey all commands issued too it via IPMI or through\nthe BMC's web interface. Additionally the blade would randomly power on when\noff, and the front light would consistently blink as if a hardware fault was\ndetected.\n\nThis has been bothering me for a while, but it was my spare blade and wasn't\naffecting my lab in anyway so I've ignored it. I finally needed it for a\nproject and looked into what may be causing the issue.\n\nA [thread][1] on the Serve the Home forums lead to me too a solution, even\nthough my symptoms didn't quite match up with what I was experiencing.\n\nI downloaded the [PowerEdge C6100 Owner's Manual][2] for the jumper\ninformation, and found it too be redundant. The board itself has each of the\njumpers clearly labeled.\n\nAfter pulling the affected chassis out of the server I connected the pins for\nthe CMOS reset, CMOS password reset, and system reset for about 15 seconds. I\npulled the jumpers, reinstalled the blade and it's happy once again. Problem\nsolved.\n\n*Update:* After performing a few commands via the web interface the issue\nreturned. I'm still looking for a solution too the problem.\n\n*Update 2:* I'm now suspecting that the issue may be related too me not\nupdating the FSB, which is responsible for handling power management of\nindividual nodes as well as reporting temperature and command response from the\nBMC.\n\n[1]: http://forums.servethehome.com/index.php?threads/dell-c6100-anyone-brick-a-board-yet.1448/\n[2]: http://ftp.dell.com/Manuals/all-products/esuprt_ser_stor_net/esuprt_cloud_products/poweredge-c6100_Owner%27s%20Manual_en-us.pdf\n","created_at":1404178982,"fuzzy_word_count":300,"path":"/blog/2014/06/fixing-erratic-bmc-controller-on-poweredge-c6100/","published_at":1404178982,"reading_time":2,"tags":["servers"],"title":"Fixing Erratic BMC Controller on PowerEdge C6100","type":"blog","updated_at":1404178982,"weight":0,"word_count":251},{"cid":"034b115ed7c98ddc4ed3c5c1cc0c3b1567edc749","content":"\nThe current large project I'm working on is going to be hosted on AWS and I was\nrequested to do a cost estimate. Looking into it, it quickly became clear that\nreserved instances could potentially save quite a bit of cash but there was a\ncatch (isn't there always?).\n\nThere is an upfront cost for reserving the instance and in exchange you get a\nreduced hourly rate. After running the numbers one thing wasn't clear too me,\nis the upfront cost credit towards running machines or a fee you never see\nagain?\n\nI immediately assumed the latter based on the numbers for one simple reason. If\nyou use the 'Light Reserved Instance' with a 1 year reservation, have your\nmachine running 24/7 the whole year it will cost your *more* than running the\nsame instance as 'on demand'. This was true for their m1.small, m3.medium, and\nm3.large which was the only ones I ran the numbers for.\n\nI searched the internet and wasn't able to find a solid answer to the question\nuntil I asked Amazon's customer service directly.\n\nUltimately there probably is a price point where 1 year light reserved\ninstances make sense, and if you're looking too run 24/7 for the whole year\nyou'll want to do a heavy anyway for the most savings but it still surprised\nme.\n\nI'll probably do a project later using [d3.js][1] to get some direct hours run\nvs total cost for various instances. It'll probably be a fun project.\n\n[1]: http://d3js.org/\n","created_at":1402075691,"fuzzy_word_count":300,"path":"/blog/2014/06/aws-reserved-instance-pricing/","published_at":1402075691,"reading_time":2,"tags":["aws"],"title":"AWS Reserved Instance Pricing","type":"blog","updated_at":1402075691,"weight":0,"word_count":247},{"cid":"aa39c3995e5759ba0376be8a366e8068d5294619","content":"\nBefore I describe the issue that I encountered, let me be very clear. This hack\nis *potentially dangerous and should absolutely only be done in development\nenvironments*. This won't affect your host system, only the docker container so\nthe most damage you'll do is prevent hostname and possibly user/group lookups\nwithin the container itself.\n\nAlright with that out of the way, I was actively working on a codebase that\nuses subdomains as part of the identifier. Rather than setup a full DNS server,\npoint my local system at it and load in the domains I wanted to simply modify\nthe /etc/hosts file inside the environment.\n\nDocker mounts an /etc/hosts file inside it's containers, read-only, and the\ncontainer's 'root' user has had it's mount permissions revoked so it's not able\nto be modified. Other users have encountered this issue, and a [novel\nworkaround was put forward][1]. The solution however makes use of Perl, and is\nspecific too Ubuntu base systems.\n\nI'll explain the solution after showing a more general way to accomplish the\nsame thing. Different linux systems will store their libraries in different\ndirectory structures. CentOS is different from Fedora, which is different from\nUbuntu and Debian. All of them name their libraries, in this case we're looking\nfor 'libnss_files.so.2'.\n\nYou can find where your copy of this library lives with the following command.\nThis should be run inside the docker container that you want to modify the\n/etc/hosts file in.\n\n```bash\nfind / -name libnss_files.so.2 -print 2\u003e /dev/null\n```\n\nPay attention to the path, multiple files may show up and you want the one that\nmatches your system's running kernel (generally x86_64 systems will have their\nlibraries in a lib64 directory).\n\nOnce you've found this add the following lines to your Dockerfile. Make sure\nyou modify the path in the copy in the first line to the path of your copy of\nthe library. Once done you'll use the /var/hosts file to modify your hosts file\ninstead.\n\n```docker\nRUN mkdir -p /override_lib \u0026\u0026 cp /etc/hosts /var/ \u0026\u0026 cp /usr/lib64/libnss_files.so.2 /override_lib\nRUN sed -ie 's:/etc/hosts:/var/hosts:g' /override_lib/libnss_files.so.2\nENV LD_LIBRARY_PATH /override_lib\n```\n\nSo what is this actually doing? On linux systems, name configurations such as\nDNS, username, and group lookups are generally handled by the `nss` or name\nservice switch configuration tools including the hosts file. The library that\nwe're copying and modifying is a very specific to reading from files on the\nsystem and includes the default paths to these files.\n\nGenerally you have to be very careful when you're manipulating strings within\ncompiled libraries. The length of the string is encoded along with it, so at a\nminimum it's important that the string is *the same length or less*. You can\nget away with less but it requires additionally writing an end of string\ncharacter as well.\n\nToo make this hack simple, we're simply replacing the 'etc' with 'var', both\nsystems directories that regular users generally should have read access but\nnot write access too.\n\nFinally we need to tell all programs that need to perform lookups using\nhostnames in the hosts file to make use of our modified library instead of the\nsystem one. Linux will look for shared libraries at runtime in any paths set in\nin the LD_LIBRARY_PATH (colon delimited just like PATH) and this doesn't\nrequire any privileges too set.\n\nAnd the result? An editable hosts file, with no extra services. I can't stress\nenough though, there could be bad ramifications from modifying libraries this\nway. This is definitely not a 'production ready' hack.\n\n[1]: https://stackoverflow.com/questions/19414543/how-can-i-make-etc-hosts-writable-by-root-in-a-docker-container\n","created_at":1401810239,"fuzzy_word_count":600,"path":"/blog/2014/06/modifying-the-hosts-file-in-a-docker-container/","published_at":1401810239,"reading_time":3,"tags":["docker","linux"],"title":"Modifying the Hosts File in a Docker Container","type":"blog","updated_at":1401810239,"weight":0,"word_count":588},{"cid":"ddc11ca9b74f95ee52fb12cc28e67c5cd4b39188","content":"\nRecently I've been playing around with building a pure javascript full text\nsearch engine for static content sites like this one. One of the challenges\nwith doing this has been working around the Markdown markup embedded in the\nwritten content.\n\nMost of the markdown syntax can be stripped out simply by removing all\nnon-alphanumeric characters from the document and move on. This doesn't solve\none of the bigger challenges I've experienced... Code blocks. Code blocks have\nplenty of regular English-ish words and can easily skew keyword detection\nwithin it.\n\nI didn't want to write my own Markdown parser, so I started with the one\nalready in use by this site's renderer ([redcarpet][1]). Another GitHub user,\n[Markus Koller or toupeira on GitHub][2] provided [the basis][3] for the code\nthat became the redcarpet \"StripDown\" formatter, which was designed to\nessentially render a Markdown document without the markup.\n\nIt does almost exactly what I want, except it still outputs raw code inside the\ncontent. The following code sample includes a modified version that excludes\nany code blocks. My content is also formatted inside the markdown documents to\nnever be longer than 80 lines, this also turns individual paragraphs and list\nitems into individual lines for paragraph detection.\n\n```ruby\nrequire 'redcarpet'\nrequire 'redcarpet/render_strip'\n\nclass ContentRenderer \u003c Redcarpet::Render::StripDown\n  def block_code(*args)\n    nil\n  end\n\n  def list_item(content, list_type)\n    content.gsub(\"\\n\", \" \") + \"\\n\\n\"\n  end\n\n  def paragraph(text)\n    text.gsub(\"\\n\", \" \") + \"\\n\\n\"\n  end\nend\n\nmarkdown = Redcarpet::Markdown.new(ContentRenderer, fenced_code_blocks: true)\nputs markdown.render(File.read('sample_markdown_article.md'))\n```\n\nThe above code will print out just the content of the markdown formatted file\n'sample_markdown_article.md'.\n\n[1]: https://github.com/vmg/redcarpet\n[2]: https://github.com/toupeira\n[3]: https://github.com/vmg/redcarpet/issues/79\n","created_at":1401489269,"fuzzy_word_count":300,"path":"/blog/2014/05/extracting-content-from-markdown/","published_at":1401489269,"reading_time":2,"tags":["development","tips"],"title":"Extracting Content From Markdown","type":"blog","updated_at":1401489269,"weight":0,"word_count":275},{"cid":"5e5380d86fc9e91faaa461fa07f71f27e0a9440d","content":"\nI've been using the PostgreSQL's hstore extension in a Rails application lately\nand kept encountering the error that is this post's namesake. It would\nspecifically happen when a database had been dropped, recreated and I freshly\nran the migrations.\n\nIt seems that while Rails 4 supports the HStore datatype, it doesn't enable the\nextension itself. I've found two ways too solve this issue in wildly different\nways.\n\n## First Solution: Enable HStore by Default\n\nThis is the common solution that is recommended too solve this issue. It\nenables the HStore extension by default on all newly created databases. Too\nunderstand this you need to know a bit about PostgreSQL's behavior.\n\nWhen a new database is created, PostgreSQL creates a copy of a special\npre-existing database named `template1` by default. Anything done too this\ndatabase will be reflected in all new databases, including enabling extensions.\n\nToo enable the HStore extension on the `template1` database you can execute the\nfollowing command (generally as the `postgres` user or with your authentication\nof choice).\n\n```\npsql -d template1 -c 'CREATE EXTENSION hstore;'\n```\n\n## Second Solution: Rails Migration\n\nThe above solution doesn't sit well with me. While it's uncommon for any\nindividual PostgreSQL server to be shared among different applications with\ndifferent databases, the possibility is there. Perhaps the application will get\ndecommissioned and the DBA will simply drop the associated database and roles\ninstead of setting up a new one.\n\nDisconnecting the requirements of the application from the application itself\nalways seems to lead too trouble.\n\nRails already has a mechanism too handle modifications too the database\novertime, migrations. They're solid, well tested, and encapsulate not only how\nto get the database to a particular state but also how to return it back to\nit's prior state (generally).\n\nWe can also do this without using raw SQL which now also seems a bit... Off to\nme. The following is a sample Rails migration that will both enable and disable\nthe extension:\n\n```ruby\nclass ManageHstore \u003c ActiveRecord::Migration\n  def change\n    reversible do |op|\n      op.up { enable_extension 'hstore' }\n      op.down { disable_extension 'hstore' }\n    end\n  end\nend\n```\n\nNow the biggest problem with this migration is that too use it, you need too\nplan ahead of time too use the extension or not worry about freshly running all\nthe migrations (generally because you dropped and created the database). This\nmigration needs to be named so it alphabetically comes before any migration in\nyour application that makes use of the HStore datatype.\n\nActiveRecord uses timestamps at the beginning of the migration names to handle\nthis alphabetic sorting, and such you'll want to fake this in before you used\nthe HStore datatype.\n","created_at":1401314455,"fuzzy_word_count":500,"path":"/blog/2014/05/pg-error-error-type-hstore-does-not-exist/","published_at":1401314455,"reading_time":3,"tags":["postgres","ruby"],"title":"PG::Error: ERROR: Type 'Hstore' Does Not Exist","type":"blog","updated_at":1401314455,"weight":0,"word_count":443},{"cid":"527b64ef990702f4612935f9f88e2caa977f0404","content":"\nI've found several places where I needed to be able to update my kernels but\nfor one reason or another can't update the kernel that gets booted initially. A\ncouple of these situations were:\n\n* Running Custom or Updated Kernels on DigitalOcean (this is one of their\n  biggest failings IMHO)\n* Allowing updating of kernels on embedded linux devices that require their\n  kernel flashed into NVRAM.\n* Running an embedded system that used an active/backup partition scheme for\n  updating.\n\nIn all cases the process was pretty much the same, though there were some\ncustom changes to the preliminary init system depending on what I needed to get\ndone, especially with the last one which I may cover in a different article.\n\nIn all cases these were done on a Red Hat based distribution like CentOS,\nScientific Linux, RHEL, or even Fedora. For those users of Debian based systems\nyou'll need to adjust the scripts too your system though I can't imagine\nanything other than the package names changing.\n\nThis assumes you already have the kernel and initramfs you want to boot\ninstalled on your local filesystem at `/boot/vmlinuz-custom` and\n`/boot/initramfs.img`.\n\nA quick background on how this works, when the linux kernel is compiled an init\nprogram is configured to be the first thing triggered, by default and in most\nsituations this will be the executable `/sbin/init`. This init process is then\nresponsible for starting the rest of the daemons and processes that make up the\nsystems we regularly interact with.\n\nThere are tools that allow you too effectively execute another kernel to run in\nplace of the kernel that is already running. There are some catches though as\nthe new kernel won't always re-initialize all devices (since they've already\nbeen initialized) and that can lead too some weird behaviors with processes\nthat already have hooks on those devices.\n\nToo prevent any issues you need to load the new kernel as early in the boot\nprocess as possible. Doing this in the init program is pretty much as early as\nyou can get and makes for a pretty stable system (I've yet to experience any\nissues with machines running this way).\n\nThere are several different init systems and they all behave a little\ndifferently, as far as I know only systemd supports a means of automatically\nexecuting a different kernel but I am personally not a systemd fan and it would\nbe too late in the boot process already for me too trust the chain load. You\ncan reliably chain load kernels regardless of what your normal init system is\nthough very easily and that's what I'm going to cover here.\n\nYou'll need to have the kexec tools installed on your system. This is pretty\nstraight-forward:\n\n```sh\nyum install kexec-tools -y\n```\n\nNext we're going to shift the standard init process off to the side, someplace\nstill accessible so we can call it later (this will need to be done as root).\n\n```sh\nmv /sbin/init /sbin/init.original\n```\n\nNow we need to create our own init script that will handle detecting if it's\nthe new or old kernel, replacing the kernel if it is indeed an old one, and\nstarting up the normal init process if it's the new kernel.\n\nNow there is a very important catch here, whatever process starts up first is\ngiven PID 1 which is very important in kernel land. Whatever process is PID 1\nwill inherit all zombie processes on the system and will need to handle them.\nSince our shell script is the first thing started up it will get PID 1 for both\nthe old and new kernel and getting the process handling code correct is not a\ntrivial issue.\n\nWhat we really need is to hand over PID 1 to the init process so it can do it's\njob normally as if the shell script never existed. There is a native function\nto do exactly this in these shell scripts: `exec`.\n\nOur simple shell script to do the chain load looks like this:\n\n```sh\n#!/bin/bash\n\n# Detect if this is the old kernel (not booted with the otherwise meaningless\n# 'kexeced' parameter.\nif [ $(grep -q ' kexeced$' /proc/cmdline) ]; then\n  kexec --load /boot/vmlinuz-custom --initrd=/boot/initramfs.img \\\n    --reuse-cmdline --append=' kexeced'\n  kexec --exec\nfi\n\n# If we made it this far we're running on the new kernel, trigger the original\n# init binary with all the options passed too this as well as having it take\n# over this process's PID.\nexec /sbin/init.original \"$@\"\n```\n\nAfter rebooting you should be in your new kernel which you can verify with\n`uname -a` and also by examining the `/proc/cmdline` file for the existence of\nthe `kexeced` flag.\n\nIf you modify the script above, be very careful as any execution error will\ncause your system to die and recovery will only be possible by mounting the\nfilesystem on another linux system and fixing it.\n\nIn a future article I'll cover how to use this trick to build an active /\nbackup system allowing you to fall back to a known good system when booting\nfails which is incredibly useful for embedded devices in the field that need\nupdates but are not easy to get too or replace when an update bricks the\nsystem.\n","created_at":1400859556,"fuzzy_word_count":900,"path":"/blog/2014/05/chain-loading-kernels/","published_at":1400859556,"reading_time":5,"tags":["linux"],"title":"Chain Loading Kernels","type":"blog","updated_at":1400859556,"weight":0,"word_count":881},{"cid":"8883b0d1a1361ee4d516cf2b8c03fc8cbfa590be","content":"\n# Calculating RSA Key Fingerprints in Ruby\n\nI regularly find myself working on projects that involve the manipulation and storage of RSA keys. In the past I've never had to worry about identification or presentation of these keys. Normally I've only got one too three pairs at most that I'm manipulating (server, certificate authority, client).\n\nI've not found myself working on a project that involves presenting the certificates to users for selection and comparison. The obvious way too do this is take a page out of other developer's books and present the key's fingerprint.\n\nFor those unfamiliar with key fingerprints, they are a condensed way to compare differing RSA with a high probability that if the fingerprints match, so do the keys. These are generally based on a cryptographic digest function such as SHA1 and MD5, and you'll see them most commonly when connecting to a new SSH host and will look like the following:.\n\n```text\nThe authenticity of host 'some.fakedomain.tld (127.0.0.1)' can't be established.\nRSA key fingerprint is 0c:6c:dd:32:b5:59:40:1d:ac:05:24:4f:04:bc:e0:f3.\nAre you sure you want to continue connecting (yes/no)?\n```\n\nThe string of 32 hex characters presented there can be compared with another known value to make sure you're connecting to the correct SSH server and will always be the same length regardless of the bit-strength of the keys used. Without the fingerprint, users would have to compare 256 hex characters for a 1024 bit key, which is a very low security key.\n\nYou can calculate the SSH fingerprint for your SSH key or a SSH host key using the ssh-keygen command like so:\n\n```console\n$ ssh-keygen -lf ~/.ssh/id_rsa\n$ ssh-keygen -lf /etc/ssh/ssh_host_key.pub\n```\n\nIt will work when the path is either a private RSA key or a public key formatted for SSH authorized key files.\n\nX509 certificates also use a key fingerprint to help identify a certificate's signing authority. What I rapidly learned through this investigation was that they are calculated slightly differently from SSH fingerprints even if they're in the same format.\n\nI couldn't find any good Ruby code that calculated either, and the alternatives were some dense C++. Luckily SSH fingerprints are pretty documented in [RFC4253][1] and [RFC4716][2]. Fingerprints on RSA keys for use with OpenSSL are less clear, and there is a different method for calculating the fingerprints of certificates.\n\nSlowly working through the undocumented bits of Ruby's OpenSSL wrapper, the RFCs and a couple of C++ implementations I finally got a set of working implementations that calculate the following fingerprints in Ruby:\n\n* MD5 \u0026 SHA1 fingerprints for RSA SSH keys\n* Fingerprints of RSA keys for use with x509 certificates\n* Fingerprints of x509 certificates\n\nThe easiest being a regular x509 certificate:\n\n```ruby\nrequire 'openssl'\n\npath_to_cert = '/tmp/sample.crt'\ncert = OpenSSL::X509::Certificate.new(File.read(path_to_cert))\nputs OpenSSL::Digest::SHA1.hexdigest(cert.to_der).scan(/../).join(':')\n```\n\nYou can compare the output of the above code with OpenSSL's implementation with the following command:\n\n```console\n$ openssl x509 -in /tmp/sample.crt -noout -fingerprint\n```\n\nPlease note that case sensitivity doesn't matter here (OpenSSL will return upper case hex codes).\n\nThe next one I got working was the SSH fingerprints thanks to the RFCs mentioned earlier.\n\n```ruby\nrequire 'openssl'\n\npath_to_key = '/tmp/ssh_key'\n\nkey = OpenSSL::PKey::RSA.new(File.read(path_to_key))\ndata_string = [7].pack('N') + 'ssh-rsa' + key.public_key.e.to_s(0) + key.public_key.n.to_s(0)\nputs OpenSSL::Digest::MD5.hexdigest(data_string).scan(/../).join(':')\n```\n\n*Please note: The above only works for RSA SSH keys.*\n\nCalculating a SHA1 fingerprint for SSH hosts is as simple as replacing the 'MD5' class with 'SHA1' or any of the other support digest algorithms.\n\nThe last one was the hardest to track down and implement, eventually I found the answer in [RFC3279][3] under section 2.3.1 for the format of the public key I would need to generate before performing a digest calculation on it.\n\n```ruby\nrequire 'openssl'\n\npath_to_key = '/tmp/x509_key.pem'\n\nkey = OpenSSL::PKey::RSA.new(File.read(path_to_key))\ndata_string = OpenSSL::ASN1::Sequence([\n  OpenSSL::ASN1::Integer.new(key.public_key.n),\n  OpenSSL::ASN1::Integer.new(key.public_key.e)\n])\nputs OpenSSL::Digest::SHA1.hexdigest(data_string.to_der).scan(/../).join(':')\n```\n\n[1]: http://www.ietf.org/rfc/rfc4253.txt\n[2]: http://www.ietf.org/rfc/rfc4716.txt\n[3]: http://www.ietf.org/rfc/rfc3279.txt\n","created_at":1398119824,"fuzzy_word_count":700,"path":"/blog/2014/04/calculating-rsa-key-fingerprints-in-ruby/","published_at":1398119824,"reading_time":3,"tags":["development","ruby","security"],"title":"Calculating RSA Key Fingerprints in Ruby","type":"blog","updated_at":1398119824,"weight":0,"word_count":636},{"cid":"5b6283bcb889b21a704e5aeb874331584dcc5724","content":"\nAn update too Fedora a while ago started causing some unexpected behavior with\nmy dotfiles. Specifically the way I was handling my SSH agent. My SSH keys when\nadded to my agent automatically expire after a couple of hours.\n\nAfter the update, when that expiration came I started receiving errors in my\nshell that looked similar to the following (Since I fixed it I am not able to\nget the exact working again):\n\n```\nWarning: Unable to connect to SSH agent\n```\n\nI also noticed that periodically I got a Gnome keyring pop-up asking for my SSH\nagent rather than my command-line client. I'm personally not a big fan of\nGnome, but I deal with because it's the default for Fedora, tends to stay out\nof your way, and switching to something else is just not a project I've had\ntime for.\n\nNow Gnome was very much getting in my way. I dealt with it for several months\nnow and finally got sick of it.\n\nI tracked this down too the `gnome-keyring-daemon` which was starting up and\nclobbering the contents of my `SSH_AUTH_SOCK` variable along with my\n`GPG_AGENT_INFO` environment. Not very friendly.\n\nThere were a couple paths that I could've gone for for solving this situation.\nThe first, and easiest way to probably have dealt with this was too put some\nlogic into my `~/.bashrc` file that detected when the `gnome-keying-agent` was\nrunning, kill it and clean up after it. It might look something like this:\n\n```sh\nif [ -n \"${GNOME_KEYRING_PID}\" ]; then\n  if $(kill -0 ${GNOME_KEYRING_PID}); then\n    kill -9 ${GNOME_KEYRING_PID}\n  fi\nfi\n\nunset GNOME_KEYRING_CONTROL SSH_AUTH_SOCK GPG_AGENT_INFO GNOME_KEYRING_PID\n```\n\nI share my dotfiles along a lot of different systems and don't like\nsystem-specific behavior getting in there. Instead I choose to find what was\nstarting up the keyring daemon and preventing it from doing so. Without a good\nplace to start and stubbornly refusing to Google this particular problem I took\nthe brute force approach of grep for the binary name in the `/etc` directory.\n\nSure enough in `/etc/xdg/autostart` I found a series of background daemons that\nI definitely did not want nor need running. As root I ran the following command\nto purge them from my system:\n\n```sh\ncd /etc/xdg/autostart\nrm -f gnome-keyring-{gpg,pkcs11,secrets,ssh}.desktop\n```\n\nThe first solution will keep your system in a default state, but this will\npermanently prevent the obnoxious behavior on your system for all users and\nprevents you from adding hacks to your bashrc to work around misbehaving\nsoftware.\n\nI hope this helps someone else!\n","created_at":1397485163,"fuzzy_word_count":500,"path":"/blog/2014/04/disabling-gnomes-keyring-in-fedora-19/","published_at":1397485163,"reading_time":2,"tags":["linux"],"title":"Disabling Gnome's Keyring in Fedora 19","type":"blog","updated_at":1397485163,"weight":0,"word_count":422},{"cid":"394848a1bfc032cc5aacac95a974d0bace2fc03e","content":"\n# One-Liner SSL Certificate Generation\n\nI regularly find myself in need of generating a quick SSL key and certificate pair. I've been using a one-liner for a while to generate these certificates. No annoying user prompts just a quick fast certificate pair.\n\n```console\necho -e \"XX\\n\\n \\n \\n\\n$(hostname)\\n\\n\" | openssl req -new -x509 -newkey \\\n  rsa:2048 -keyout service.key -nodes -days 90 -out service.crt \u0026\u003e /dev/null\n```\n\nThe cert uses the hostname of whatever machine you generated it on. It should under no circumstances be used for a production service. It's a 2048 bit key and only valid for for roughly three months.\n","created_at":1396032771,"fuzzy_word_count":100,"path":"/blog/2014/03/one-liner-ssl-certificate-generation/","published_at":1396032771,"reading_time":1,"tags":["linux","tips"],"title":"One-Liner SSL Certificate Generation","type":"blog","updated_at":1396032771,"weight":0,"word_count":97},{"cid":"4d4c8309ddcd30cea86601c43d52b30446124ccd","content":"\n# Preventing Tmux Lockups\n\nAnyone that has used SSH, Tmux or Screen for a while will have inevitably\ndumped excessive output to their terminal. Depending on the size of the output\nyou may have experienced the dreaded lockup. That horrible realization seconds\nafter you hit the command where signals just stop working and you just have to\nsit there and wait for your terminal to catch up.\n\nThere is a piece of remote connection software called Mosh that I've been told\nhandles this pretty well, but I don't yet trust its security model and it\ndoesn't prevent the same thing from happening locally.\n\nThis is especially bad if you're working in a multi-pane tmux window as it's\nlocks up all the terminals in the same window, and prevents you from changing\nto the other windows.\n\nI've had this issue happen to me one too many times but never thought of\nlooking for a solution until a friend of mine, [Gabe Koss][1], made a passing\ncomment along the lines of \"Too bad tmux can't rate limit the output of a\nterminal\".\n\nA quick search through the doc and two relatively recent configuration options\npopped out doing exactly what I was looking for (c0-change-internal, and\nc0-change-trigger). Googling around for good values, left me wanting. A lot of\npeople were recommending setting the values to 100 and 250 respectively; These\nare the defaults and since I still experience the issue are clearly not working\nfor me.\n\nTo set the variables to something more reasonable I had to understand what they\nwere doing. A 'C0' sequence is one that modifies the screen beyond a normal\ncharacter sequence, think newlines, carriage returns, backspaces. According to\nthe tmux man page, the trigger will catch if the number of c0 sequences per\n**millisecond** exceeds the number in the configuration file, at which point it\nwill start displaying an update once every interval number of milliseconds.\n\nI can't see faster than my eye's refresh rate so that seems like a decent\nstarting point. According to [Wikipedia][2] the human eye/brain interface can\nprocess 10-12 images per second but we can notice 'choppiness' below 48 FPS.\nSince I won't be reading anything flying by that fast I settled on a maximum\nrate of 10 FPS updated in my shell, or an interval of '100ms'.\n\nFor the trigger I was significantly less scientific, I dropped the trigger by\n50, reloaded my tmux configuration, `cat`'d a large file and tested whether I\ncould immediately kill the process and move between panes. I finally settled on\na value of '75' for the trigger rate. It does make the output seem a little\nchoppy but it is significantly nicer to not kill my terminal.\n\nTL;DR Add the following lines to your `~/.tmux.conf` file and you'll be in a\nmuch better shape:\n\n```\nsetw -g c0-change-interval 50\nsetw -g c0-change-trigger  75\n```\n\n[1]: http://gabekoss.com/\n[2]: http://en.wikipedia.org/wiki/Frame_rate\n","created_at":1396025772,"fuzzy_word_count":500,"path":"/blog/2014/03/preventing-tmux-lockups/","published_at":1396025772,"reading_time":3,"tags":["linux"],"title":"Preventing Tmux Lockups","type":"blog","updated_at":1396025772,"weight":0,"word_count":471},{"cid":"79b814d806ddb8a2906e3209c9fcdd2067923491","content":"\n# Finding Ruby Subclasses\n\nWhile working through a problem I found it would be immensely useful to be able\nto enumerate all of the current subclasses of a particular class. After\nthinking about this for a while I settled on a good old friend of mine,\n`ObjectSpace`.\n\nFor those not familiar with the `ObjectSpace` module, it is a means to inspect\nand access the items being tracked by Ruby's garbage collector. This means it\nhas a hook into every living object, and more dangerously, every near-death\nobject.\n\n`ObjectSpace` provides a method for enumerating instances of a specific class,\nspecifically named `each_object` which takes a class. With Ruby all classes are\nin fact instances of the `Class` class. This allows us to enumerate every\navailable class by passing it to the enumerator like so:\n\n```ruby\nObjectSpace.each_object(Class).to_a\n```\n\nAlright so we now have an array of every single class that could possibly be\ninstantiated, how do we narrow it down to just the ones we're interested in?\nOnce again Ruby provides with the `ancestors` method, combine that with a\nselect and we can quickly narrow it down. You can see it in the following\nexample:\n\n```\n[1] pry(main)\u003e TargetSubclass = Class.new(String)\n=\u003e TargetSubclass\n[2] pry(main)\u003e ObjectSpace.each_object(Class).select { |k| k.ancestors.include?(String) }\n=\u003e [String, TargetSubclass]\n```\n\nHmm, that's not quite right though. We have found all the subclasses but we've\nalso grabbed the parent class. With one small modification we eliminate that as\nwell.\n\n```\n[1] pry(main)\u003e TargetSubclass = Class.new(String)\n=\u003e TargetSubclass\n[2] pry(main)\u003e ObjectSpace.each_object(Class).select { |k| k.ancestors.include?(String) \u0026\u0026 k != String }\n=\u003e [TargetSubclass]\n```\n\nThat line is rather long though, and I generally like to avoid multiple tests\nin a select block. There is a tad bit of syntactic sugar provided by Ruby\nallowing us to accomplish the same thing, our final example is ultimately the\nsolution I went with:\n\n```\n[1] pry(main)\u003e TargetSubclass = Class.new(String)\n=\u003e TargetSubclass\n[2] pry(main)\u003e ObjectSpace.each_object(Class).select { |k| k \u003c String }\n=\u003e [TargetSubclass]\n```\n\nPutting this into a method:\n\n```ruby\ndef subclasses(klass)\n  ObjectSpace.each_object(Class).select { |k| k \u003c klass }\nend\n```\n\nIf you were so inclined you could extend the `Class` class with a method to\nmake this available anywhere like so:\n\n```ruby\nclass Class\n  def self.subclasses\n    ObjectSpace.each_object(Class).select { |k| k \u003c self }\n  end\nend\n```\n\nI'm personally not a fan of extending any of the core classes unless absolutely\nnecessary, but too each there own.\n","created_at":1392900504,"fuzzy_word_count":400,"path":"/blog/2014/02/finding-ruby-subclasses/","published_at":1392900504,"reading_time":2,"tags":["ruby","tips"],"title":"Finding Ruby Subclasses","type":"blog","updated_at":1392900504,"weight":0,"word_count":391},{"cid":"c8c1ae8bde99220e013af51ca6064872c715c5ef","content":"\n# Creating Crypt Style SHA512 Passwords With Ruby\n\nI needed to generate crypt-style SHA512 passwords in ruby for an `/etc/shadow`\nfile. After a bunch of Googling and messing around with the OpenSSL library I\nfinally found a very simple built-in way to handle this.\n\n```ruby\nrequire 'securerandom'\n\n'password'.crypt('$6$' + SecureRandom.random_number(36 ** 8).to_s(36))\n```\n\nYou'll get a string that looks like:\n\n```\n$6$4dksjo1b$Lt194Dwy7r/7WbM8MezYZysmGcxjaiisgTrTBbHkyBZFXeqQTG0J5hep4wLM/AmYxlGNLRy0OWATLDZCqjwCk.\n```\n\nIf you don't want to use the `SecureRandom` module you can replace the random\ncall with simply `rand(36 ** 8)` though this isn't recommended.\n\nEnjoy!\n","created_at":1392668907,"fuzzy_word_count":100,"path":"/blog/2014/02/creating-crypt-style-sha512-passwords-with-ruby/","published_at":1392668907,"reading_time":1,"tags":["development","linux","ruby","security"],"title":"Creating Crypt Style SHA512 Passwords With Ruby","type":"blog","updated_at":1392668907,"weight":0,"word_count":78},{"cid":"8500e847dfcf43ebd1804b8017fcdf5509d91f95","content":"\n# Setting Linux System Timezone\n\nI change the timezone on the linux systems so rarely that I almost always have\nto look it up. I'm writing it up here for my own personal reference. With any\nluck it'll also help others.\n\nThe system timezone is controlled by the `/etc/localtime` file and is generally\nsymlinked to locale files stored in `/usr/share/zoneinfo`. Generally I like to\nkeep my systems on UTC as I my machines are in several timezones and it makes\nall the logs have consistent times.\n\nTo set the system time to UTC you'd run the following command as root:\n\n```sh\nln -sf /usr/share/zoneinfo/UTC /etc/localtime\n```\n\nOther timezones can be found in the `/usr/share/zoneinfo` and are generally\nbroken up by continent with a few exceptions.\n\nAs a user it's obviously more useful to see the time in my local timezone and\nthis can be overridden on a per-user basis using the `TZ` environment variable.\nI stick this in my `~/.bashrc` file and it just works transparently:\n\n```sh\nexport TZ=\"America/Los_Angeles\"\n```\n","created_at":1391280646,"fuzzy_word_count":200,"path":"/blog/2014/02/setting-linux-system-timezone/","published_at":1391280646,"reading_time":1,"tags":["linux","tips"],"title":"Setting Linux System Timezone","type":"blog","updated_at":1391280646,"weight":0,"word_count":163},{"cid":"058ff488f9040fc2eefcc1acfe53884e2fcbcd84","content":"\n# Starting Puppet Master on Fedora 19\n\nI was trying to get puppet running out of the box on Fedora 19 and found a bug\nexists in their systemd service file. After installing `puppet` and\n`puppet-server`, whenever I tried to start the server with the following\ncommand:\n\n```sh\nsystemctl start puppetmaster.service\n```\n\nIt would hang for a long time and the following error message would show up in\nthe log:\n\n```\nJan 19 03:42:18 puppet-01 puppet-master[1166]: Starting Puppet master version 3.3.1\nJan 19 03:42:18 puppet-01 systemd[1]: PID file /run/puppet/master.pid not readable (yet?) after start.\nJan 19 03:43:07 puppet-01 systemd[1]: puppetmaster.service operation timed out. Terminating.\nJan 19 03:43:07 puppet-01 puppet-master[1166]: Could not run: can't be called from trap context\n```\n\nStarting puppet directly from the command line using the same command specified\nin the service file would work fine, but that wasn't really a solution. Turns\nout puppet, additionally I would briefly see the `puppetmaster` service open up\nport 8140 before systemd would kill it.\n\nTurns out the systemd service script is looking in the wrong location for the\npid file. All of the pids are stored in `/var/run/puppet/` with a filename of\neither `agent.pid` or `master.pid` depending on the mode it was run as. The\nsystemd script, as the log indicates is looking for the pid files in\n`/run/puppet`.\n\nThe real solution would be to bring this too the attention of the script\nmaintainers, but I haven't had a lot of luck going through those processes.\nInstead you can work around the issue without any bureaucracy by changing the\n`rundir` configuration option in `/etc/puppet/puppet.conf` to `/run/puppet`,\nand creating `/run/puppet` (with puppet as the user and group owning the\ndirectory).\n\nAfter that, voila! The service starts up. You'd think a QA process would catch\nthat the service script doesn't work...\n","created_at":1390103257,"fuzzy_word_count":300,"path":"/blog/2014/01/starting-puppetmaster-on-fedora-19/","published_at":1390103257,"reading_time":2,"tags":["linux"],"title":"Starting Puppet Master on Fedora 19","type":"blog","updated_at":1390103257,"weight":0,"word_count":288},{"cid":"e0ab50cfe633209b78265cd3bc152eb0b86008d3","content":"\n# Playing With the Linux Bluetooth Stack\n\nList all available bluetooth interfaces:\n\n```sh\nhciconfig -a\n```\n\nIf you get an error like the following:\n\n```\nOperation not possible due to RF-kill\n```\n\nYou'll need to unblock access to the resource using `rfkill`. You can unblock\nall blocked devices like so:\n\n```sh\nrfkill unblock all\n```\n\nBefore doing any iBeacon stuff you should disable scanning:\n\n```sh\nhciconfig hci0 noscan\n```\n\n```sh\nhcitool -i hci0 cmd 0x08 0x0008 1E 02 01 1A 1A FF 4C 00 02 15 [ 92 77 83 0A B2 EB 49 0F A1 DD 7F E3 8C 49 2E DE ] [ 00 00 ] [ 00 00 ] C5 00\nhcitool -i hci0 leadv\n```\n\n```\nLE set advertise enable on hci1 returned status 12\n```\n","created_at":1387914807,"fuzzy_word_count":200,"path":"/blog/2013/12/playing-with-the-linux-bluetooth-stack/","published_at":1387914807,"reading_time":1,"tags":["linux"],"title":"Playing With the Linux Bluetooth Stack","type":"blog","updated_at":1387914807,"weight":0,"word_count":115},{"cid":"73ad6f15668518b562fdc70dd986be79303a2ddb","content":"\n# Updating BMC on Dell PowerEdge C6100\n\nI just received my Dell PowerEdge C6100 and found it's software quite a bit\noutdated. After searching around quite a bit I found the resources lacking for\nexplaining how to perform these updates. So in this post I'm going to quickly\ncover updating the BMC firmware on each blade.\n\nThe system I received had four different versions of the BMC software\ninstalled, additionally Two were branded as MegaRAC and the others branded as\nDell. This update didn't fix the branding (and I'd love to remove the Dell\nbranding as it's kind of annoying) it did, however, fix a number of other\nissues that I was experiencing such as:\n\n1. Console Redirection failing to connect\n2. BMC losing it's network connection after a couple of minutes\n3. Slow responses, with occasional failures to load pages\n4. Remote IPMI tools being unable to read sensors status\n\nThe first step is too download the latest version of of the BMC software from\n[Dell's support site][1] (I've also taken the liberty of [hosting a copy\nmyself][2]). I recommend you go through the process of entering the service tag\nof each of the blades and make sure that Dell recognizes them as existing even\nif they're out of support.\n\nThere has been mention of versions of these blades that had custom\nmodifications for DCS and any attempts to modify the BIOS or BMC will likely\ncause you to end up bricking the remote management board or the motherboard.\n\nEven with the regular board there is always a risk of bricking it, though\nfirmware updates have gotten a lot more reliable and I haven't experienced an\nincorrectly flashed motherboard in years. You've been warned.\n\nThe BMC was fairly straight forward. I installed the 64-bit version of Fedora\n19 on a thumb drive, downloaded version 1.30 of the BMC software (get the file\nnamed `PEC6100BMC130.exe`). The file itself is a self-extracting zip archive\nwhich can be extracted using the regular unzip utility.\n\n```sh\nunzip PEC6100BMC130.exe\n```\n\nInside you'll find two folders, KCSFlash and SOCFlash should both be put on the\nlive drive within the KCSFlash. You'll need to set the execute bit on the\ncontents of the linux directory and the linux.sh file. You'll also need to\ninstall the `glibc.i686` package. Afterwards it's as simple as booting each\nchassis off the drive and as root run the linux.sh script.\n\nIf the KCSFlash fails, the SOCFlash will more likely than not work but it is\nslightly more dangerous. If you need it mark the `linux/flash8.sh`,\n`linux/socflash`, and `linux/socflash_x64` as executable in the SOCFlash folder\nand run the flash8.sh script.\n\nAfter that you're going to want to reboot into the BIOS and ensure the IPMI\nethernet port is set to dedicated, as this switched it back to \"Shared\" on me.\n\n[1]: https://support.dell.com/\n[2]: http://static.stelfox.net/files/PEC6100BMC130.exe\n","created_at":1387247173,"fuzzy_word_count":500,"path":"/blog/2013/12/updating-bmc-on-dell-poweredge-c6100/","published_at":1387247173,"reading_time":3,"tags":["servers","tips"],"title":"Updating BMC on Dell PowerEdge C6100","type":"blog","updated_at":1387247173,"weight":0,"word_count":456},{"cid":"0bbdb0f4db85b6de79d1d6b2b12cda774f2def9a","content":"\n# Updating the BIOS on Dell PowerEdge C6100\n\nThe BIOS was quite a bit more complicated and there was a few options that I\nhad available to try, all of which require either Windows or DOS environments.\nI don't have any legal copies of Windows to put on my server and didn't want to\ngo through all that effort`\n\nIt really needs to be done within a DOS environment. I downloaded the file\n`PEC6100BIOS017000.exe` from [Dell's support website][1] ([locally hosted\ncopy][2]) as well as the [2.88Mb version of FreeDOS][3] ([locally hosted\ncopy][4]).\n\n[1]: http://downloads.dell.com/Pages/Drivers/poweredge-c6100-all.html\n[2]: http://static.stelfox.net/files/PEC6100BIOS017000.exe\n[3]: http://www.fdos.org/bootdisks/autogen/FDSTD.288.imz\n[4]: http://static.stelfox.net/files/FDSTD.288.imz\n","created_at":1387204742,"fuzzy_word_count":100,"path":"/blog/2013/12/updating-the-bios-on-dell-poweredge-c6100/","published_at":1387204742,"reading_time":1,"tags":["servers","tips"],"title":"Updating the BIOS on Dell PowerEdge C6100","type":"blog","updated_at":1387204742,"weight":0,"word_count":84},{"cid":"26e8ee458f2f15aeb58d4a9caa8889e4e05c3e64","content":"\n# Using Dnsmasq as a Standalone TFTP Server\n\n*If you've come across this blog post with the intention of setting up TFTP on\nan modern version of OpenWRT I have a [more recent blog post][1] detailing how\ntoo configure your system.*\n\nI found myself in need of a TFTP server but wanted to avoid having all of the\nxinet.d packages and services on my system (even if they were disabled). While\nlooking for alternatives I found out that `dnsmasq` has a built-in read-only\nTFTP server.\n\nI already have a DNS and DHCP server on my network and didn't want dnsmasq to\ntake on either of those roles so my first challenge was finding a way to\nprevent dnsmasq from running those bits of it's code, or failing that I would\njust firewall off the service. Luckily it's quite easy to disable both bits of\nfunctionality.\n\nFor DHCP you simply have to leave out any of the dhcp option in the\nconfiguration file, DNS you just tell it to operate on port 0 and it will be\ndisabled.\n\nSo my whole configuration starting out looks like this:\n\n```\n# Disable DNS\nport=0\n```\n\nNow I need to configure the TFTP bits of dnsmasq. This too was rather simple\nonly requiring me to add the following to my already terse configuration file:\n\n```\n# Enable the TFTP server\nenable-tftp\ntftp-root=/var/lib/tftp\n```\n\nI created the root directory for my TFTP server and started it up with the\nfollowing commands:\n\n```bash\nmkdir /var/lib/tftp\nsystemctl enable dnsmasq.service\nsystemctl start dnsmasq.service\n```\n\nVoila, TFTP running and happy. If you have a firewall running you'll also want\nto open ports `69/tcp` and `69/udp` (though I suspect only the UDP one is\nneeded).\n\n[1]: {{\u003c relref \"2014-07-01-using-openwrts-dnsmasq-as-a-tftp-server.md\" \u003e}}\n","created_at":1386890986,"fuzzy_word_count":300,"path":"/blog/2013/12/using-dnsmasq-as-a-standalone-tftp-server/","published_at":1386890986,"reading_time":2,"tags":["linux","tips"],"title":"Using Dnsmasq as a Standalone TFTP Server","type":"blog","updated_at":1386890986,"weight":0,"word_count":272},{"cid":"83220ee8ae83aed45f0d4efe8cbc031d844f27e2","content":"\n# Configuring PXE Booting on OpenWRT\n\nI needed to support PXE booting on my home network. I use OpenWRT as my main\nrouter and DHCP server and it took me a bit of searching how to configure the\nBOOTP next server to redirect local clients to my Arch TFTP/NFS server for\nbooting, so I'm placing the configuration here to help others who might be\nlooking to do the same thing.\n\nIt's worth noting that this isn't a guide on setting up PXE booting completely\non an OpenWRT, you'll need another system that is running a configured TFTP\nserver. I'll write up how I setup my Arch box as a TFTP server at a later date.\n\nThe configuration itself was very simple; You just need to add a couple lines\nto `/etc/config/dhcp`. You'll want to replace 10.0.0.45 with whatever your\nlocal TFTP server is.\n\n```\nconfig boot linux\n  option filename      'pxelinux.0'\n  option serveraddress '10.0.0.45'\n  option servername    'Arch-Pixie'\n```\n\nThe filename `pxelinux.0` is from Syslinux, and the `servername` has no\ntechnical meaning, but it provides nice information to the clients. In this\ncase I've used the name of my Arch linux server that I'll be booting off of.\n\nHope this helps someone out. Cheers!\n","created_at":1386769244,"fuzzy_word_count":200,"path":"/blog/2013/12/configuring-pxe-booting-on-openwrt/","published_at":1386769244,"reading_time":1,"tags":["linux"],"title":"Configuring PXE Booting on OpenWRT","type":"blog","updated_at":1386769244,"weight":0,"word_count":193},{"cid":"4326fb410da01c58beb2e4bb206c6b4c4dd3c554","content":"\n# Running Emails Through Ruby\n\nFollowing up on my [earlier post][1] where I covered how to backup your Gmail\naccount using `fetchmail` and `procmail`; I wanted to cover how I was\nadditionally processing received mail through ruby.\n\nThis was part of a larger project where I was doing statistical analysis on my\nemail while evaluating various data stores. To get the emails into the various\ndata stores, I used the ruby script to parse, process and store the emails as\nthey came in.\n\nIf you're going to be doing any form of mail manipulation or statistics I\nhighly recommend the [mail][2] gem. It did almost everything I needed out of\nthe box, though it didn't correctly enumerate any of the additional headers.\n\nProcmail is a highly flexible mail filtering and local delivery agent. Without\nmuch effort you can pass the mail it is handling through a series of filters\nwhich can manipulate and reject mail before eventually delivering it to your\ninbox. In light of this, we're going to make a filter that simply counts the\ntotal number of emails the script has processed, and add a header to the\nmessage that indicates this count.\n\n```ruby\n#!/usr/bin/env ruby\n\nrequire 'mail'\n\n# Get the email message from STDIN or a passed filename\nmessage = \"\"\nwhile input = ARGF.gets\n  message += input\nend\n\n# Parse the email into a ruby object\nmsg = Mail.new(message)\n\n# Location of our count file\ncount_file = \"#{ENV['HOME']}/.mail_counter.txt\"\n\n# Load or initialize our count value and increment it\ncount = File.exists?(count_file) ? File.read(count_file).to_i : 0\ncount += 1\n\n# Update our count on disk\nFile.write(count_file, count.to_s)\n\n# Add our header with the count\nmsg.header.fields \u003c\u003c Mail::Field.new(\"X-Mail-Counter: #{count}\")\n\n# Output the now modified message back out to $stdout\nbegin\n  $stdout.puts msg.to_s\nrescue Errno::EPIPE\n  exit(74)\nend\n```\n\nMake sure you mark the script executable after saving it.\n\nIf you followed along with [my earlier post][1] the only change we need to make\nis to add our ruby mail processor as a procmail filter. I've stored the script\nin `~/.bin/mail-counter.rb`, if you've stored it in a different location you'll\nwant to update your path to reflect that.\n\nFilters in procmail are handled by using the pipe helper. The following is a\nminimum working example of a `procmailrc` file to make use of our filter:\n\n```bash\nMAILDIR=$HOME\nVERBOSE=on\n\n:0fw\n| /home/sstelfox/Documents/ruby/riak-mail-indexer/counter.rb\n\n:0\nMaildir/\n```\n\nStore the above file in `~/.procmailrc`. The next time you run `fetchmail`\nthose headers will be added to the messages before being delivered and you can\nwatch the count increment by looking at the contents of `~/.mail_counter.txt`.\n\nThe following are a few additional sources I made use of while writing this\narticle:\n\n* http://stackoverflow.com/questions/273262/best-practices-with-stdin-in-ruby\n* http://www.jstorimer.com/blogs/workingwithcode/7766125-writing-ruby-scripts-that-respect-pipelines\n\n[1]: {{\u003c ref \"./2013-11-19-backing-up-gmail-with-fetchmail.md\" \u003e}}\n[2]: https://github.com/mikel/mail\n","created_at":1386513125,"fuzzy_word_count":500,"path":"/blog/2013/12/running-emails-through-ruby/","published_at":1386513125,"reading_time":3,"tags":["linux","ruby","tips"],"title":"Running Emails Through Ruby","type":"blog","updated_at":1386513125,"weight":0,"word_count":479},{"cid":"31a42eb1241c89c6937b34ebd801cbd60a93efc9","content":"\nI've been working on a pure javascript based search engine for this static\nwebsite and needed to access a get parameter within the URL.\n\nI found a few solutions online but they usually made use of jQuery or weren't\nin coffeescript. A few others would only extract an individual named parameter\nat a time. The following will return all of them in Javascript's equivalent of\na hash (or dictionary if you prefer) in the form of an object.\n\n```coffeescript\ngetParams = -\u003e\n  query = window.location.search.substring(1)\n  raw_vars = query.split(\"\u0026\")\n\n  params = {}\n\n  for v in raw_vars\n    [key, val] = v.split(\"=\")\n    params[key] = decodeURIComponent(val)\n\n  params\n\nconsole.log(getParams())\n```\n\nIf compiled and included in a page the above will print out the parameters as a\nhash object to the console.\n","created_at":1386458458,"fuzzy_word_count":200,"path":"/blog/2013/12/access-get-parameters-with-coffeescript/","published_at":1386458458,"reading_time":1,"tags":["development"],"title":"Access GET Parameters With Coffeescript","type":"blog","updated_at":1386458458,"weight":0,"word_count":137},{"cid":"63e52ad8f46b1ad77b8a74a93e6b1dec59a2eb33","content":"\nI [recently posted][1] a guide on backing up your Gmail with `fetchmail`. This\nunfortunately doesn't include your calendar data. It seems like backing up was\na hot enough topic that the Google Gmail team are [releasing an official backup\nmethod][2].  It's not completely in the wild yet but I definitely look forward\nto poking around in it.\n\nNow if only Google let you download everything they know about you as well...\nWould definitely make for an interesting read.\n\n[1]: {{\u003c ref \"./2013-11-19-backing-up-gmail-with-fetchmail.md\" \u003e}}\n[2]: http://gmailblog.blogspot.com/2013/12/download-copy-of-your-gmail-and-google.html\n","created_at":1386259458,"fuzzy_word_count":100,"path":"/blog/2013/12/downloading-google-mail-and-calendar-data/","published_at":1386259458,"reading_time":1,"tags":["tips"],"title":"Downloading Google Mail and Calendar Data","type":"blog","updated_at":1386259458,"weight":0,"word_count":78},{"cid":"eb5b5bdee595ec942ac69097d7f181c7abf1ed80","content":"\nDuring my daily review of various new sources I came across one particular\narticle that was both concerning and very amusing. Drones have been getting\nmore and more popular, and more accessible. They've been getting used by the\nmilitary, law enforcement, [recently Amazon][1] (though they've abandoned that\nfor now), you can even purchase one for your iPhone at airports.\n\nThe security of these systems hasn't been thoroughly tested publicly, though\n[there is at least][2] one report of a military drone being stolen already.\nWith the beginnings of [various commercial uses][3] of drones, expanding beyond\nhobbiests exploring [what they can do with them][4].\n\nSomeone had to start the testing eventually and Samy Kamkar took the lead this\ntime with his new project [Skyjack][5]. The article that tipped me off to this\nproject can be found [over on Threatpost][6].\n\nThe gist of the project is a drone that can forcibly disconnect other drone's\ncontroller and take it's place to \"steal\" the drone.\n\nI can imagine a whole cyberpunk thriller action scene where corporate\nanarchists hijack a drone and use it's trusted status within the drone's\nnetwork to hack in and take control of the entire CNC drone system to further\ntheir goals.\n\n[1]: http://www.cnn.com/2013/12/02/tech/innovation/amazon-drones-questions/\n[2]: http://rt.com/news/iran-us-drone-gulf-216/\n[3]: http://www.fastcompany.com/3019913/watch-the-skies-tonight-for-a-taco-delivering-drone-brought-to-you-by-taco-bell\n[4]: http://gizmodo.com/5947033/this-team-of-quadrocopters-can-throw-and-catch-better-than-you\n[5]: http://samy.pl/skyjack/\n[6]: http://threatpost.com/how-to-skyjack-drones-in-an-hour-for-less-than-400/103086\n","created_at":1386181095,"fuzzy_word_count":300,"path":"/blog/2013/12/taking-back-the-sky/","published_at":1386181095,"reading_time":1,"tags":["news"],"title":"Taking Back the Sky","type":"blog","updated_at":1386181095,"weight":0,"word_count":200},{"cid":"2a95947f7b4113f23cbd996d602a2284a57976cf","content":"\n# Fail Fast in Bash Scripts\n\nI found myself writing another bash script that should exit should any of the\nfew commands within it fail to run. As I began writing some error handling\nafter each command, and isolating the sections into bash functions I figured\nthere had to be a better way. After a little Googling and a trip through the\nbash man pages sure enough:\n\n```sh\n#!/bin/bash\n\nfunction error_handler() {\n  echo \"Error occurred in script at line: ${1}.\"\n  echo \"Line exited with status: ${2}\"\n}\n\ntrap 'error_handler ${LINENO} $?' ERR\n\nset -o errexit\nset -o errtrace\nset -o errpipe\nset -o nounset\n\necho \"Everything is running fine...\"\n\n# A command outside of a conditional that will always return a exit code of 1\ntest 1 -eq 0\n\necho \"This will never run, as a command has failed\"\necho \"Using unset variable ${TEST} will also cause this script to exit\"\n```\n\nThe first piece of that is setting up an error handler that will get run\nwhenever an error condition occurs with the script. You can use this section to\nroll back any changes or cleanup your environment as well as give you some\ndebug information about the failure.\n\nI'm then setting a few bash options, The following is a description taken more\nor less directly from the bash man pages:\n\n\u003e -o errexit: Exit immediately if a pipeline (which may consist of a single\n\u003e simple command), a subshell command enclosed in parentheses, or one of the\n\u003e commands executed as part of a command list enclosed by braces exits with a\n\u003e non-zero status.\n\u003e\n\u003e -o errtrace: If set, any trap on ERR is inherited by shell functions, command\n\u003e substitutions, and commands execute in a subshell environment.\n\u003e\n\u003e -o nounset: Treat unset variables and parameters other than the special\n\u003e parameters \"@\" and \"*\" as an error when performing parameter expansion.\n\nIf anything goes wrong in the script it will fail once, fail fast, and let you\nknow where it died.\n","created_at":1385497180,"fuzzy_word_count":400,"path":"/blog/2013/11/fail-fast-in-bash-scripts/","published_at":1385497180,"reading_time":2,"tags":["linux","tips"],"title":"Fail Fast in Bash Scripts","type":"blog","updated_at":1385497180,"weight":0,"word_count":340},{"cid":"48560ad747cb9427245245b95f810cda3df87ec7","content":"\n# Using VIM as Your Password Manager\n\nThere are all kinds of password managers out there. Everything from web\nservices that are quite solid and respectable, to [native][1] [desktop][2]\napps.\n\nA lot of these are simply too heavy for me, involve installing software on a\ncomputer to access in addition to sharing the file around, or required you to\nremember multiple account details before you could get access to any individual\npassword.\n\nDue too the various complexities and lack of matching use cases a couple years\nago I set out to develop my own open-source version of PassPack. In the interim\nthough I needed a solution for keeping track of my hundreds of various accounts\nand their passwords.\n\nAround this time I was ramping up my usage of vim and happened to come across a\nvery fortunate command entirely by accident. Enter *vimcrypt*.\n\nFor any plaintext file you, while in command mode you can type the command `:X`\nand it will ask you for a password to encrypt your file with. By default this\nuses a remarkably weak algorithm called [pkzip][4] which isn't secure enough\nfor me to trust it with my keys.\n\nSince vim 7.3 and later, `:X` has also supported an additional cipher; The much\nstronger blowfish algorithm. You can enable this by running the command `:set\ncryptmethod=blowfish`. I chose to add the following lines to my `~/.vimrc`\nfile:\n\n```\n\" When encrypting any file, use the much stronger blowfish algorithm\nset cryptmethod=blowfish\n```\n\nThis was a fantastic interim solution as I have yet to find a development or\nproduction linux system that hasn't been excessively locked down (and probably\nnot somewhere I'd put my password file anyway) that didn't already have vim\ninstalled.\n\nUsing this personally required me coming up with a pseudo-file format that\nwould allow me to quickly and easily find the credentials I needed. I settled\non the simple format shown off below:\n\n```\nOneline Account Description\n  Site: \u003cURL of Site's login page\u003e\n  Username: \u003cusername for the site\u003e\n  Password: \u003cpassword for the site\u003e\n  Email: \u003cemail I used to register\u003e\n\n  Login with: \u003cemail|username\u003e # Only necessary when I have both\n\n  ** Address on file **\n  ** Phone on file **\n```\n\nYou'll notice I also used this to keep track of whether an account had physical\ninformation tied to it. When I moved this made it very quick for me to search\nfor accounts that I needed to update with my new mailing address.\n\nAs with many solutions this \"temporary\" one became more and more permanent as\nmy motivation to build the Passpack competitor dwindled. My problem had been\nsolved and I was no longer compelled to put any effort into a solution.\n\nIf this still isn't strong enough for your tastes, the [vim wiki][5] has some\nadditional ways you can encrypt your files. These all require additional setup\nand failed my requirements in that they generally require additional files or\nsetup before I can access my passwords.\n\nHope this helps some other weary CLI warrior some trouble. Cheers!\n\n***Update***: I received a recommendation from a user named [sigzero][6] over\non Reddit. For additional security they added the following line to their\n`~/.vimrc` file.\n\n```\nautocmd BufReadPost * if \u0026key != \"\" | set noswapfile nowritebackup viminfo= nobackup noshelltemp history=0 secure | endif\n```\n\nIt disables additional files that vim may write copies to such as swap files\nand backups, prevents dangerous shell commands, and prevents vim from storing a\nhistory of commands.\n\n***Update 2***: I received another recommendation from another reddit user,\nthis time from [NinlyOne][7].  At their recommendation, I've prepended the\nfollowing modeline to my password.  It automatically folds each password entry\nto prevent potential shoulder surfing. You can open up an entry using the\ncommand `zo` and close it back up with `zc`. It's worth noting that this is\ntied to my indented file format.\n\n```\n# vim: fdm=indent fdn=1 sw=2:\n```\n\n[1]: https://lastpass.com/\n[2]: http://keepass.info/\n[4]: https://en.wikipedia.org/wiki/PKZIP\n[5]: http://vim.wikia.com/wiki/Encryption\n[6]: http://www.reddit.com/r/vim/comments/1rg3ji/wrote_up_my_thoughts_on_using_vim_as_a_password/cdn20o8\n[7]: http://www.reddit.com/r/vim/comments/1rg3ji/wrote_up_my_thoughts_on_using_vim_as_a_password/cdnn94z\n","created_at":1385410246,"fuzzy_word_count":700,"path":"/blog/2013/11/using-vim-as-your-password-manager/","published_at":1385410246,"reading_time":3,"tags":["linux","security"],"title":"Using VIM as Your Password Manager","type":"blog","updated_at":1385410246,"weight":0,"word_count":632},{"cid":"680a23e0570169b6ea9f6d7d0872749ccb61b9bb","content":"\n# Backing up Gmail with fetchmail\n\nThis morning I found myself in need of a large set of emails to test a\nparticular set of code. Ideally these emails would be broken out into easily\ndigestible pieces, and it was strictly for my own personal testing so I wasn't\nconcerned with using my own live data for this test (There will probably be\nanother post on this project later on).\n\nHaving used `fetchmail` with good results in the past I decided it was a good\nidea to take this opportunity to also backup my Gmail account into the common\n`Maildir` format (which essentially breaks out emails into individual files\nmeeting my requirements).\n\nThe first step was to enable POP access to my account through Gmail's\ninterface. You can accomplish this with the following steps.\n\n1. Login to Gmail\n2. Click on the gear icon\n3. Choose settings\n4. Forwarding and POP/IMAP\n5. Enable POP for all mail\n6. When messages are accessed with POP... Keep\"\n7. Save Changes.\n\nEnsure you have `fetchmail` and `procmail` installed. For me on Fedora this can\nbe accomplished using yum by running the following commands:\n\n```sh\nsudo yum install fetchmail procmail -y\n```\n\nWe need to configure fetchmail to let it know where to retrieve our mail from.\nThis configuration file lives at `$HOME/.fetchmailrc`. By default fetchmail\nwill send all retrieved mail to the local SMTP server over a normal TCP\nconnection. This isn't necessary or ideal, rather we'll additionally supply a\nlocal mail delivery agent (procmail) to handle processing the mail into the\nMaildir format.\n\n```\npoll pop.gmail.com\nprotocol pop3\ntimeout 300\nport 995\nusername \"full_email@withdomain.tld\" password \"yourpassword\"\nkeep\nssl\nsslcertck\nsslproto TLS1\nmda \"/usr/bin/procmail -m '/home/\u003cusername\u003e/.procmailrc'\"\n```\n\nBe sure to set the permissions on the `.fetchmailrc` file to 0600:\n\n```sh\nchmod 0600 $HOME/.fetchmailrc\n```\n\nWe'll now need to configure procmail to properly deliver our mail to the local\n`Maildir` folder. Procmail's configuration by default lives in\n`$HOME/.procmailrc`\n\n```sh\nLOGFILE=$HOME/.procmail.log\nMAILDIR=$HOME\nVERBOSE=on\n\n:0\nMaildir/\n```\n\nWith that done, simply run the `fetchmail` command. In my experience this can\ntake a while process and it seems like Google limits the number of emails you\ncan download at a time, so you may need to run the command a couple of times to\nget all your emails.\n","created_at":1384872940,"fuzzy_word_count":400,"path":"/blog/2013/11/backing-up-gmail-with-fetchmail/","published_at":1384872940,"reading_time":2,"tags":["linux","tips"],"title":"Backing up Gmail with fetchmail","type":"blog","updated_at":1384872940,"weight":0,"word_count":367},{"cid":"6b1effc442ff0d8901e2cd6db546aadb0b0ca353","content":"\n# Ruby's Option Parser - a More Complete Example\n\nRecently while writing a Ruby program I needed to parse some command line\noptions. Helpfully Ruby provides a module named `OptionParser` to make this\neasy. I found a few parts of the documentation ambiguous and a few others down\nright confusing.\n\nThe catch I hit was the required field. In my mind the definition of a required\nargument is something that needs to be passed on the command line to continue.\nWhat`OptionParser` actually means is that a value isn't required when the\nargument is passed.`OptionParser` already provides boolean switches, so when\nsomeone would use an optional switch is beyond me.\n\nTo make it a little more clear and to have something to work from in the future\nI created the following chunk of code that includes a Configuration singleton\nthat can be used anywhere within your codebase to access the run-time\nconfiguration, a sample parser with a wide range of different types of options,\nand it will load configuration from a file named `config.yml` in the same\ndirectory.\n\nI feel like the following is a much more complete explanation of how\n`OptionParser` is supposed to be used with supporting code.\n\n```ruby\n#!/usr/bin/env ruby\n\n# This file provides an example of creating a command line application with a\n# wide variety of command line options, parsing and the like as well as global\n# configuration singleton that can be relied on throughout a program.\n#\n# This entire setup lives within the \"Example\" module. These are really common\n# names and it would be a shame to override required functionality in other code\n# that wasn't properly namespaced.\n\nrequire 'optparse'\nrequire 'singleton'\nrequire 'yaml'\n\nmodule Example\n  # Defines the available configuration options for the configuration\n  ConfigurationStruct = Struct.new(:enum, :list, :required, :optional, :verbose, :float)\n\n  class Configuration\n    include Singleton\n\n    # Initialize the configuration and set defaults:\n    @@config = ConfigurationStruct.new\n\n    # This is where the defaults are being set\n    @@config.enum = :one\n    @@config.list = []\n    @@config.optional = nil\n    @@config.verbose = false\n\n    def self.config\n      yield(@@config) if block_given?\n      @@config\n    end\n\n    # Loads a YAML configuration file and sets each of the configuration values to\n    # whats in the file.\n    def self.load(file)\n      YAML::load_file(file).each do |key, value|\n        self.send(\"#{key}=\", value)\n      end\n    end\n\n    # This provides an easy way to dump the configuration as a hash\n    def self.to_hash\n      Hash[@@config.each_pair.to_a]\n    end\n\n    # Pass any other calls (most likely attribute setters/getters on to the\n    # configuration as a way to easily set/get attribute values \n    def self.method_missing(method, *args, \u0026block)\n      if @@config.respond_to?(method)\n        @@config.send(method, *args, \u0026block)\n      else\n        raise NoMethodError\n      end\n    end\n\n    # Handles validating the configuration that has been loaded/configured\n    def self.validate!\n      valid = true\n\n      valid = false if Configuration.required.nil?\n\n      raise ArgumentError unless valid\n    end\n  end\n\n  class ConfigurationParser\n    def self.parse(args)\n      opts = OptionParser.new do |parser|\n\n        parser.separator \"\"\n        parser.separator \"Specific options:\"\n\n        parser.on(\"--enum ENUM\", [:one, :two, :three], \"This field requires one of a set of predefined values be\", \"set. If wrapped in brackets this option can be set to nil.\") do |setting|\n          Configuration.enum = setting\n        end\n\n        parser.on(\"-l\", \"--list x,y\", Array, \"This command flag takes a comma separated list (without\", \"spaces) of values and turns it into an array. This requires\", \"at least one argument.\") do |setting|\n          Configuration.list = setting\n        end\n\n        parser.on(\"--[no-]verbose\", \"This is a common boolean flag, setting verbosity to either\", \"true or false.\") do |setting|\n          Configuration.verbose = setting\n        end\n\n        parser.on(\"--optional [STR]\", \"This command doesn't require a string to be passed to it, if\", \"nothing is passed it will be nil. No error will be raised if\", \"nothing is passed to it that logic needs to be handled\", \"yourself.\") do |setting|\n          Configuration.optional = setting\n        end\n\n        parser.on(\"-r\", \"--required STR\", \"This command requires a string to be passed to it.\") do |setting|\n          Configuration.required = setting\n        end\n\n        parser.on(\"--float NUM\", Float, \"This command will only accept an integer or a float.\") do |setting|\n          Configuration.float = setting\n        end\n\n        parser.on_tail(\"-h\", \"--help\", \"--usage\", \"Show this usage message and quit.\") do |setting|\n          puts parser.help\n          exit\n        end\n\n        parser.on_tail(\"-v\", \"--version\", \"Show version information about this program and quit.\") do\n          puts \"Option Parser Example v1.0.0\"\n          exit\n        end\n      end\n\n      opts.parse!(args)\n    end\n  end\nend\n\nif File.exists?(\"config.yml\")\n  Example::Configuration.load(\"config.yml\")\nend\n\nExample::ConfigurationParser.parse(ARGV)\nExample::Configuration.validate!\n\nrequire \"json\"\nputs JSON.pretty_generate(Example::Configuration.to_hash)\n```\n","created_at":1354507140,"fuzzy_word_count":900,"path":"/blog/2012/12/rubys-option-parser-a-more-complete-example/","published_at":1354507140,"reading_time":4,"tags":["development","ruby"],"title":"Ruby's Option Parser - a More Complete Example","type":"blog","updated_at":1354507140,"weight":0,"word_count":824},{"cid":"e86ab96d363292c5cde987e69d0b8f4ef8596f81","content":"\n# Keep Your Gems Updated\n\nI recently went back through my backups recently and found quite a few old abandoned projects. Looking back on the code I see some things I'm impressed with, but the majority of the code I wouldn't write today. That's not to say the code is bad, or doesn't function. It did exactly what I wanted to accomplish at the time, just not necessarily in the most efficient way.\n\nThis archive of old code made me start wondering how much old code I'm using in the projects that I'm currently writing. Not code that I've written but code that I'm depending on, specifically gems. As of this writing I have 26 active ruby projects in various states of development all of which make use of RVM and bundler.\n\nConveniently enough, bundler provides an easy way to update all the gems installed in a project unless specific version information was provided in the \"Gemfile\". None of my projects have had a version directly specified in the \"Gemfile\" with the exception of Rails. Each project also has solid test\ncoverage (though I must admit it's usually not complete).\n\nFor each project I went through and ran \"bundle update\" and kept track of the results. I did not keep track of unique gems so the four Rails projects probably had a lot of duplicate gems each one more or less likely to have different versions of different gems installed depending on when I started the project.\n\nAcross all of the different projects I had 2214 gems installed. Of those 813 had updates. My initial plan was to go through the updates and see how many of those updates were security or bug fixes, how many were added features, or performance improvements, but I wasn't counting on the shear number of gems that my projects were depending on.\n\nThe big question for myself after I updated the Gems was how much will this be now? Running through the thousands of tests in all of the projects I had exactly 7 tests that were now failing and they were all due too projects that removed or renamed a piece of functionality that I was making use of. In one case I had to extend the core Hash method to replace the functionality. All in all it took me about a quarter of an hour to fix all the tests after updating my Gems.\n\nSince I didn't actually go through all of the gems I don't know for sure that my projects are in anyway more secure, faster, or more stable but I can't imagine they're in a worse state. If you have test coverage on your projects you should try and update the gems and see for yourself.\n","created_at":1354043820,"fuzzy_word_count":500,"path":"/blog/2012/11/keep-your-gems-updated/","published_at":1354043820,"reading_time":3,"tags":["ruby","security"],"title":"Keep Your Gems Updated","type":"blog","updated_at":1354043820,"weight":0,"word_count":452},{"cid":"9c467f77d83e46934324af395510d4480d6e7e07","content":"\n# Auditing Heroku SSH Keys\n\nA good friend of mine recently left the organization I work for and the task of resetting our passwords and auditing credentials fell on me. Since we use [Heroku](https://www.heroku.com/) for our development platform I needed to not only reset the credentials for the web portion (which conveniently also handles resetting the API key) but also revoke any SSH keys he may have added to access it.\n\nSadly Heroku does not seem to provide any web interface that I could find for examining what keys were associated with the account. Searching for this information also didn't turn up very valuable results; most people were looking to add keys or resolve issues with missing keys rather than revoking them. I suspect not many people think of SSH keys when it comes time to revoke access which is a dire mistake.\n\nI took to the command line to solve my issue as I knew you could list and add keys that way, so it was a minor leap of logic to assume they could revoke keys as well. I ran \"heroku help keys\" to get the syntax for the commands and was pleasantly surprised to see an additional option listed in there:\n\n```console\nkeys:clear       #  remove all authentication keys from the current user\n```\n\nAs a now two person web-shop it's not a terrible amount of work to add our keys back in and looking through there were already some keys in there that should have been revoked long ago. One command and our applications were safe from mischief, though I know my former associate wouldn't abuse that privilege beyond perhaps pointing out the security flaw I'd allowed.\n","created_at":1353943080,"fuzzy_word_count":300,"path":"/blog/2012/11/auditing-heroku-ssh-keys/","published_at":1353943080,"reading_time":2,"tags":["ruby","security"],"title":"Auditing Heroku SSH Keys","type":"blog","updated_at":1353943080,"weight":0,"word_count":274},{"cid":"9da745c171dd64e836a41b0087added33413a153","content":"\n# CarrierWave, S3 and Filenames\n\nThis is going to be a real quick post. I'm using the \"carrier_wave\" gem with\n\"fog\" for one of my projects and found that when a file is stored on S3 the\n\"identifier\", and \"filename\" methods return nil. I got around this issue in two\nseparate ways neither of which I'm particularly happy about.\n\nOutside of the uploader, you can use the File utility and the URL of the object\nto get the base filename like so:\n\n```ruby\nFile.basename(Model.asset.url)\n```\n\nIf you try and do this within the uploader itself like this:\n\n```ruby\nFile.basename(self.url)\n```\n\nIt will work, but not when creating additional versions such as thumbnails as\nthe file hasn't actually been created yet so a URL can't be built and you'll\nget an error trying to perform `File.basename(nil)`. You'd need to go back up\nto the model and get the normal version's URL like so:\n\n```ruby\nFile.basename(self.model.asset.url)\n```\n\nNow if you're trying to get the file name to build part of the store_dir,\nyou've just created an infinite loop! Ruby will be happy to tell you that the\nstack level too deep (`SystemStackError`). So ultimately how did I end up\ngetting it into my store_dir?\n\n```ruby\nself.model.attributes[\"asset\"]\n```\n\nThe file name gets stored raw directly in the database, and thus you can pull\nit out by accessing the value directly without going through the accessor that\nget overridden by CarrierWave. I'm pretty sure this is a bug, and will report\nit with example code and a test (as is appropriate for any bug report *hint*)\nas soon as my dead line has passed.\n","created_at":1344542457,"fuzzy_word_count":300,"path":"/blog/2012/08/carrierwave-s3-and-filenames/","published_at":1344542457,"reading_time":2,"tags":["development","ruby"],"title":"CarrierWave, S3 and Filenames","type":"blog","updated_at":1344542457,"weight":0,"word_count":261},{"cid":"ca97c064f3a61ffac30ef6033838e0e67e197147","content":"\n# Security Through Obesity\n\nJeremy Spilman recently [proposed changes][1] to how user's hashes are stored\nin website's and companies databases. This post was originally going to look at\nsome of the issues involved in the scheme he envisioned, however, he rather\nquickly posted a [followup article][2] with a well thought out solution that\ncountered all of the issues that other people and myself were able to come up\nwith. I'd strongly recommend reading both if you haven't done so. Instead of\nannouncing flaws, I'm turning this into a post with a simple functional\nimplementation of the described scheme in Ruby using DataMapper.\n\nAt first I'd like to point out that this is one of those few examples where a\nform of security through obscurity is actually increasing not only the\nperceived security but the cost to attack a system as well.\n\nPlease note this code is a minimal, functional, example and should not be used\nin production. It is missing a lot of things that I personally would add before\nattempting to use this but that is an exercise for the reader. It is licensed\nunder the MIT license. I'll walk through the code briefly afterwards going over\nsome bits.\n\n```ruby\n# encoding: utf-8\n\nrequire \"rubygems\"           # You only need this if you use bundler\nrequire \"dm-core\"\nrequire \"dm-migrations\"\nrequire \"dm-sqlite-adapter\"\nrequire \"dm-validations\"\nrequire \"scrypt\"\n\nDataMapper.setup :default, \"sqlite:hash.db\"\n\nclass User\n  include DataMapper::Resource \n\n  property :id,             Serial\n  property :username,       String, :required =\u003e true,\n                                    :unique =\u003e true \n  property :crypt_hash,     String, :required =\u003e true,\n                                    :length =\u003e 64\n  property :salt,           String, :required =\u003e true,\n                                    :length =\u003e 25 \n\n  def check_password(plaintext_password)\n    encrypted_hash = scrypt_helper(plaintext_password, self.salt)\n    hash_obj = SiteHash.first(:crypt_hash =\u003e encrypted_hash)\n\n    if hash_obj.nil?\n      puts \"Invalid password\"\n      return false\n    end\n\n    verification_hash = scrypt_helper(plaintext_password, hash_obj.salt)\n\n    if self.crypt_hash == verification_hash\n      return true\n    else\n      puts \"WARNING: Found matching hash, but verification failed.\"\n      return false\n    end\n  end\n\n  def password=(plaintext_password)\n    generate_salt\n\n    encrypted_password = SiteHash.new\n    encrypted_password.crypt_hash = scrypt_helper(plaintext_password,\n                                                  self.salt)\n    encrypted_password.save\n\n    self.crypt_hash = scrypt_helper(plaintext_password,\n                                    encrypted_password.salt)\n  end\n\n  private\n\n  def generate_salt\n    self.salt = SCrypt::Engine.generate_salt(:max_time =\u003e 1.0)\n  end\n\n  def scrypt_helper(plaintext_password, salt)\n    SCrypt::Engine.scrypt(plaintext_password, salt,\n                          SCrypt::Engine.autodetect_cost(salt),\n                          32).unpack('H*').first\n  end\nend\n\nclass SiteHash\n  include DataMapper::Resource\n\n  property :id,             Serial\n  property :crypt_hash,     String,   :required =\u003e true,\n                                      :length =\u003e 64\n  property :salt,           String,   :required =\u003e true,\n                                      :length =\u003e 25\n\n  def initialize(*args)\n    super\n    generate_salt\n  end\n\n  private\n\n  def generate_salt\n    self.salt = SCrypt::Engine.generate_salt(:max_time =\u003e 1.0)\n  end\nend\n\nDataMapper.finalize\nDataMapper.auto_upgrade!\n```\n\nI tried to keep this as a simple minimum implementation without playing golf.\nStrictly speaking the validations on the data_mapper models aren't necessary\nand could have been removed, in this case, however, the length fields do\nactually indicate a bit more of what you might expect to see in the database,\nwhile the requires are just good habits living on.\n\nBoth of the two models are required to have both a salt and a hash, the name\n'crypt_hash' was chosen do too a conflict with one of data_mapper's reserved\nwords 'hash', the same goes for the model name, however, that class comes from\nelsewhere. Raw scrypt'd hashes are 256 bits long or 64 hex characters long,\nwhile the salts are 64 bits (16 hex characters) plus some meta-data totaling 25\nhex characters in this example.\n\nSalts are hashes are computed by the 'scrypt' gem. In this example I've bumped\nup the max time option to create a hash from the default of 0.2 seconds up to 1\nsecond. This is one of those things that I could have left out as the default\nis fine for an example, but it also couldn't hurt slightly increasing it in\ncase someone did copy-paste this into production.\n\nThe one thing that I'd like to point out is a couple of 'puts' statements I\ndropped in the check_password method on the User model. The first one simply\nannounces an invalid password. A lot of these could indicate a brute force\nattack. The second one is more serious, it indicates that there is either a bug\nin the code, a hash collision has occurred, or an attacker has been able to\ndrop in hash of their choosing into the site_hashes table, but haven't updated\nthe verification hash on the user model yet. I'd strongly recommend reading\nthrough both of Jeremy's posts if you want to understand how this threat works\nand specifically the second post to see how the verification hash protects what\nit does.\n\nSo how would you use this code? Well you'd want to create a user with a\npassword and then check if their password is valid or not later on like so:\n\n```ruby\nUser.create(:username =\u003e 'admin', :password =\u003e 'admin')\nUser.first(:username =\u003e 'admin').check_password('admin')\n```\n\nOne of the key ways this separation increases the security of real users hashes\nis by having a large number of fake hashes in the hash table that the attackers\nwill have to crack at the same time. As a bonus I've written a module to handle\njust that for the code I've already provided. Once again this is licensed under\nthe MIT license and should not be considered production ready.\n\n```ruby\n# This is the code above, you can also include everything below\n# this in the same file if you're into that sort of thing\nrequire \"user_hash_example\"\n\nmodule HashFaker\n  def self.fast_hash\n    SiteHash.create(:crypt_hash =\u003e get_bytes(32))\n  end\n\n  def self.hash\n    SiteHash.create(:crypt_hash =\u003e scrypt_helper(get_bytes(24),\n                                                 generate_salt))\n  end\n\n  def self.generate_hashes(count = 5000, fast = false)\n    count.times do\n      fast ? fast_hash : hash\n    end\n  end\n\n  private\n\n  def self.generate_salt\n    SCrypt::Engine.generate_salt(:max_time =\u003e 1.0)\n  end\n\n  def self.get_bytes(num)\n    OpenSSL::Random.random_bytes(num).unpack('H*').first\n  end\n\n  def self.scrypt_helper(plaintext_password, salt)\n    SCrypt::Engine.scrypt(plaintext_password, salt,\n                          SCrypt::Engine.autodetect_cost(salt),\n                          32).unpack('H*').first\n  end\nend\n```\n\n[1]: http://www.opine.me/a-better-way-to-store-password-hashes/\n[2]: http://www.opine.me/all-your-hashes-arent-belong-to-us/\n","created_at":1344438416,"fuzzy_word_count":1100,"path":"/blog/2012/08/security-through-obesity/","published_at":1344438416,"reading_time":5,"tags":["development","ruby","security"],"title":"Security Through Obesity","type":"blog","updated_at":1344438416,"weight":0,"word_count":1021},{"cid":"ec068df397d1428c038be1c336ddfe4c1841ff12","content":"\n# Adding a Table Prefix to DataMapper Tables\n\nSo I recently encountered a situation where I needed to define a prefix on the\ntables used by the \"data_mapper\" gem. When I went searching I found quite a bit\nof information about similar projects in Python, and PHP named DataMapper but\nnothing about the ruby \"data_mapper\". The search continued eventually ending in\nmy reading through the source of the data_mapper gem only to find that there\nwas no feature for simply defining a prefix.\n\nReading through the source though did allow me to find any easy way to\nimplement such functionality. The following snippet is a minimalist data_mapper\ninitialization and setup of one model with a table prefix of \"source_\" (chosen\nat random and of no significance).\n\n```ruby\n# encoding: utf-8\n\n# (1)\nrequire \"dm-core\"\nrequire \"dm-migrations\"\n\n# (2)\nmodule PrefixNamingConvention\n  def self.call(model_name)\n    # (3)\n    prefix = \"source_\"\n    # (4)\n    table_name = DataMapper::NamingConventions::Resource::UnderscoredAndPluralized.call(model_name)\n\n    \"#{prefix}#{table_name}\"\n  end\nend\n\n# (5)\nDataMapper::Logger.new($stdout, :debug)\n\n# (6)\nDataMapper.setup(:default, \"sqlite:example.db\")\nDataMapper.repository(:default).adapter.resource_naming_convention = PrefixNamingConvention\n\n# (7)\nclass Person\n  include DataMapper::Resource\n\n  property :id, Serial\n  property :first_name, String\n  property :last_name, String\n  property :email, String\nend\n\n# (8)\nDataMapper.finalize\nDataMapper.auto_upgrade!\n```\n\nSo here are some notes on what's going on in this snippet. Each area that I\nwill be discussing has been annotated with a number like \"# (1)\" to make it\neasier to find a section you have questions about.\n\n1. Since this is an example I'm only including the bare minimum data mapper\n   gems to accomplish the task. If you're using bundler you may need to also\n   require \"rubygems\" to get this too work.\n2. This is where the real work happens, DataMapper uses external modules that\n   receive the \"call\" method to handle the conversion of class names to table\n   names. By default DataMapper uses the module\n   \"DataMapper::NamingConventions::Resource::UnderscoredAndPluralized\", which\n   I'll use later to maintain the same names.\n3. This is where I'm defining the table prefix. This could be defined in a\n   global, call another method or class, whatever your heart desires to get a\n   string that will be used as a prefix.\n4. Here I'm getting what DataMapper would have named the table if I wasn't\n   interferring\n5. I'm logging to standard out so that I can see the queries called to verify\n   that DataMapper is creating tables with the names that I want. This is used\n   later on in this post to demonstrate this solution working, however, it\n   could be left out without affecting anything.\n6. Initial setup of a sqlite database, and then the good stuff. Once a database\n   has been setup with a specific adapter you can change the naming convention\n   DataMapper will use to generate table names. This is accomplished by passing\n   the module constant name through the repositories adapter and too\n   \"resource_naming_convention\" as demonstrated in the code.\n7. Here I'm defining an example model of no importance. This is purely for\n   demonstration, normally DataMapper would name this model \"people\".\n8. Inform DataMapper we're done setting it up and to run the migrations to\n   create the model defined.\n\nWhen you run this ruby file (assuming you have the \"data_mapper\" and\n\"dm-sqlite-adapter\" gem installed) you'll see output very similar too this:\n\n```\n~ (0.001402) PRAGMA table_info(\"source_people\")\n~ (0.000089) SELECT sqlite_version(*)\n~ (0.077840) CREATE TABLE \"source_people\" (\"id\" INTEGER NOT NULL PRIMARY KEY AUTOINCREMENT, \"first_name\" VARCHAR(50), \"last_name\" VARCHAR(50), \"email\" VARCHAR(50))\n```\n\nNotice the third line? Specifically the name of the table? It's named exactly\nas it would have been except now it has a prefix of \"source_\".\n\nHope this saves someone else some trouble. Cheers!\n","created_at":1344348573,"fuzzy_word_count":700,"path":"/blog/2012/08/adding-a-table-prefix-to-datamapper-tables/","published_at":1344348573,"reading_time":3,"tags":["development","ruby"],"title":"Adding a Table Prefix to DataMapper Tables","type":"blog","updated_at":1344348573,"weight":0,"word_count":605},{"cid":"3d07fe55992222ce9681baf9df6a916a2f696076","content":"\n# Wireless Troubleshooting\n\n## BCM4311\n\nThere is a known issue with Fedora 15+ and BCM4311 wireless cards where the wireless will connect, stay connected for a few minutes, then disconnect until either the machine is rebooted or wpa_supplicant and NetworkManager are rebooted. The latter does not always solve the problem. I've personally observed this issue being more prominent when large file transfers are taking place.\n\nBefore taking these steps you should verify that you do indeed have a Broadcom BCM4311 wireless card. This can be done with the following command:\n\n```console\n$ lspci | grep -i bcm4311\n```\n\nYou should see something along the following if you have the card:\n\n```console\n0c:00.0 Network controller: Broadcom Corporation BCM4311 802.11b/g WLAN (rev 01)\n```\n\nEnsure you have a wired connection as you will lose access to the wireless card during this process. You have been warned :). Please also note that the following commands (with the exception of the reboots) won't do anything unless you have the RPMFusion repository installed.\n\nAs root install the akmod-wl package and reboot like so:\n\n```console\n$ yum install akmod-wl -y\n$ reboot\n```\n\nOnce the system has finished rebooting, perform a system update which will grab a few additional packages, and reboot once again:\n\n```console\n$ yum update -y\n$ reboot\n```\n\nIf you are unable to see the wireless card then this solution probably hasn't worked for you. You can bring the card back online by running the following as root:\n\n```console\n# modprobe b43\n```\n","created_at":1342483200,"fuzzy_word_count":300,"path":"/notes/wireless-troubleshooting/","published_at":1342483200,"reading_time":2,"tags":["linux","fedora","wireless"],"title":"Wireless Troubleshooting","type":"notes","updated_at":1342483200,"weight":0,"word_count":243},{"cid":"156b55b41a8025972e4a4fc84da1f0f9769480c9","content":"\n# Ruby's XMLRPC::Client and SSL\n\nFor the past few days I've been working on a Ruby project that needed to\ninteract with a remote XMLRPC API. This isn't particularly unusual but it was\nthe first time from within a Ruby application. Luckily enough Ruby has a built\nin XMLRPC client that handles a lot of the messy bits.\n\nThe XMLRPC::Client class itself seems fairly simple. There are only a handful\nof methods, five of which are for opening a new connection in a few different\nways, and at least two ways to open each type of connection.\n\nAs a starting point this was a simplified chunk of code that I was using to\nconnect to the remote API:\n\n```ruby\nrequire 'xmlrpc/client'\n\nclass APIConnection\n  def initialize(username, password, host)\n    # Build the arguments for the XMLRPC::Client object\n    conn_args = {\n      :user =\u003e username,\n      :password =\u003e password,\n      :host =\u003e host,\n      :use_ssl =\u003e true,\n      :path =\u003e \"/api\"\n    }\n\n    @connection = XMLRPC::Client.new_from_hash(conn_args)\n  end\n\n  def version\n    @connection.call(\"version\")\n  end\nend\n```\n\nThe problem I ran into was when connecting to a server using HTTPS. I knew that\nthis certificate was good however I continued to get the message:\n\n```\nwarning: peer certificate won't be verified in this SSL session\n```\n\nRuby has taken the approach of by default not including any trusted certificate\nauthorities which I greatly appreciate especially considering that in 2010 and\n2011 12 certificate authorities were known to have been hacked including major\nones such as VeriSign, and [DigiNotar][2]. Some of which were [proven][3] to\nhave issued false certificates.\n\nSince XMLRPC::Client doesn't expose it's SSL trust settings through it's\nmethods I went on a bit of a journey through Google to find an answer. What I\nfound was overly disturbing, a lot of people don't seem to understand what SSL\nis actually for. The solutions I found from the most egregious to least:\n\n* Disabling OpenSSL certificate checking globally with\n  OpenSSL::SSL::VERIFY_NONE\n* Overriding the Net::HTTP certificate checking\n* Disabling OpenSSL certificate checking locally by extending XMLRPC::Client\n  and over-riding how it was establishing connections\n* Using an SSL stripping proxy\n\nI couldn't find a solution out there that didn't the security conscious voice\nin my head scream in despair. I asked on [StackOverflow][4] for a good\nsolution. When I asked I didn't have a good grasp on how Ruby was handling SSL\ncertificates at all. The thorough answer from [emboss][5] didn't quite answer\nmy question but it gave me more than enough to really hunt down what I wanted.\n\nFirst stop, I needed the certificates that I'll be using to verify the\nconnection. Every single certificate authority that issues certificates for\npublic websites makes the public portion of their certificates available and\nthis is what we need to verify the connection. To find out which ones you\nspecifically need you can go to the API server's address and look at it's\ncertificate information by clicking on the site's lock icon. Every browser is a\nlittle different so you'll have to find this out on your own. With Chrome (and\nperhaps others) you can download each of the certificates in the chain that\nyou'll need to verify the server's certificate.\n\nThe server I was connecting to was using a [RapidSSL][6] certificate, who has\nbeen verified by [GeoTrust][7]. You want to grab their certificates base64\nencoded in PEM format. Stick them all in a `ca.crt` file.\n\nHow do we get XMLRPC::Client to actually use that information without hacking\nit all to pieces? Net::HTTP has a few methods that allow you to set the\nappropriate connection settings and XMLRPC::Client uses Net::HTTP. If\nXMLRPC::Client allowed to you specify this directly somehow I would've been a\nlot happier.\n\nHere's that code snippet again, this time forcing certificate verification with\nthe `ca.crt` file. This code assumes that the `ca.crt` file lives in the same\ndirectory as the connection script:\n\n```ruby\nrequire 'xmlrpc/client'\n\nclass APIConnection\n  def initialize(username, password, host)\n    # Build the arguments for the XMLRPC::Client object\n    conn_args = {\n      :user =\u003e username,\n      :password =\u003e password,\n      :host =\u003e host,\n      :use_ssl =\u003e true,\n      :path =\u003e \"/api\"\n    }\n\n    @connection = XMLRPC::Client.new_from_hash(conn_args)\n\n    @connection.instance_variable_get(\"@http\").verify_mode = OpenSSL::SSL::VERIFY_PEER\n    @connection.instance_variable_get(\"@http\").ca_file = File.join(File.dirname(__FILE__), \"ca.crt\")\n  end\n\n  def version\n    @connection.call(\"version\")\n  end\nend\n```\n\nThose last two lines in the initialize method first dive into the connection\nwe've already setup (but before it's been called), grab the of Net::HTTP and\ntells it to force peer verification and to use the certificate file we created\nbefore. No more warning, and we're actually safe.\n\n[2]: http://www.symantec.com/connect/blogs/diginotar-ssl-breach-update\n[3]: http://nakedsecurity.sophos.com/2011/08/29/falsely-issued-google-ssl-certificate-in-the-wild-for-more-than-5-weeks/\n[4]: http://stackoverflow.com/questions/9199660/why-is-ruby-unable-to-verify-an-ssl-certificate\n[5]: http://stackoverflow.com/a/9238221/95114\n[6]: http://www.rapidssl.com/\n[7]: http://www.geotrust.com/\n","created_at":1329242931,"fuzzy_word_count":800,"path":"/blog/2012/02/rubys-xmlrpcclient-and-ssl/","published_at":1329242931,"reading_time":4,"tags":["development","ruby"],"title":"Ruby's XMLRPC::Client and SSL","type":"blog","updated_at":1329242931,"weight":0,"word_count":785},{"cid":"1cf77b45d7dcb76db77c2fbe7c6d889944427a69","content":"\n# Exploration of an ACN Iris 3000\n\nSo I found a dirt cheap video SIP phone (ACN Iris 3000) at a local HAM fest. After looking around I found the vendor has locked in the phone with their specific service with an iron grip and had gone out of business. I guess I should expect that kind of anti-competitive behavior from a business that Donald Trump has a vested interest in.\n\nI've come across one post on a forum that seems to have been crawled and copied out every where. The poster had cracked it and got it working with an Asterisk server which is what my ultimate goal for this phone is, however they claim to have done it by getting root through telnet. The problem being that port 23 (telnet) is not open so this was a dead end.\n\nThis is a running document of how I'm doing it, you'll notice that I'm writing this as I go.\n\n## Reconnaissance\n\nFirst thing's first a little run down of what I've found. I can change the network address and the way it's handling networking (either bridged or NAT). I can not get into the Administrators menu which is where all the juicy bits seem to be. I've found that the factory reset code is 7517517, though that doesn't get me anything beyond cleaning up it's last known phone number.\n\nThrough a very thorough nmap scan I've found that ports TCP 21, 79, 113, 513, 514, 554, 5060, 7022, and 8080 are all open. There doesn't appear to be any UDP ports available which actually surprised me, since SIP over UDP is pretty common.\n\n7022 and 8080 both immediately caught my eye. 7022 looks like someone moved SSH (port 22) to a non-standard port, and 8080 is a very common alternate port for HTTP. Connecting to 7022 via telnet confirmed my suspicions of SSH. I received this prompt:\n\n```console\nConnected to 10.0.0.85.\nEscape character is '^]'.\nSSH-2.0-dropbear_0.45\n```\n\nBingo. SSH it is, and an old version of dropbear at that.  Unfortunately as the one poster I found said the password was neither blank nor 'root'. I suspect that they had an older firmware revision and these 'bugs' were ironed out in a later revision. That's OK though it'll just take a bit more work.\n\nAs for port 8080 it is definitely running a web configuration interface. All it asks for is a password (which we don't have). The extension for the login page (esp) makes me suspect that the Iris device is running a copy of AppWebServer or something similar and using embedded javascript as the server side processing. For now that doesn't provide much but it could be very useful later on.\n\n## Attack\n\nSo while looking for an exploit for DropBear 0.45 I started up a SSH dictionary attack and encountered by first real problem. The screen started blinking while running three or more threads trying to break in, at first I thought it was kind of funny but then it turned off completely.\n\nTurns out the adapter I have for it is only rated for pushing out 500mA and the phone itself takes up to 1500mA, apparently I hit that limit and browned-out the phone. It still seems to work but if I want to take this route I'll need a more robust power supply. Looking around I found a 1500mA supply and after checking the boards for damage I gave it a shot and everything seems to be working OK.\n\nUnfortunately I wasn't able to find any viable exploits for that particular DropBear version as the vulnerabilities that had been found were either DoS vulnerabilities or were only useful with valid credentials.\n\nThe basic dictionary attack failed and I started up a more comprehensive one. I could easily start brute forcing this but it would take a very long time, especially if the company realized that a weak password wasn't cutting it.\n","created_at":1304264888,"fuzzy_word_count":700,"path":"/blog/2011/05/exploration-of-a-acn-iris-3000/","published_at":1304264888,"reading_time":4,"tags":["hardware"],"title":"Exploration of an ACN Iris 3000","type":"blog","updated_at":1304264888,"weight":0,"word_count":646},{"cid":"71ecbdcc20e98a4a46bc017572788b77322d1507","content":"\n# Linux N Issues \u0026 KDE Multi-Monitor Woes\n\nSo I recently did a fresh install of Fedora 14 with KDE installed (not the KDE spin mind you) on my ThinkPad. I'm pleasantly surprised with hows it's working everything seems to be working out the box very stably. I used it without issue for a solid month and a half without a single issue.\n\nEarlier this week I started having issues with my wireless card on some networks, but not at all of them. The most prominent one being my home network. I've had issues with my access point dropping connections before on a wide array of machines and not actually dropping it (IE: my laptop would see it as connected but the AP wouldn't exchange traffic with it). So when I started seeing this behavior I expected that issue to have cropped up again.\n\nThe actual behavior that I was witnessing was this:\n\n1. Connect to wireless network\n2. Use the connection for 5-10 seconds\n3. Pages would start timing out even though the connection was still 'active'\n\nDisconnecting and reconnecting to the wireless would start the situation all over again, which quickly became frustrating but I didn't have time to mess with it so I just plugged into an ethernet port and went about my business.\n\nThe next day I received my the logwatch email from my laptop and it mentioned more than 20,000 new entries of an error I've never seen before:\n\n```text\niwlagn 0000:03:00.0: BA scd_flow 0 does not match txq_id 10\n```\n\nAfter poking around a bit online I found that the issue is with a recent kernel update (I'm currently running 2.6.35.11-83) changing the behavior of some sanity checks to wireless connections that support 'N'. Turns out my wireless card and all the networks I was having issues with support 'N'. Good to know.\n\nIt was easy enough to solve that issue, the kernel module just needed an option passed to it that I believe just disables 'N'. This is all well and good but I haven't had to manually pass options to a kernel module for a couple of releases now (I think the last was Fedora 10). Since then the file \"/etc/modules.conf\" has been deprecated in favor of placing files in \"/etc/modules.d/\". There are some files that come stock in there but none are passing parameters to modules and the naming scheme doesn't seem to conform to anything.\n\nI was unsure if there was something specific I had to name the file or if it needed to be in one of the existing files. I ended up creating the file \"/etc/modprobe.d/iwlagn.conf\" and putting the following in it:\n\n```modconf\noptions iwlagn 11n_disable=1\n# This one might be needed instead\n#options iwlagn 11n_disable50=1\n```\n\nAfter I rebooted the problem vanished like it was never there. If you notice there is a second option in there that is commented out. I found some people where the first option didn't work but replacing it with that second option did, so if one doesn't work for you try the second option.\n\nThe second issue that I encountered was just this morning. I'm working at a remote site today that I'm not at very often, and am using my laptop as my desktop workhorse. Usually when I'm at a remote site, I steal an office that isn't in use and claim it as my own, and today was no different. There was a screen in this office that had its power plugged it but it's VGA cable was just sitting there. I figured 'why not?' so I plugged it in and KDE happily announced that it detected a new display and offered to take me to the settings interface that would allow me to configure it.\n\n\"Awesome!\" I thought, multiple desktops has always been one of those things I had to tweak and search around for and it looks like KDE is making some serious strides in their support for it. When I turned it on I wasn't really paying attention to the settings and my laptop display ended up on the wrong side of the screen and my primary desktop on the LCD. Not exactly what I wanted but it's cool that it was that easy to setup.\n\nI went back into the configuration options switched things around, but no matter what I did, the 'primary desktop' was always on the external monitor. What's more is that there isn't any option for selecting the primary display! That *used to* be there...\n\nI hunted around before getting frustrated and searching around online. Sure enough other people were annoyed by this regression but the solution was very easy (though it appears you have to do it every time).\n\nTo set the primary monitor to my laptop screen (LCDS1) I just opened a shell and put this in:\n\n```console\n$ xrandr --output LCDS1 --primary\n```\n\nPoof! Everything is all set and I'm happy once again. I hope that the KDE developers put back the primary display selection in the settings but for now it's easy enough. Hopefully this will help other people on the net.\n","created_at":1298644659,"fuzzy_word_count":900,"path":"/blog/2011/02/linux-n-issues-kde-multi-monitor-woes/","published_at":1298644659,"reading_time":5,"tags":["kde","linux","operations"],"title":"Linux N Issues \u0026 KDE Multi-Monitor Woes","type":"blog","updated_at":1298644659,"weight":0,"word_count":863},{"cid":"67c88a0d232a7eb6a25b79ddc879ee727ac80603","content":"\n# The Home Network and NAT as a Security Layer\n\nOne of the hot-topics for IPv6 (which I have been thinking about a lot lately) is NAT. I normally wouldn't go into detail about specifics that are obvious to people in my field but for the sake of this post I will. NAT or Network Address Translation, is a way for a large number of computers to share a single public IP address.\n\nThe router that is handling the NAT will keep track of connections coming in and out of it and re-write the destination IP to an internal address to keep the traffic flowing.\n\nNAT was necessary with IPv4 because IPv4 only had 4,294,967,296 addresses on the internet and quite a few those were unroutable or reserved. With a world population of 7 billion only half the population of the planet could have a single device online at a time. IPv6 solves this issue by increasing the number of public routable address to 3.4×10^38. That means that each person alive could have 4.9x10^28 addresses online at any given time.\n\nSo what does this mean for NAT? Clearly we don't need it any more right? There is no way I'll ever use 4.9x10^28 addresses. I'm willing to bet Google doesn't own that many machines. Well this is where the debate starts. NAT was never designed to be used as a security tool and it has even had some security ramifications because of it.\n\nThe weakness of NAT is also it's primary strength. What do I mean by that? You can't attack a computer if you can't talk to it. In my opinion, this alone has protected innumerable regular home users from all kinds of terrible things online. A standard COTS router that comes with most internet connections will stop port-scans and automated attack tools at the door.\n\nSounds good right? So why would people be opposed to it? Simple. It complicates things. There are a limited number of simultaneous connections that can go through a single NAT device. This hard limit of 65,536 connections (in reality this number is an order of magnitude less - 32,768) isn't changed between IPv4 and IPv6 and there really isn't a good reason to change it. Sounds like a lot but trust me it gets used up quickly.\n\nThere is also identity reasons, behind a NAT could be 100 people or 1 and to the rest of the world it will all look the same. If someone breaks into a home network there isn't any way to differentiate that cracker from a normal user to the outside world. This privacy also gives home users plausible deniability for anything that happens on their network.\n\nBut those arguments against have very little to do with security. So what is all this hype about NAT being a form of security through obscurity? The argument I come across whenever I ask naysayers about NAT, is that if a user gets infected then the network can still be enumerated behind the NAT as if all the computers were on the Internet. This argument has one fatal flaw. It is depending on a user to get infected. A firewall has this exact same \"vulnerability\" so would they argue that a firewall is not a layer of security? I thought not.\n\nNAT has it's problems, but claiming it is not a security layer is just plain wrong. IPv6 is here to stay and we should really start looking at the security implications of everything involving it. New security models need to be created and lots of of research needs to be done in this area still. In the mean time I suspect a lot of malware and viruses will start making use of IPv6 and how relatively unknown it really is.\n","created_at":1297966502,"fuzzy_word_count":700,"path":"/blog/2011/02/the-home-network-and-nat-as-a-security-layer/","published_at":1297966502,"reading_time":3,"tags":["networking","security"],"title":"The Home Network and NAT as a Security Layer","type":"blog","updated_at":1297966502,"weight":0,"word_count":620},{"cid":"8546300aa76c26abfeacdfca9ce90c6ad8653c07","content":"\n# Image Crawler Meets rm -f *\n\nI wrote a simple web crawler that archived any images it found from a site with a large number of backgrounds. I wanted to have rolling backgrounds that almost never repeated.\n\nI let my crawler go and stopped it after a 24 hour period. I listed the content of the directory the images were being saved in to see the results and my ssh session locked up... Or so I thought. I hit Ctrl-C and nothing happened... So I closed my window and opened a new one.\n\nMaybe there was a file name that hit some weird glitch in ls causing it too lock up. Not a big deal I can start from scratch. I switched to the directory and tried to delete all the files inside the directory:\n\n```console\n$ rm -f *\n/bin/rm: Argument list too long.\n```\n\nThis elusive error message doesn't give you a whole lot to work with. The '*' was being expanded, and making the list too long? After hunting in the man pages, then the info pages before finding this sentence that may have been relevant:\n\n\u003e GNU 'rm', like every program that uses the 'getopt' function to parse its\n\u003e arguments...\n\nAfter some creative Googling I found the answer: getopt's argument limit is 1024. How many files did I have? I wanted to give ls one more try... I typed it in and sure enough console froze... Or did it? I walked away and did other things. When I came back I had a list of files longer than my console buffer. I was smarter the second time which still took about five minutes:\n\n```console\n$ ls | wc -l\n177654\n```\n\nThat was it indeed, but I needed to come up with an alternate way to clean up this directory\n\n```console\n$ find . -exec rm -f {} \\;\n```\n\nVictory! The directory is clean and happy again. Now I'll just have to organize those downloaded files into a directory hierarchy so the terminal is still useful to interact with them...\n","created_at":1257370605,"fuzzy_word_count":400,"path":"/blog/2009/11/image-crawler-meets-rm-f/","published_at":1257370605,"reading_time":2,"tags":["linux","tips"],"title":"Image Crawler Meets rm -f *","type":"blog","updated_at":1257370605,"weight":0,"word_count":337},{"cid":"f06eb3af4fc3acb685c0553c3b02c9ef114d5776","content":"\n# About Myself\n\nI never know how to describe myself appropriately, but I like to think I'm a welcoming, encouraging, cheerful, and reasonable intelligent soul. I read everything I can get my hands on, create things using many tools of mind and hand, teach where I can, laugh as mush as possible, and always makes time for friends and nature.\n\nIf you're looking for a my professional history, I encourage you to peruse my [LinkedIn](https://www.linkedin.com/in/samstelfox) profile. If you'd like to get a hold of me to ask about a project, myself, or just want a friend you can reach me by:\n\n* send me a message [via email](mailto:sam@stelfox.net) (Probably the best choice)\n* toot at me on [Mastodon](https://infosec.exchange/@pasties)\n* ping me on [Github](https://github.com/sstelfox)\n\nI'd kindly like recruiters, and those looking to sell me goods or services to refrain from reaching out. I exclusively use personal referrals or my own research for products and services and do not appreciate the spam.\n\n## Like my Work?\n\nAll of the open source stuff I work on is usually licensed MIT or APLv2 and I almost always include license information in the root of my repository. Detailed license information [for this site]({{\u003c ref \"licenses\" \u003e}}) is also available.\n\nI currently don't accept donations for any of my work as I see it as my contribution to the betterment of the internet but reach out if you'd like to sponsor a project or work together on something.\n","created_at":-62135596800,"fuzzy_word_count":300,"path":"/about/","published_at":1721005407,"reading_time":2,"tags":null,"title":"About Myself","type":"page","updated_at":1721005407,"weight":0,"word_count":231},{"cid":"d4f719bf06c1baba63e048b5ab877339d914df9b","content":"\n***Note: This page is quite old and is likely out of date. My opinions may have\nalso changed dramatically since this was written. It is here as a reference\nuntil I get around to updating it.***\n\nAsterisk is a software implementation of a telephone private branch exchange\n(PBX) originally created in 1999 by Mark Spencer of Digium. Like any PBX, it\nallows attached telephones to make calls to one another, and to connect to\nother telephone services including the public switched telephone network (PSTN)\nand Voice over Internet Protocol (VoIP) services. Its name comes from the\nasterisk symbol, “*”.\n\nNOTE: FreeSWITCH may be a solid replacement for asterisk as it has support for\nthe Linksys SPA3000 as well as ZRTP and SRTP support. It might be wise to look\ninto using kamailio as a front end SIP router though this doesn't seem to be\nnecessary unless we want to start handling multi-thousands of calls\nconcurrently.\n\nA few [references][2] for help when building FreeSWITCH.\n\n## Previous Reasoning\n\nThis was setup on my private network to provide a land line phone to my office\nusing our existing internet connection. I wanted to go about doing this without\nhaving to pay for the services as I don't expect this to get a lot of use.\n\nThis Asterisk setup involves the use of [Google Voice][3] and [SIPGate][4].\nSIPGate provides a single phone number, SIP connectivity and call forwarding.\nGoogle Voice doesn't provide services to receive phone calls, however, it can\nbe setup as an intermediary that will call you and then connect to another\nnumber on your behalf, while also providing call forwarding.\n\nCombining the three will get free incoming and outgoing to anywhere in the US\nand Canada. Incoming calls will come from Google Voice, which will forward the\ncall to the SIPGate phone number, which the Asterisk PBX will be tied to and\nwe'll in turn be able to receive the call.\n\nOutgoing calls are a bit more tricky. Implemented using [pygooglevoice][5], the\nAsterisk box will actually connect to Google Voice's APIs to dial the number\nand call back (making it an incoming call and thus free) to make the outgoing\ncall.\n\n## Links ##\n\n* http://ofps.oreilly.com/titles/9780596517342/asterisk-Arch.html\n* http://www.asteriskdocs.org/\n* http://www.voip-info.org/\n* http://nerdvittles.com/index.php?p=65\n* http://www.voip-info.org/wiki/index.php?page=Asterisk+LDAP\n* http://download.ag-projects.com/\n\n## Security Notes ##\n\nAsterisk/SIP are going to be a significant security hole in the network if I\nallow outside access to the SIP services (In case I want to say be able to pick\nup the phone from my laptop while at a cafe or from my office). I intend to\nresearch making this considerably more secure before allowing this kind of\nsetup and will document it here.\n\nOne of the things for me to note ahead of time is to not put incoming calls\ninto the same context as my dial plans. This alone will be a significant\nincrease in any kind of security.\n\n### Encryption\n\nThe configurations provided have TLS enabled BUT it won't do any good until the\nserver has a certificate. A normal webserver certificate in PKCS12 format. FOR\nTESTING ONLY you can generate a self signed certificate. You'll need both the\ncertificate authority's cert and the key/cert pair for the server.\n\nYou can use the following to create a self-signed one:\n\n```\n[root@localhost ~]# openssl genrsa -out ca.key 4096\n[root@localhost ~]# openssl req -new -x509 -days 3650 -key ca.key -out ca.crt\n[root@localhost ~]# openssl genrsa -des3 -out pbx.key 4096\n[root@localhost ~]# openssl req -new -key argus.key -out pbx.csr\n[root@localhost ~]# openssl x509 -req -days 3650 -in pbx.csr -CA ca.crt -CAkey ca.key -set_serial 01 -out pbx.crt\n[root@localhost ~]# openssl rsa -in pbx.key -out pbx.key.insecure\n[root@localhost ~]# cat pbx.crt \u003e pbx.pem\n[root@localhost ~]# cat pbx.key.insecure \u003e\u003e pbx.pem\n[root@localhost ~]# mv pbx.pem ca.crt /var/lib/asterisk/\n[root@localhost ~]# restorecon -R /var/lib/asterisk/\n[root@localhost ~]# chown asterisk:asterisk /var/lib/asterisk\n```\n\n### Firewall Adjustments\n\n```\n# Allow incoming SIP calls from the local network\n-A PRIMARYSERVICES -s 10.13.37.0/24 -m udp -p udp --dport 5060 -j ACCEPT\n-A PRIMARYSERVICES -s 10.13.37.0/24 -m tcp -p tcp --dport 5060:5061 -j ACCEPT\n\n# Log and allow SIP traffic from other people\n-A PRIMARYSERVICES -m state --state NEW -m udp -p udp --dport 5060 -j LOG --log-prefix \"SIP Traffic \"\n-A PRIMARYSERVICES -m state --state NEW -m tcp -p tcp --dport 5060:5061 -j LOG --log-prefix \"SIP Traffic \"\n-A PRIMARYSERVICES -m udp -p udp --dport 5060 -j ACCEPT\n-A PRIMARYSERVICES -m tcp -p tcp --dport 5060:5061 -j ACCEPT\n\n# Allow incoming RTP traffic from the local network\n-A PRIMARYSERVICES -s 10.13.37.0/24 -m udp -p udp --dport 10000:10100 -j ACCEPT\n\n# Log and allow RTP traffic from other people\n-A PRIMARYSERVICES -m state --state NEW -m udp -p udp --dport 10000:10100 -j LOG --log-prefix \"RTP Traffic \"\n-A PRIMARYSERVICES -m udp -p udp --dport 10000:10100 -j ACCEPT\n```\n\n### Fail2Ban\n\nDue to the overwhelmingly large number of attackers trying to exploit unsecured\nAsterisk boxes, configuring [Fail2Ban][7] with Asterisk is HIGHLY recommended.\nI'm updating the regular expressions in that template as I see attacks, and in\nsome cases where I intentionally generate the logs myself.\n\n## Config Files\n\nThere are a significant number of configuration files created when asterisk is\ninstalled, these all reside in `/etc/asterisk`. I'll go over the ones that I\nmade modifications too including full source (without the comments that come\nincluded, there are a lot of them). I copied the original files to *.conf.o and\nblew away most of the files I edited (including the stock comments). \n\n### /etc/asterisk/asterisk.conf\n\n```ini\n; --- Asterisk's Primary Configuration ---\n; Gentlemen\n\n[directories](!)\n  astetcdir =\u003e /etc/asterisk                ; The Asterisk configuration files\n  astmoddir =\u003e /usr/lib64/asterisk/modules  ; The loadable modules\n  astvarlibdir =\u003e /usr/share/asterisk       ; The base location for variable state information used by\n                                            ; various parts of Asterisk. This includes items that are\n                                            ; written out by Asterisk at runtime\n  astdbdir =\u003e /var/spool/asterisk           ; Asterisk will store its internal database in this directory\n                                            ; as a file called astdb\n  astkeydir =\u003e /var/lib/asterisk            ; Asterisk will use a subdirectory called keys in this\n                                            ; directory as the default location for loading keys for\n                                            ; encryption\n  astdatadir =\u003e /usr/share/asterisk         ; This is the base directory for system-provided data, such as\n                                            ; the sound files that come with Asterisk\n  astagidir =\u003e /usr/share/asterisk/agi-bin  ; Asterisk will use a subdirectory called agi-bin in this\n                                            ; directory as the default location for loading AGI scripts\n  astspooldir =\u003e /var/spool/asterisk        ; The Asterisk spool directory, where voicemail, call\n                                            ; recordings, and the call origination spool are stored.\n  astrundir =\u003e /var/run/asterisk            ; The location where Asterisk will write out it's unix control\n                                            ; socket as well as it PID file\n  astlogdir =\u003e /var/log/asterisk            ; The asterisk log directory\n\n[options]\n  verbose = 3                     ; Sets the default verbose setting for the Asterisk logger\n  timestamp = yes                 ; Adds timestamps to all output except output from a CLI command\n  initcrypto = yes                ; Load keys from the astkeydir at startup\n  systemname = jeeves             ; The name of the PBX, yeah ours is called Jeeves, what you going to do about\n                                  ; it?\n  maxload = 0.8                   ; Sets a maximum load average. If the load average is at or above this\n                                  ; threshold, Asterisk will not accept new calls.\n  minmemfree = 10                 ; Sets the minimum number of megabytes of free memory required for Asterisk to\n                                  ; continue accepting calls.\n  cache_record_files = yes        ; When doing recording, stores the file in record_cache_dir until recording is\n                                  ; complete.\n  runuser = asterisk              ; Run Asterisk under the user specified\n  rungroup = asterisk             ; Run Asterisk under the group specified\n  defaultlanguage = en            ; Set the default language\n  documentation_language = en_US  ; Set the preferred language for the command help\n\n[files]\n  astctlpermissions = 0660  ; Sets the permissions for the Asterisk control socket\n  astctlowner = root        ; Sets the owner for the Asterisk control socket\n  astctlgroup = asterisk    ; Sets the group for the Asterisk control socket\n  astctl = asterisk.ctl     ; Sets the filename for the Asterisk control socket\n```\n\n### /etc/asterisk/ccss.conf\n\n```ini\n; --- Call Completion Supplementary Services ---\n; Gentlemen\n\n[general]\n  cc_max_requests = 20  ; Global limit on the number of CC requests that may be in the Asterisk\n                        ; system at any one time.\n```\n\n### /etc/asterisk/cdr_adaptive_odbc.conf\n\n```ini\n; --- ODBC Database CDR storage ---\n; Gentlemen\n\n[default]\n  connection = gentlemen  ; Name of the connections (references res_odbc.conf)\n  table = asterisk        ; Name of the table in the database\n  usegmtime = yes         ; Use Greenwich Mean Time in the database\n```\n\n### /etc/asterisk/cdr.conf\n\n```ini\n; --- Asterisk Call Detail Record Engine Configuration ---\n; Gentlemen\n\n[general]\n  batch = yes\n  enable = yes\n  safeshutdown = yes\n  scheduleronly = no\n  size = 100\n  time = 300\n  unanswered = yes\n\n[csv]\n  accountlogs = yes   ; Create separate log file for each account code\n  loguniqueid = yes   ; Log uniqueid for the call\n  loguserfield = yes  ; Log user field\n  usegmtime = yes     ; Log date and time in GMT\n```\n\n### /etc/asterisk/cdr_syslog.conf\n\n```ini\n; --- Asterisk Call Detail Records (CDR) - Syslog Backend ---\n; Gentlemen\n\n; This line needs to be added to /etc/rsyslog.conf:\n; local4.info                   /var/log/asterisk-cdr.log\n\n[cdr]\n  facility=local4\n  priority=info\n\n  ; High Resolution Time for billsec and duration fields\n  template = '${CDR(clid)}','${CDR(src)}','${CDR(dst)}','${CDR(dcontext)}','${CDR(channel)}','${CDR(dstchannel)}','${CDR(lastapp)}','${CDR(lastdata)}','${CDR(start)}','${CDR(answer)}','${CDR(end)}','${CDR(duration,f)}','${CDR(billsec,f)}','${CDR(disposition)}','${CDR(amaflags)}','${CDR(accountcode)}','${CDR(uniqueid)}','${CDR(userfield)}'\n```\n\n### /etc/asterisk/extensions.conf\n\nThe only thing that I have changed in the below configuration is that where it\nsays 'SPA3000' in the variables I used the MAC address of the actual SPA3000\ndevice as it is defined in sip.conf (this has also been changed there). This\nwill allow it to remain unique even if another is added.\n\n```ini\n; --- Asterisk Extension Configuration ---\n; Gentlemen\n\n; Global variables to be used in the rest of the dial plan\n[globals]\n  ; Area information\n  LOCALAREACODE=802\n\n  ; User extensions\n  U1=SIP/user1\n  U2=SIP/user2\n  U3=SIP/user3\n\n  ; Device extensions\n  PSTNOUT=SIP/SPA3000_PSTNOUT\n  PSTNIN=SIP/SPA3000_PSTNIN\n  LANDLINE=SIP/SPA3000_LINE\n\n; This context contains the menu for callers coming in over the PSTN line\n[pstnmenu]\n  exten =\u003e s,1,Verbose(3,${CALLERID(all)})  ; Print the Caller ID to the console if it's available\n           same =\u003e n,GoTo(debug,echo,1)\n           same =\u003e n,Answer()\n           same =\u003e n,HangUp()\n\n  exten =\u003e i,1,GoTo(s,1)\n\n; This context is where unauthenticated SIP clients from the internet are going\n; to end up This will provide a certain measure of spam and robot protection by\n; requiring the user to answer a simple math question generated on the fly\n[unauthenticated]\n  ; If we allow numeric extension calls from the internet:\n  ;       exten =\u003e _XXX,1,GoTo(internal-public,1,${EXTEN})\n\n  ; If we allow alpha extension calls from the internet (assumes all extensions are lower case)\n  ;       exten =\u003e _X.,1,Set(SAFE_EXTEN=${FILTER(a-z,${EXTEN})})\n  ;               same =\u003e n,GoTo(internal-public,1,${SAFE_EXTEN})\n\n  ; Catch any unknown or unallowed extensions and give them a \"Network out of order\" response\n  ; This might trip up some brute force tools, and is better than not passing a reason\n  exten =\u003e i,1,HangUp(38)\n\n; Our trunk lines should never be making calls into the system, by putting them into this context\n; if anyone manages to collect a valid username/password set from one of our trunk they will not be\n; able to make calls through our system. This trunk should never have any dialplan in it beyonds 'hang up'\n[from-trunk]\n  ; Catch any unknown or unallowed extensions and give them a \"Network out of order\" response\n  ; This might trip up some brute force tools, and is better than not passing a reason\n  exten =\u003e i,1,HangUp(38)\n\n; This is by far the most dangerous context. This one makes outgoing calls from\n[callout]\n  exten =\u003e _NXXNXXXXXX,1,Dial(${PSTNOUT}/1${EXTEN})     ; 10-digit pattern match for NANP\n  exten =\u003e _9NXXNXXXXXX,1,Dial(${PSTNOUT}/1${EXTEN:1})  ; 10-digit pattern match for NANP (9 first)\n\n  exten =\u003e _NXXXXXX,1,Dial(${PSTNOUT}/${LOCALAREACODE}${EXTEN})     ; 7-digit pattern match for NANP\n  exten =\u003e _9NXXXXXX,1,Dial(${PSTNOUT}/${LOCALAREACODE}${EXTEN:1})  ; 7-digit pattern match for NANP\n                                                                    ; (9 first)\n\n  ;exten =\u003e _1NXXNXXXXXX,1,Dial(${PSTNOUT}/${EXTEN})     ; Long-distance pattern match for NANP\n  ;exten =\u003e _91NXXNXXXXXX,1,Dial(${PSTNOUT}/${EXTEN:1})  ; Long-distance pattern match for NANP (9 first)\n\n  ;exten =\u003e _011.,1,HangUp(63)   ; Deny the International pattern match for calls made from NANP\n                                 ; with 'Service or option unavailable'\n  ;exten =\u003e _9011.,1,HangUp(63)  ; Deny the International pattern match for calls made from NANP\n                                 ; with 'Service or option unavailable' (9 first)\n\n  ; The number they're reaching doesn't match anything we know about, deny it\n  exten =\u003e i,1,HangUp()\n\n[internal]\n  include =\u003e callout          ; Allow internal phones to call out\n  include =\u003e internal-public  ; Include public extensions (user extensions should only be defined once,\n                              ; either here or there)\n\n  exten =\u003e 600,1,GoTo(debug,echo,1)\n\n; This context holds the private internal extensions that will be available publicly either\n; through a PSTN line or through anonymous SIP connections from the internet. These lines\n; will be directly dialable from the internal context\n[internal-public]\n  exten =\u003e 301,1,Dial(${U1})\n  exten =\u003e user1,1,Dial(${U1})\n\n  exten =\u003e 302,1,Dial(${U2})\n  exten =\u003e user2,1,Dial(${U2})\n\n  exten =\u003e 303,1,Dial(${U3})\n  exten =\u003e user3,1,Dial(${U3})\n\n  exten =\u003e 304,1,Dial(${LANDLINE})\n  exten =\u003e landline,1,Dial(${LANDLINE})\n\n; Various extensions that can be used to debug troublesome clients\n[debug]\n  ; This channel will echo back whatever sounds this server receives from the client\n  ; including DTMF tones until they hangup or they press #\n  exten =\u003e echo,1,Answer()\n    same =\u003e n,Playback(silence/1)   ; Prevent the beginning of our audio from getting cut off\n    same =\u003e n,Playback(demo-echotest)\n    same =\u003e n,Echo()\n    same =\u003e n,Playback(demo-echodone)\n    same =\u003e n,Playback(vm-goodbye)\n    same =\u003e n,Playback(silence/1)   ; Prevent the end of our audio from getting cut off\n    same =\u003e n,HangUp()\n\n  ; This can be used anywhere in any other dialplan through the GoSub() routine like so:\n  ;       exten =\u003e example,1,GoSub(debug,log,1(1,[${CHANNEL}] This is the message))\n  ; By default all logging is off, to turn it on you need to execute the following commands\n  ; at the asterisk console:\n  ;       *CLI\u003e core set verbose 0\n  ;       *CLI\u003e database put Log all 1\n  ;\n  ; You can optionally only turn on logging for a single channel by running the following\n  ; command in place of the latter command (this example is for the \"incoming\" channel)\n  ;       *CLI\u003e database put Log/channel/incoming 1\n  ;\n  exten =\u003e log,1,GotoIf($[${DB_EXISTS(Log/all)} = 0]?checkchan1)\n    same =\u003e n,GotoIf($[${ARG1} \u003c= ${DB(Log/all)}]?log)\n    same =\u003e n(checkchan1),Set(KEY=Log/channel/${CHANNEL})\n    same =\u003e n,GotoIf($[${DB_EXISTS(${KEY})} = 0]?checkchan2)\n    same =\u003e n,GotoIf($[${ARG1} \u003c= ${DB(${KEY})}]?log)\n    same =\u003e n(checkchan2),Set(KEY=Log/channel/${CUT(CHANNEL,-,1)})\n    same =\u003e n,GotoIf($[${DB_EXISTS(${KEY})} = 0]?return)\n    same =\u003e n,GotoIf($[${ARG1} \u003c= ${DB(${KEY})}]?log)\n    same =\u003e n(return),Return()              ; Logging is not turned on return without doing anything\n    same =\u003e n(log),Verbose(0,${ARG2})       ; Log the message to the console\n    same =\u003e n,Return()\n```\n\n### /etc/asterisk/features.conf\n\n```ini\n; Asterisk Call Features (parking, transfer, etc) Configuration\n; Gentlemen\n\n[general]\n  ; Nothing has been configured yet\n```\n\n### /etc/asterisk/indications.conf\n\nI never got around to configuring this file, I left the section here as the\ndefault asterisk install adds this file in. For now you're on your own in\nconfiguring indications.\n\n### /etc/asterisk/logger.conf\n\n```ini\n; --- Logging Configuration ---\n; Gentlemen\n\n[general]\n  rotatestrategy = rotate\n  exec_after_rotate=gzip -9 ${filename}.2\n\n[logfiles]\n  console =\u003e notice,warning,error,dtmf\n  ;full =\u003e notice,warning,error,debug,verbose,dtmf,fax\n  ;verbose =\u003e notice,warning,error,verbose\n  messages =\u003e notice,warning,error\n\n  syslog.local4 =\u003e notice,warning,error\n```\n\n### /etc/asterisk/manager.conf\n\n```ini\n; --- AMI - The Asterisk Manager Interface Configuration ---\n; Gentlemen\n\n[general]\n  ;bindaddr = 10.13.37.17\n  enabled = no\n  ;port = 5038\n  webenabled = no\n```\n\n### /etc/asterisk/modules.conf\n\n```ini\n; --- Module Loader configuration file ---\n; Gentlemen\n\n[modules]\n  ; I much prefer the whitelist approach of loading modules as I won't get anything that I don't need,\n  ; security through simplicity\n  autoload=no\n\n  ; Uncomment the following if you wish to use the Speech Recognition API\n  ;preload =\u003e res_speech.so\n\n  ; Essential modules needed for basic operations\n  load =\u003e app_dial.so           ; Used to connection channels together (i.e., make phone calls)\n  load =\u003e app_stack.so          ; Provides GoSub(), GoSubIf(), Return(), StackPop(), LOCAL(),\n                                ; and LOCAL_PEEK()\n  load =\u003e chan_sip.so           ; Session Initiation Protocol channel driver\n  load =\u003e pbx_config.so         ; This is the traditional, and most popular, dialplan language\n                                ; for Asterisk. Without this module, Asterisk cannot read\n                                ; extensions.conf\n  load =\u003e res_rtp_asterisk.so   ; Provides RTP\n\n  ; Security Modules\n  load =\u003e cdr_adaptive_odbc.so  ; Writes CDRs through ODBC framework to a database\n  load =\u003e cdr_csv.so            ; Write CDRs to a CSV file\n  load =\u003e cdr_syslog.so         ; Writes CDRs to syslog\n  load =\u003e res_security_log.so   ; Enables security logging\n\n  ; Audio Codecs\n\n  ; Recommended Order:\n  ;       ulaw, alaw, gsm, g722, g726, speex\n\n  ; Description:          A-law PCM codec used all over the world (except Canada/USA) on the PSTN\n  ; Codec:                G.711 (alaw)\n  ; Nominal Bandwidth:    64 kbit/s\n  ; Algorithmic Latency:  0.125ms\n  ; Mean Opinion Score:   4.2\n  ; Fax Capable:          Yes\n  ; Video Capable:        No\n  load =\u003e codec_alaw.so\n\n  ; Description:          Global System for Mobile Communications (GSM) codec\n  ; Codec:                GSM-FR\n  ; Nominal Bandwidth:    13 kbit/s\n  ; Algorithmic Latency:\n  ; Mean Opinion Score:   3.7\n  ; Fax Capable:          No\n  ; Video Capable:        No\n  load =\u003e codec_gsm.so\n\n  ; Description:          Wideband audio codec\n  ; Codec:                G.722\n  ; Nominal Bandwidth:    64 kbit/s\n  ; Algorithmic Latency:\n  ; Mean Opinion Score:\n  ; Fax Capable:          Maybe\n  ; Video Capable:        No\n  load =\u003e codec_g722.so\n\n  ; Description:          Flavor of ADPCM\n  ; Codec:                G.726\n  ; Nominal Bandwidth:    16-40 kbit/s\n  ; Algorithmic Latency:  0.125ms\n  ; Mean Opinion Score:   3.85\n  ; Fax Capable:          No\n  ; Video Capable:        No\n  load =\u003e codec_g726.so\n\n  ; Open source speech codec\n  ; Codec:                Speex\n  ; Nominal Bandwidth:    2.15-22.4 kbit/s\n  ; Algorithmic Latency:  30-34ms\n  ; Mean Opinion Score:\n  ; Fax Capable:          No\n  ; Video Capable:        No\n  load =\u003e codec_speex.so\n\n  ; Description:          Mu-law PCM codec used in Canada/USA on PSTN\n  ; Codec:                G.711 (ulaw)\n  ; Nominal Bandwidth:    64 kbit/s\n  ; Algorithmic Latency:  0.125ms\n  ; Mean Opinion Score:   4.2\n  ; Fax Capable:          Yes\n  ; Video Capable:        No\n  load =\u003e codec_ulaw.so\n\n  ; Audio Codec Converters\n  load =\u003e codec_resample.so  ; Re-samples between 8-bit and 16-bit signed linear\n  load =\u003e codec_a_mu.so      ; A-law to mu-law direct converter\n\n  ; Format Interpreters (used to read audio files from the disk)\n  load =\u003e format_ogg_vorbis.so  ; Play ogg vorbis files\n  load =\u003e format_wav.so         ; Play WAV files\n\n  ; Resources \u0026 Function Modules\n  load =\u003e func_cdr.so         ; Used by cdr_syslog module to format it's output\n  load =\u003e func_strings.so     ; Used for security reasons on incoming calls from the PSTN lines\n  load =\u003e res_adsi.so         ; Used by voicemail\n  load =\u003e res_odbc.so         ; Used by the cdr_adaptive_odbc module\n  load =\u003e res_musiconhold.so  ; Provides music on hold resources\n\n  ; Voicemail\n  load =\u003e app_voicemail_plain.so  ; Stores voicemails in files\n\n  ; Production Debug Tools\n  load =\u003e app_echo.so     ; Used in the echo channel to ensure two way audio is working\n  load =\u003e app_verbose.so  ; Used in the various channels to send messages to the console and logs\n\n  ; Used in various dialplans\n  load =\u003e app_playback.so  ; Plays back a pre-recorded audio file\n\n  ; For Caller ID information\n  load =\u003e app_setcallerid.so\n  load =\u003e func_callerid.so\n```\n\n### /etc/asterisk/musiconhold.conf\n\n```ini\n; --- Music on Hold Configuration ---\n; Gentlemen\n\n[default]\n  directory = moh  ; Directory relative to the astdatadir that has the music on hold files\n  mode = files     ; Read files from the configured directory\n  sort = random    ; Sort the files in random order\n```\n\n### /etc/asterisk/res_odbc.conf\n\n```ini\n; --- ODBC Configuration File ---\n; Gentlemen\n\n; This 'context' name is the name that other files will reference this by\n[gentlemen]\n  enabled =\u003e yes                    ; Not strictly necessary but a quick way to remind myself this can\n                                    ; be disabled without deleting or commenting out the whole thing\n  dsn =\u003e asterisk-gentlemen         ; This value should match an entry in /etc/odbc.ini\n  username =\u003e databaseuser          ; The username used to connect to the database\n  password =\u003e databasepass          ; Password for connecting to the database\n  pre-connect =\u003e yes                ; Open a connection as soon as asterisk starts\n  idlecheck =\u003e 3600                 ; How often should we check that the database connection is still\n                                    ; open? (in seconds)\n  share_connections =\u003e yes          ; Allow connections to be shared, this reduces overhead but doesn't\n                                    ; work on all databases\n  limit =\u003e 5                        ; What is the maximum number of database connections we can have open\n                                    ; at any one time?\n  connect_timeout =\u003e 5              ; How long should we attempt to connect before considering the\n                                    ; connection dead?\n  negative_connection_cache =\u003e 300  ; When a connection fails, how long should we cache that information\n                                    ; before we attempt another connection?\n```\n\n### /etc/asterisk/rtp.conf\n\n```ini\n; --- RTP Configuration ---\n; Gentlemen\n\n[general]\n  rtpstart=10000\n  rtpend=10100\n\n  ;rtpchecksums=no\n  ;dtmftimeout=3000\n  ;rtcpinterval = 5000    ; Milliseconds between rtcp reports\n                          ;(min 500, max 60000, default 5000)\n\n  ; Enable strict RTP protection. This will drop RTP packets that\n  ; do not come from the source of the RTP stream. This option is\n  ; disabled by default.\n  ;strictrtp=yes\n```\n\n### /etc/asterisk/sip.conf\n\n```ini\n; --- SIP Configuration ---\n; Gentlemen\n\n[general]\n  ; Features/SIP Defaults\n  allowguest = no            ; Disable unauthenticated call\n  alwaysauthreject = yes     ; Always respond as if every extension is valid, this makes brute force\n                             ; scanning of extensions pointless\n  canreinvite = no           ; Force all calls to be relayed through asterisk since SIP clients\n                             ; outside the firewall won't be able to directly talk to internal extensions\n  context = unauthenticated  ; Default context for incoming calls\n  dtmfmode = auto            ; Automatically detect the DTMF tranmission type\n  srvlookup = no             ; Disable DNS SRV record lookup on outbound calls\n  videosupport = yes         ; May be useful later when setting up video calls\n\n  ; Network options\n  bindport = 5060                      ; Explicitely bind to the default SIP port\n  externip = x.x.x.x                   ; The current external IP address of The Gentlemens Lounge\n  localnet = 10.13.37.0/255.255.255.0  ; Our internal subnet\n  tcpenable = yes                      ; We'll allow incoming connections via TCP as well\n  udpbindaddr = 10.13.37.xx            ; Listen for UDP requests on the primary interface\n\n  ; Encryption settings\n  tlsenable = yes                          ; Turn encryption support on\n  tlsbindbindaddr = 10.13.37.xx            ; Listen for encrypted calls on the primary interface\n  tlscafile = /var/lib/asterisk/ca.crt     ; Certificate authority's cert\n  tlscertfile = /var/lib/asterisk/pbx.pem  ; This server's private key and certificate\n\n  ; Clear the list of codecs\n  disallow = all\n\n  ; Set a default order for our codecs, providing all of them will increase compatibility with other\n  ; internet clients. Notes on the quality tradeoff can be found in the module.conf file where these\n  ; codecs are loaded\n  allow = ulaw\n  allow = alaw\n  allow = gsm\n  allow = g722\n  allow = g726\n  allow = speex\n  allow = g729\n\n; Template to restrict SIP users to only the local network\n[localonly](!)\n  ; Start by denying everyone\n  deny = 0.0.0.0/0.0.0.0\n  ; Allow connetion that originate from 10.13.37.X to attempt to authenticate against this account\n  permit = 10.13.37.0/255.255.255.0\n\n; A template for VoIP extensions that might connect from the outside\n[sipclient](!)\n  context = internal  ; They should be in our internal context\n  host = dynamic      ; The clients will not be static (they could be anywhere)\n  type = friend       ; User type\n  qualify = yes       ; Talk to them frequently to make sure they're still there (and too hold\n                      ; firewall connections open)\n\n; Slight adjustment of the sipclient template to adjust that these extensions will be connecting from the outside\n[roadwarrior](!,sipclient)\n  nat = yes  ; Yes, they're probably on the other side of our NAT\n\n; This is specifically for my SPA-3000, I've found that even though my device has a static addresses\n; I should leave the host type as dynamic as registrations cause errors in the asterisk log, I suppose it\n; would be possible to configure the LINE and PSTNIN as peers and turn off registrations in the Sipura's\n; configuration, which might be the better route to go. I will have to investigate this in the future but\n; this works for now.\n[atadevice](!,sipclient,localonly)\n  nat = never  ; These devices should never be outside our NAT\n\n; Configuration for internal POTS line on the Linksys SPA-3000\n[SPA3000_LINE](atadevice)\n  context = internal\n  port = 5060\n  secret = setyourownpassword\n\n; Configuration for incoming phone calls from the POTS line through the SPA-3000\n[SPA3000_PSTNIN](atadevice)\n  context = pstnmenu\n  secret = setyourownpassword\n\n; Configuration for incoming telephone line on the Linksys SPA-3000\n[SPA3000_PSTNOUT](atadevice)\n  context = from-trunk\n  default-user = from-pbx\n  host = 10.13.37.xx\n  port = 5061\n  secret = setyourownpassword\n\n; SIP Users, these should have especially strong passwords. I use the following to generate\n; passwords for them:\n;      dd if=/dev/random count=2 bs=8 2\u003e/dev/null | base64 | sed -e 's/=*$//'\n; It will create passwords like:\n;      9VkLRZz0s9GNFcAica0ONA\n;      SOhiPwA0pTupjFx/QCu7BA\n;      5Zw+5B8435lrMLKvsSNVpQ\n[user1](roadwarrior)\n  callerid = \"User 1\"\n  secret = setyourownpassword\n\n[user2](roadwarrior)\n  callerid =\"User 2\"\n  secret = setyourownpassword\n\n[user3](roadwarrior)\n  callerid = \"User 3\"\n  secret = setyourownpassword\n```\n\n### /etc/odbc.ini\n\n```ini\n[asterisk-gentlemen]\nDescription = Database driver for Asterisk call logs\nTrace       = Off\nTraceFile   = stderr\nDriver      = MySQL\nSERVER      = mysqlserver.localhost\nPORT        = 3306\nDATABASE    = gentlemen\n```\n\nHere is the schema for the table that I use:\n\n```sql\n--\n-- Database: `gentlemen`\n--\n\n-- --------------------------------------------------------\n\n--\n-- Table structure for table `asterisk`\n--\n\nDROP TABLE IF EXISTS `asterisk`;\nCREATE TABLE `asterisk` (\n  `call_id` int(10) unsigned NOT NULL AUTO_INCREMENT,\n  `accountcode` varchar(255) DEFAULT NULL COMMENT 'An account ID. This field is user-defined and is empty by default.',\n  `src` varchar(255) NOT NULL COMMENT 'The calling party’s caller ID number. It is set automatically and is read-only.',\n  `dst` varchar(255) NOT NULL COMMENT 'The destination extension for the call. This field is set automatically and is read-only.',\n  `dcontext` varchar(255) NOT NULL COMMENT 'The destination context for the call. This field is set automatically and is read-only.',\n  `clid` varchar(255) NOT NULL COMMENT 'The full caller ID, including the name, of the calling party. This field is set automatically and is read-only.',\n  `channel` varchar(255) NOT NULL COMMENT 'The calling party’s channel. This field is set automatically and is read-only.',\n  `dstchannel` varchar(255) NOT NULL COMMENT 'The called party’s channel. This field is set automatically and is read-only.',\n  `lastapp` varchar(255) NOT NULL COMMENT 'The last dialplan application that was executed. This field is set automatically and is read-only.',\n  `lastdata` varchar(255) NOT NULL COMMENT 'The arguments passed to the lastapp. This field is set automatically and is read-only.',\n  `start` varchar(255) NOT NULL COMMENT 'The start time of the call. This field is set automatically and is read-only.',\n  `answer` varchar(255) NOT NULL COMMENT 'The answered time of the call. This field is set automatically and is read-only.',\n  `end` varchar(255) NOT NULL COMMENT 'The end time of the call. This field is set automatically and is read-only.',\n  `duration` varchar(255) NOT NULL COMMENT 'The number of seconds between the start and end times for the call. This field is set automatically and is read-only.',\n  `billsec` varchar(255) NOT NULL COMMENT 'The number of seconds between the answer and end times for the call. This field is set automatically and is read-only.',\n  `disposition` varchar(255) NOT NULL COMMENT 'An indication of what happened to the call. This may be NO ANSWER, FAILED, BUSY, ANSWERED, or UNKNOWN.',\n  `amaflags` varchar(255) NOT NULL COMMENT 'The Automatic Message Accounting (AMA) flag associated with this call. This may be one of the following: OMIT, BILLING, DOCUMENTATION, or Unknown.',\n  `userfield` varchar(255) DEFAULT NULL COMMENT 'A general-purpose user field. This field is empty by default and can be set to a user-defined string.',\n  `uniqueid` varchar(255) NOT NULL COMMENT 'The unique ID for the src channel. This field is set automatically and is read-only.',\n  PRIMARY KEY (`call_id`)\n) ENGINE=MyISAM  DEFAULT CHARSET=latin1 ;\n```\n\n## Music On Hold\n\nThe way that music on hold is configured in the dial plan on this page it will\nlook for files in the directory `/usr/share/asterisk/moh/`. Files will be\nchosen at random from this directory as long as asterisk can read them (that is\nit has a codec for the audio file loaded). I strongly suggest the music be in\n`ogg` format.\n\n## Text to Speech\n\nPlease refer to my notes on [Festival][7] for more information on text to\nspeech with asterisk.\n\n[2]: http://robsmart.co.uk/2009/06/02/freeswitch_linksys3102/\n[3]: http://google.com/voice\n[4]: http://www.sipgate.com/\n[5]: http://code.google.com/p/pygooglevoice/\n[6]: {{\u003c ref \"./fail2ban.md\" \u003e}}\n[7]: {{\u003c ref \"./festival.md\" \u003e}}\n","created_at":-62135596800,"fuzzy_word_count":5e3,"path":"/notes/asterisk/","published_at":1540945455,"reading_time":24,"tags":null,"title":"Asterisk","type":"notes","updated_at":1540945455,"weight":0,"word_count":4991},{"cid":"167f5babf35b9943aa4ed1097d9b1b2c3008a6fb","content":"\n***Note: This page is quite old and is likely out of date. My opinions may have\nalso changed dramatically since this was written. It is here as a reference\nuntil I get around to updating it.***\n\n[Documentation][1]\n\n## Installation\n\n```\nsudo yum install python-pip chromaprint-tools -y\nsudo pip-python install beets\nsudo pip-python install pyacoustid\nsudo pip-python install rgain\nsudo pip-python install pylast\n```\n\nConfigure ~/.beetsconfig like so:\n\n```ini\n[beets]\npath_format: $artist/$album/$track - $title\nimport_copy: yes\nimport_write: yes\nimport_resume: ask\nimport_art: yes\nimport_quiet_fallback: skip\nimport_timid: no\nignore: .AppleDouble ._* *~ .DS_Store\nthreaded: yes\ncolor: yes\nplugins: chroma embedart lastgenre replaygain scrub\n\n[paths]\ndefault: $albumartist/$album/$track - $title\nsingleton: $artist/$title\ncomp: Compilations/$album/$track - $title\n\n[replaygain]\noverwrite: yes\n```\n\n## Usage\n\nThe following command will import music from the given path and copy it into\nthe directory specified in the configuration file.\n\n```\nbeet import /path/to/music\n```\n\nAdding the `-C` will do the same thing but will update the files in place\nrather than copying them elsewhere:\n\n```\nbeet import -C /path/to/music\n```\n\nAnd finally if you want the metadata to only exist in the database (not update\nthe files) you can pass it the `-W` flag like so:\n\n```\nbeet import -W /path/to/music\n```\n\nThe `-A` flag will import the music without checking the tags. And finally the\n`-q` will suppress the prompts and only import the files that have a 95% chance\nof matching.\n\n[1]: http://readthedocs.org/docs/beets/en/1.0b12/index.html\n","created_at":-62135596800,"fuzzy_word_count":300,"path":"/notes/beets/","published_at":1507583005,"reading_time":2,"tags":null,"title":"Beets","type":"notes","updated_at":1507583005,"weight":0,"word_count":234},{"cid":"7f8ff0658794a4db57d9decea6c6c8d41caf4700","content":"\n***Note: This page is quite old and is likely out of date. My opinions may have\nalso changed dramatically since this was written. It is here as a reference\nuntil I get around to updating it.***\n\n## Firewall Adjustments\n\nA DNS server isn't very good unless other machines are able to query it. The\nfollowing allows incoming DNS queries universally.\n\n```\n-A INPUT -p udp -m udp --dport 53 -j ACCEPT\n-A INPUT -p tcp -m tcp --dport 53 -j ACCEPT\n```\n\nIf you're running named in a chroot you'll also need to add the following line\nto rsyslog's configuration file so that the chroot doesn't break named's\nlogging.\n\n```\n$AddUnixListenSocket /var/named/chroot/dev/log\n```\n\n## Config Files\n\n### /etc/named.conf\n\n```\ninclude \"/etc/named.root.key\";\n\n// Allow no zone transfers. Any slaves should be added here.\nacl \"xfer\" {\n  none;\n};\n\n// This should include any internal and DMZ subnets so intranet and servers\n// can query our internal zones. This also prevents outside hosts from using\n// our name server as a resolver for other domains.\nacl \"trusted\" {\n  10.87.19.0/24;\n  2001:abcd:ef::/64;\n  fc00::/7;\n  fe80::/10;\n  127.0.0.1;\n  ::1;\n};\n\nkey \"control-key\" {\n  algorithm hmac-md5;\n  secret \"##!!pulled-from-rndc.conf-generation!!##\";\n};\n\ncontrols {\n  inet 127.0.0.1 port 953 allow { 127.0.0.1; } keys { \"control-key\"; };\n};\n\nlogging {\n  channel default_syslog {\n    severity debug;\n    syslog local2;\n  };\n\n  channel audit_syslog {\n    severity debug;\n    syslog local3;\n  };\n\n  category default      { default_syslog; };\n\n  category client       { audit_syslog; };\n  category config       { default_syslog; };\n  category dnssec       { audit_syslog; };\n  category general      { default_syslog; };\n  category lame-servers { audit_syslog; };\n  category network      { audit_syslog; };\n  category notify       { audit_syslog; };\n  category queries      { audit_syslog; };\n  category resolver     { audit_syslog; };\n  category security     { audit_syslog; default_syslog; };\n  category update       { audit_syslog; };\n  category xfer-in      { audit_syslog; };\n  category xfer-out     { audit_syslog; };\n};\n\noptions {\n  listen-on port 53     { any; };\n  listen-on-v6 port 53  { any; };\n\n  directory           \"/var/named\";\n  dump-file           \"/var/named/data/cache_dump.db\";\n  statistics-file     \"/var/named/data/named_stats.txt\";\n  memstatistics-file  \"/var/named/data/named_mem_stats.txt\";\n  zone-statistics     yes;\n\n  // Override the version information to reduce enumeration options\n  version \"It's over 9000\";\n\n  // More efficient zone transfers\n  transfer-format many-answers;\n\n  // Set the maximum time a zone transfer can take. Any zone transfer that\n  // takes longer than 15 minutes is unlikely to ever complete. If there are\n  // HUGE zone files this may become an issue.\n  max-transfer-time-in 15;\n\n  // With no dynamic interfaces, bind doesn't need to poll for interface state\n  interface-interval 0;\n\n  dnssec-enable     yes;\n  dnssec-validation yes;\n  dnssec-lookaside  auto;\n\n  // ISC DLV key\n  bindkeys-file           \"/etc/named.iscdlv.key\";\n  managed-keys-directory  \"/var/named/dynamic\";\n\n  // Only accept queries and cached queries from the trusted ACL\n  allow-query       { trusted; };\n  allow-query-cache { trusted; };\n};\n\n// Trusted portion of the split-horizon DNS containing internal domains,\n// private records, as well as allow recursive lookups.\nview \"internal-in\" in {\n  match-clients { trusted; };\n  recursion yes;\n\n  additional-from-auth yes;\n  additional-from-cache yes;\n\n  // Zone transfers limited to members of the \"xfer\" ACL\n  allow-transfer { xfer; };\n  zone \".\" IN {\n    type hint;\n    file \"named.ca\";\n  };\n\n  zone \"1057.name\" IN {\n    type master;\n    file \"data/internal/1057.name.zone.db\";\n    allow-update { none; };\n  };\n\n  zone \"19.87.10.in-addr.arpa\" IN {\n    type master;\n    file \"data/internal/19.87.10.in-addr.arpa.zone.db\";\n    allow-update { none; };\n  };\n\n  zone \"0.0.0.0.f.e.0.0.d.c.b.a.1.0.0.2.ip6.arpa\" IN {\n    type master;\n    file \"data/internal/0.0.0.0.f.e.0.0.d.c.b.a.1.0.0.2.arpa.zone.db\";\n    allow-update { none; };\n  };\n\n  include \"/etc/named.rfc1912.zones\";\n};\n\n// Untrusted/External portion of the split-horizon DNS, only allow internal\nview \"external-in\" in {\n  match-clients { any; };\n  recursion no;\n\n  additional-from-auth no;\n  additional-from-cache no;\n\n  zone \".\" IN {\n    type hint;\n    file \"named.ca\";\n  };\n\n  zone \"1057.name\" IN {\n    type master;\n    file \"data/public/1057.name.zone.db\";\n    allow-query   { any; };\n    allow-update  { none; };\n  };\n\n  zone \"0.0.0.0.f.e.0.0.d.c.b.a.1.0.0.2.ip6.arpa\" IN {\n    type master;\n    file \"data/public/0.0.0.0.f.e.0.0.d.c.b.a.1.0.0.2.arpa.zone.db\";\n    allow-query   { any; };\n    allow-update  { none; };\n  };\n};\n\n// View for users attempting to query the server using the CHAOS class. Trusted\n// users can still use this to query for the servers version number.\nview \"bind-chaos\" chaos {\n  match-clients { any; };\n  recursion no;\n\n  zone \"bind\" {\n    type master;\n    file \"db.bind\";\n\n    allow-query     { trusted; };\n    allow-transfer  { none; };\n  };\n};\n```\n\n### /etc/rndc.conf\n\nThis needs to be generated on your own which can be done using the following\ncommand:\n\n```\nrndc-confgen -b 512 \u003e /etc/rndc.conf\n```\n\nThe commented out code belongs in `/etc/named.conf`, just replace the following\ncomment with the code:\n\n```\n## INCLUDE KEY AND CONFIG FROM /etc/rndc.conf ##\n```\n\n512 is unfortunately the strongest bit size available for authentication so it\nis strongly recommended to firewall off and limit the IP addresses the control\nchannel is running on. By default this is exclusively the IPv4 loopback\naddress.\n\n### /var/named/db.bind\n\n```\n$TTL 86400\n@   CHAOS    SOA   @   rname.invalid.  (\n    0   ; serial\n    1D  ; refresh\n    1H  ; retry\n    1W  ; expire\n    3H  ; minimum\n)\n\n    NS    @\n\nversion.bind.   CHAOS TXT \"It's over 9000\"\nauthors.bind.   CHAOS TXT \"Nom de plume\"\n```\n\n### /var/named/data/internal/1057.name.zone.db\n\n```\n$TTL 86400\n@   IN    SOA   ns1.1057.name.    dns.1057.name.  (\n  2012080801  ; Serial number YYYYMMDDNN\n  43200       ; Refresh\n  7200        ; Retry\n  2592000     ; Expire\n  3600        ; Min TTL\n)\n\n      NS          ns1.1057.name.\n\nns1   IN    A         10.87.19.15\nns1   IN    AAAA      2001:abcd:ef::1059:afc:5bb:aa92\n```\n\n### /var/named/data/internal/19.87.10.in-addr.arpa.zone.db\n\n```\n$TTL 86400\n@   IN    SOA   ns1.1057.name.    dns.1057.name.  (\n  2013020801  ; Serial number YYYYMMDDNN\n  43200       ; Refresh\n  7200        ; Retry\n  2592000     ; Expire\n  3600        ; Min TTL\n)\n\n      NS          ns1.1057.name.\n\n15     IN    PTR       ns1.1057.name.\n```\n\n### /var/named/data/internal/0.0.0.0.f.e.0.0.d.c.b.a.1.0.0.2.arpa.zone.db\n\n```\n$TTL 86400\n@   IN    SOA   ns1.1057.name.    dns.1057.name.  (\n  2013020801  ; Serial number YYYYMMDDNN\n  43200       ; Refresh\n  7200        ; Retry\n  2592000     ; Expire\n  3600        ; Min TTL\n)\n\n      NS          ns1.1057.name.\n\n2.9.1.1.b.b.5.0.c.f.a.0.9.5.0.1    IN    PTR    ns1.1057.name.\n```\n\n### /var/named/data/public/1057.name.zone.db\n\n```\n$TTL 86400\n@   IN    SOA   ns1.1057.name.    dns.1057.name.  (\n  2012080801  ; Serial number YYYYMMDDNN\n  43200       ; Refresh\n  7200        ; Retry\n  2592000     ; Expire\n  3600        ; Min TTL\n)\n\n      NS          ns1.1057.name.\n\nns1   IN    A         4.2.2.2\nns1   IN    AAAA      2001:abcd:ef::1059:afc:5bb:aa92\n```\n\n### /var/named/data/public/0.0.0.0.f.e.0.0.d.c.b.a.1.0.0.2.arpa.zone.db\n\n```\n$TTL 86400\n@   IN    SOA   ns1.1057.name.    dns.1057.name.  (\n  2013020801  ; Serial number YYYYMMDDNN\n  43200       ; Refresh\n  7200        ; Retry\n  2592000     ; Expire\n  3600        ; Min TTL\n)\n\n      NS          ns1.1057.name.\n\n2.9.1.1.b.b.5.0.c.f.a.0.9.5.0.1    IN    PTR    ns1.1057.name.\n```\n\n## LDAP Backend\n\nUsing the package `bind-dyndb-ldap`, an ldap backend can be used either\nexclusively or in addition to other zones. Configuring a simple authenticated\nldap lookup can be done with something along the line of the following\nconfiguration:\n\n```\ndynamic-db \"my_db_name\" {\n  library \"ldap.so\";\n  arg \"uri ldap://ldap.example.com\";\n  arg \"base cn=dns, dc=example, dc=com\";\n  arg \"auth_method none\";\n  arg \"cache_ttl 300\";\n};\n```\n\nWith this configuration, the LDAP back-end will try to connect to server\nldap.example.com with simple authentication, without any password. It will then\ndo an LDAP subtree search in the `cn=dns,dc=example,dc=com` base for entries\nwith object class idnsZone, for which the idnsZoneActive attribute is set to\nTrue.\n\nFor each entry it will find, it will register a new zone with BIND. The LDAP\nback-end will keep each record it gets from LDAP in its cache for 5 minutes.\n\nIt also supports SASL authentication methods which means we can use encrypted\nauthentication and/or kerberos.\n\n## IPv6 Slow down\n\nIf there is a local IPv6 network but it is unrouted bind will regularily\nattempt to contact other nameservers using IPv6 and doesn't seem to cache\nwhether it was able to reach them or not.\n\nThis has a dramatically visible impact on the time it takes to query for a\nname. To prevent this there are really only two options, the first is to make\nthe IPv6 network routeable, and the second is to put bind in IPv4 only mode.\nNeither solution is good, but the latter is the only feasible one (This does\nmean it won't even listen on an IPv6 port).\n\n```\necho 'OPTIONS=\"-4\"' \u003e\u003e /etc/sysconfig/named\nservice named restart\n```\n","created_at":-62135596800,"fuzzy_word_count":1200,"path":"/notes/bind/","published_at":1507583005,"reading_time":6,"tags":null,"title":"Bind","type":"notes","updated_at":1507583005,"weight":0,"word_count":1179},{"cid":"0900d00d7e40789c788a6c4e706333750d750480","content":"\n***Note: This page is quite old and is likely out of date. My opinions may have\nalso changed dramatically since this was written. It is here as a reference\nuntil I get around to updating it.***\n\nThis page goes through how to create a local PKI infrastructure for use with\nall the other components listed in my notes and many may not mention it at all.\nIt uses OpenSSL and provides scripts, information about choices made during the\nprocess and where to go next.\n\n## Security Notes\n\nThere are varying levels of security that need to be taken into account\nthroughout this process. The first and most important certificate is the 'root'\ncertificate authority's certificate. If the private key for this is compromised\neverything else will need to be done by scratch. Additionally the private key\nneeds to be generated from a true random source. This is important as a little\nbit of predictability will impair all future operations done with this\ncertificate and thus all encrypted operations done from certificates at the\nheart of this system.\n\nSecond level certificate authorities should still be protected with utmost\ncaution, but these are for secondary services such as one for the [Linux/389\nDirectory Server] to hand out certificates, another for [Linux/OpenVPN]\ncertificates, one for websites and perhaps even another for personal\ncertificates for email and code signing. If one of these gets compromised the\nroot certificate can issue a CRL automatically invalidating all certificates\nfrom that authority and limits the damage done.\n\n## Root CA\n\nCaution needs to be taken when working with the root certificate from the get\ngo. It should be generated on a clean machine disconnected from the network.\n\n### Support Structure\n\nThe following set of commands builds a directory structure in the current\nuser's home drive in a folder called pki. It adjusts the permissions on the\nfolders to ensure that it is somewhat protected (though not protected from the\nroot user). Once again this should be performed on a trusted machine.\n\n```\nmkdir -p ~/pki/root/{certs,crl,newcerts,private}\nchown -R `id -u`:`id -g` ~/pki\nchmod -R 700 ~/pki\n```\n\n### Create the Configuration File\n\nCreate a file in the root CA's directory named openssl.cnf with the following\ncontents:\n\n```ini\nHOME                    = .\nRANDFILE                = $ENV::HOME/.rnd\noid_section             = new_oids\n\n[ new_oids ]\n\n[ ca ]\ndefault_ca = CA_default\n\n[ CA_default ]\ndir = /home/sstelfox/pki             # The root directory\ncerts = $dir/certs                   # Where the issued certs are kept\ncrl_dir = $dir/crl                   # Where the issued crl are kept\ndatabase = $dir/index.txt            # Database index file.\nunique_subject = yes\nnew_certs_dir = $dir/newcerts        # Default place for new certs.\n\ncertificate = $dir/cacert.pem        # The CA certificate\nserial = $dir/serial                 # The current serial number\ncrlnumber = $dir/crlnumber           # The current crl number\ncrl = $dir/crl.pem                   # The current CRL\nprivate_key = $dir/private/ca.key # The private key\nRANDFILE = $dir/private/.rand        # Private random number file\n\nx509_extensions = usr_cert           # The extentions to add to the cert\n\nname_opt = ca_default                # Subject Name options\ncert_opt = ca_default                # Certificate field options\n\ncrl_extensions = crl_ext\n\ndefault_days = 365                   # How long to certify for\ndefault_crl_days = 7                 # How long before next CRL\ndefault_md = sha256                  # Use sha256 for the hash\npreserve = no                        # Keep passed DN ordering\npolicy = policy_match\n\n# For the CA policy\n[ policy_match ]\ncountryName = match\nstateOrProvinceName = match\norganizationName = match\norganizationalUnitName = optional\ncommonName = supplied\nemailAddress = optional\n\n[ policy_anything ]\ncountryName = optional\nstateOrProvinceName = optional\nlocalityName = optional\norganizationName = optional\norganizationalUnitName = optional\ncommonName = supplied\nemailAddress = optional\n\n###################### CA Request Defaults ##########################\n[ req ]\ndefault_bits = 4096\ndefault_md = sha256\ndefault_keyfile = priv.key\ndistinguished_name = req_distinguished_name\nattributes = req_attributes\nx509_extensions = v3_ca # The extentions to add to the self signed cert\n\nstring_mask = utf8only\n\n[ req_distinguished_name ]\ncountryName = Country Name\ncountryName_default = AC\ncountryName_min = 2\ncountryName_max = 2\n\nstateOrProvinceName = State or Province Name (full name)\nstateOrProvinceName_default = YmVkcm9vbXByb2dyYW1tZXJzLm5ldA==\n\nlocalityName = Locality Name\nlocalityName_default = dHJ1ZWR1YWxpdHk=\n\n0.organizationName = Organization Name\n0.organizationName_default = QmVkcm9vbSBQcm9ncmFtbWVycyAtIENvZGUgVGhlIFBsYW5ldA==\n\norganizationalUnitName = Organizational Unit Name (eg, section)\norganizationalUnitName_default = QWRtaW5pc3RyYXRpb24=\n\ncommonName  = Common Name (User\\'s name the server\\'s hostname)\ncommonName_max = 64\n\nemailAddress = Email Address\nemailAddress_max = 64\n\n[ req_attributes ]\nchallengePassword = A challenge password\nchallengePassword_min = 4\nchallengePassword_max = 24\n\n[ usr_cert ]\nbasicConstraints = CA:false\n\n# These two are the only expected use cases at the time for this CA. If it is\n# omitted the certificate can be used for anything *except* object signing.\n# nsCertType = server\n# nsCertType = client, email, objsign\n\n# This is typical in keyUsage for a client certificate.\n# keyUsage = nonRepudiation, digitalSignature, keyEncipherment\n\nnsComment = \"Certificate issued by BedroomProgrammers.net\"\n\n# PKIX recommendations harmless if included in all certificates.\nsubjectKeyIdentifier=hash\nauthorityKeyIdentifier=keyid,issuer\n\n# This stuff is for subjectAltName and issuerAltname.\n# Import the email address.\n# subjectAltName=email:copy\n# An alternative to produce certificates that aren't\n# deprecated according to PKIX.\n# subjectAltName=email:move\n\n# Location of the public cert\nissuerAltName = URI:http://bedroomprogrammers.net/bpca.crt\n\n# Information about the certificates\nnsCaRevocationUrl = http://bedroomprogrammers.net/bpca.crl\nnsBaseUrl = http://bedroomprogrammers.net/ssl/\nnsRevocationUrl = http://bedroomprogrammers.net/ssl/revoke/?\nnsRenewalUrl = http://bedroomprogrammers.net/ssl/renew/?\nnsCaPolicyUrl = http://bedroomprogrammers.net/ssl/policy/\n\n# This is required for TSA certificates.\n# extendedKeyUsage = critical,timeStamping\n\n[ v3_req ]\n# Extensions to add to a certificate request\n\nbasicConstraints = CA:false\nkeyUsage = nonRepudiation, digitalSignature, keyEncipherment\n\n[ v3_ca ]\n# Extensions for a typical CA\n\n# PKIX recommendation.\nsubjectKeyIdentifier=hash\nauthorityKeyIdentifier=keyid:always,issuer\n\n# This may break some older clients\nbasicConstraints = critical,CA:true\n# If this happens the certificate will have to be remade using this option:\n#basicConstraints = CA:true\n\n# Some might want this also\n# nsCertType = sslCA, emailCA\n\n# Include email address in subject alt name: another PKIX recommendation\n# subjectAltName=email:copy\n# Copy issuer details\nissuerAltName = URI:http://bedroomprogrammers.net/bpca.crt\n\n[ crl_ext ]\nissuerAltName = URI:http://bedroomprogrammers.net/bpca.crt\nauthorityKeyIdentifier = keyid:always\n\n[ proxy_cert_ext ]\nbasicConstraints=CA:false\n\n# nsCertType = server\n# nsCertType = objsign\n# nsCertType = client, email\n# nsCertType = client, email, objsign\n\n# This is typical in keyUsage for a client certificate.\n# keyUsage = nonRepudiation, digitalSignature, keyEncipherment\n\nnsComment = \"Certificate issued by BedroomProgrammers.net\"\n\n# PKIX recommendations harmless if included in all certificates.\nsubjectKeyIdentifier = hash\nauthorityKeyIdentifier = keyid,issuer\n\n# Copy subject details\nissuerAltName = issuer:copy\n\nnsCaRevocationUrl = http://bedroomprogrammers.net/bpca.crl\nnsBaseUrl = http://bedroomprogrammers.net/ssl/\nnsRevocationUrl = http://bedroomprogrammers.net/ssl/revoke/?\nnsRenewalUrl = http://bedroomprogrammers.net/ssl/renew/?\nnsCaPolicyUrl = http://bedroomprogrammers.net/ssl/policy/\n\n# This really needs to be in place for it to be a proxy certificate.\nproxyCertInfo=critical,language:id-ppl-anyLanguage,pathlen:3,policy:foo\n```\n\n### Create CA Key \u0026 Certificate\n\nThis section creates the CA private key, public certificate, and serial file\nusing the `openssl.cnf` file above.\n\n```\nopenssl req -new -keyout private/ca.key -out ca.csr -config openssl.cnf\nopenssl ca -create_serial -config ./openssl.cnf -out ca.crt -days 1095 -batch \\\n  -keyfile private/ca.key -selfsign -extensions v3_ca -infiles ca.csr\n```\n\nYou will now want to distribute \"ca.crt\" to any client that will use a service\nwith your CA.\n\n## Common Tasks\n\n### Simple Client Certificate Creation\n\nFirst we'll need to create a key for client, this should be done on the client\nitself. It may be easier if the CA config (openssl.cnf) was distributed as well\nas there are some strict signing requirements in the above configuration where\nthe country, state, and org name need to match exactly.\n\n```\nopenssl genrsa 2048 \u003e host.example.org.key\nopenssl req -new -key host.example.org.key -out host.example.org.csr \\\n  -extensions v3_req\n```\n\nOnce the CSR has been generated transfer it to the host containing the CA, this\nassumes you have placed the file (and are currently within) the CA's directory.\n\n```\nopenssl ca -config ./openssl.cnf -out host.example.org.crt -days 365 -batch \\\n  -keyfile private/ca.key -cert ca.crt -infiles host.example.org.csr\n```\n\n### Advanced Client Certificate Creation\n\nCreating a certificate with multiple domains requires one extra step, create a\nkey and certificate as normal but create an additional file\n`test.domain.net.cnf` that includes something along the following lines:\n\n```\nsubjectAltName=DNS:www.test.domain.net,DNS:*.test.domain.net,DNS:other.domain.net\n```\n\nWhen you sign the key you'll need to add the flag `-extfile` and specify the\nfile with the above contents. This will append those domains to the\ncertificate. An example way to sign the CSR:\n\n```\nopenssl ca -config ./openssl.cnf -out test.domain.net.crt -days 365 -batch \\\n  -keyfile private/ca.key -cert ca.crt -infiles test.domain.net.csr \\\n  -extfile test.domain.net.cnf\n```\n\n### Convert a Certificate for Microsoft Certificate Store\n\nThis will convert a standard OpenSSL certificate/key pair into a pfx for use by\nMicrosoft products (fuck them for being different).\n\n```\nopenssl pkcs12 -export -out keycert.pfx -inkey priv.key -in cert.crt\n```\n\n### View the Contents of a Certificate\n\nTo view the text content of a certificate you can use the following command:\n\n```\nopenssl x509 -text -noout -in cert.crt\n```\n\nYou can also view the contents of a signing request using:\n\n```\nopenssl req -text -noout -in cert.csr\n```\n\n## Quick Self Signed Cert\n\n```\nopenssl req -new -x509 -newkey rsa:4096 -keyout server.key -nodes -days 365 \\\n  -out server.crt\n```\n\n## Add / Change a Password on a Keyfile\n\n```\nopenssl rsa -des -in unprotected.key -out encrypted.key\n```\n","created_at":-62135596800,"fuzzy_word_count":1700,"path":"/notes/building-a-certificate-authority/","published_at":1508540507,"reading_time":8,"tags":null,"title":"Building a Certificate Authority","type":"notes","updated_at":1508540507,"weight":0,"word_count":1614},{"cid":"b0fdf28a6c01c9d4173970241f5a8cd59b4ac149","content":"\n***Note: This page is quite old and is likely out of date. My opinions may have\nalso changed dramatically since this was written. It is here as a reference\nuntil I get around to updating it.***\n\nReplacement for the ntpd daemon.\n\n## Installation\n\n```\nyum install chrony -y\n```\n\nAfter configuration enable the service like so\n\n```\nsystemctl enable chronyd.service\n```\n\n## Server\n\n### /etc/chrony.conf\n\n```\nbindaddress \u003cServer IP\u003e\nbindcmdaddress 127.0.0.1\n\n# IPv4/IPv6:\nserver clock.nyc.he.net iburst\nserver clock.sjc.he.net iburst\n\n# IPv4 only:\nserver time.keneli.org iburst\nserver bonehed.lcs.mit.edu iburst\nserver gnomon.cc.columbia.edu iburst\n\n# Record the rate at which the system clock gains/losses time.\ndriftfile /var/lib/chrony/drift\n\n# Enable kernel RTC synchronization.\nrtcsync\n\n# In first three updates step the system clock instead of slew\n# if the adjustment is larger than 100 seconds.\nmakestep 100 3\n\n# Allow client access from local network.\nallow 10.13.37\n\n# Serve time even if not synchronized to any NTP server.\nlocal stratum 6\n\nkeyfile /etc/chrony.keys\n\n# Specify the key used as password for chronyc.\ncommandkey 1\ncmdallow 127.0.0.1\n\n# Send a message to syslog if a clock adjustment is larger than 0.5 seconds.\nlogchange 0.5\n\nlogdir /var/log/chrony\nlog measurements statistics tracking\n```\n\nIf you want to have multiple chrony servers on the local network it's a good\nidea to mark them as peers with each other with the following directive:\n\n```\npeer \u003cOther Server IP\u003e\n```\n\nThis will automatically generate a unique key in /etc/chrony.keys\n\n## Client (always on)\n\n### /etc/chrony.conf\n\n```\nserver \u003cServer IP\u003e\ndriftfile /var/lib/chrony/drift\nrtcsync\nkeyfile /etc/chrony.keys\ncommandkey 2\ncmdallow 127.0.0.1\ninitstepslew 20 \u003cServer IP\u003e\nlogchange 0.5\nlogdir /var/log/chrony\nlog measurements statistics tracking\n```\n\nThis will automatically generate a unique key in /etc/chrony.keys\n\n## Client (Intermittant Connection)\n\nThis section relies on NetworkManager's dispatcher to inform chronyd when we\nhave and don't have a network connection to take the synchronization on and\noffline.\n\n### /etc/chrony.conf\n\n```\nserver \u003cServer IP\u003e offline\ndriftfile /var/lib/chrony/drift\nrtcsync\nkeyfile /etc/chrony.keys\ncommandkey 2\ncmdallow 127.0.0.1\ninitstepslew 20 \u003cServer IP\u003e\nlogchange 0.5\nlogdir /var/log/chrony\nlog measurements statistics tracking\n```\n\nThis will automatically generate a unique key in /etc/chrony.keys\n\n### /etc/NetworkManager/dispatcher.d/20-chrony\n\nThis file needs to be executable and should be owned by root. A similar version\nof this script comes with Fedora, however, it isn't quite as friendly as mine\n:). Just replace it.\n\n```sh\n#!/bin/sh\n\nINTERFACE=$1    # The interface which is brought up or down\nSTATUS=$2       # Interface status\n\ncase \"$STATUS\" in\n  'up')\n    # Check to see if the interface added a default route\n    /sbin/ip route list dev \"$INTERFACE\" | grep -q '^default' \u0026\u0026\n      /usr/libexec/chrony-helper command online \u003e /dev/null 2\u003e\u00261\n    ;;\n  'down')\n    # If we don't have a default route anymore mark chrony as offline\n    /sbin/ip route list | grep -q '^default' ||\n      /usr/libexec/chrony-helper command offline \u003e /dev/null 2\u003e\u00261\n    ;;\nesac\n```\n","created_at":-62135596800,"fuzzy_word_count":500,"path":"/notes/chronyd/","published_at":1507583668,"reading_time":3,"tags":null,"title":"Chronyd","type":"notes","updated_at":1507583668,"weight":0,"word_count":442},{"cid":"0da9dfff456d4a9a4276f45313e3c6a5cbd05580","content":"\n## Recovering Data from Swap\n\nSometimes useful bits of information can be recovered from swap. Whether it's\nencryption keys, documents that were being worked on or anything else that\nmight've ended up in RAM. To search through the swap for interesting bits (and\ndepending on the size this might take a while) you can execute the following\ncommand as root or sudo to do it:\n\n```\n[root@localhost ~]# strings `/bin/swapon -s | tail -1 | awk '{print $1}'` | less\n```\n\nThe command above uses the swapon utility to list all of the swap devices in\nuse; look at the last line of the output (most people only have one swap\ndevice); extract only the path to the device node. Run the strings utility\n(which prints only printable strings of text from whatever you run through it)\non the swap device. Break the output down by pages.\n\nA lot of the output will look like junk, but if you're patient some interesting\nthings can pop out at you.\n","created_at":-62135596800,"fuzzy_word_count":200,"path":"/notes/data-recovery/","published_at":1507583668,"reading_time":1,"tags":["linux","tips"],"title":"Data Recovery","type":"notes","updated_at":1507583668,"weight":0,"word_count":163},{"cid":"bc89ecba163d215161a65dacf8b7d7bb487bab5f","content":"\n***Note: This page is quite old and is likely out of date. My opinions may have\nalso changed dramatically since this was written. It is here as a reference\nuntil I get around to updating it.***\n\n## Firewall Adjustments\n\n```\n# Accept DHCP requests\n-A INPUT -m udp -p udp --dport 67 --sport 68 -j ACCEPT\n```\n\n## Configuration\n\n### /etc/dhcp/dhcpd.conf\n\n```\n# Default lease time information\nmin-lease-time 300;\nmax-lease-time 86400;\ndefault-lease-time 86400;\n\n# We are the only DHCP server there should be...\nauthoritative;\n\n# No updates, might deal with this later\nddns-updates off;\nddns-update-style none;\n\n# Security measures\nignore bootp;\nignore client-updates;\ndeny declines;\ndeny duplicates;\n\n# Verify the address is unused before assigning\nping-check true;\nping-timeout 1;\n\n# Logging information\nlog-facility local1;\n\n# Default DNS servers\noption domain-name-servers 8.8.8.8, 4.2.2.2;\n\n# Room Mate's network\nsubnet 192.168.100.0 netmask 255.255.255.0 {\n  option routers 192.168.100.1;\n  option broadcast-address 192.168.100.255;\n\n  range 192.168.100.50 192.168.100.250;\n\n  # I don't monitor nor care about the devices my room mates\n  # put on this subnet. Allow them all.\n  allow unknown-clients;\n}\n\n# Public network\nsubnet 10.13.37.0 netmask 255.255.255.192 {\n  option routers 10.13.37.1;\n  option broadcast-address 10.13.37.63;\n\n  range 10.13.37.10 10.13.37.40;\n\n  # They probably won't be around long... no need to hold onto\n  # resources they don't need\n  max-lease-time 1800;\n  default-lease-time 600;\n\n  # I don't know who will get on this... besides I want to have\n  # some fun with strangers...\n  allow unknown-clients;\n}\n\n# Private/Trusted LAN\nsubnet 10.13.37.128 netmask 255.255.255.192 {\n  option routers 10.13.37.129;\n  option broadcast-address 10.13.37.191;\n\n  option domain-name \"home.bedroomprogrammers.net\";\n  option ntp-servers 10.13.37.129;\n  option time-offset -18000;\n\n  range 10.13.37.140 10.13.37.170;\n\n  deny unknown-clients;\n}\n\n# Include my known clients configurations\ninclude \"/etc/dhcp/known-hosts.conf\";\n```\n\n### /etc/dhcp/known-clients.conf\n\nThis file needs to be created by hand. Initially it is empty, clients should be\nadded as needed.\n\n```\nhost caerleon {\n    hardware ethernet 00:25:22:0d:6d:66;\n    fixed-address 10.13.37.140;\n}\n```\n","created_at":-62135596800,"fuzzy_word_count":300,"path":"/notes/dhcpd/","published_at":1507583668,"reading_time":2,"tags":null,"title":"DHCPd","type":"notes","updated_at":1507583668,"weight":0,"word_count":286},{"cid":"d8e0824a3454e30cd1dc674e8a1954b1012e6afd","content":"\n***Note: This page is quite old and is likely out of date. My opinions may have\nalso changed dramatically since this was written. It is here as a reference\nuntil I get around to updating it.***\n\nDuplicity is a command line backup utility that makes use of the rsync\nlibraries to perform incremental backups. It also can use GPG public/private\nkeypairs to handle backing up and restoring files.\n\nI personally want to use public / private key pairs for my backups and contrary\nwith what I do most of the time I share a single public / private key pair for\nmy backups, with a strong pass string on the private key.\n\nThis is an adequate level of security for me as the backups will all be\nencrypted and can be decrypted and restored from any of the other machines as\nneeded.\n\nThe backups are performed to a remote system using SFTP and thus I handle\nper-machine authentication using SSH keys.\n\nDuplicity supports more than SFTP as a backend, however, it will just make my\nscripts less relevant too you.\n\n## Installation\n\nFedora happily has a package available so this is as easy as:\n\n```\nsudo yum install duplicity python-paramiko -y\n```\n\nIt also happily doesn't have a whole lot of dependencies.\n\nYou'll also need GPG installed if you want to make use of my scripts, this\nwon't trample on any GPG setup you may currently have. On Fedora this is\nprovided with the gpgme package and can be installed like so:\n\n```\nsudo yum install gpgme -y\n```\n\n### GPG Key Creation\n\nYou'll only need to perform this process once, afterwards you'll have the\nexported keys to move and backup as you please. I've chosen to use a 4096 bit\nRSA and RSA key that doesn't expire and has the name \"System Backups\" without\nan email address or a comment.\n\nI've also set a very long pass string on the key as you'll be relying on this\nto protect all the data on all your systems you'll be backing up with this.\nThese settings are of course personal preference and a 4096 bit key may be\noverkill for most uses, though I really enjoy overkill when it comes to\nsecurity and peace of mind.\n\n```\ngpg --keyring backup-pub.gpg --secret-keyring backup-sec.gpg \\\n  --no-default-keyring --gen-key\n```\n\nNow we need a way to securely move the generated keys around without worrying\nabout. I've chosen to export the public and private keys, encrypt them with a\nsymmetric pass string and move the resulting file between machines.\n\nThe private key should be encrypted anyway but it's one extra layer that can be\nadded on that allows me to feel comfortable enough to leave the whole blob in\nthe script without fear of compromising the private key in case that gets\ncracked.\n\nI refer to this password as the \"installation key\" as the script will use it to\ndecrypt and import the keys into the local GPG on other machines.\n\nFirst you'll need to get the key ID of the key we just generated. This can be\ndone like so:\n\n```\ngpg --keyring backup-pub.gpg --list-keys\n/home/user/.gnupg/backup-pub.gpg\n------------------------------------\npub   4096R/01234567 2012-09-24\nuid                  System Backups\nsub   4096R/89ABCDEF 2012-09-24\n```\n\nGiven the output above our key ID is `01234567`, you'll want to replace that in\nthe following commands with your own key ID. First we'll export the public key,\nthere isn't any security concerns here:\n\n```\ngpg --keyring backup-pub.gpg --output backup-pubkey.gpg --export 01234567\n```\n\nYou now have the public key in the file `backup-pubkey.gpg`. Now we're going to\nexport the private key without saving it, combine it with the public key, and\nencrypt the whole shebang into one file. Watch closely...\n\n```\ngpg --secret-keyring backup-sec.gpg --export-secret-key 01234567 |\n  cat backup-pubkey.gpg - | gpg --armor --symmetric --cipher-algo AES256 \\\n  --output backup-keys.asc\n```\n\nYou now have a block of base64 encoded encrypted goodness in the\n`backup-keys.asc` file which you can import later on.\n\n### GPG Key Import\n\n```\ncat export.asc | gpg2 --decrypt --batch --passphrase \"PASSWORD\" | gpg2 \\\n  --keyring backup-pub.gpg --secret-keyring backup-sec.gpg --import\n```\n","created_at":-62135596800,"fuzzy_word_count":700,"path":"/notes/duplicity/","published_at":1507583668,"reading_time":4,"tags":null,"title":"Duplicity","type":"notes","updated_at":1507583668,"weight":0,"word_count":649},{"cid":"b26281a362c292821888608ccd7ac0353c45a6bb","content":"\n***Note: This page is quite old and is likely out of date. My opinions may have\nalso changed dramatically since this was written. It is here as a reference\nuntil I get around to updating it.***\n\nEZStream is a media streamer and re-encoder that can be used to feed\n[Icecast][1].\n\n## Mounting Content\n\nI have a [Samba][2] share \"Media\" setup on the server \"samba-srv\" that holds\nall my media and allows anonymous read access to the content. You'll need to\ninstall `cifs-utils` to mount the samba share. To make it available to my\nstream server I've performed the following:\n\n```\nmkdir /media/content\nmount -t cifs -o username=guest /samba-srv/Media /media/content\n```\n\nThe above command could be added to an init script like an `rc.local` or\nalternatively you can do it right and add it to the fstab file to be mounted\nautomatically. You'll want to add the following to `/etc/fstab`:\n\n```\n/samba-srv/Media  /media/content  cifs  auto,guest,_netdev  0 0\n```\n\nNice and easy.\n\n## Installation \u0026 Configuration\n\nQuick and Easy:\n\n```\nyum install ezstream -y\n```\n\nezstream.xml\n\n```xml\n\u003cezstream\u003e\n  \u003curl\u003ehttp://127.0.0.1:8000/automation.ogg\u003c/url\u003e\n\n  \u003csourceuser\u003eautomation\u003c/sourceuser\u003e\n  \u003csourcepassword\u003ehackme\u003c/sourcepassword\u003e\n\n  \u003cformat\u003eVORBIS\u003c/format\u003e\n\n  \u003cfilename\u003e./song_request.rb\u003c/filename\u003e\n  \u003cplaylist_program\u003e1\u003c/playlist_program\u003e\n\n  \u003csvrinfobitrate\u003e128\u003c/svrinfobitrate\u003e\n  \u003csvrinfochannels\u003e2\u003c/svrinfochannels\u003e\n  \u003csvrinfosamplerate\u003e44100\u003c/svrinfosamplerate\u003e\n\n  \u003cmetadata_format\u003e@a@ - @t@\u003c/metadata_format\u003e\n\n  \u003creencode\u003e\n    \u003cenable\u003e1\u003c/enable\u003e\n    \u003cencdec\u003e\n      \u003cformat\u003eFLAC\u003c/format\u003e\n      \u003cmatch\u003e.flac\u003c/match\u003e\n      \u003cdecode\u003eflac -s -d --force-raw-format --sign=signed --endian=little -o - \"@T@\"\u003c/decode\u003e\n      \u003cencode\u003eNot supported Yet\u003c/encode\u003e\n    \u003c/encdec\u003e\n    \u003cencdec\u003e\n      \u003cformat\u003eMP3\u003c/format\u003e\n      \u003cmatch\u003e.mp3\u003c/match\u003e\n      \u003cdecode\u003emadplay -b 16 -R 44100 -S -o raw:- \"@T@\"\u003c/decode\u003e\n      \u003cencode\u003eNot supported Yet\u003c/encode\u003e\n    \u003c/encdec\u003e\n    \u003cencdec\u003e\n      \u003cformat\u003eVORBIS\u003c/format\u003e\n      \u003cmatch\u003e.ogg\u003c/match\u003e\n      \u003cdecode\u003eoggdec -R -b 16 -e 0 -s 1 -o - \"@T@\"\u003c/decode\u003e\n      \u003cencode\u003eoggenc -r -B 16 -C 2 -R 44100 --raw-endianness 0 -q 1.5 -t \"@M@\" -\u003c/encode\u003e\n    \u003c/encdec\u003e\n  \u003c/reencode\u003e\n\u003c/ezstream\u003e\n```\n\n[1]: {{\u003c ref \"./icecast.md\" \u003e}}\n[2]: {{\u003c ref \"./samba.md\" \u003e}}\n","created_at":-62135596800,"fuzzy_word_count":300,"path":"/notes/ezstream/","published_at":1540945455,"reading_time":2,"tags":null,"title":"EZStream","type":"notes","updated_at":1540945455,"weight":0,"word_count":279},{"cid":"58a8ca8c41826f0343d66517ead6027759937c98","content":"\n***Note: This page is quite old and is likely out of date. My opinions may have\nalso changed dramatically since this was written. It is here as a reference\nuntil I get around to updating it.***\n\nfail2ban provides a vital service of blocking troublesome IPs from attempting\nbrute force logins.\n\nMy immediate issue is that it requires a few packages off the bat that I do not\nwant on my system. Specifically tcpwrappers and shorewall. My firewall scripts\nare stronger, easier to use, and IMHO more secure than what shorewall provides\nand I don't need it. Neither does the fail2ban package as indicated in [this\nticket][1] on the redhat bug tracker.\n\nTODO: [MySQL][2] logs now that the documented configuration logs authorization\nfailures.\n\n## Installation\n\nSo what's my solution? Fuck the man we're doing this my way. Regular install\nthrough yum, force the old packages out using rpm, and exclude fail2ban from\nupdating (and preventing those damn packages from coming back). This is done\nusing the following commands:\n\n```\n[root@localhost ~]# yum install fail2ban -y\n[root@localhost ~]# rpm -v --nodeps -e shorewall tcp_wrappers\n```\n\nFinally add this line to `/etc/yum.conf`.\n\n```\nexclude=fail2ban shorewall tcp_wrappers\n```\n\nThere are of course security implication too not updating a package (in this\ncase fail2ban won't ever get updated) however it's an exception that I'm\nwilling to make to get fail2ban and not have the shitty shittiness of shorewall\nand tcp_wrappers on my systems.\n\n## Firewall Modifications\n\nThere are some small changes that need to be made to the firewall script that\nshould probably be merged with the default as they can't hurt and having\nfail2ban easily available to my default config would be a significant benefit.\n\nThe changes are specifically:\n\n* Add an \"OFFENDINGIPS\" chain\n* Push all incoming traffic through the OFFENDINGIPS chain before continuing\n  through the INPUT chain\n* Add a rule to the OFFENDINGIPS chain to prevent ongoing attacks from getting\n  through for the duration of the attack\n\nThe chain definition (near the top) should look like this:\n\n```\n:OFFENDINGIPS - [0:0]\n```\n\nPush all the traffic through the chain looks like this (this belongs above all other INPUT chain rules):\n\n```\n# [DEF-RULESET] Pass all traffic through the OFFENDINGIPS table to block\n# any hosts that have been caught being malicious in some way\n-A INPUT -j OFFENDINGIPS\n```\n\nRule to prevent ongoing attacks:\n\n```\n# [DEF-RULESET] If something keeps trying to connect after we have marked\n# them as an attacker, keep blocking them until they stop for a full hour.\n# When fail2ban is adding IPs to this table, it will initially mark them\n# as an ATTACKER and they will be banned for a minimum of the fail2ban time,\n# but it will be indefinite as long as they keep trying \n-A OFFENDINGIPS -m recent --name ATTACKER --update --seconds 3600 -j DROP\n```\n\n## Configuration\n\nI've noticed that the documentation on the actual raw config files for fail2ban\nis really poor. Most of the documentation I've found is drop in files for\nspecific services but completely unable to find coherent documentation on all\nthe options in the actions files. Sure signs of amateur work which disheartens\nme. This package is still useful though...\n\n### /etc/fail2ban/fail2ban.conf\n\n```\n[Definition]\n# Log informational level messages\nloglevel = 3\n\n# Log to syslog\nlogtarget = SYSLOG\n\nsocket = /var/run/fail2ban/fail2ban.sock\n```\n\n### /etc/fail2ban/jail.conf\n\n```ini\n[DEFAULT]\n\n# Never ban localhost or the internal network\nignoreip = 127.0.0.1 10.13.37.0/24\n\n# Number of seconds that a host is banned by default (12 Hours)\nbantime  = 43200\n\n# The time period to look for number of failed attempts\nfindtime = 600\n\n# The number of failures before a host get banned\nmaxretry = 5\n\n# Pick the best backend based on what's available on the system\nbackend  = auto\n\n[ssh-iptables]\nenabled  = true\nfilter   = sshd\naction   = iptables\nlogpath  = /var/log/secure\n\n# If asterisk is installed on this machine turn this on. The log path assumes you've followed the configuration in the FKAM wiki.\n[asterisk-iptables]\nenabled  = false\nfilter   = asterisk\naction   = iptables\nlogpath  = /var/log/asterisk.log\n```\n\n### /etc/fail2ban/filter.d/sshd.conf\n\n```\n[INCLUDES]\n# Include common.conf before this file\nbefore = common.conf\n\n[Definition]\n_daemon = sshd\n\nfailregex = ^%(__prefix_line)s(?:error: PAM: )?Authentication failure for .* from \u003cHOST\u003e\\s*$\n            ^%(__prefix_line)s(?:error: PAM: )?User not known to the underlying authentication module for .* from \u003cHOST\u003e\\s*$\n            ^%(__prefix_line)sFailed (?:password|publickey) for .* from \u003cHOST\u003e(?: port \\d*)?(?: ssh\\d*)?$\n            ^%(__prefix_line)sROOT LOGIN REFUSED.* FROM \u003cHOST\u003e\\s*$\n            ^%(__prefix_line)s[iI](?:llegal|nvalid) user .* from \u003cHOST\u003e\\s*$\n            ^%(__prefix_line)sUser \\S+ from \u003cHOST\u003e not allowed because not listed in AllowUsers$\n            ^%(__prefix_line)sauthentication failure; logname=\\S* uid=\\S* euid=\\S* tty=\\S* ruser=\\S* rhost=\u003cHOST\u003e(?:\\s+user=.*)?\\s*$\n            ^%(__prefix_line)srefused connect from \\S+ \\(\u003cHOST\u003e\\)\\s*$\n            ^%(__prefix_line)sAddress \u003cHOST\u003e .* POSSIBLE BREAK-IN ATTEMPT!*\\s*$\n            ^%(__prefix_line)sUser \\S+ from \u003cHOST\u003e not allowed because none of user's groups are listed in AllowGroups$\n\nignoreregex =\n```\n\n### /etc/fail2ban/filter.d/asterisk.conf\n\nThis file required a bit of special attention, I noticed that the one set of\nregular expressions that have been copied and pasted everywhere DIDN'T MATCH\nactual logs. They didn't take into account the port, and frankly I like\nmatching the whole line rather than just part of it. I modified the expressions\nwhich are now below. They are perfectly backwards compatible with older\nversions of asterisk that may not include the port and as a bonus will match\nagainst syslog output if you are logging to syslog as well.\n\n```ini\n[INCLUDES]\nbefore = common.conf\n\n[Definition]\n_daemon = asterisk\n\n# These are the regular expressions that will trigger fail2ban into blocking an\n# IP address for an asterisk 1.8 installation. The first regular expression\n# should match against the following line taken directly from a misconfigured\n# client:\n#\n# Nov  9 18:47:21 pbx asterisk[3432]: NOTICE[3445]: chan_sip.c:24331 in handle_request_register: Registration from 'INCOMING CALL \u003csip:SPA3000_PSTN@10.13.37.102\u003e' failed for '10.13.37.101:5061' - No matching peer found\nfailregex = ^%(__prefix_line)s?NOTICE\\[[0-9]+\\]: chan_sip.c:[0-9]+ in handle_request_register: Registration from '.*' failed for '\u003cHOST\u003e(:[0-9]+)?' - No matching peer found\\s*$\n#\n# The following should match against the following line log taken directly from\n# an attack:\n# Nov 14 06:32:16 pbx asterisk: NOTICE[1640]: chan_sip.c:21975 in handle_request_invite: Sending fake auth rejection for device \"sip\" \u003csip:sip@91.226.97.107\u003e;tag=L7922NDHSn\n            ^%(__prefix_line)s?NOTICE\\[[0-9]+\\]: chan_sip.c:[0-9]+ in handle_request_invite: Sending fake auth rejection for device \".*\" \u003csip:.*@\u003cHOST\u003e\u003e;tag=[a-zA-Z0-9]+\\s*$\n#\n# The following regexes were provided through an asterisk forum, they are a bit\n# sloppy and might be outdated. I've already updated one (the first one above)\n# to reflect what I actually see in my logs. The rest will be updated as\n# I see the attacks\n#\n#           ^%(__prefix_line)s?NOTICE.*: Registration from '.*' failed for '\u003cHOST\u003e(:[0-9]+)?' - Wrong Password\\s*$\n#           ^%(__prefix_line)s?NOTICE.*: Registration from '.*' failed for '\u003cHOST\u003e(:[0-9]+)?' - Username/auth name mismatch\\s*$\n#           ^%(__prefix_line)s?NOTICE.*: Registration from '.*' failed for '\u003cHOST\u003e(:[0-9]+)?' - Device does not match ACL\\s*$\n#           ^%(__prefix_line)s?NOTICE.* \u003cHOST\u003e(:[0-9]+)? failed to authenticate as '.*'\\s*$\n#           ^%(__prefix_line)s?NOTICE.*: No registration for peer '.*' \\(from \u003cHOST\u003e(:[0-9]+)?\\)\\s*$\n#           ^%(__prefix_line)s?NOTICE.*: Host \u003cHOST\u003e(:[0-9]+)? failed MD5 authentication for '.*' (.*)\\s*$\n#           ^%(__prefix_line)s?NOTICE.*: Failed to authenticate user .*@\u003cHOST\u003e(:[0-9]+)?.*\\s*$\n\nignoreregex =\n```\n\nSome notes on this specific file. I found 'pre-made' asterisk configurations\nfor fail2ban on the internet and found them to be very lacking. The did not\ndeal with an attack log I saw actually hitting my server the 'Sending fake\nauth rejection' log specifically.\n\nI was also greatly displeased by their performance. By making the regular\nexpression more specific (without sacrificing positive matches) I was able to\nget a real world performance increase of 442%.\n\nThis was measured using the unix 'time' program and fail2ban's regular\nexpression tester 'fail2ban-regex'. As an example with one rule (the 'No\nmatching peer found' log) turned on in each test, first the old than my updated\none gave the following results:\n\n```\n[root@localhost ~]# time fail2ban-regex /var/log/asterisk.log /etc/fail2ban/filter.d/old-asterisk.conf \u003e /dev/null\n\nreal    0m1.621s\nuser    0m1.534s\nsys     0m0.049s\n[root@localhost ~]# time fail2ban-regex /var/log/asterisk.log /etc/fail2ban/filter.d/asterisk.conf \u003e /dev/null\n\nreal    0m0.367s\nuser    0m0.298s\nsys     0m0.047s\n```\n\n### /etc/fail2ban/actions.d/iptables.conf\n\n```ini\n[Init]\n# The following had defaults that I wasn't happy with, but now they're not being\n# used anyways as I don't want offending hosts talking to *any* service on this\n# server until the ban expires. IF you just want to block access to the offending\n# service these SHOULD NOT have defaults, the triggering rule should apply them\n# so they actually block just that service rather than mess up whatever the default\n# is\n#name =\n#port =\n#protocol =\n\n# This is the manual table that I setup to handle blocked hosts\ntable = OFFENDINGIPS\n\n[Definition]\n\n# The commands that get executed when fail2ban starts up\n# We don't need to do anything since I take care of this manually and permanently\nactionstart =\n\n# The commands that get executed when fail2ban shuts down\n# We don't need to do anything since I take care of this manually and permanently\nactionstop =\n\n# There doesn't really seem to be any documentation on what this actually does,\n# I've modified it so it will look at the appropriate table and it appears that\n# it's just checking to make sure that the INPUT table is properly redirecting\n# to the table that has the offending hosts in it\nactioncheck = iptables -n -L INPUT | grep -q \u003ctable\u003e\n\n# This command bans the offending IP and marks them as an attacker\nactionban = iptables -A \u003ctable\u003e -s \u003cip\u003e -j LOG --log-prefix \"Attacker's Back \"\n            iptables -A \u003ctable\u003e -s \u003cip\u003e -m recent --set --name ATTACKER -j DROP\n\n# This command removes an IP from the ban-list\nactionunban = iptables -D \u003ctable\u003e -s \u003cip\u003e -j LOG --log-prefix \"Attacker's Back \"\n              iptables -D \u003ctable\u003e -s \u003cip\u003e -j DROP\n```\n\n## Testing Matches/Regexes/Bans Without Making the System Live\n\n### Validation Configuration\n\nUsing the following command you can verify that the configuration is valid and\nthat fail2ban will be happy with what you told it. It's output isn't very\nfriendly but it'll do in a pinch.\n\n```\n[root@localhost ~]# fail2ban-client -d\n```\n\n### Validation Regular Expressions\n\nFirst off you'll need a sample of the log that your going to be matching\nagainst. If the message is already in your log more power to you, you can use\nthat logfile as the input (and it's what I did).\n\nAs for the regex, the tool supports passing the full thing on the command line\nHOWEVER it doesn't expand fail2ban REGEX variables beyond \u003cHOST\u003e (that is those\ndefined in the common.conf file) which means you won't get an accurate\nrepresentation on a match.\n\nThe best way is to define the regex in a filter file for fail2ban and run the\ntool like so (This is from me testing asterisk with the output from my test):\n\n```\n[root@localhost ~]# fail2ban-regex /var/log/asterisk.log /etc/fail2ban/filter.d/asterisk.conf\n\nRunning tests\n=============\n\nUse regex file : /etc/fail2ban/filter.d/asterisk.conf\nUse log file   : /var/log/asterisk.log\n\n\nResults\n=======\n\nFailregex\n|- Regular expressions:\n|  [1] ^\\s*(?:\\S+ )?(?:@vserver_\\S+ )?(?:(?:\\[\\d+\\])?:\\s+[\\[\\(]?asterisk(?:\\(\\S+\\))?[\\]\\)]?:?|[\\[\\(]?asterisk(?:\\(\\S+\\))?[\\]\\)]?:?(?:\\[\\d+\\])?:)?\\s*?NOTICE.*: Registration from '.*' failed for '\u003cHOST\u003e(:[0-9]+)?' - Wrong Password\\s*$\n|  [2] ^\\s*(?:\\S+ )?(?:@vserver_\\S+ )?(?:(?:\\[\\d+\\])?:\\s+[\\[\\(]?asterisk(?:\\(\\S+\\))?[\\]\\)]?:?|[\\[\\(]?asterisk(?:\\(\\S+\\))?[\\]\\)]?:?(?:\\[\\d+\\])?:)?\\s*?NOTICE.*: Registration from '.*' failed for '\u003cHOST\u003e(:[0-9]+)?' - No matching peer found\\s*$\n|  [3] ^\\s*(?:\\S+ )?(?:@vserver_\\S+ )?(?:(?:\\[\\d+\\])?:\\s+[\\[\\(]?asterisk(?:\\(\\S+\\))?[\\]\\)]?:?|[\\[\\(]?asterisk(?:\\(\\S+\\))?[\\]\\)]?:?(?:\\[\\d+\\])?:)?\\s*?NOTICE.*: Registration from '.*' failed for '\u003cHOST\u003e(:[0-9]+)?' - Username/auth name mismatch\\s*$\n|  [4] ^\\s*(?:\\S+ )?(?:@vserver_\\S+ )?(?:(?:\\[\\d+\\])?:\\s+[\\[\\(]?asterisk(?:\\(\\S+\\))?[\\]\\)]?:?|[\\[\\(]?asterisk(?:\\(\\S+\\))?[\\]\\)]?:?(?:\\[\\d+\\])?:)?\\s*?NOTICE.*: Registration from '.*' failed for '\u003cHOST\u003e(:[0-9]+)?' - Device does not match ACL\\s*$\n|  [5] ^\\s*(?:\\S+ )?(?:@vserver_\\S+ )?(?:(?:\\[\\d+\\])?:\\s+[\\[\\(]?asterisk(?:\\(\\S+\\))?[\\]\\)]?:?|[\\[\\(]?asterisk(?:\\(\\S+\\))?[\\]\\)]?:?(?:\\[\\d+\\])?:)?\\s*?NOTICE.* \u003cHOST\u003e(:[0-9]+)? failed to authenticate as '.*'\\s*$\n|  [6] ^\\s*(?:\\S+ )?(?:@vserver_\\S+ )?(?:(?:\\[\\d+\\])?:\\s+[\\[\\(]?asterisk(?:\\(\\S+\\))?[\\]\\)]?:?|[\\[\\(]?asterisk(?:\\(\\S+\\))?[\\]\\)]?:?(?:\\[\\d+\\])?:)?\\s*?NOTICE.*: No registration for peer '.*' \\(from \u003cHOST\u003e(:[0-9]+)?\\)\\s*$\n|  [7] ^\\s*(?:\\S+ )?(?:@vserver_\\S+ )?(?:(?:\\[\\d+\\])?:\\s+[\\[\\(]?asterisk(?:\\(\\S+\\))?[\\]\\)]?:?|[\\[\\(]?asterisk(?:\\(\\S+\\))?[\\]\\)]?:?(?:\\[\\d+\\])?:)?\\s*?NOTICE.*: Host \u003cHOST\u003e(:[0-9]+)? failed MD5 authentication for '.*' (.*)\\s*$\n|  [8] ^\\s*(?:\\S+ )?(?:@vserver_\\S+ )?(?:(?:\\[\\d+\\])?:\\s+[\\[\\(]?asterisk(?:\\(\\S+\\))?[\\]\\)]?:?|[\\[\\(]?asterisk(?:\\(\\S+\\))?[\\]\\)]?:?(?:\\[\\d+\\])?:)?\\s*?NOTICE.*: Failed to authenticate user .*@\u003cHOST\u003e(:[0-9]+)?.*\\s*$\n|\n`- Number of matches:\n   [1] 0 match(es)\n   [2] 75 match(es)\n   [3] 0 match(es)\n   [4] 0 match(es)\n   [5] 0 match(es)\n   [6] 0 match(es)\n   [7] 0 match(es)\n   [8] 0 match(es)\n\nIgnoreregex\n|- Regular expressions:\n|\n`- Number of matches:\n\nSummary\n=======\n\nAddresses found:\n[1]\n[2]\n    X.X.X.X (Mon Nov 07 11:55:38 2011)\n    X.X.X.X (Mon Nov 07 11:55:38 2011)\n    X.X.X.X (Mon Nov 07 11:55:39 2011)\n    X.X.X.X (Mon Nov 07 11:55:39 2011)\n    Y.Y.Y.Y (Wed Nov 09 04:13:36 2011)\n    Y.Y.Y.Y (Wed Nov 09 04:33:36 2011)\n    Y.Y.Y.Y (Wed Nov 09 08:33:37 2011)\n    Y.Y.Y.Y (Wed Nov 09 08:33:37 2011)\n[3]\n[4]\n[5]\n[6]\n[7]\n[8]\n\nDate template hits:\n13339 hit(s): MONTH Day Hour:Minute:Second\n0 hit(s): WEEKDAY MONTH Day Hour:Minute:Second Year\n0 hit(s): WEEKDAY MONTH Day Hour:Minute:Second\n0 hit(s): Year/Month/Day Hour:Minute:Second\n0 hit(s): Day/Month/Year Hour:Minute:Second\n0 hit(s): Day/MONTH/Year:Hour:Minute:Second\n0 hit(s): Month/Day/Year:Hour:Minute:Second\n0 hit(s): Year-Month-Day Hour:Minute:Second\n0 hit(s): Day-MONTH-Year Hour:Minute:Second[.Millisecond]\n0 hit(s): Day-Month-Year Hour:Minute:Second\n0 hit(s): TAI64N\n0 hit(s): Epoch\n0 hit(s): ISO 8601\n0 hit(s): Hour:Minute:Second\n0 hit(s): \u003cMonth/Day/Year@Hour:Minute:Second\u003e\n\nSuccess, the total number of match is 75\n\nHowever, look at the above section 'Running tests' which could contain\nimportant information.\n```\n\nThe above shows that I have 75 matches and the IPs (removed) that matched\nagainst which rule.\n\n[1]: https://bugzilla.redhat.com/show_bug.cgi?id=244275\n[2]: {{\u003c ref \"./mysql.md\" \u003e}}\n","created_at":-62135596800,"fuzzy_word_count":2200,"path":"/notes/fail2ban/","published_at":1540945455,"reading_time":10,"tags":null,"title":"fail2ban","type":"notes","updated_at":1540945455,"weight":0,"word_count":2125},{"cid":"0cd2660b6e698798a39645f5d518818218127d74","content":"\n***Note: This page is quite old and is likely out of date. My opinions may have\nalso changed dramatically since this was written. It is here as a reference\nuntil I get around to updating it.***\n\nFestival is packaged in a way on Fedora in a standalone mode, it does not come\nwith an init script to startup it's server on boot so the following is an\nadaptation of another one custom written to handle this. It's primarily used in\nFedora for Gnome's usability support which doesn't run the server as a daemon.\n\nListening to the samples of the stock voices, I've found that `ked` sounds the\nbest in my opinion and at least until we here try and make our own diphone\ndatabase this is the voice that I'm going to use.\n\nPackages:\n\n* festival\n* asterisk-festival\n* festvox-ked-diphone\n\nIn Fedora the voices are stored in the directory\n`/usr/share/festival/lib/voices/*/` directories. These are the voice that you\nhave to choose from when configuring festival. The configuration files live in\n`/etc/festival/`.\n\nWe're only interested in `/etc/festival/festival.scm` which doesn't exist by\ndefault. The following file has the `ked` voice set to be the default. I left\nthe other voice in there commented out in case I wanted to switch back some\nother time.\n\n```\n(defvar server_home \".\")\n\n;; Enable access to localhost so asterisk can connect (no one else needs to connect)\n(set! server_access_list '(\"[^.]+\" \"127.0.0.1\" \"localhost.*\" \"argus.lounge.gentlemenslounge.org\"))\n\n(cd server_home)\n(set! default_access_strategy 'direct)\n\n;; Set the voice to 'ked' the other one is left in case I wish to switch later\n(set! voice_default 'voice_ked_diphone)\n;(set! voice_default 'voice_nitech_us_awb_arctic_hts)\n\n; Preload ked voice to make the server more responsive\n(voice_ked_diphone)\n\n;; Asterisk command\n\"(define (tts_textasterisk STRING MODE)\nApply tts to STRING. This function is specifically designed for use in\nserver mode so a single function call may synthesize the string. This function\nname may be added to the server safe functions.\"\n(let ((wholeutt (utt.synth (eval (list 'Utterance 'Text string)))))\n(utt.wave.resample wholeutt 8000)\n(utt.wave.rescale wholeutt 5)\n(utt.send.wave.client.wholeutt)))\n;; End Asterisk command\n\n;(provide 'siteinit)\n```\n","created_at":-62135596800,"fuzzy_word_count":400,"path":"/notes/festival/","published_at":1507583005,"reading_time":2,"tags":null,"title":"Festival","type":"notes","updated_at":1507583005,"weight":0,"word_count":334},{"cid":"3b3961a02f1212bf5b79e379d0f16fc6691e96c3","content":"\nIf you've already implemented the SSH keys for authentication and have a\npassword on your key then you've already achieved multi-factor authentication\nto a certain degree.\n\nThe gatekeeper script is something that I haven't come across on any other\nsystems that other people administer. Perhaps it's too much trouble for them\nwithout much gain. I had the idea for this while watching a James Bond movie.\n\nA Russian systems engineer put riddles on one of his machines that you had to\ngo through in order to access the system, while this alone isn't secure, asking\nthe user random questions after authentication couldn't hurt security. I highly\nrecommend using some logic that changes periodically, but the client can\nremotely deduce.\n\nFirst we need to make a script to handle the additional authentication. Please\nnote that the following script is just an example, most of the security is in\nplace but you'll want to implement your own logic for the questions and\nanswers.\n\n```bash\n#!/bin/sh\n# gatekeeper.sh - Post-login access question script\n\n# In the event that a user tries to get crafty and Ctrl-C out of the script\n# we'll just kill the connection\ntrap jail INT\njail() {\n  kill -9 $PPID\n  exit 0\n}\n\n# Once a user logs in, check to see if they just wanted a shell.\n# SSH_ORIGINAL_COMMAND will be null (-z) if they did\nif [ -z \"$SSH_ORIGINAL_COMMAND\" ]; then\n  # The answer the user needs to know, a function or call to another script\n  # could be used to generate this answer based on any number of resources\n  # available to the system. This could include querying an internal web\n  # script to get a daily password user's of a site have access to (and\n  # perhaps are supposed to be checking)\n  local CORRECT_ANSWER=\"muffins\"\n\n  # Message displayed to the user before being prompted, I'm going to assume\n  # the user knows the question in this case what's the admin's favorite\n  # breakfast\n  echo -n \"Gatekeeper token authentication required: \"\n\n  # Get the user's answer. If the answer is correct execute the command.\n  # If the answer is wrong, log the attempt and kill the connection.\n  while read -s inputline; do\n    RESPONSE=\"$inputline\"\n    echo\n\n    if [ $CORRECT_ANSWER = \"${RESPONSE}\" ]; then\n      echo \"Gatekeeper authentication accepted.\"\n      $SHELL -l\n      exit 0\n    else\n      logger \"Gatekeeper: $USER login failed from $SSH_CLIENT\"\n      kill -9 $PPID\n      exit 0\n    fi\n  done\nfi\n\n# This command will bypass the gatekeeper script if the user tries to rsync as\n# this script will break rsync. It creates a fresh shell just for good measure.\n#if [ `echo $SSH_ORIGINAL_COMMAND | awk '{print $1}'` = rsync ]; then\n#  $SHELL -c \"$SSH_ORIGINAL_COMMAND\"\n#  exit 0\n#fi\n\n# If a user tried to execute something other than an 'approved' command just\n# kill the session. This will prevent SCP and SFTP unless they are configured\n# to bypass the script.\nkill -9 $PPID\nexit 0\n```\n\nPut this script in `/etc/ssh/gatekeeper.sh` and change it's permissions to 755\nwith the owner being root. To make the SSH server pass off control to the\nGatekeeper script once it's done authenticating a user, we'll use the\n'ForceCommand' command in SSHd's config file (`/etc/ssh/sshd_config`). Add the\nfollowing line to the end of the config and restart the SSH daemon.\n\n```\nForceCommand /etc/ssh/gatekeeper.sh\n```\n\nAssuming everything went according to plan, when you SSH into the remote server\nonce your done authenticating it'll ask for the token. Putting in 'muffins'\nshould get you to your shell, while anything else will kill the connection.\n","created_at":-62135596800,"fuzzy_word_count":700,"path":"/notes/gatekeeper-script-for-ssh/","published_at":1507565383,"reading_time":3,"tags":null,"title":"Gatekeeper Script for SSH","type":"notes","updated_at":1507565383,"weight":0,"word_count":639},{"cid":"2c44a018b90b5fffa1c8ae33edb2a42a138d4e6e","content":"\n***Note: This page is quite old and is likely out of date. My opinions may have\nalso changed dramatically since this was written. It is here as a reference\nuntil I get around to updating it.***\n\nI came across this python script that accesses GPIO pins in linux. It's\npotentially very useful so I'm including it here, in it's entirety. I can later\non reference this for use in my own programs.\n\n```\n\"\"\"Arduino-like library for Python on BeagleBone\"\"\"\nimport time\n\nHIGH = \"HIGH\"\nLOW = \"LOW\"\nOUTPUT = \"OUTPUT\"\nINPUT = \"INPUT\"\npinList = [] # needed for unexport()\nstartTime = time.time() # needed for millis()\ndigitalPinDef = {\n  \"P8.3\":   38,\n  \"P8.4\":   39,\n  \"P8.5\":   34,\n  \"P8.6\":   35,\n  \"P8.11\":  45,\n  \"P8.12\":  44,\n  \"P8.14\":  26,\n  \"P8.15\":  47,\n  \"P8.16\":  46,\n  \"P8.17\":  27,\n  \"P8.18\":  65,\n  \"P8.20\":  63,\n  \"P8.21\":  62,\n  \"P8.22\":  37,\n  \"P8.23\":  36,\n  \"P8.24\":  33,\n  \"P8.25\":  32,\n  \"P8.26\":  61,\n  \"P8.27\":  86,\n  \"P8.28\":  88,\n  \"P8.29\":  87,\n  \"P8.30\":  89,\n  \"P8.39\":  76,\n  \"P8.40\":  77,\n  \"P8.41\":  74,\n  \"P8.42\":  75,\n  \"P8.43\":  72,\n  \"P8.44\":  73,\n  \"P8.45\":  70,\n  \"P8.46\":  71,\n  \"P9.12\":  60,\n  \"P9.15\":  48,\n  \"P9.23\":  49,\n  \"P9.25\":  117,\n  \"P9.27\":  115,\n  \"P9.42\":  7\n}\n\nanalogPinDef = {\n  \"P9.33\":  \"ain4\",\n  \"P9.35\":  \"ain6\",\n  \"P9.36\":  \"ain5\",\n  \"P9.37\":  \"ain2\",\n  \"P9.38\":  \"ain3\",\n  \"P9.39\":  \"ain0\",\n  \"P9.40\":  \"ain1\"\n}\n\ndef pinMode(pin, direction):\n  \"\"\"pinMode(pin, direction) opens (exports)  a pin for use and \n  sets the direction\"\"\"\n  if pin in digitalPinDef:\n    fw = file(\"/sys/class/gpio/export\", \"w\")\n    fw.write(\"%d\" % (digitalPinDef[pin]))\n    fw.close()\n    fileName = \"/sys/class/gpio/gpio%d/direction\" % (digitalPinDef[pin])\n    fw = file(fileName, \"w\")\n    if direction == INPUT:\n      fw.write(\"in\")\n    else:\n      fw.write(\"out\")\n    fw.close()\n    pinList.append(digitalPinDef[pin])\n  else:\n    print \"pinMode error: Pin \" + pin + \" is not defined as a digital I/O pin in the pin definition.\"\n\n\ndef digitalWrite(pin, status):\n  \"\"\"digitalWrite(pin, status) sets a pin HIGH or LOW\"\"\"\n  if pin in digitalPinDef:\n    fileName = \"/sys/class/gpio/gpio%d/value\" % (digitalPinDef[pin])\n    fw = file(fileName, \"w\")\n    if status == HIGH:\n      fw.write(\"1\")\n    if status == LOW:\n      fw.write(\"0\")\n    fw.close()\n  else:\n    print \"digitalWrite error: Pin \" + pin + \" is not defined as a digital I/O pin in the pin definition.\"\n\ndef digitalRead(pin):\n  \"\"\"digitalRead(pin) returns HIGH or LOW for a given pin.\"\"\"\n  if pin in digitalPinDef:\n    fileName = \"/sys/class/gpio/gpio%d/value\" % (digitalPinDef[pin])\n    fw = file(fileName, \"r\")\n    inData = fw.read()\n    fw.close()\n    if inData == \"0\\n\":\n      return LOW\n    if inData == \"1\\n\":\n      return HIGH\n  else:\n    print \"digitalRead error: Pin \" + pin + \" is not defined as a digital I/O pin in the pin definition.\"\n    return -1;\n\ndef analogRead(pin):\n  \"\"\"analogRead(pin) returns analog value for a given pin.\"\"\"\n  if pin in analogPinDef:\n    fileName = \"/sys/devices/platform/tsc/\" + (analogPinDef[pin])\n    fw = file(fileName, \"r\")\n    return fw.read()\n  fw.close()\nelse:\n  print \"analogRead error: Pin \" + pin + \" is not defined as an analog in pin in the pin definition.\"\n    return -1;\n\ndef pinUnexport(pin):\n  \"\"\"pinUnexport(pin) closes a pin in sysfs. This is susally \n  called by cleanup() when a script is exiting.\"\"\"\n  fw = file(\"/sys/class/gpio/unexport\", \"w\")\n  fw.write(\"%d\" % (pin))\n  fw.close()\n\ndef cleanup():\n  \"\"\" takes care of stepping through pins that were set with\n  pinMode and unExports them. Prints result\"\"\"\n  def find_key(dic, val):\n    return [k for k, v in dic.iteritems() if v == val][0]\n  print \"\"\n  print \"Cleaning up. Unexporting the following pins:\",\n  for pin in pinList:\n    pinUnexport(pin)\n    print find_key(digitalPinDef, pin),\n\ndef delay(millis):\n  \"\"\"delay(millis) sleeps the script for a given number of \n  milliseconds\"\"\"\n  time.sleep(millis/1000.0)\n\ndef millis():\n  \"\"\"millis() returns an int for the number of milliseconds since \n  the script started.\"\"\"\n  return int((time.time() - startTime) * 1000)\n\ndef run(setup, main): # from PyBBIO by Alexander Hiam - ahiam@marlboro.edu - www.alexanderhiam.com https://github.com/alexanderhiam/PyBBIO\n  \"\"\" The main loop; must be passed a setup and a main function.\n  First the setup function will be called once, then the main\n  function wil be continuously until a stop signal is raised, \n  e.g. CTRL-C or a call to the stop() function from within the\n  main function. \"\"\"\n  try:\n    setup()\n    while (True):\n      main()\n  except KeyboardInterrupt:\n    # Manual exit signal, clean up and exit happy\n    cleanup()\n  except Exception, e:\n    # Something may have gone wrong, clean up and print exception\n    cleanup()\n    print e\n```\n","created_at":-62135596800,"fuzzy_word_count":700,"path":"/notes/gpio/","published_at":1507584110,"reading_time":4,"tags":null,"title":"GPIO","type":"notes","updated_at":1507584110,"weight":0,"word_count":646},{"cid":"befe051bf06d8d13f32be5ace3cbfbe0476d333d","content":"\n***Note: This page is quite old and is likely out of date. My opinions may have\nalso changed dramatically since this was written. It is here as a reference\nuntil I get around to updating it.***\n\n## Installation\n\nI've been using a BU-353 which if I remember correctly was ~$20 and has been\nfairly reliable. It's also got a magnetic base allowing you to attach it to the\nroof of a car when war-driving.\n\n```\nyum install gpsd gpsd-clients -y\n```\n\nIf you want to use gpsd on a headless server you'll want to exclude the\n`gpsd-clients` as they will install `xgps` and all of X as a dependency.\n\n## Configuration\n\nMy GPS device is identified by udev as a serial port, which is fairly common as\nNMEA specifies a serial connection with a baud rate of 4800. There is an issue,\nhowever, with a program running in the background called `modem-manager`.\n\nIt will actively take control over any serial device that gets plugged in and\nsend \"AT\" commands at it to try and determine whether or not the device is a\nmodem.  It will release control if it fails after 10 seconds but that initial\ncontrol breaks GPSd auto-device detection.\n\nTo prevent this from happening we need to blacklist the device to\nmodem-manager. If you use a modem (hah) then you'll want to make sure the\ndevice IDs don't conflict. To get your device ID run the following command:\n\n```\nudevadm monitor --env\n```\n\nNow plug in your GPS device and you should see something similar to the\nfollowing output:\n\n```\n**snip**\n\nUDEV  [10963.023977] add      /devices/pci0000:00/0000:00:1d.0/usb2/2-1/2-1.2/2-1.2:1.0/ttyUSB4/tty/ttyUSB4 (tty)\nACTION=add\nDEVLINKS=/dev/gps4 /dev/serial/by-id/usb-Prolific_Technology_Inc._USB-Serial_Controller-if00-port0 /dev/serial/by-path/pci-0000:00:1d.0-usb-0:1.2:1.0-port0\nDEVNAME=/dev/ttyUSB4\nDEVPATH=/devices/pci0000:00/0000:00:1d.0/usb2/2-1/2-1.2/2-1.2:1.0/ttyUSB4/tty/ttyUSB4\nID_BUS=usb\nID_MM_CANDIDATE=1\nID_MODEL=USB-Serial_Controller\nID_MODEL_ENC=USB-Serial\\x20Controller\nID_MODEL_FROM_DATABASE=PL2303 Serial Port\nID_MODEL_ID=2303\nID_PATH=pci-0000:00:1d.0-usb-0:1.2:1.0\nID_PATH_TAG=pci-0000_00_1d_0-usb-0_1_2_1_0\nID_REVISION=0300ID_SERIAL=Prolific_Technology_Inc._USB-Serial_Controller\nID_TYPE=generic\nID_USB_DRIVER=pl2303\nID_USB_INTERFACES=:ff0000:\nID_USB_INTERFACE_NUM=00\nID_VENDOR=Prolific_Technology_Inc.\nID_VENDOR_ENC=Prolific\\x20Technology\\x20Inc.\nID_VENDOR_FROM_DATABASE=Prolific Technology, Inc.\nID_VENDOR_ID=067b\nMAJOR=188\nMINOR=4\nSEQNUM=2470\nSUBSYSTEM=tty\nTAGS=:systemd:\nUSEC_INITIALIZED=10963017963\n\n**snip**\n```\n\nYou'll probably see this a couple time as udev announces all kinds of\ncrazyness. The important fields to look for are `ID_VENDOR_ID` and\n`ID_MODEL_ID`, these will become your `idVendor` and `idProduct` the udev rules\nto come. The values for mine are `067b` and `2303` respectively.\n\nCreate the file `/etc/udev/rules.d/77-user-mm-usb-device-blacklist.rules` with\nthe following contents, you'll want to replace the values of `idVendor` and\n`idProduct` if yours differ from mine:\n\n```\nACTION!=\"add|change\", GOTO=\"user_mm_usb_device_blacklist_end\"\nSUBSYSTEM!=\"usb\", GOTO=\"user_mm_usb_device_blacklist_end\"\nENV{DEVTYPE}!=\"usb_device\", GOTO=\"user_mm_usb_device_blacklist_end\"\n\nATTRS{idVendor}==\"067b\", ATTRS{idProduct}==\"2303\", ENV{ID_MM_DEVICE_IGNORE}=\"1\"\n\nLABEL=\"user_mm_usb_device_blacklist_end\"\n```\n\nThe changes should be picked up immediately so no need to restart or anything\nsilly like that.\n\nNow you'll want to start up gpsd and ensure it starts up on boot as well:\n\n```\nsystemctl enable gpsd.service\nsystemctl start gpsd.service\n```\n\nThats the end of the configuration, whenever you plug in one or more GPS\ndevices hotplug will take care of notifying gpsd of the devices, and will\nremove it when it's been removed. You can view information on your position as\nwell as a fix of your location with `xgps` which is installed with the\n`gpsd-clients`.\n\n## Using as a Time Source\n\nTODO\n\n## Extending Accuracy with NTRIP / RTCM data\n\nTODO\n\n* http://en.wikipedia.org/wiki/Networked_Transport_of_RTCM_via_Internet_Protocol\n* http://igs.bkg.bund.de/ntrip/caster\n* http://www.linuxcertif.com/man/5/rtcm-104/173922/\n* http://software.rtcm-ntrip.org/ (BNC is the relevant one)\n\n## Profiling Accuracy of GPS Receivers\n\nIncluded with the gpsd-clients is a utility called `gpsprof` which can take\nsamples from a GPS receiver and test the margin of error in x,y,z spaces. By\ndefault it collects 100 samples.\n\nSince NMEA specifies producing 1/sample/sec this will by default take 100\nseconds or 1 minute 40 seconds to complete. It outputs the information in\ngnuplot format so we'll need that to view the data:\n\n```\nyum install gnuplot -y\n```\n\nDuring the test you'll want to ensure that the GPS receiver is in a fixed\nlocation. Personally I trust data more with a larger sample size so I opted to\ntest my receiver over a period of two hours. Like so:\n\n```\ngpsprof -n 7200 | tee two-hour-gps-prof.gnuplot | gnuplot -persist\n```\n\nThis also saves the data into a file so you can regenerate the graph without\nrunning the test for two hours again.\n\nWhat I found while running the test indoors is that my GPS receiver isn't that\naccurate (big surprise for $20).\n\nOver the course of two hours of testing is that 50% of the fixes were with 9.24\nmeters of the average, 95% were within 27.32 meters of the average, and 99% of\nthe fixes were within 37.5 meters (this is only for latitude and longitude).\n\nThe latitude varied by as much as 60 meters and the logitude by as much as 40\nmeters. The altitude varied by as much as 80 meters though I don't have a\nstatistical breakdown for that. Of the 7200 data point there was also 45 where\nthe receiver was unable to calculate altitude. The average location (at least\nfor lat/long) was incredibly accurate as verified by Google Maps so at least\nthat's something.\n\nA second test run over the course of an hour with a clear view of the sky\nthrough the two separate windows provided an accuracy with 50% of fixes within\n6.23 meters of average, 95% of fixes within 25.01 meters, and 99% wihin 31.94\nmeters.\n\nThe latitude varied by as much as 40 meters and the longitude by as much as 25\nmeters. The altitude varied by as much as 40 meters. All fixes had altitude\nfixes as well.\n\nI suspect some if not quite a bit of the error was because I was indoors and\nnowhere near a window (my office is in the heart of a building unfortunately).\n\n## Misc References\n\n* http://catb.org/gpsd/hacking.html\n","created_at":-62135596800,"fuzzy_word_count":900,"path":"/notes/gpsd/","published_at":1507584110,"reading_time":5,"tags":null,"title":"GPSd","type":"notes","updated_at":1507584110,"weight":0,"word_count":877},{"cid":"5f9f51b88369aa0735f750690d30423db64eac06","content":"\n***Note: This page is quite old and is likely out of date. My opinions may have\nalso changed dramatically since this was written. It is here as a reference\nuntil I get around to updating it.***\n\n## Security\n\n### Password Protection\n\nIf left unchecked a malicious user with console access can add or remove kernel\nparameters or chain-boot onto another device when booting using grub. You can\nprevent this by requiring the user to enter a password to modify the run-time\nconfiguration of grub on boot.\n\nYou can use the 'Bootloader Password' option during a CentOS or Fedora install\nhowever it's uses md5 to hash the password which is a known broken scheme.\n`grub-crypt` which comes installed by default on Fedora allows you to use\nsha512 to hash the password like so:\n\n```\n[root@localhost ~]# grub-crypt --sha-512\nPassword:\nRetype password:\n$6$Um4l/Bido.ySrD.H$uuQjipx3uCu/XwGAfqOQsdIw1m1dphRbUbKOsoT5EpCt4LGi0kGdckDE3SPj2eS3pJ9DCJy3V/TqlqJOjjMvJ1\n```\n\nNote: Do not use the hash displayed it above, it is not secure (I used\n\"password\")\n\nYou would then add the following line (or replace an existing one) to\n`/etc/grub.conf`\n\n```\npassword --sha-512 $6$Um4l/Bido.ySrD.H$uuQjipx3uCu/XwGAfqOQsdIw1m1dphRbUbKOsoT5EpCt4LGi0kGdckDE3SPj2eS3pJ9DCJy3V/TqlqJOjjMvJ1\n```\n\n### Protect the Configuration\n\nSince the grub configuration file has a password hash in it (regardless of the\ncurrent state of security of that hash type), as additional information about\nthe server it should be restricted to only be readable and writeable by root\nlike so:\n\n```\n[root@localhost ~]# chmod 600 /etc/grub.conf\n```\n","created_at":-62135596800,"fuzzy_word_count":300,"path":"/notes/grub/","published_at":1507584890,"reading_time":2,"tags":null,"title":"Grub","type":"notes","updated_at":1507584890,"weight":0,"word_count":216},{"cid":"b69e377d49f6f7e8465a3858da5181f93a0003fb","content":"\n***Note: This page is quite old and is likely out of date. My opinions may have\nalso changed dramatically since this was written. It is here as a reference\nuntil I get around to updating it.***\n\n## Notes on Setup\n\nI setup two simple nginx webservers to test this configuration. They both\nserved up a simple static page whose only contents was an indication of which\nserver it was served from.\n\n```sh\nyum install haproxy keepalived -y\n```\n\nSetup the logging haproxy will use\n\n```\ncat \u003c\u003c EOF \u003e /etc/rsyslog.d/haproxy.conf\nlocal2.*        /var/log/haproxy.log\nEOF\n\nservice rsyslog restart\n```\n\nThe following is the config that worked as an initial pass setting up haproxy.\nThe configuration lives at `/etc/haproxy/haproxy.cfg`.\n\n```\nglobal\n  log         /dev/log local2 info\n\n  chroot      /var/lib/haproxy\n  pidfile     /var/run/haproxy.pid\n  maxconn     10000\n  user        haproxy\n  group       haproxy\n  daemon\n\n  stats socket /var/lib/haproxy/stats\n\ndefaults\n  mode                    http\n  log                     global\n  option                  httplog\n  option                  dontlognull\n  option http-server-close\n  option forwardfor       except 127.0.0.0/8\n  option                  redispatch\n  retries                 3\n  timeout http-request    5s\n  timeout queue           1m\n  timeout connect         5s\n  timeout client          1m\n  timeout server          1m\n  timeout http-keep-alive 10s\n  timeout check           5s\n  maxconn                 5000\n\nfrontend  main *:80\n  default_backend             app\n\nbackend app\n  balance leastconn\n  server  nginx-01 192.168.122.61:80 check\n  server  nginx-02 192.168.122.62:80 check\n\nlisten ssl :443\n  balance leastconn\n  mode    tcp\n  server  nginx-01 192.168.122.61:443 check\n  server  nginx-02 192.168.122.62:443 check\n```\n\nYou will also need to open up the outbound firewall to nginx and the inbound\nfirewall to the port.\n\n```\n-A INPUT  -m tcp -p tcp --dport 80 -m conntrack --ctstate NEW -j ACCEPT\n-A OUTPUT -m tcp -p tcp --dport 80 -j ACCEPT\n```\n\nAnd setup haproxy to run.\n\n```\nsystemctl enable haproxy.service\nsystemctl start haproxy.service\n```\n","created_at":-62135596800,"fuzzy_word_count":300,"path":"/notes/haproxy/","published_at":1507584890,"reading_time":2,"tags":null,"title":"HAProxy","type":"notes","updated_at":1507584890,"weight":0,"word_count":255},{"cid":"dbd4c708ddf852e826510227e0fa290f0b4838c4","content":"\n***Note: This page is quite old and is likely out of date. My opinions may have\nalso changed dramatically since this was written. It is here as a reference\nuntil I get around to updating it.***\n\nPackages of interest:\n\n* keepalived\n* ipvsadm\n* pacemaker\n\n## IPVS\n\nIPVS (IP Virtual Server) is used to present a single address in a high\navailability scenario for one more services.\n\n## Installation / Setup\n\n```\nyum install ipvsadm -y\n```\n\nA note about LXC containers: In order to make use of ipvsadm within an LXC\ncontainer, you will also need to install the ipvsadm package on the LXC host\nand reboot it (optionally just load the kernel module though I don't know it by\nname). Additionally the simplest version, VS/NAT, requires enabling IPv4 packet\nforwarding which requires modification of the read-only proc filesystem and\nthus doesn't seem to be an easy option, there may be work arounds for this, and\nit may effect the other forms but they haven't been tested yet.\n\nFirst pass at commands:\n\n```\nipvsadm -A -t 192.168.122.30:443 -s wlc\nipvsadm -a -t 192.168.122.30:443 -r 192.168.122.61:443 -g -w 1\nipvsadm -a -t 192.168.122.30:443 -r 192.168.122.62:443 -g -w 1\n```\n\nThat didn't work so I rebooted to clear it all out.\n\n```\nyum install keepalived -y\n```\n\nAh ha, Eureka moment. Inside the LXC container I can unmount the `/proc/sys`\noverlay as long as I'm root...\n\n```\numount /proc/sys\necho 1 \u003e /proc/sys/net/ipv4/ip_forward\n```\n\nIt seems that `net.ipv4.ip_nonlocal_bind` isn't available within an LXC\ncontainer: `cat /proc/sys/net/ipv4/ip_nonlocal_bind`.\n\nValuable sites:\n\n* http://mojobojo.com/blog/2011/01/14/lvs-nginx-nodejs-mongodb-cluster-setup-on-rackspace/\n* https://www.rackspace.com/blog/installing-and-configuring-lvs-tun/\n\nInteresting use for this as a firewall:\n\n* http://keepalived.org/Keepalived-LVS-NAT-Director-ProxyArp-Firewall-HOWTO.html\n* http://backreference.org/2013/04/03/firewall-ha-with-conntrackd-and-keepalived/\n","created_at":-62135596800,"fuzzy_word_count":300,"path":"/notes/high-availability/","published_at":1507584890,"reading_time":2,"tags":null,"title":"High Availability","type":"notes","updated_at":1507584890,"weight":0,"word_count":249},{"cid":"258035afaeeda82db8b96ec8e5522d50d8b2856f","content":"\n***Note: This page is quite old and is likely out of date. My opinions may have\nalso changed dramatically since this was written. It is here as a reference\nuntil I get around to updating it.***\n\nInstalling and setting up hipache. To actually install this I had to open up\nport `tcp/9418` outbound on the server.\n\n```\nyum install git npm -y\nnpm install hipache -g\n```\n\nCreate an initial configuration file:\n\n```\ncat \u003c\u003c EOF \u003e /etc/hipache.json\n{\n  \"server\": {\n    \"accessLog\": \"/var/log/hipache_access.log\",\n    \"port\": 80,\n    \"workers\": 5,\n    \"maxSockets\": 100,\n    \"deadBackendTTL\": 30,\n    \"address\": [\"0.0.0.0\"],\n    \"address6\": [\"::\"]\n  },\n  \"redisHost\": \"192.168.122.101\",\n  \"redisPort\": 6379,\n  \"redisDatabase\": 0,\n  \"redisPassword\": \"password\"\n}\nEOF\n```\n\nAllow redis access to the local redis servers:\n\n```\n-A OUTPUT -m tcp -p tcp --dport 6379 -d 192.168.122.0/24 -j ACCEPT\n```\n","created_at":-62135596800,"fuzzy_word_count":200,"path":"/notes/hipache/","published_at":1507584890,"reading_time":1,"tags":null,"title":"Hipache","type":"notes","updated_at":1507584890,"weight":0,"word_count":121},{"cid":"7725fd500d786124f7bf425e480f5ba1973b4330","content":"\n***Note: This page is quite old and is likely out of date. My opinions may have\nalso changed dramatically since this was written. It is here as a reference\nuntil I get around to updating it.***\n\nApache or httpd is a strong and well tested webserver.\n\n## Installation\n\nTo install the Apache web server for this hardening guide on fedora run the\nfollowing command as root:\n\n```\n[root@localhost] ~# yum install httpd mod_evasive mod_gnutls mod_security \\\n  mod_selinux -y\n```\n\nA few modules I need to look into (TODO):\n\n* `mod_log_post`\n\n## Security Notes\n\nApache starts up using root privileges initially. Dropping the privileges is\nhighly recommended as soon as possible (which it does if alternative\ncredentials are supplied in it's configuration).\n\nAdditionally, the Apache web server can be used to remotely execute dynamic\ncode if a vulnerability is found with the privileges the process is running as.\nThis could be very dangerous... The languages allowed to be executed should be\nchosen wisely and hardened appropriately.\n\nUser credentials can be passed through the web server so it is strongly\nrecommended that SSL certificates be used. If SSL certificates are available\nthere are very few reasons not to use SSL everywhere.\n\nThe only exception I have been able to come up with is for an application that\nis extremely latency sensitive, contains no sensitive data, and have the data\nverified in a different way (through data signatures like HMAC or GPG). The\nlatter is not necessarily required.\n\n### Firewall Adjustments\n\nBy default Apache only runs on port 80 using TCP, if SSL is enabled it will\nalso use port 443 over SSL. Additional ports can be configured by the admin.\n\n```\n-A SERVICES -m tcp -p tcp --dport 80 -j ACCEPT\n-A SERVICES -m tcp -p tcp --dport 443 -j ACCEPT\n```\n\nThis service is very sensitive to the flood attack rules. It is recommended\nthese be adjusted to allow twice the maximum number of connections expected\nduring a time frame or alternatively to allow traffic on these two ports too\nbypass the anti-flood rules.\n\n## Performance Notes\n\nIf you use `mod_security` you should note that in my testing each Apache thread\nincreases it's resident memory size by ~27Mb using the stock configuration. For\nservers that need to handle a high volume of requests this may not be\nacceptable. If using `mod_security` you DEFINITELY need to adjust the\n`ServerLimit` and `MaxClients` (they are aliases of each other so should always\nbe the same).\n\n### Tuning ServerLimit/MaxClients with the worker MPM\n\n* Start up the apache server\n* Use the `ps` utility to determine the size of client threads (will be in Kb)\n* Determine that max amount of RAM the apache process should be allowed to use\n* Subtract the `parent` processes RAM from that\n* Divide the remaining RAM by the size of the client threads, this is your\n  ServerLimit / MaxClient value\n\nAn example of this is provided below:\n\n```\n[root@localhost ~]# /etc/init.d/httpd start\nStarting httpd:                                            [  OK  ]\n[root@localhost ~]# ps aux -u apache | grep httpd\nroot      2238  0.7  0.4 177132  9312 ?        Ss   10:45   0:00 /usr/sbin/httpd\napache    2240  0.0  0.2 177132  5124 ?        S    10:45   0:00 /usr/sbin/httpd\napache    2241  0.0  0.2 177132  5124 ?        S    10:45   0:00 /usr/sbin/httpd\napache    2242  0.0  0.2 177132  5124 ?        S    10:45   0:00 /usr/sbin/httpd\napache    2243  0.0  0.2 177132  5124 ?        S    10:45   0:00 /usr/sbin/httpd\napache    2244  0.0  0.2 177132  5124 ?        S    10:45   0:00 /usr/sbin/httpd\napache    2245  0.0  0.2 177132  5124 ?        S    10:45   0:00 /usr/sbin/httpd\napache    2246  0.0  0.2 177132  5124 ?        S    10:45   0:00 /usr/sbin/httpd\napache    2247  0.0  0.2 177132  5124 ?        S    10:45   0:00 /usr/sbin/httpd\nroot      2249  0.0  0.0 103376   832 pts/0    S+   10:45   0:00 grep --color=auto httpd\n[root@localhost ~]# PARENTMEMORY=$((9312/1024))\n[root@localhost ~]# CHILDMEMORY=$((5124/1024))\n[root@localhost ~]# APACHERAM=1024\n[root@localhost ~]# echo $(($(($APACHERAM-$PARENTMEMORY))/$CHILDMEMORY))\n203\n```\n\nThe parent process will have a running user of 'root', all of the children\nprocesses will have 'apache'. In the above example the parent process is using\n9312Kb of memory or ~9Mb, the child process is using 5124Kb or ~5Mb, and we're\nallowing Apache to use up to 1Gb of memory (1024Mb). With that knowledge we can\nsee that we should set ServerLimit and MaxClients to \"203\" each.\n\n## Configuration\n\n### /etc/httpd/conf/httpd.conf\n\nThis is the main configuration file. This differs quite a bit from the\nconfiguration files that come with most distributions, most notably most of the\nmodules are disabled and sections of the config are only applicable if certain\nmodules are loaded. This preserves compatibility with any functionality I may\nneed in the future while removing the bloat of the modules.\n\nTo use this configuration you'll need to create the directory\n`/var/www/html/default`. Any additional domains should be created in\n`/var/www/html` with their domain name as the folder name.\n\n```\n### Section 1: Global Environment\n\nServerTokens Prod\nServerRoot \"/etc/httpd\"\nPidFile run/httpd.pid\nTimeout 60\nKeepAlive On\nMaxKeepAliveRequests 100\nKeepAliveTimeout 5\n\n\u003cIfModule prefork.c\u003e\n  StartServers    8\n  MinSpareServers   5\n  MaxSpareServers   20\n  ServerLimit   256\n  MaxClients    256\n  MaxRequestsPerChild 5000\n\u003c/IfModule\u003e\n\n# This isn't used by default in Fedora but can be by adjusting\n# /etc/sysconfig/httpd\n\u003cIfModule worker.c\u003e\n  StartServers    4\n  MaxClients    300\n  MinSpareThreads   25\n  MaxSpareThreads   75\n  ThreadsPerChild   25\n  MaxRequestsPerChild 0\n\u003c/IfModule\u003e\n\nListen 80\n\n# Use to restrict by IP Address/Range\nLoadModule authz_host_module modules/mod_authz_host.so\n\n# Logging Modules\nLoadModule log_forensic_module modules/mod_log_forensic.so\nLoadModule log_config_module modules/mod_log_config.so\nLoadModule logio_module modules/mod_logio.so\n\n# Allow sending and detecting of mime types\nLoadModule mime_module modules/mod_mime.so\n\n# Header magic and compression\nLoadModule headers_module modules/mod_headers.so\nLoadModule expires_module modules/mod_expires.so\nLoadModule deflate_module modules/mod_deflate.so\n\n# Provides DirectoryIndex directive\nLoadModule dir_module modules/mod_dir.so\n\n# Disable if you don't need directory indexes\nLoadModule autoindex_module modules/mod_autoindex.so\n\n# Used by most MVC stuff\nLoadModule rewrite_module modules/mod_rewrite.so\n\n#LoadModule env_module modules/mod_env.so\n#LoadModule setenvif_module modules/mod_setenvif.so\n#LoadModule auth_basic_module modules/mod_auth_basic.so\n#LoadModule authn_file_module modules/mod_authn_file.so\n#LoadModule authn_alias_module modules/mod_authn_alias.so\n#LoadModule authn_dbm_module modules/mod_authn_dbm.so\n#LoadModule authn_default_module modules/mod_authn_default.so\n#LoadModule authz_user_module modules/mod_authz_user.so\n#LoadModule authz_owner_module modules/mod_authz_owner.so\n#LoadModule authz_groupfile_module modules/mod_authz_groupfile.so\n#LoadModule authz_dbm_module modules/mod_authz_dbm.so\n#LoadModule authz_default_module modules/mod_authz_default.so\n#LoadModule mime_magic_module modules/mod_mime_magic.so\n#LoadModule vhost_alias_module modules/mod_vhost_alias.so\n#LoadModule alias_module modules/mod_alias.so\n#LoadModule cache_module modules/mod_cache.so\n#LoadModule disk_cache_module modules/mod_disk_cache.so\n#LoadModule ext_filter_module modules/mod_ext_filter.so\n#LoadModule usertrack_module modules/mod_usertrack.so\n#LoadModule dav_module modules/mod_dav.so\n#LoadModule dav_fs_module modules/mod_dav_fs.so\n#LoadModule actions_module modules/mod_actions.so\n#LoadModule speling_module modules/mod_speling.so\n#LoadModule suexec_module modules/mod_suexec.so\n#LoadModule cgi_module modules/mod_cgi.so\n#LoadModule ldap_module modules/mod_ldap.so\n#LoadModule auth_digest_module modules/mod_auth_digest.so\n#LoadModule authn_anon_module modules/mod_authn_anon.so\n#LoadModule authnz_ldap_module modules/mod_authnz_ldap.so\n#LoadModule userdir_module modules/mod_userdir.so\n#LoadModule status_module modules/mod_status.so\n#LoadModule include_module modules/mod_include.so\n#LoadModule info_module modules/mod_info.so\n#LoadModule negotiation_module modules/mod_negotiation.so\n#LoadModule proxy_module modules/mod_proxy.so\n#LoadModule proxy_balancer_module modules/mod_proxy_balancer.so\n#LoadModule proxy_http_module modules/mod_proxy_http.so\n#LoadModule proxy_connect_module modules/mod_proxy_connect.so\n#LoadModule proxy_ftp_module modules/mod_proxy_ftp.so\n#LoadModule cern_meta_module modules/mod_cern_meta.so\n#LoadModule asis_module modules/mod_asis.so\n#LoadModule unique_id_module modules/mod_unique_id.so\n\nInclude conf.d/*.conf\n\n\u003cIfModule status_module\u003e\n  ExtendedStatus Off\n\u003c/IfModule\u003e\n\nUser apache\nGroup apache\n\n### Section 2: 'Main' server configuration\n\nServerAdmin sstelfox@bedroomprogrammers.net\nUseCanonicalName Off\nDocumentRoot \"/var/www/html/default\"\n\n\u003cDirectory /\u003e\n  Options FollowSymLinks\n  AllowOverride None\n\n  Order Deny,Allow\n  Deny from all\n\u003c/Directory\u003e\n\n\u003cDirectory \"/var/www/html\"\u003e\n  # Indexes Includes FollowSymLinks SymLinksifOwnerMatch ExecCGI MultiViews\n  Options Indexes SymLinksifOwnerMatch\n\n  AllowOverride FileInfo AuthConfig Limit\n\n  Order allow,deny\n  Allow from all\n\u003c/Directory\u003e\n\n\u003cIfModule mod_userdir.c\u003e\n  # This module won't get loaded unless we want to use it so we should\n  # make sure that it's properly configured\n  UserDir disabled root\n  UserDir public_html\n\n  \u003cDirectory /home/*/public_html\u003e\n    AllowOverride AuthConfig Limit\n    Options Indexes SymLinksIfOwnerMatch\n    \u003cLimit GET POST\u003e\n      Order allow,deny\n      Allow from all\n    \u003c/Limit\u003e\n    \u003cLimitExcept GET POST\u003e\n      Order deny,allow\n      Deny from all\n    \u003c/LimitExcept\u003e\n  \u003c/Directory\u003e\n\u003c/IfModule\u003e\n\nDirectoryIndex index.html\n\nAccessFileName .htaccess\n\u003cFiles ~ \"^\\.ht\"\u003e\n  Order allow,deny\n  Deny from all\n\u003c/Files\u003e\n\nTypesConfig /etc/mime.types\nDefaultType text/plain\n\n\u003cIfModule mod_mime_magic.c\u003e\n  MIMEMagicFile conf/magic\n\u003c/IfModule\u003e\n\nHostnameLookups Off\n\n# Disable these if we need to serve files from NFS\nEnableMMAP On\nEnableSendfile On\n\nErrorLog logs/error_log\nLogLevel warn\n\nLogFormat \"%v %h %t \\\"%r\\\" %\u003es %b \\\"%{Referer}i\\\" \\\"%{User-Agent}i\\\" \\\"%{forensic-id}n\\\" %I %O %T\" combined\nLogFormat \"%v %h %t \\\"%r\\\" %\u003es %b\" common\nLogFormat \"%v %{Referer}i -\u003e %U\" referer\nLogFormat \"%v %{User-agent}i\" agent\n\nCustomLog logs/access_log combined\nForensicLog logs/forensic_log\n\nServerSignature Off\n\n\u003cIfModule mod_dav_fs.c\u003e\n  DAVLockDB /var/lib/dav/lockdb\n\u003c/IfModule\u003e\n\n\u003cIfModule autoindex_module\u003e\n  Alias /icons/ \"/var/www/icons/\"\n  \u003cDirectory \"/var/www/icons\"\u003e\n    Options SymLinksIfOwnerMatch\n    AllowOverride None\n\n    Order allow,deny\n    Allow from all\n  \u003c/Directory\u003e\n\n  IndexOptions FancyIndexing VersionSort NameWidth=* HTMLTable Charset=UTF-8\n\n  AddIconByEncoding (CMP,/icons/compressed.gif) x-compress x-gzip\n\n  AddIconByType (TXT,/icons/text.gif) text/*\n  AddIconByType (IMG,/icons/image2.gif) image/*\n  AddIconByType (SND,/icons/sound2.gif) audio/*\n  AddIconByType (VID,/icons/movie.gif) video/*\n\n  AddIcon /icons/binary.gif .bin .exe\n  AddIcon /icons/binhex.gif .hqx\n  AddIcon /icons/tar.gif .tar\n  AddIcon /icons/world2.gif .wrl .wrl.gz .vrml .vrm .iv\n  AddIcon /icons/compressed.gif .Z .z .tgz .gz .zip\n  AddIcon /icons/a.gif .ps .ai .eps\n  AddIcon /icons/layout.gif .html .shtml .htm .pdf\n  AddIcon /icons/text.gif .txt\n  AddIcon /icons/c.gif .c\n  AddIcon /icons/p.gif .pl .py\n  AddIcon /icons/f.gif .for\n  AddIcon /icons/dvi.gif .dvi\n  AddIcon /icons/uuencoded.gif .uu\n  AddIcon /icons/script.gif .conf .sh .shar .csh .ksh .tcl\n  AddIcon /icons/tex.gif .tex\n  AddIcon /icons/bomb.gif core\n\n  AddIcon /icons/back.gif ..\n  AddIcon /icons/hand.right.gif README\n  AddIcon /icons/folder.gif ^^DIRECTORY^^\n  AddIcon /icons/blank.gif ^^BLANKICON^^\n\n  DefaultIcon /icons/unknown.gif\n\n  AddDescription \"GZIP compressed document\" .gz\n  AddDescription \"tar archive\" .tar\n  AddDescription \"GZIP compressed tar archive\" .tgz\n  AddDescription \"BZIP2 compressed tar archive\" .tar.bz2\n\n  ReadmeName README.html\n  HeaderName HEADER.html\n\n  IndexIgnore .??* *~ *# HEADER* README* RCS CVS *,v *,t\n\u003c/IfModule\u003e\n\nAddDefaultCharset UTF-8\n\n# Compressed\nAddType application/x-tar      .tgz\nAddType application/x-compress .Z\nAddType application/x-gzip     .gz .tgz\n\n# Certificates\nAddType application/x-x509-ca-cert .crt\nAddType application/x-pkcs7-crl    .crl\n\n# Audio\nAddType audio/ogg     oga ogg\n\n# Video\nAddType video/ogg     ogv\nAddType video/mp4     mp4\nAddType video/webm    webm\n\n# SVG\nAddType image/svg+xmlsvg svgz\nAddEncoding gzip         svgz\n\n# Web Fonts\nAddType application/vnd.ms-fontobject eot\nAddType font/truetype                 ttf\nAddType font/opentype                 otf\nAddType application/x-font-woff       woff\n\n# Assorted\nAddType image/x-icon                   ico\nAddType image/webp                     webp\nAddType text/cache-manifest            appcache manifest\nAddType text/x-component               htc\nAddType application/x-chrome-extension crx\nAddType application/x-xpinstall        xpi\nAddType application/octet-stream       safariextz\n\n# Add a bunch of compression directives\n\u003cIfModule mod_deflate.c\u003e\n  # 0 - 9 CPU/Bandwidth tradeoff\n  DeflateCompressionLevel 7\n\n  \u003cIfModule mod_setenvif.c\u003e\n    \u003cIfModule mod_headers.c\u003e\n      SetEnvIfNoCase ^(Accept-EncodXng|X-cept-Encoding|X{15}|~{15}|-{15})$ ^((gzip|deflate)\\s,?\\s(gzip|deflate)?|X{4,13}|~{4,13}|-{4,13})$ HAVE_Accept-Encoding\n      RequestHeader append Accept-Encoding \"gzip,deflate\" env=HAVE_Accept-Encoding\n    \u003c/IfModule\u003e\n  \u003c/IfModule\u003e\n\n  # html, txt, css, js, json, xml, etc:\n  \u003cIfModule filter_module\u003e\n    FilterDeclare COMPRESS\n    FilterProvider  COMPRESS DEFLATE resp=Content-Type /text/(html|css|javascript|plain|x(ml|-component))/\n    FilterProvider  COMPRESS DEFLATE resp=Content-Type /application/(javascript|json|xml|x-javascript)/\n    FilterChain COMPRESS\n    FilterProtocol  COMPRESS change=yes;byteranges=no\n  \u003c/IfModule\u003e\n\n  # webfonts and svg:\n  \u003cFilesMatch \"\\.(ttf|otf|eot|svg)$\" \u003e\n    SetOutputFilter DEFLATE\n  \u003c/FilesMatch\u003e\n\u003c/IfModule\u003e\n\n\u003cIfModule mod_include\u003e\n  AddType text/html .shtml\n  AddOutputFilter INCLUDES .shtml\n\u003c/IfModule\u003e\n\n#ErrorDocument 500 \"The server just diagnosed itself with schizophrenia.\"\n#ErrorDocument 404 /missing.html\n#ErrorDocument 404 \"/cgi-bin/missing_handler.pl\"\n#ErrorDocument 402 http://www.example.com/subscription_info.html\n\n\u003cIfModule mod_setenvif\u003e\n  BrowserMatch \"Mozilla/2\" nokeepalive\n  BrowserMatch \"MSIE 4\\.0b2;\" nokeepalive downgrade-1.0 force-response-1.0\n  BrowserMatch \"RealPlayer 4\\.0\" force-response-1.0\n  BrowserMatch \"Java/1\\.0\" force-response-1.0\n  BrowserMatch \"JDK/1\\.0\" force-response-1.0\n\n  BrowserMatch \"Microsoft Data Access Internet Publishing Provider\" redirect-carefully\n  BrowserMatch \"MS FrontPage\" redirect-carefully\n  BrowserMatch \"^WebDrive\" redirect-carefully\n  BrowserMatch \"^WebDAVFS/1.[0123]\" redirect-carefully\n  BrowserMatch \"^gnome-vfs/1.0\" redirect-carefully\n  BrowserMatch \"^XML Spy\" redirect-carefully\n  BrowserMatch \"^Dreamweaver-WebDAV-SCM1\" redirect-carefully\n\u003c/IfModule\u003e\n\n\u003cIfModule status_module\u003e\n  \u003cLocation /server-status\u003e\n    SetHandler server-status\n    Order deny,allow\n    Deny from all\n    Allow from 10.13.37.\n        \u003c/Location\u003e\n\u003c/IfModule\u003e\n\n\u003cIfModule info_module\u003e\n  \u003cLocation /server-info\u003e\n    SetHandler server-info\n    Order deny,allow\n    Deny from all\n    Allow from 10.13.37.\n  \u003c/Location\u003e\n\u003c/IfModule\u003e\n\n\u003cIfModule mod_proxy\u003e\n  ProxyRequests On\n  ProxyVia Block\n\n  \u003cProxy *\u003e\n    Order deny,allow\n    Deny from all\n    Allow from 10.13.37.\n  \u003c/Proxy\u003e\n  \u003cIfModule mod_disk_cache.c\u003e\n    CacheEnable disk /\n    CacheRoot \"/var/cache/mod_proxy\"\n  \u003c/IfModule\u003e\n\u003c/IfModule\u003e\n```\n\n### /etc/httpd/conf.d/php.conf\n\nThis file is being included here as it will be needed most places that I run\nApache. Please refer to the PHP section below for more information.\n\n```\n\u003cIfModule prefork.c\u003e\n  LoadModule php5_module modules/libphp5.so\n\u003c/IfModule\u003e\n\u003cIfModule worker.c\u003e\n  LoadModule php5_module modules/libphp5-zts.so\n\u003c/IfModule\u003e\n\nAddHandler php5-script .php\nAddType text/html .php\nAddType application/x-httpd-php-source .phps\n\nDirectoryIndex index.php\n```\n\n### /etc/httpd/conf.d/mod_evasive.conf\n\n`mod_evasive` is a crafty little module designed to limit the impact of DoS and\nDDoS attacks. This won't stop them but it will help defend against them. It\nwill also send notices to an admin if desired (it's an optional feature which\ncan be disabled).\n\nThe one potential problem I can see cropping up is with AJAX requests. Those\ncan come in fast and hard, and they are intentional and necessary, if that\nbecomes a problem (you'll get a 403 response when you trip and it will last for\nDOSBlockingPeriod seconds) then play with the DOSPageCount setting and the\nDOSSiteCount setting. Alternatively you can just program your AJAX clients to\nunderstand what happened and to react accordingly.\n\n```\nLoadModule evasive20_module modules/mod_evasive20.so\n\n\u003cIfModule mod_evasive20.c\u003e\n  DOSHashTableSize  3097\n  DOSPageCount    2\n  DOSSiteCount    50\n  DOSPageInterval   1\n  DOSSiteInterval   1\n  DOSBlockingPeriod 10\n  #DOSEmailNotify   alerts@example.org\n  DOSLogDir   \"/var/lock/mod_evasive\"\n  #DOSWhitelist   192.168.0.*\n\u003c/IfModule\u003e\n```\n\n### /etc/httpd/conf.d/mod_gnutls.conf\n\nThe GnuTLS module provides SSL encryption for web requests. It's quite a bit\nmore flexible than `mod_ssl` though it hasn't been audited as thoroughly. It is\nalso considered an 'experimental' module for apache even though it's been in\nthe wild for two years.\n\n```\nLoadModule gnutls_module modules/mod_gnutls.so\n\nGnuTLSCache dbm \"/var/cache/mod_gnutls\"\nGnuTLSCacheTimeout 300\n\nListen 443\n```\n\n### /etc/httpd/conf.d/mod_security.conf\n\nWARNING: While this is a good module to have loaded it sextuples the amount of\nmemory used by child processes. You will need to tune the server's performance\nsettings accordingly. This may have a huge impact on high-traffic sites.\n\nWith that out of the way this is a very good application firewall to have as\npart of the defense-in-depth doctrine, though it's configuration can be a\nburden. The `mod_security` module gives your Apache Web server increased\nability to inspect and process input from Web clients before it's acted on by\nthe scripts or processes waiting for the input.\n\nThe `mod_security` module even lets you inspect Web server output before it's\ntransmitted back to clients. I love this feature: it allows you to watch out\nfor server responses that might indicate that other filters have failed and an\nattack has succeeded!\n\nWith that said this isn't the file you should actually be looking at. Yes this\nis where everything will get loaded but the stock Fedora `mod_security` for the\nmost parts loads up `mod_security` rules in other places including the Core\nModSecurity Rule Set which will get updated with the rest of your server.\n\nThe variables that you'll want to play around with are located in\n`/etc/httpd/modsecurity.d/modsecurity_crs_10_config.conf` and if you want to\nwrite your own rules in `/etc/httpd/modsecurity.d/modsecurity_localrules.conf`.\n\nSome changes I'd recommend in\n`/etc/httpd/modsecurity.d/modsecurity_crs_10_config.conf`:\n\n* Uncomment the rule that limits argument name length to 100 characters\n* Uncomment the rule that limits the value of an argument's length to 400\n  characters\n* Uncomment the rule that limits the total argument length to 64000 characters\n* Review the restricted extension types\n\nAfter keeping an eye on the logs for a few weeks and ensuring that there aren't\nany false positives, you should change the line `SecDefaultAction\n\"phase:2,pass` to `SecDefaultAction \"phase:2,deny,log,status:403\"`.\n\n### /etc/httpd/conf.d/mod_selinux.conf\n\nThe `mod_selinux` module allows us to extend SELinux contexts into individual\nweb applications or virtual hosts without impacting the memory usage of\nindividual child processes (The parent process seems to use about 100Kb of\nmemory more but this is negligible).\n\nSince I don't use the built in apache authentication I'm limited to\nrestrictions based on virtual hosts, which is still a rather large gain of\nsecurity.\n\nYou can additionally adjust contexts based on the IP the user is connecting\nfrom.\n\n```\nLoadModule selinux_module modules/mod_selinux.so\n\nselinuxServerDomain *:s0\nselinuxDomainEnv  SELINUX_DOMAIN\n```\n\nIf you enable the environment module (`env_module`) in apache the domain may be\navailable for use within the application (it would be `SELINUX_DOMAIN`). This\nhasn't been tested. With PHP you may need to add the `E` to the string\n`variable_order`, and adjust `auto_globals_jit` in the `php.ini` file.\n\nChanging contexts with the stock SELinux rules is denied! You will need to\ncreate a SELinux policy to allow it.\n\n#### Virtual Host Contexts\n\nTo make use of this feature you need to define contexts within each of the\nvirtual host configuration directives after enabling `mod_selinux` by adding\nthe following line:\n\n```\nselinuxDomainVal    *:s0:c1\n```\n\nThe trailing domain should be different for virtual host (the next one would be\nc2).\n\n#### IP Based Contexts\n\nYou can set contexts based on the IP address being connected from by adding the\nfollowing line within a Directory definition:\n\n```\nSetEnvIf Remote_Addr \"10.13.37.(25[0-5]|2[0-4][0-9]|[1-9]?[0-9])$\" SELINUX_DOMAIN=*:s0:c1\n```\n\n### /etc/httpd/conf.d/virtual_hosts.conf\n\nThis does not exist upon the default installation and will need to be created.\nNo certificates are automatically generated when you install the GnuTLS module,\nif you need them you will need to generate them yourself.\n\n```\nNameVirtualHost *:80\n\n\u003cIfModule gnutls_module\u003e\n  NameVirtualHost *:443\n\n  # Redirect all unencrypted connections which haven't been defined to the secure ones\n  \u003cVirtualHost _default_:80\u003e\n    RewriteEngine On\n    RewriteCond %{HTTPS} off\n    RewriteRule (.*) https://%{HTTP_HOST}%{REQUEST_URI}\n  \u003c/VirtualHost\u003e\n\u003c/IfModule\u003e\n\n\u003cVirtualHost _default_:443\u003e\n  GnuTLSEnable On\n  GnuTLSPriorities SECURE\n  GnuTLSCertificateFile /etc/pki/tls/certs/localhost.crt\n  GnuTLSKeyFile /etc/pki/tls/private/localhost.key\n\u003c/VirtualHost\u003e\n\n\u003cVirtualHost *:443\u003e\n  ServerName something.example.org\n  GnuTLSEnable On\n  GnuTLSPriorities SECURE\n  GnuTLSCertificateFile /etc/pki/tls/certs/something.example.org.crt\n  GnuTLSKeyFile /etc/pki/tls/private/something.example.org.key\n  DocumentRoot \"/var/www/html/something.example.org\"\n\u003c/VirtualHost\u003e\n```\n\n### /etc/httpd/conf.d/welcome.conf\n\nDelete this file, it has no use and should be removed.\n\n## Virtual Hosts\n\nPlease note that name based virtual hosts are not supported on SSL connections\nwhen using `mod_ssl`. They work quite well with `mod_gnutls` and browsers that\nsupport SNI (Server Name Indication, as described in section 3.1 of\n[RFC3546][1]). Compatibility with older browsers may be impacted.\n\n### Name Based\n\nName based virtual hosts allow you to re-use an IP to host multiple websites.\nThis is only officially supported for unencrypted connections, however by using\nthe GnuTLS module you can easily set this up with SSL based hosts as well. An\nexample of how to use this can be seen in the `virtual_hosts.conf` file on this\npage.\n\n### IP Based\n\nI don't really use these so I'm not including documentation, this section is\nmostly so other people referencing this documentation can be aware that they\ncan have different virtual hosts bound to specific IP addresses rather than\nhostnames.\n\n## PHP\n\n```\n[root@localhost] ~# yum install php php-suhosin\n```\n\nAdditional PHP packages that may be needed and that I've found very useful:\n\n* php-mcrypt\n* php-bcmath\n* php-ldap\n* php-snmp\n* php-pdo\n* php-mysql\n* php-pecl-xdebug\n\n### /etc/php.ini\n\nThe following is for production use, `display_errors`,\n`display_startup_errors`, `mysql.trace_mode` may be useful in development.\n\n```ini\n[PHP]\nshort_open_tag = Off\nasp_tags = Off\nprecision = 14\ny2k_compliance = On\noutput_buffering = 4096\nzlib.output_compression = Off\nimplicit_flush = Off\nserialize_precision = 100\nallow_call_time_pass_reference = Off\nsafe_mode = Off\nopen_basedir /var/www/html:/tmp\ndisable_functions = system, show_source, symlink, exec, dl, shell_exec, passthru, escapeshellarg, escapeshellcmd, pcntl_exec, phpinfo\nexpose_php = Off\nmax_execution_time = 30\nmax_input_time = 60\nmemory_limit = 128M\nerror_reporting = E_ALL | E_STRICT\ndisplay_errors = Off\ndisplay_startup_errors = Off\nlog_errors = On\nlog_errors_max_len = 1024\nignore_repeated_errors = Off\nignore_repeated_source = Off\nreport_memleaks = On\ntrack_errors = Off\nvariables_order = \"GPCS\"\nrequest_order = \"GP\"\nregister_globals = Off\nregister_long_arrays = Off\nregister_argc_argv = Off\nauto_globals_jit = On\npost_max_size = 8M\nmagic_quotes_gpc = Off\nmagic_quotes_runtime = Off\nmagic_quotes_sybase = Off\ndefault_mimetype = \"text/html\"\nenable_dl = Off\nfile_uploads = On\nupload_max_filesize = 2M\nallow_url_fopen = Off\nallow_url_include = Off\ndefault_socket_timeout = 15\n\n[Date]\ndate.timezone = America/Montreal\n\n[Syslog]\ndefine_syslog_variables  = Off\n\n[mail function]\nsendmail_path = /usr/sbin/sendmail -t -i\nmail.add_x_header = Off\n\n[SQL]\nsql.safe_mode = Off\n\n[ODBC]\nodbc.allow_persistent = On\nodbc.check_persistent = On\nodbc.max_persistent = -1\nodbc.max_links = -1\nodbc.defaultlrl = 4096\nodbc.defaultbinmode = 1\n\n[MySQL]\nmysql.allow_persistent = On\nmysql.max_persistent = -1\nmysql.max_links = -1\nmysql.default_port = 3306\nmysql.connect_timeout = 15\nmysql.trace_mode = Off\n\n[MySQLi]\nmysqli.max_links = -1\nmysqli.default_port = 3306\nmysqli.reconnect = Off\n\n[PostgresSQL]\npgsql.allow_persistent = On\npgsql.auto_reset_persistent = Off\npgsql.max_persistent = -1\npgsql.max_links = -1\npgsql.ignore_notice = 0\npgsql.log_notice = 0\n\n[Sybase-CT]\nsybct.allow_persistent = On\nsybct.max_persistent = -1\nsybct.max_links = -1\nsybct.min_server_severity = 10\nsybct.min_client_severity = 10\n\n[bcmath]\nbcmath.scale = 0\n\n[Session]\nsession.save_handler = files\nsession.save_path = \"/var/lib/php/session\"\nsession.use_cookies = 1\nsession.use_only_cookies = 1\nsession.name = UNCID\nsession.auto_start = 0\nsession.cookie_lifetime = 21600\nsession.cookie_path = /\nsession.serialize_handler = php\nsession.gc_probability = 1\nsession.gc_divisor = 1000\nsession.gc_maxlifetime = 1440\nsession.bug_compat_42 = Off\nsession.bug_compat_warn = Off\nsession.entropy_length = 16\nsession.entropy_file = /dev/urandom\nsession.cache_limiter = nocache\nsession.cache_expire = 180\nsession.use_trans_sid = 0\nsession.hash_function = 1\nsession.hash_bits_per_character = 5\n\n[MSSQL]\nmssql.allow_persistent = On\nmssql.max_persistent = -1\nmssql.max_links = -1\nmssql.min_error_severity = 10\nmssql.min_message_severity = 10\nmssql.compatability_mode = Off\nmssql.connect_timeout = 5\nmssql.timeout = 15\nmssql.textlimit = 4096\nmssql.textsize = 4096\nmssql.batchsize = 0\nmssql.datetimeconvert = On\n; This may need to be adjusted\nmssql.secure_connection = Off\nmssql.max_procs = -1\n\n[Assertion]\nassert.active = On\nassert.warning = On\nassert.bail = Off\nassert.callback = 0\n\n[Tidy]\ntidy.clean_output = Off\n\n[soap]\nsoap.wsdl_cache_enabled=1\nsoap.wsdl_cache_dir=\"/tmp\"\nsoap.wsdl_cache_ttl=86400\n```\n\n### /etc/php.d/suhosin.ini\n\nThis is the config file for the suhosin extension. If an application isn't\nworking check the logs generated by suhosin to see if it is the issue.\n\n```\nextension = suhosin.so\n\n[suhosin]\nsuhosin.log.syslog.facility = LOG_LOCAL2\nsuhosin.executor.max_depth = 15\nsuhosin.executor.include.max_traversal = 2\nsuhosin.executor.disable_eval = On\nsuhosin.executor.disable_emodifier = On\nsuhosin.mail.protect = 2\nsuhosin.session.cryptraddr = 2\nsuhosin.cookie.cryptraddr = 2\n; This is not an official response, just funny :D\nsuhosin.filter.action = 418\nsuhosin.upload.disallow_elf = On\n```\n\n## Basic Authentication\n\nSometimes it's just needed to prevent access to something for a little while,\nHTTP basic authentication can help! If you're using the hardened configuration\nabove you'll need to enable a few modules to actually use it and make sure that\na .htaccess file has permission to override `AuthConfig`. Add the following\nlines if they aren't already there in the section where you load modules:\n\n```\nLoadModule auth_basic_module modules/mod_auth_basic.so\nLoadModule authn_file_module modules/mod_authn_file.so\nLoadModule authz_user_module modules/mod_authz_user.so\n```\n\nAdditionally if you want to use group based authentication add:\n\n```\nLoadModule authz_groupfile_module modules/mod_authz_groupfile.so\n```\n\nFirst create a user to use for the login:\n\n```\nhtpasswd -c .htpasswd demouser\n```\n\nAdditional users can be added by omitting the `-c`.\n\nCreate or append the following to a `.htaccess` file in the directory you want\nto protect:\n\n```\nAuthType Basic\nAuthName \"Some Name Describing the site\"\nAuthUserFile /path/to/.htpasswd\nRequire valid-user\n```\n\nThat's it you'll have HTTP Basic authentication. If you want to use a group\nfile you'll need to change the setting in the `.htaccess` file to match:\n\n```\nAuthType Basic\nAuthName \"Some Name Describing the site\"\nAuthUserFile /path/to/.htpasswd\nAuthGroupFile /path/to/.htgroups\nRequire group demogroup\n```\n\nAnd create a group file `.htgroups` with the following:\n\n```\ndemogroup: demouser someotheruser\n```\n\n## Kerberos Authentication\n\nhttp://www.grolmsnet.de/kerbtut/\n\n## Passenger\n\nBefore going through this you need to ensure that the tools for compiling ruby\nare installed on the system these are listed in [Ruby][2].\n\nEnsure that ruby is installed on the system as well as the development headers\nneeded to compile the passenger module, this is all done as root:\n\n```\nyum install ruby curl-devel ruby-devel httpd-devel apr-devel apr-util-devel -y\n```\n\n```\ngem install passenger\npassenger-install-apache2-module\n```\n\nI wasn't able to get this working at the current time as there is a bug in\npassenger 3.0.12 that prevents it from being compiled with GCC 4.6.\n\n[1]: http://www.ietf.org/rfc/rfc3546.txt\n[2]: {{\u003c ref \"./ruby.md\" \u003e}}\n","created_at":-62135596800,"fuzzy_word_count":3700,"path":"/notes/httpd/","published_at":1540945455,"reading_time":17,"tags":null,"title":"HTTPd","type":"notes","updated_at":1540945455,"weight":0,"word_count":3601},{"cid":"40d78109140d9822f52db145f51c9c3fe3fcf8bc","content":"\n***Note: This page is quite old and is likely out of date. My opinions may have\nalso changed dramatically since this was written. It is here as a reference\nuntil I get around to updating it.***\n\nWhile it's described and primarily used as an audio streaming server, it would\nbe more accurate to describe it as a media streaming server as it's perfectly\ncapable of rebroadcasting video streams with or without audio.\n\n## Setup Process\n\nAs of this writing Fedora 16 comes with Icecast version 2.3.2, the associated\ndocumentation can be found on the Icecast website. I found some\ndocumentation had been removed rather than updated along with the software and\nthat documentation can be found in version 2.0.1's documentation.\n\n### Networking\n\nSetup a dedicated IP for use by the Icecast server (this is optional but a good\npractice even if it's just an internal address). You'll want to note this down\nfor what the server will bind it's addresses to later on.\n\nFor the purposes of this example configuration I'm using `eth0:1` interface\nwith an IP Address of 192.168.20.45 configured like so in\n`/etc/sysconfig/network-script/ifcfg-eth0:1`\n\n```\n# eth0:1 - For Icecast Server\nDEVICE=\"eth0:1\"\nNM_CONTROLLED=\"no\"\nONBOOT=\"yes\"\nBOOTPROTO=\"static\"\n\nIPADDR=\"192.168.20.45\"\nNETMASK=\"255.255.255.0\"\nNETWORK=\"192.168.20.0\"\nBROADCAST=\"192.168.20.255\"\n\nIPV4_FAILURE_FATAL=\"yes\"\nIPV6_AUTOCONF=\"no\"\nIPV6INIT=\"no\"\n\nNAME=\"IceCast Interface\"\n```\n\nSetup an A/CNAME record pointing at the server that will be hosting the Icecast\nserver, take note of this as it will be needed for the hostname param in the\nconfiguration file. This example will use streams.example.org for the hostname.\n\n### Firewall\n\nAdd the following [IPTables][3] rules:\n\n```\n-A SERVICES -d 192.168.20.45 -m tcp -p tcp --dport 8000 -j ACCEPT\n-A SERVICES -d 192.168.20.45 -m tcp -p tcp --dport 8001 -j ACCEPT\n```\n\n### Setup the chroot Environment\n\nThe configuration I have here uses a chrooted environment within\n`/usr/share/icecast`. By default it isn't fully setup.\n\n```\nmkdir /usr/share/icecast/log\nchown -R icecast:icecast /usr/share/icecast\nrm -rf /var/log/icecast\nln -s /usr/share/icecast/log/ /var/log/icecast\n```\n\n### Installation \u0026 Configuration\n\n```\nyum install icecast -y\n```\n\nReplace `/etc/icecast.xml` with the following minimized (read comments and\nmount points removed) configuration. You'll need to change the values,\nespecially [passwords][4] listed in the configuration before making it live.\n\n```xml\n\u003cicecast\u003e\n  \u003chostname\u003estreams.example.org\u003c/hostname\u003e\n  \u003cfileserve\u003e1\u003c/fileserve\u003e\n  \u003cserver-id\u003eIcecast 2.3.2\u003c/server-id\u003e\n  \u003clocation\u003eEarth\u003c/location\u003e\n  \u003cshoutcast-mount\u003e/stream\u003c/shoutcast-mount\u003e\n  \u003clisten-socket\u003e\n    \u003cbind-address\u003e192.168.20.45\u003c/bind-address\u003e\n    \u003cport\u003e8000\u003c/port\u003e\n  \u003c/listen-socket\u003e\n  \u003clisten-socket\u003e\n    \u003cbind-address\u003e192.168.20.45\u003c/bind-address\u003e\n    \u003cport\u003e8001\u003c/port\u003e\n    \u003cshoutcast-compat\u003e1\u003c/shoutcast-compat\u003e\n  \u003c/listen-socket\u003e\n  \u003csecurity\u003e\n    \u003cchroot\u003e1\u003c/chroot\u003e\n    \u003cchangeowner\u003e\n      \u003cuser\u003eicecast\u003c/user\u003e\n      \u003cgroup\u003eicecast\u003c/group\u003e\n    \u003c/changeowner\u003e\n  \u003c/security\u003e\n  \u003cpaths\u003e\n    \u003cpidfile\u003e/var/run/icecast/icecast.pid\u003c/pidfile\u003e\n    \u003cbasedir\u003e/usr/share/icecast\u003c/basedir\u003e\n    \u003cwebroot\u003e/web\u003c/webroot\u003e\n    \u003cadminroot\u003e/admin\u003c/adminroot\u003e\n    \u003clogdir\u003e/log\u003c/logdir\u003e\n    \u003calias source=\"/\" dest=\"/status.xsl\"/\u003e\n  \u003c/paths\u003e\n  \u003clogging\u003e\n    \u003caccesslog\u003eaccess.log\u003c/accesslog\u003e\n    \u003cerrorlog\u003eerror.log\u003c/errorlog\u003e\n    \u003cplaylistlog\u003eplaylist.log\u003c/playlistlog\u003e\n    \u003cloglevel\u003e3\u003c/loglevel\u003e\n    \u003clogarchive\u003e0\u003c/logarchive\u003e\n    \u003clogsize\u003e102400\u003c/logsize\u003e\n  \u003c/logging\u003e\n  \u003climits\u003e\n    \u003cclients\u003e100\u003c/clients\u003e\n    \u003csources\u003e2\u003c/sources\u003e\n    \u003cthreadpool\u003e5\u003c/threadpool\u003e\n    \u003cqueue-size\u003e524288\u003c/queue-size\u003e\n    \u003cclient-timeout\u003e30\u003c/client-timeout\u003e\n    \u003cheader-timeout\u003e15\u003c/header-timeout\u003e\n    \u003csource-timeout\u003e10\u003c/source-timeout\u003e\n    \u003cburst-on-connect\u003e1\u003c/burst-on-connect\u003e\n    \u003cburst-size\u003e65535\u003c/burst-size\u003e\n  \u003c/limits\u003e\n  \u003cauthentication\u003e\n    \u003csource-user\u003esource\u003c/source-user\u003e\n    \u003csource-password\u003ehackme\u003c/source-password\u003e\n    \u003crelay-user\u003erelay\u003c/relay-user\u003e\n    \u003crelay-password\u003ehackme\u003c/relay-password\u003e\n    \u003cadmin-user\u003eadmin\u003c/admin-user\u003e\n    \u003cadmin-password\u003ehackme\u003c/admin-password\u003e\n  \u003c/authentication\u003e\n\u003c/icecast\u003e\n```\n\nIcecast hasn't been ported over to systemd yet so you mind as well use the old\nconfiguration options to set it to start up on each boot:\n\n```\nchkconfig icecast on\nservice icecast start\n```\n\nIf you configured everything properly you now have a happy Icecast server ready\nto have a source authenticate to it and listeners receive it.\n\n## Example Mount Definitions\n\n### LiveDJ with Automation Fallback\n\nThe following configuration provides a public and a hidden mount. If there is a\nlive DJ authenticated and streaming content it will pull users out of the\nautomated stream and to listen to the live DJ, if the automated DJ goes down it\nwill play silence but won't kill the stream. This requires that you put a file\nin the web directory named 'silence.ogg'.\n\nYou can download the one [I use here][5].  With this you'll want to use the URL\n`http://streams.example.org/radio.ogg.m3u` to access the stream.\n\n```xml\n\u003cmount\u003e\n  \u003cmount-name\u003e/automation.ogg\u003c/mount-name\u003e\n\n  \u003cusername\u003eautomation\u003c/username\u003e\n  \u003cpassword\u003erobot-password-hackme\u003c/password\u003e\n\n  \u003cfallback-mount\u003e/silence.ogg\u003c/fallback-mount\u003e\n  \u003cfallback-override\u003e1\u003c/fallback-override\u003e\n\n  \u003ccharset\u003eUTF8\u003c/charset\u003e\n\n  \u003cstream-name\u003eRadio - Automation System\u003c/stream-name\u003e\n  \u003cstream-description\u003eA Radio Station Automaton\u003c/stream-description\u003e\n  \u003cstream-url\u003ehttp://streams.example.org/\u003c/stream-url\u003e\n  \u003cgenre\u003eA Genre!\u003c/genre\u003e\n\n  \u003cbitrate\u003e128\u003c/bitrate\u003e\n\n  \u003cpublic\u003e0\u003c/public\u003e\n  \u003chidden\u003e1\u003c/hidden\u003e\n\u003c/mount\u003e\n\n\u003cmount\u003e\n  \u003cmount-name\u003e/radio.ogg\u003c/mount-name\u003e\n\n  \u003cusername\u003elivedj\u003c/username\u003e\n  \u003cpassword\u003ethe-live-djs-password\u003c/password\u003e\n\n  \u003cfallback-mount\u003e/automation.ogg\u003c/fallback-mount\u003e\n  \u003cfallback-override\u003e1\u003c/fallback-override\u003e\n\n  \u003ccharset\u003eUTF8\u003c/charset\u003e\n\n  \u003cstream-name\u003eRadio - Automation System\u003c/stream-name\u003e\n  \u003cstream-description\u003eA Radio Station Automaton\u003c/stream-description\u003e\n  \u003cstream-url\u003ehttp://streams.example.org/\u003c/stream-url\u003e\n  \u003cgenre\u003eA Genre!\u003c/genre\u003e\n\n  \u003cbitrate\u003e128\u003c/bitrate\u003e\n\n  \u003cpublic\u003e1\u003c/public\u003e\n  \u003chidden\u003e0\u003c/hidden\u003e\n\u003c/mount\u003e\n```\n\n## Logrotate Stanza\n\nTODO\n\n## ffmpeg2theora Source Client\n\nTODO\n\n## Using Icecast stream in HTML5\n\n### Audio\n\nHere is a snippet of HTML5 for connecting to the stream\n`http://streams.example.org:8000/radio.ogg` with some javascript controls,\npretty straight forward. The loop is included in case the stream dies or is\ncurrently in fallback mode to a file.\n\n```html\n\u003c!DOCTYPE html\u003e\n\u003chtml lang=en\u003e\n  \u003chead\u003e\n    \u003cmeta charset=utf-8 /\u003e\n    \u003cmeta name=viewport content=\"width=device-width\"/\u003e\n    \u003ctitle\u003eHTML5 Radio Player Test\u003c/title\u003e\n  \u003c/head\u003e\n  \u003cbody\u003e\n    \u003cdiv id=container\u003e\n      \u003caudio id=radioStream preload=metadata controls loop\u003e\n      \u003csource src=\"http://streams.example.org:8000/radio.ogg\" type=\"audio/ogg\"/\u003e\n      \u003c/audio\u003e\n      \u003cbr/\u003e\n      \u003ca href=\"#\" onclick=\"javascript:rs=document.getElementById('radioStream');rs.play();\"\u003ePlay\u003c/a\u003e\n      \u003ca href=\"#\" onclick=\"javascript:rs=document.getElementById('radioStream');rs.pause();\"\u003ePause\u003c/a\u003e\n      \u003ca href=\"#\" onclick=\"javascript:rs=document.getElementById('radioStream');rs.pause();rs.currentTime=0;\"\u003eStop\u003c/a\u003e\n      \u003cinput id=volume value=100 type=text /\u003e\u003ca href=\"#\" onclick=\"javascript:rs=document.getElementById('radioStream');vo=document.getElementById('volume');rs.volume=parseInt(vo.value)/100.0;\"\u003eSet Volume\u003c/a\u003e\n    \u003c/div\u003e\n  \u003c/body\u003e\n\u003c/html\u003e\n```\n\n### Video\n\nTodo...\n\n## Fully Documented Configuration File\n\n```xml\n\u003cicecast\u003e\n  \u003c!--\n    This is the DNS name or IP address that will be used for the stream directory\n    lookups or possibily the playlist generation if a Host header is not\n    provided. While localhost is shown as an example, in fact you will want\n    something that your listeners can use.\n  --\u003e\n  \u003chostname\u003estreams.example.org\u003c/hostname\u003e\n\n  \u003c!--\n    This flag turns on the icecast2 fileserver from which static files can be\n    served. All files are served relative to the path specified in the\n    paths-\u003ewebroot configuration setting. By default the setting is enabled so\n    that requests for the images on the status page are retrievable.\n\n    This service should not be used for general file serving, use a service\n    designed for that task such as Apache.\n  --\u003e\n  \u003cfileserve\u003e1\u003c/fileserve\u003e\n\n  \u003c!--\n    This optional setting allows for the administrator of the server to override\n    the default server identification. The default is icecast followed by a\n    version number and most will not care to change it however this setting will\n    change that. \n  --\u003e\n  \u003cserver-id\u003eIcecast 2.3.2\u003c/server-id\u003e\n\n  \u003c!--\n    This is an undocumented setting, it probably gets announced to the icecast\n    directory and defaults to \"Earth\" the value of this is also visible on the\n    Icecast Administration page.\n  --\u003e\n  \u003clocation\u003eEarth\u003c/location\u003e\n\n  \u003c!--\n    An optional mountpoint setting to be used when shoutcast DSP compatible\n    clients connect. The default global setting is /stream but can be overridden\n    here to use an alternative name which may include an extension that some\n    clients require for certain formats.\n\n    Defining this within a listen-socket group tells icecast that this port and\n    the subsequent port are to be used for shoutcast compatible source clients.\n    This is an alternative to the shoutcast-compat approach as this implicitly\n    defines the second listening socket and allows for specifying multiple\n    sockets using different mountpoints for shoutcast source clients.\n\n    The shoutcast-mount outside of a listen-socket group is the global setting of\n    the mountpoint to use. \n  --\u003e\n  \u003cshoutcast-mount\u003e/stream\u003c/shoutcast-mount\u003e\n\n  \u003c!--\n    The listen-sockets define the ports and addresses that the daemon will listen\n    on. Best practice dictates that you bind a service only to the IP addresses\n    that you want the service to listen on. Without specifying one Icecast will\n    listen on all IPv4 and IPv6 addresses the host is aware of.\n  --\u003e\n  \u003clisten-socket\u003e\n    \u003c!--\n      An optional IP address that can be used to bind to a specific network card.\n      If not supplied, then it will bind to all interfaces. \n    --\u003e\n    \u003cbind-address\u003e192.168.20.45\u003c/bind-address\u003e\n\n    \u003c!--\n      The TCP port that will be used to accept client connections.\n    --\u003e\n    \u003cport\u003e8000\u003c/port\u003e\n\n    \u003c!--\n      This could optionally be defined here as well/instead of in the global\n      configuration. Refer to it's documentation else where in this config\n    --\u003e\n    \u003c!-- \u003cshoutcast-mount\u003e/stream\u003c/shoutcast-mount\u003e --\u003e\n  \u003c/listen-socket\u003e\n  \u003clisten-socket\u003e\n    \u003cbind-address\u003e192.168.20.45\u003c/bind-address\u003e\n    \u003cport\u003e8001\u003c/port\u003e\n\n    \u003c!--\n      This optional flag will indicate that this port will operate in\n      'shoutcast-compatibility' mode. Due to major differences in the source client\n      connection protocol, if you wish to use any of the shoutcast DJ tools, you\n      will need to configure at least one socket as shoutcast-compatible. Note that\n      when in this mode, only source clients (and specifically shoutcast source\n      clients) will be able to attach to this port. All listeners may connect to any\n      of the ports defined without this flag. Also, for proper Shoutcast DSP\n      compatibility, you must define a listen socket with a port one less than the\n      one defined as 'shoutcast-compat'. This means if you define 8001 as\n      shoutcast-compat, then you will need to define a listen port of 8000 and it\n      must not also be defined as shoutcast-compat.\n    --\u003e\n    \u003cshoutcast-compat\u003e1\u003c/shoutcast-compat\u003e\n  \u003c/listen-socket\u003e\n\n  \u003c!--\n    This section contains configuration settings that can be used to secure the\n    icecast server by performing a chroot to a secured location.\n  --\u003e\n  \u003csecurity\u003e\n    \u003c!--\n      An indicator which specifies whether a chroot() will be done when the\n      server is started. The chrooted path is specified by the \u003cbasedir\u003e\n      configuration value the \u003cpaths\u003e section.\n    --\u003e\n    \u003cchroot\u003e1\u003c/chroot\u003e\n    \u003c!--\n      This section indicates the user and group that will own the icecast process\n      when it is started. These need to be valid users on the system. This allows\n      the daemon to drop it's root privileges. This really shouldn't be optional.\n    --\u003e\n    \u003cchangeowner\u003e\n      \u003cuser\u003eicecast\u003c/user\u003e\n      \u003cgroup\u003eicecast\u003c/group\u003e\n    \u003c/changeowner\u003e\n  \u003c/security\u003e\n\n  \u003cpaths\u003e\n    \u003c!--\n      This path is used in conjunction with the chroot settings, and specified the\n      base directory that is chrooted to when the server is started (and is not used\n      if not chroot'd).\n    --\u003e\n    \u003cbasedir\u003e/usr/share/icecast\u003c/basedir\u003e\n\n    \u003c!--\n      Note that since chroot is turned on, these paths are all relative to\n      \u003cbasedir\u003e and need to exist within that directory\n    --\u003e\n\n    \u003c!--\n      This path specifies the base directory used for all static file requests.\n      This directory can contain all standard file types (including mp3s and ogg\n      vorbis files). For example, if webroot is set to /usr/share/icecast, and a\n      request for http://server:port/mp3/stuff.mp3 comes in, then the file\n      /usr/share/icecast/mp3/stuff.mp3 will be served.\n    --\u003e\n    \u003cwebroot\u003e/web\u003c/webroot\u003e\n\n    \u003c!--\n      This path specifies the base directory used for all admin requests. More\n      specifically, this is used to hold the XSLT scripts used for the web-based\n      admin interface. The admin directory contained within the icecast\n      distribution contains these files. \n    --\u003e\n    \u003cadminroot\u003e/admin\u003c/adminroot\u003e\n\n    \u003c!--\n      This path specifies the base directory used for logging. Both the error.log\n      and access.log will be created relative to this directory. \n    --\u003e\n    \u003clogdir\u003e/log\u003c/logdir\u003e\n\n    \u003c!--\n      This pathname specifies the file to write at startup and to remove at\n      normal shutdown. The file contains the process id of the icecast process.\n      This could be read and used for sending signals icecast. This is not\n      affected by basedir and needs a full path\n    --\u003e\n    \u003cpidfile\u003e/var/run/icecast/icecast.pid\u003c/pidfile\u003e\n\n    \u003c!--\n      Aliases treat requests for 'source' path as being for 'dest' path May be\n      made specific to a port or bound address using the \"port\" and \"bind-address\"\n      attributes. The following alias serves up the status page as the root path.\n    --\u003e\n    \u003calias source=\"/\" dest=\"/status.xsl\"/\u003e\n\n    \u003c!--\n      There are two additional settings that may be applied within this context\n      that may be of use later on, though, they're not a good replacement for a\n      good firewall. They are documented here:\n\n      allow-ip:\n      If specified, this specifies the location of a file that contains a list\n      of IP addresses that will be allowed to connect to icecast. This could be\n      useful in cases where a master only feeds known slaves. The format of the\n      file is simple, one IP per line.\n\n      deny-ip:\n      If specified, this specifies the location of a file that contains a list\n      of IP addressess that will be dropped immediately. This is mainly for\n      problem clients when you have no access to any firewall configuration.\n      The format of the file is simple, one IP per line. \n    --\u003e\n  \u003c/paths\u003e\n\n  \u003c!--\n    This section contains information relating to logging within icecast. There\n    are two logfiles currently generated by icecast, an error.log (where all log\n    messages are placed) and an access.log (where all stream/admin/http requests\n    are logged).\n\n    Note that a HUP signal should be sent to icecast when the log files need to\n    ne re-opened after moving or deleting the log files. \n  --\u003e\n  \u003clogging\u003e\n    \u003c!--\n      All requests made to the icecast daemon will be logged in this file. It's\n      location is relative to the path specified by the \u003clogdir\u003e config value. \n    --\u003e\n    \u003caccesslog\u003eaccess.log\u003c/accesslog\u003e\n\n    \u003c!--\n      All icecast generated log messages will be written to this file. If the\n      loglevel is set too high (Debug for instance) then this file can grow\n      fairly large over time. Log-rotation should be handled with logrotate,\n      and a HUP signal should be generated after the logs have been\n      moved/truncated to inform the icecast daemon of the change.\n    --\u003e\n    \u003cerrorlog\u003eerror.log\u003c/errorlog\u003e\n\n    \u003c!--\n      Into this file, a log of all metadata for each mountpoint will be written.\n      The developers note that the format of this logfile will most likely\n      change over time as they haven't decided on a format.\n\n      Currently, the file is pipe delimited. This option is optional.\n    --\u003e\n    \u003cplaylistlog\u003eplaylist.log\u003c/playlistlog\u003e\n\n    \u003c!--\n      Indicates what messages are logged by icecast. Log messages are categorized\n      into one of 4 types, Debug, Info, Warn, and Error. Icecast will log all\n      messages matching the value set here and below (so level 3 will log info,\n      warn, and error messages).\n\n      4-Debug, 3-Info, 2-Warn, 1-Error\n    --\u003e\n    \u003cloglevel\u003e3\u003c/loglevel\u003e\n\n    \u003c!--\n      I let logrotate handle all of my log rotations and as such I don't have any\n      use for the built in log rotation. These values are set with this in mind,\n      and I don't expect the logs to ever actually reach this size. As such I\n      don't have to worry about what Icecast will rename the file when it does\n      reach that size.\n    --\u003e\n\n    \u003c!--\n      If this value is set, then icecast will append a timestamp to the end of\n      the logfile name when logsize has been reached. If disabled, then the\n      default behavior is to rename the logfile to logfile.old (overwriting any\n      previously saved logfiles). We disable this by default to prevent the\n      filling up of filesystems for people who don't care (or know) that their\n      logs are growing.\n    --\u003e\n    \u003clogarchive\u003e0\u003c/logarchive\u003e\n\n    \u003c!--\n      This value specifies (in Kb) the maxmimum size of any of the log files.\n      When the logfile grows beyond this value, icecast will either rename it to\n      logfile.old, or add a timestamp to the archived file (if logarchive is\n      enabled). \n\n      Here I have it set to 100Mb\n    --\u003e\n    \u003clogsize\u003e102400\u003c/logsize\u003e\n  \u003c/logging\u003e\n\n  \u003c!--\n    This section contains server level settings that, in general, do not need to\n    be changed. Only modify this section if you are know what you are doing.\n  --\u003e\n  \u003climits\u003e\n    \u003c!--\n      Total number of concurrent clients supported by the server. Listeners are\n      considered clients, but so are accesses to any static content (i.e.\n      fileserved content) and also any requests to gather stats. These are max\n      *concurrent* connections for the entire server (not per mountpoint). \n    --\u003e\n    \u003cclients\u003e100\u003c/clients\u003e\n\n    \u003c!--\n      Maximum number of connected sources supported by the server. This includes\n      active relays and source clients.\n    --\u003e\n    \u003csources\u003e2\u003c/sources\u003e\n\n    \u003c!--\n      This is the number of threads that are started to handle client connections.\n      You may need to increase this value if you are running a high traffic\n      stream. This recommended value is for a small to medium traffic server. \n    --\u003e\n    \u003cthreadpool\u003e5\u003c/threadpool\u003e\n\n    \u003c!--\n      This is the maximum size (in bytes) of the stream queue. A listener may\n      temporarily lag behind due to network congestion and in this case an\n      internal queue is maintained for the listeners. If the queue grows larger\n      than this config value, then it is truncated and any listeners found will be\n      removed from the stream.\n\n      This will be the default setting for the streams which is 512k unless\n      overridden here. You can override this in the individual mount settings which\n      can be useful if you have a mixture of high bandwidth video and low bitrate \n      audio streams. \n    --\u003e\n    \u003cqueue-size\u003e524288\u003c/queue-size\u003e\n\n    \u003c!--\n      This does not seem to be used. (Note from official documentation)\n    --\u003e\n    \u003cclient-timeout\u003e30\u003c/client-timeout\u003e\n\n    \u003c!--\n      The maximum time (in seconds) to wait for a request to come in once the\n      client has made a connection to the server. In general this value should not\n      need to be tweaked. \n    --\u003e\n    \u003cheader-timeout\u003e15\u003c/header-timeout\u003e\n\n    \u003c!--\n      If a connected source does not send any data within this timeout period (in\n      seconds), then the source connection will be removed from the server. \n    --\u003e\n    \u003csource-timeout\u003e10\u003c/source-timeout\u003e\n\n    \u003c!--\n      This setting is really just an alias for burst-size. When enabled the\n      burst-size is 64 kbytes and disabled the burst-size is 0 kbytes. This option\n      is deprecated, use burst-size instead.\n    --\u003e\n    \u003cburst-on-connect\u003e1\u003c/burst-on-connect\u003e\n\n    \u003c!--\n      The burst size is the amount of data (in bytes) to burst to a client at\n      connection time. Like burst-on-connect, this is to quickly fill the \n      pre-buffer used by media players. The default is 64 kbytes which is a typical\n      size used by most clients so changing it is not usually required. This\n      setting applies to all mountpoints unless overridden in the mount settings.\n    --\u003e\n    \u003cburst-size\u003e65535\u003c/burst-size\u003e\n  \u003c/limits\u003e\n\n  \u003c!--\n    This section contains all the usernames and passwords used for administration\n    purposes or to connect sources and relays. \n  --\u003e\n  \u003cauthentication\u003e\n    \u003c!--\n      This is undocumented but icecast seems to accept it, it is kind of implied\n      that it exists from the source-password documentation\n    --\u003e\n    \u003csource-user\u003esource\u003c/source-user\u003e\n\n    \u003c!--\n      The unencrypted password used by sources to connect to icecast2. The default\n      username for all source connections is 'source' but this option allows to\n      specify a default password. This and the username can be changed in the\n      individual mount sections. \n    --\u003e\n    \u003csource-password\u003ehackme\u003c/source-password\u003e\n\n    \u003c!--\n      Used in the master server as part of the authentication when a slave requests\n      the list of streams to relay. The default username is 'relay' \n    --\u003e\n    \u003crelay-user\u003erelay\u003c/relay-user\u003e\n\n    \u003c!--\n      Used in the master server as part of the authentication when a slave requests\n      the list of streams to relay. \n    --\u003e\n    \u003crelay-password\u003ehackme\u003c/relay-password\u003e\n\n    \u003c!--\n      The username/password used for all administration functions. This includes\n      retrieving statistics, accessing the web-based administration screens, etc.\n      A list of these functions can be found in the \"Administration\" section of\n      the manual. \n    --\u003e\n    \u003cadmin-user\u003eadmin\u003c/admin-user\u003e\n    \u003cadmin-password\u003ehackme\u003c/admin-password\u003e\n  \u003c/authentication\u003e\n\n  \u003c!--\n    This section contains all the settings for listing a stream on any of the\n    Icecast2 YP Directory servers. Multiple occurances of this section can be\n    specified in order to be listed on multiple directory servers. \n\n    Right now it is commented out, if you want your stream listed publicly on\n    Icecast's directory then uncomment this whole section. \n\n    Comments inside this section would mess it up, so I've included the\n    documentation here in the parent block:\n\n    yp-url:\n    The URL which icecast2 uses to communicate with the Directory server. The\n    value for this setting is provided by the owner of the Directory server.\n\n    yp-url-timeout:\n    This value is the maximum time icecast2 will wait for a response from a\n    particular directory server. The recommended value should be sufficient\n    for most directory servers. \n  --\u003e\n  \u003c!--\n  \u003cdirectory\u003e\n    \u003cyp-url\u003ehttp://dir.xiph.org/cgi-bin/yp-cgi\u003c/yp-url\u003e\n    \u003cyp-url-timeout\u003e15\u003c/yp-url-timeout\u003e\n  \u003c/directory\u003e\n   --\u003e\n\n  \u003c!--\n    If this server is going to be a slave relaying all of the mount points for\n    another Icecast server, you can use these settings to configure it to be a\n    slave. None of these need to be setup on the master server.\n\n    master-server:\n    This is the IP or domain name for the master server that is actually\n    hosting the streams.\n\n    master-server-port:\n    This is the TCP port on the master server to connect to, while not\n    documented anywhere I suspect that this port needs to be a shoutcast-compat\n    configured port.\n\n    master-update-interval:\n    The interval (in seconds) of how often to update the list of streams\n    available on the master. If streams are pretty stable and don't change very\n    often (such as an internet radio station) it might be a good idea to\n    increase this up to something like 10 or 15 minutes.\n\n    master-username:\n    This is the username that has been configured on the master server that the\n    slave will use to authenticate. This defaults to 'relay' on both sides and\n    is optional.\n\n    master-password:\n    This is the password that has been configured on the master server that the\n    slave will use to authenticate.\n\n    relays-on-demand:\n    When this is set the slave will only connect to a master's stream to\n    rebroadcast it if the slave has at least one listener. Setting this avoids\n    using bandwidth unecessarily.\n  --\u003e\n  \u003c!--\n    \u003cmaster-server\u003eother-icecast-server.local\u003c/master-server\u003e\n    \u003cmaster-server-port\u003e8001\u003c/master-server-port\u003e\n    \u003cmaster-update-interval\u003e120\u003c/master-update-interval\u003e\n    \u003cmaster-username\u003erelay\u003c/master-password\u003e\n    \u003cmaster-password\u003ehackme\u003c/master-password\u003e\n    \u003crelays-on-demand\u003e1\u003c/relays-on-demand\u003e\n  --\u003e\n\n  \u003c!--\n    If only specific mountpoints need to be relayed, then you can configure an\n    Icecast slave with a \"Specific Mountpoint Relay\". Using a Specific\n    Mountpoint Relay, only those mountpoints specified will be relayed. This\n    only needs to be configured on the Icecast slaves.\n\n    server:\n    This is the IP or domain name for the server which contains the mountpoint\n    to be relayed (The master server).\n\n    port:\n    This is the TCP Port for the server which contains the mountpoint to be\n    relayed (The master server).\n\n    mount:\n    The mountpoint located on the remote server. If you are relaying a shoutcast\n    stream, this should be a '/' or '/;name'. \n\n    local-mount:\n    The name to use for the local mountpoint. This is what the mount will be\n    named on the relaying server. By default the remote mountpoint name is used. \n\n    username:\n    This is the username the slave will use to authenticate to the remote\n    (master) server. Usually this is 'relay'.\n\n    password:\n    This is the password the slave will use to authenticate to the remote\n    (master) server.\n\n    on-demand:\n    An on-demand relay will only retrieve the stream if there are listeners\n    requesting the stream. This is useful in cases where you want to limit\n    bandwidth when no one is listening. \n\n    relay-shoutcast-metadata:\n    If you are relaying a Shoutcast stream, you may want to specify this\n    indicator to also relay the metadata (song titles) that are part of the\n    Shoutcast data stream. By default this is enabled but it is up to the\n    remote server on whether it sends any. \n  --\u003e\n  \u003c!--\n  \u003crelay\u003e\n    \u003cserver\u003eother-icecast-server.local\u003c/server\u003e\n    \u003cport\u003e8001\u003c/port\u003e\n    \u003cusername\u003erelay\u003c/username\u003e\n    \u003cpassword\u003ehackme\u003c/password\u003e\n\n    \u003cmount\u003e/remote-stream\u003c/mount\u003e\n    \u003clocal-mount\u003e/local-stream\u003c/local-mount\u003e\n\n    \u003con-demand\u003e1\u003c/on-demand\u003e\n    \u003crelay-shoutcast-metadata\u003e1\u003c/relay-shoutcast-metadata\u003e\n  \u003c/relay\u003e\n  --\u003e\n\n  \u003c!--\n    This section contains the settings which apply only to a specific mountpoint\n    and applies to an incoming stream whether it is a relay or a source client.\n    The purpose of the mount definition is to state certain information that can\n    override either global/default settings or settings provided from the\n    incoming stream.\n\n    A mount does not need to be stated for each incoming source although you may\n    want to specific certain settings like the maximum number of listeners or a\n    mountpoint specific username/password. As a general rule, only define what\n    you need to but each mount definition needs at least the mount-name.\n    Changes to most of these will apply across a configuration file re-read even\n    on active streams, however some only apply when the stream starts or ends.\n  --\u003e\n  \u003cmount\u003e\n    \u003c!--\n      The name of the mount point for which these settings apply\n    --\u003e\n    \u003cmount-name\u003e/example.ogg\u003c/mount-name\u003e\n\n    \u003c!--\n      An optional value which will set the username that a source must use to\n      connect using this mountpoint \n    --\u003e\n    \u003cusername\u003eothersource\u003c/username\u003e\n\n    \u003c!--\n      An optional value which will set the password that a source must use to\n      connect using this mountpoint\n    --\u003e\n    \u003cpassword\u003ehackmemore\u003c/password\u003e\n\n    \u003c!--\n      An optional value which will set the maximum number of listeners that can\n      be attached to this mountpoint.\n    --\u003e\n    \u003cmax-listeners\u003e1\u003c/max-listeners\u003e\n\n    \u003c!--\n      An optional value which will set the length of time a listener will stay\n      connected to the stream. An auth component may override this\n    --\u003e\n    \u003cmax-listener-duration\u003e3600\u003c/max-listener-duration\u003e\n\n    \u003c!--\n      An optional value which will set the filename which will be a dump of the\n      stream coming through on this mountpoint \n    --\u003e\n    \u003cdump-file\u003e/tmp/dump-example1.ogg\u003c/dump-file\u003e\n\n    \u003c!--\n      An optional value which will specify the file those contents will be sent\n      to new listeners when they connect but before the normal stream is sent.\n      Make sure the format of the file specified matches the streaming format.\n      The specified file is appended to webroot before being opened\n    --\u003e\n    \u003cintro\u003e/intro.ogg\u003c/intro\u003e\n\n    \u003c!--\n      This optional value specifies a mountpoint that clients are automatically\n      moved to if the source shuts down or is not streaming at the time a\n      listener connects. Only one can be listed in each mount and should refer\n      to another mountpoint on the same server that is streaming in the same\n      streaming format.\n\n      If clients cannot fallback to another mountpoint, due to a missing\n      fallback-mount or it states a mountpoint that is just not available, then\n      those clients will be disconnected. If clients are falling back to a\n      mountpoint and the fallback-mount is not actively streaming but defines a\n      fallback-mount itself then those clients may be moved there instead. This\n      multi-level fallback allows clients to cascade several mountpoints.\n\n      A fallback mount can also state a file that is located in webroot. This is\n      useful for playing a pre-recorded file in the case of a stream going down.\n      It will repeat until either the listener disconnects or a stream comes\n      back available and takes the listeners back. As per usual, the file format\n      should match the stream format, failing to do so may cause problems with\n      playback.\n\n      Note that the fallback file is not timed so be careful if you intend to\n      relay this. They are fine on slave streams but don't use them on master\n      streams, if you do then the relay will consume stream data at a faster\n      rate and the listeners on the relay would eventually get kicked off.\n    --\u003e\n    \u003cfallback-mount\u003e/example2.ogg\u003c/fallback-mount\u003e\n\n    \u003c!--\n      When enabled, this allows a connecting source client or relay on this\n      mountpoint to move listening clients back from the fallback mount\n    --\u003e\n    \u003cfallback-override\u003e1\u003c/fallback-override\u003e\n\n    \u003c!--\n      When set to 1, this will cause new listeners, when the max listener count\n      for the mountpoint has been reached, to move to the fallback mount if\n      there is one specified\n    --\u003e\n    \u003cfallback-when-full\u003e1\u003c/fallback-when-full\u003e\n\n    \u003c!--\n      For non-Ogg streams like MP3, the metadata that is inserted into the stream\n      often has no defined character set. We have traditionally assumed UTF8 as\n      it allows for multiple language sets on the web pages and stream directory,\n      however many source clients for MP3 type streams have assumed Latin1\n      (ISO-8859-1) or leave it to whatever character set is in use on the source\n      client system.\n\n      This character mismatch has been known to cause a problem as the stats\n      engine and stream directory servers want UTF8 so now we assume Latin1 for\n      non-Ogg streams (to handle the common case) but you can specify an\n      alternative character set with this option.\n\n      The source clients can also specify a charset= parameter to the metadata\n      update URL if they so wish.\n    --\u003e\n    \u003ccharset\u003eISO8859-1\u003c/charset\u003e\n\n    \u003c!--\n      The default setting for this is -1 indicating that it is up to the source\n      client or relay to determine if this mountpoint should advertise. A setting\n      of 0 will prevent any advertising and a setting of 1 will force it to\n      advertise. If you do force advertising you may need to set other settings\n      listed below as the YP server can refuse to advertise if there is not\n      enough information provided.\n    --\u003e\n    \u003cpublic\u003e1\u003c/public\u003e\n\n    \u003c!--\n      Setting this will add the specified name to the stats (and therefore YP)\n      for this mountpoint even if the source client/relay provide one.\n    --\u003e\n    \u003cstream-name\u003eMy stream\u003c/stream-name\u003e\n\n    \u003c!--\n      Setting this will add the specified description to the stats (and therefore\n      YP) for this mountpoint even if the source client/relay provide one.\n    --\u003e\n    \u003cstream-description\u003eMy description\u003c/stream-description\u003e\n\n    \u003c!--\n      Setting this will add the specified URL to the stats (and therefore YP)\n      for this mountpoint even if the source client/relay provide one. The URL\n      is generally for directing people to a website.\n    --\u003e\n    \u003cstream-url\u003ehttp://streams.example.org/\u003c/stream-url\u003e\n\n    \u003c!--\n      Setting this will add the specified genre to the stats (and therefore YP)\n      for this mountpoint even if the source client/relay provide one. This can\n      be anything be using certain key words can help searches in the YP\n      directories.\n    --\u003e\n    \u003cgenre\u003emy-genre\u003c/genre\u003e\n\n    \u003c!--\n      Setting this will add the specified bitrate to the stats (and therefore YP)\n      for this mountpoint even if the source client/relay provide one. This is\n      stated in kbps.\n    --\u003e\n    \u003cbitrate\u003e128\u003c/bitrate\u003e\n\n    \u003c!--\n      Setting this will add the specified mime type to the stats (and therefore\n      YP) for this mountpoint even if the source client/relay provide one. It is\n      very unlikely that this will be needed.\n    --\u003e\n    \u003ctype\u003eapplication/ogg\u003c/type\u003e\n\n    \u003c!--\n      Setting this will add the specified subtype to the stats (and therefore\n      YP) for this mountpoint. The subtype is really to help the YP server to\n      identify the components of the type. An example setting is vorbis/theora\n      do indicate the codecs in an Ogg stream.\n    --\u003e\n    \u003csubtype\u003evorbis\u003c/subtype\u003e\n\n    \u003c!--\n      Enable this to prevent this mount from being shown on the xsl pages. This\n      is mainly for cases where a local relay is configured and you do not want\n      the source of the local relay to be shown.\n    --\u003e\n    \u003chidden\u003e1\u003c/hidden\u003e\n\n    \u003c!--\n      This optional setting allows for providing a burst size which overrides\n      the default burst size as defined in limits. The value is in bytes.\n    --\u003e\n    \u003cburst-size\u003e65536\u003c/burst-size\u003e\n\n    \u003c!--\n      This optional setting specifies what interval, in bytes, there is between\n      metadata updates within shoutcast compatible streams. This only applies to\n      new listeners connecting on this mountpoint, not existing listeners\n      falling back to this mountpoint. The default is either the hardcoded server\n      default or the value passed from a relay.\n    --\u003e\n    \u003cmp3-metadata-interval\u003e4096\u003c/mp3-metadata-interval\u003e\n\n    \u003c!--\n      This specifies that the named mount point will require listener\n      authentication using HTTP basic auth. Despite the official documentation\n      claiming to only support \"htpasswd\" type authentication, it also supports\n      \"url\" type authentication and even provides examples elsewhere in their\n      documentation.. They really need to go through and clean up everything...\n\n      Anyway here is an example of htpasswd type authentication...\n\n      The htpasswd authenticator requires a few parameters. The first, filename,\n      specifies the name of the file to use to store users and passwords. Note\n      that this file need not exist (and probably will not exist when you first\n      set it up). Icecast has built-in support for managing users and passwords\n      via the web admin interface. The second option, allow_duplicate_users, if\n      set to 0, will prevent multiple connections using the same username.\n      Setting this value to 1 will enable mutltiple connections from the same\n      username on a given mountpoint. Note there is no way to specify a \"max\n      connections\" for a particular user.\n    --\u003e\n    \u003cauthentication type=\"htpasswd\"\u003e\n      \u003coption name=\"filename\" value=\"users-auth-file\" /\u003e\n      \u003coption name=\"allow_duplicate_users\" value=\"0\" /\u003e\n    \u003c/authentication\u003e\n\n    \u003c!--\n      And an example for URL authentication...\n\n      Authenticating listeners via the URL method involves icecast, when a\n      listener connects, issuing requests to a web server and checking the\n      response headers. If a certain header is sent back then the listener\n      connecting is allowed to continue, if not, an error is sent back to the\n      listener.\n\n      The URLs specified will invoke some web server scripts like PHP to do any\n      work that they may choose to do. All that is required of the scripting\n      language is that POST information can be handled and response headers can\n      be sent back. libcurl is used for the requesting so https connections may\n      be possible, but be aware of the extra overhead involved.\n\n      The useragent sent in each curl request will represent the icecast server\n      version. The response headers will depend on whether the listener is to be\n      accepted. Acceptance is determined by the auth_header option you can also\n      passed a failed reason in a header that looks like the following:\n\n        icecast-auth-message: some reason for failure\n\n      Each of the options described below are optional. Each option sends the\n      parameters action, mount, server, and port. The action is the name of the\n      option, the server is either the hostname or IP the user is connected to\n      (haven't tested this), the port is the port the user is connected on and\n      the mount is the stream mount point with leading slash that the user is\n      attempting to connect to. The last catch is that listener_add and\n      listener_remove have additional parameters, which are documented in their\n      sections..\n\n      The documentation mentions that these will be sent as a POST request,\n      however, the way they are shown look like they are actually a GET\n      request...\n    --\u003e\n    \u003cauthentication type=\"url\"\u003e\n      \u003c!--\n        This URL is for informing the auth server of a stream starting. No\n        listener information is passed for this, but can be used to initialise\n        any details the auth server may have.\n      --\u003e\n      \u003coption name=\"mount_add\" value=\"http://myauthserver.com/stream_start.php\"/\u003e\n\n      \u003c!--\n        This URL is for informing the auth server of a stream finishing, like\n        the start option, no listener details are passed.\n      --\u003e\n      \u003coption name=\"mount_remove\" value=\"http://myauthserver.com/stream_end.php\"/\u003e\n\n      \u003c!--\n        This is most likely to be used if anything. When a listener connects,\n        before anything is sent back to them, this request is processed. The\n        default action is to reject a listener unless the auth server sends back\n        a response header which may be stated in the 'header' option.\n\n        listener_add also provides, client, user, pass, ip, and agent. client is\n        a unique number for the client that is connected. I don't know if the\n        client number is re-used after a client disconnects, I don't know if this\n        count gets reset every time a source connect or only when the server\n        restarts or if it ever resets at all. I suspect user is filled in with\n        the value of any basic auth user that has been sent, same with password\n        but this might also get filled in by the the 'username' and 'password'\n        options that are undocumented, ip is the IP address of the client and\n        agent is the user agent the client provided. This is quite a bit of\n        useful information for authenticating a user though...\n      --\u003e\n      \u003coption name=\"listener_add\" value=\"http://myauthserver.com/listener_joined.php\"/\u003e\n\n      \u003c!--\n        This URL is for when a listener connection closes.\n\n        listener_remove also provides the client, user, pass, and duration. I'm\n        assuming client is the same number as that sent in listener_add, user and\n        pass are the same as for listener_add in that I assume it's the clients\n        basic auth but it could also be the username and password fields passed\n        as other options in this section... duration is an interesting one\n        though, it is the length in seconds that the listener was connected to\n        the string.\n      --\u003e\n      \u003coption name=\"listener_remove\" value=\"http://myauthserver.com/listener_left.php\"/\u003e\n\n      \u003c!--\n        The expected response header to be returned that allows the authencation\n        to take place may be specified here. The default is:\n\n          icecast-auth-user: 1\n      --\u003e\n      \u003coption name=\"auth_header\" value=\"icecast-auth-user: 1\"/\u003e\n\n      \u003c!--\n        Listeners could have a time limit imposed on them, and if this header is\n        sent back with a figure (which represents seconds) then the Icecast\n        server will disconnect them after this duration has elapsed.\n      --\u003e\n      \u003coption name=\"timelimit_header\" value=\"icecast-auth-timelimit:\"/\u003e\n\n      \u003c!--\n        These are quite the mystery... mentioned yet undocumented... I suspect\n        they are passed to the listener_{add,remove} script as authentication\n        for the server to the authentication server but I have no way to be\n        sure... Maybe they're a form of default user/pass?\n      --\u003e\n      \u003coption name=\"username\" value=\"user\"/\u003e\n      \u003coption name=\"password\" value=\"pass\"/\u003e\n    \u003c/authentication\u003e\n\n    \u003c!--\n      State a program that is run when the source is started. It is passed a\n      parameter which is the name of the mountpoint that is starting. The\n      processing of the stream does not wait for the script to end.\n    --\u003e\n    \u003con-connect\u003e/home/icecast/bin/source-start\u003c/on-connect\u003e\n\n    \u003c!--\n      State a program that is run when the source ends. It is passed a parameter\n      which is the name of the mountpoint that has ended. The processing of the\n      stream does not wait for the script to end.\n    --\u003e\n    \u003con-disconnect\u003e/home/icecast/bin/source-end\u003c/on-disconnect\u003e\n  \u003c/mount\u003e\n\u003c/icecast\u003e\n```\n\n## Next Steps\n\n* `http://koorenneef.nl/content/run-your-own-online-radio-station-icecast2-and-ezstream-howto` -\u003e Dead link\n* http://www.linuxcertif.com/man/1/ezstream/\n\n[3]: {{\u003c ref \"./iptables.md\" \u003e}}\n[4]: {{\u003c ref \"./password_security.md\" \u003e}}\n[5]: /note_files/icecast/silence.ogg\n","created_at":-62135596800,"fuzzy_word_count":6600,"path":"/notes/icecast/","published_at":1540945455,"reading_time":31,"tags":null,"title":"IceCast","type":"notes","updated_at":1540945455,"weight":0,"word_count":6572},{"cid":"4e1db1c9c673e9c3b4e786ac9a58d8cb426f2afb","content":"\n***Note: This page is quite old and is likely out of date. My opinions may have\nalso changed dramatically since this was written. It is here as a reference\nuntil I get around to updating it.***\n\nIcinga is a serious and awesome port of nagios. It properly modernizes it and\ngives it all the stuff the nagios developers made into an \"Enterprise only\"\nrelease just to be tool bags. Unfortunately at this time there aren't any handy\nFedora packages, but this should be [coming soon][1] with any luck.\n\nHowever I seem to be lucky and found that the RHEL6 packages seem to work\nperfectly on Fedora 15. There is a doc, gui, idoutils, api and plain icinga\npackages, they should all be installed.  (Tested with 1.5.1-1).\n\nInstall `httpd` and `mysql-server`. Set everything to start up. Might need to\ncreate a database like so:\n\n```\n[user@host ~]$ mysql -u root -p\nmysql\u003e CREATE DATABASE icinga;\n GRANT USAGE ON *.* TO 'icinga'@'localhost'\n   IDENTIFIED BY 'icinga'\n   WITH MAX_QUERIES_PER_HOUR 0\n   MAX_CONNECTIONS_PER_HOUR 0\n   MAX_UPDATES_PER_HOUR 0;\n GRANT SELECT , INSERT , UPDATE , DELETE\n   ON icinga.* TO 'icinga'@'localhost';\n FLUSH PRIVILEGES ;\n quit\n```\n\nYou need to turn selinux off for now... Yeah not my favorite either.\n\nCreate an htpasswd file in the icinga configuration directory and give apache\naccess too it:\n\n```\n[root@localhost ~]# htpasswd -c /etc/icinga/htpasswd.users admin\nNew password:\nRe-type new password:\nAdding password for user admin\n[root@localhost ~]# chown icinga:apache /etc/icinga/htpasswd.users\n```\n\nIt occurs to me that isn't enough...\n\n```\n[root@icinga etc]# chown -R icinga:apache /etc/icinga/\n```\n\nI edited this value in the /etc/icinga/icinga.cfg file:\n\n```\nicinga_group=apache\n```\n\n* http://docs.icinga.org/latest/en/icinga-web-scratch.html\n* http://docs.icinga.org/latest/en/quickstart-idoutils.html\n\n[1]: https://bugzilla.redhat.com/show_bug.cgi?id=693608\n","created_at":-62135596800,"fuzzy_word_count":300,"path":"/notes/icinga/","published_at":1508540507,"reading_time":2,"tags":null,"title":"Icinga","type":"notes","updated_at":1508540507,"weight":0,"word_count":255},{"cid":"efee24683522e9f48993afdc67471e6970f1c27c","content":"\n***Note: This page is quite old and is likely out of date. My opinions may have\nalso changed dramatically since this was written. It is here as a reference\nuntil I get around to updating it.***\n\nOn the server side:\n\n```\nyum install iodine-server -y\n```\n\nEdit the `/etc/sysconfig/iodine-server` file and put the following options in:\n\n```\nOPTIONS=\"-P somepassword 172.16.0.1 t.0x378.net\"\n```\n\nCreate an `A` or `CNAME` record for `iodine-01.0x378.net` pointing at the FQDN\nof the server running `iodine` and a `NS` record pointing at\n`iodine-01.0x378.net` for the domain `t.0x378.net` (shorter is better, allows\nfor higher speed).\n\nAnd firewall rules...\n\nfilter table:\n\n```\n# Allow access to the iodine server\n-A INPUT -i eth0 -m udp -p udp --dport 53 -j ACCEPT\n-A INPUT -i eth0 -m tcp -p tcp --dport 53 -j ACCEPT\n\n# Accept tunneled traffic from iodine\n-A FORWARD -i dns+ -o eth0 -j ACCEPT\n-A FORWARD -i eth0 -o dns+ -m conntrack --ctstate RELATED,ESTABLISHED -j ACCEPT\n```\n\nnat table:\n\n```\n*nat\n:INPUT ACCEPT [0:0]\n:OUTPUT ACCEPT [0:0]\n\n-A POSTROUTING -s 172.16.0.0/24 -o eth0 -j MASQUERADE\n\nCOMMIT\n```\n","created_at":-62135596800,"fuzzy_word_count":200,"path":"/notes/iodine/","published_at":1507585385,"reading_time":1,"tags":null,"title":"Iodine","type":"notes","updated_at":1507585385,"weight":0,"word_count":170},{"cid":"31c24fe000d9431086290c99de1c928b70133774","content":"\n***Note: This page is quite old and is likely out of date. My opinions may have\nalso changed dramatically since this was written. It is here as a reference\nuntil I get around to updating it.***\n\nThis is the client configuration for iSCSI, please refer to [iSCSId][1] for the\nserver portion.\n\n## Configuration\n\nyum install iscsi-initiator-utils -y\n\n```\niscsid.startup = /etc/rc.d/init.d/iscsid force-start\nnode.startup = automatic\n\nnode.session.auth.authmethod = CHAP\nnode.session.auth.username = client1\nnode.session.auth.password = XXXXXXXXXXXX\n\ndiscovery.sendtargets.auth.authmethod = CHAP\ndiscovery.sendtargets.auth.username = client1\ndiscovery.sendtargets.auth.password = XXXXXXXXXXXX\n\nnode.conn[0].timeo.login_timeout = 10\nnode.conn[0].timeo.logout_timeout = 10\nnode.conn[0].timeo.noop_out_interval = 5\nnode.conn[0].timeo.noop_out_timeout = 5\n\nnode.session.cmds_max = 128\nnode.session.err_timeo.abort_timeout = 15\nnode.session.err_timeo.lu_reset_timeout = 30\nnode.session.err_timeo.tgt_reset_timeout = 30\nnode.session.initial_login_retry_max = 5\nnode.session.timeo.replacement_timeout = 120\nnode.session.queue_depth = 32\nnode.session.xmit_thread_priority = -20\n\n#node.session.iscsi.InitialR2T = Yes\nnode.session.iscsi.InitialR2T = No\n\n#node.session.iscsi.ImmediateData = No\nnode.session.iscsi.ImmediateData = Yes\n\nnode.session.iscsi.FirstBurstLength = 262144\nnode.session.iscsi.MaxBurstLength = 16776192\nnode.conn[0].iscsi.MaxRecvDataSegmentLength = 262144\nnode.conn[0].iscsi.MaxXmitDataSegmentLength = 0\ndiscovery.sendtargets.iscsi.MaxRecvDataSegmentLength = 32768\n\n#node.conn[0].iscsi.HeaderDigest = CRC32C,None\nnode.conn[0].iscsi.HeaderDigest = None\n\nnode.session.iscsi.FastAbort = Yes\n```\n\n```\n[root@localhost ~]# chkconfig iscsi on\n[root@localhost ~]# chkconfig iscsid on\n[root@localhost ~]# service iscsi start\n[root@localhost ~]# service iscsid start\n[root@localhost ~]# iscsiadm --mode discovery --type sendtargets --portal \u003cservice_ip\u003e\n```\n\n[1]: {{\u003c ref \"./iscsid.md\" \u003e}}\n","created_at":-62135596800,"fuzzy_word_count":200,"path":"/notes/iscsi/","published_at":1540945455,"reading_time":1,"tags":null,"title":"iSCSI","type":"notes","updated_at":1540945455,"weight":0,"word_count":180},{"cid":"2d653c1865e6a33a3f81b3968eb6ca514c75cb34","content":"\n***Note: This page is quite old and is likely out of date. My opinions may have\nalso changed dramatically since this was written. It is here as a reference\nuntil I get around to updating it.***\n\nThis is the server configuration for iSCSI, please refer to [iSCSI][1] for the\nclient portion.\n\n## Configuration\n\nInstall the package `scsi-target-utils`.\n\nExample config:\n\n```\ndefault-driver iscsi\ninitiator-address 10.0.0.100\n#ignore-errors yes\n\n\u003ctarget iqm.2011-09.net.bedroomprogrammers.lab:example-srv.storage\u003e\n  backing-store /dev/vdb\n\n  incominguser client1 XXXXXXXXXXXX\n  incominguser client2 XXXXXXXXXXXX\n\n  write-cache off\n  vendor_id Bedroom Programmers\n\u003c/target\u003e\n```\n\n```\n[root@localhost ~]# chkconfig tgtd on\n[root@localhost ~]# service tgtd start\n```\n\n## References\n\n* http://fedoraproject.org/wiki/Scsi-target-utils_Quickstart_Guide - High Quality\n* `http://www.ryanuber.com/a-quick-introduction-to-gfs2-over-iscsi.html` (dead link) - Transition from iSCSI to GFS2\n\n[1]: {{\u003c ref \"./iscsi.md\" \u003e}}\n","created_at":-62135596800,"fuzzy_word_count":200,"path":"/notes/iscsid/","published_at":1540945455,"reading_time":1,"tags":null,"title":"iSCSId","type":"notes","updated_at":1540945455,"weight":0,"word_count":101},{"cid":"ffe63dda6f008e100844e84de7b10637d8c3b4d1","content":"\n***Note: This page is quite old and is likely out of date. My opinions may have\nalso changed dramatically since this was written. It is here as a reference\nuntil I get around to updating it.***\n\nKerberos is a secure network authentication system.\n\nIt is very important that system times are all very close for successful\nauthentication. You should configure [NTPd][1] or [Chronyd][2] to ensure the\nsystems stay in sync.\n\n## Master Configuration\n\nEdit the configuration files (provided at the bottom).\n\nCreate the initial database for the realm, it will ask for a master password\nfor the database. DO NOT FORGET THIS!!\n\n```\nkdb5_util create -s\n```\n\nCreate an administrator user, it will ask for the password ensure that it is\nstrong as this account will be able to create, delete, and see principal's\nkeys.\n\n```\nkadmin.local -q \"addprinc \u003c\u003cusername\u003e\u003e/admin\"\n```\n\nSet the services to startup automatically and start them up:\n\n```\nchkconfig krb5kdc on\nchkconfig kadmin on\n/etc/init.d/krb5kdc start\n/etc/init.d/kadmin start\n```\n\nAfter the database has been started you'll need to create at least one normal\nuser, it will ask for a password for the account.\n\n```\n[root@localhost ~]# kadmin -p \u003c\u003cusername\u003e\u003e/admin\nkadmin:  addprinc \u003c\u003cusername\u003e\u003e\n```\n\n### Slave\n\nPlease note these instructions are untested but they are believed to be\ncorrect.\n\nFirst we'll need to create an ACL file for the replication utility `kprop`. It\nshould live `/var/Kerberos/krb5kdc/kpropd.acl` and have the contents:\n\n```\nhost/kdc1.home.bedroomprogrammers.net@BEDROOMPROGRAMMERS.NET\nhost/kdc2.home.bedroomprogrammers.net@BEDROOMPROGRAMMERS.NET\n```\n\nYou'll need to create host keys for each of the kerberos servers if they\nhaven't been created already.\n\n```\n[root@localhost ~]# kadmin -p \u003c\u003cusername\u003e\u003e/admin\nkadmin:  addprinc -rand host/kdc1.home.bedroomprogrammers.net\nkadmin:  addprinc -rand host/kdc2.home.bedroomprogrammers.net\n```\n\nYou'll need to add the keys to the local keytab file on the primary KDC.\n\n```\n[root@localhost ~]# kadmin -p \u003c\u003cusername\u003e\u003e/admin\nkadmin:  ktadd host/kdc1.home.bedroomprogrammers.net\nkadmin:  ktadd host/kdc2.home.bedroomprogrammers.net\n```\n\nCopy the keytab file from the primary KDC to the secondary server using an\nencrypted transfer mechanism such as SCP.\n\n```\n[root@localhost ~]# scp /etc/krb5.keytab root@krb2.home.bedroomprogrammers.net:/etc/krb5.keytab\n```\n\nAt this point you'll want to restart the master's service and then start up the\nkdc on the slave server:\n\n```\n[root@slave ~]# chkconfig krb5kdc start\n[root@slave ~]# /etc/init.d/krb5kdc start\n```\n\nOnce the Slave is setup you'll need to modify the `/etc/krb5.conf` on the\nclient machines and the master machine to include the FQDN of the slave in the\n`[realms]` section for the appropriate domain. The `[realms]` section would\nthen look like:\n\n```ini\n[realms]\n  BEDROOMPROGRAMMERS.NET = {\n    kdc = kdc1.home.bedroomprogrammers.net\n    kdc = kdc2.home.bedroomprogrammers.net\n    admin_server = kdc1.home.bedroomprogrammers.net\n  }\n```\n\n## Adding a New Host to the Domain\n\nHosts need `krb5-workstation` and `krb5-libs` installed. Make sure the client\nfirewall rules have been applied.\n\nBe sure to copy the config file `/etc/krb5.conf` mentioned on this page to each\nclient machine.\n\n```\n[root@kerb-client ~]# kadmin -p \u003c\u003cusername\u003e\u003e/admin\nkadmin:  addprinc -randkey host/\u003c\u003cFQDN\u003e\u003e\nkadmin:  ktadd -k /etc/krb5.keytab host/\u003c\u003cFQDN\u003e\u003e\n```\n\nYou'll need to replace \u003c\u003cFQDN\u003e\u003e with the domain name of the host you're adding.\nIt will need a DNS entry matching this hostname.\n\n## Firewall Configuration\n\n### Server Adjustments\n\n```\n# Allow authentication to the KDC\n-A INPUT -m tcp -p tcp --dport 88 -j ACCEPT\n# Allow remote kerberos administration. This should probably be restricted more\n-A INPUT -m tcp -p tcp --dport 749 -j ACCEPT\n```\n\n### Client Adjustments\n\n```\n# Allow authentication to the KDC\n-A OUTPUT -m tcp -p tcp --dport 88 -j ACCEPT\n```\n\nSetting up a client machine to talk to the KDC will initially require this rule\nas well:\n\n```\n# Temporarily need this to setup the connection with the KDC\n-A OUTPUT -m tcp -p tcp --dport 749 -j ACCEPT\n```\n\n## Configuration Files\n\n### /etc/krb5.conf\n\n```ini\n[logging]\n  default = FILE:/var/log/krb5libs.log\n  kdc = FILE:/var/log/krb5kdc.log\n  admin_server = FILE:/var/log/kadmind.log\n\n[libdefaults]\n  default_realm = BEDROOMPROGRAMMERS.NET\n  dns_lookup_realm = false\n  dns_lookup_kdc = false\n  ticket_lifetime = 24h\n  renew_lifetime = 3d\n  forwardable = true\n\n[realms]\n  BEDROOMPROGRAMMERS.NET = {\n    kdc = kdc1.home.bedroomprogrammers.net\n    admin_server = kdc1.home.bedroomprogrammers.net\n  }\n\n[domain_realm]\n  .bedroomprogrammers.net = BEDROOMPROGRAMMERS.NET\n  bedroomprogrammers.net = BEDROOMPROGRAMMERS.NET\n  .home.bedroomprogrammers.net = BEDROOMPROGRAMMERS.NET\n  home.bedroomprogrammers.net = BEDROOMPROGRAMMERS.NET\n```\n\n### /var/kerberos/krb5kdc/kadm5.acl\n\n```\n*/admin@BEDROOMPROGRAMMERS.NET  *\n```\n\n### /var/kerberos/krb5kdc/kdc.conf\n\n```ini\n[kdcdefaults]\n  kdc_ports = 88\n  kdc_tcp_ports = 88\n\n[realms]\n  BEDROOMPROGRAMMERS.NET = {\n    master_key_type = aes256-cts\n    acl_file = /var/kerberos/krb5kdc/kadm5.acl\n    dict_file = /usr/share/dict/words\n    admin_keytab = /var/kerberos/krb5kdc/kadm5.keytab\n    supported_enctypes = aes256-cts:normal aes128-cts:normal \n  }\n```\n\n## Notes on Cached Client Login\n\n* http://redhatcat.blogspot.com/2008/07/caching-kerberos-credentials-for.html\n* http://ubuntuforums.org/showthread.php?t=1205604\n* http://www.techrepublic.com/blog/opensource/authentication-caching-with-nscd/127\n* http://people.skolelinux.org/pere/blog/Caching_password__user_and_group_on_a_roaming_Debian_laptop.html\n\n[1]: {{\u003c ref \"./ntpd.md\" \u003e}}\n[2]: {{\u003c ref \"./chronyd.md\" \u003e}}\n","created_at":-62135596800,"fuzzy_word_count":700,"path":"/notes/kerberos/","published_at":1540945455,"reading_time":4,"tags":null,"title":"Kerberos","type":"notes","updated_at":1540945455,"weight":0,"word_count":680},{"cid":"7664a7ae954cb3db7214d6cbc52a05001ddbbe5e","content":"\n***Note: This page is quite old and is likely out of date. My opinions may have\nalso changed dramatically since this was written. It is here as a reference\nuntil I get around to updating it.***\n\nlibvirt is an open source API and management tool for managing platform\nvirtualization. It is used to manage Linux KVM and Xen virtual machines through\ngraphical interfaces such as Virtual Machine Manager and higher level tools\nsuch as oVirt. In this case the backend is KVM.\n\n## Security Notes\n\nSince libvirtd distributes resources to guest machines a tight control needs to\nbe placed on the guests to prevent the host from becoming unmanageable. Luckily\nthrough the use of CGroups this can be accomplished.\n\nNetworking is a tricky topic as there are several ways to do this in libvirtd.\nMy personal choice is to make the host completely transparent in regards to the\nnetworking and that is bridge mode. There is also routed and NAT either way\nexposes the host's IP.\n\nRouted mode also requires additional configuration in the network hardware or\non the client machines and NAT requires port forwarding within the host\nmachine. I will not cover either of these in this wiki.\n\nPrivilege separation is very important. In the event that a guest gets\ncompromised and the attacker finds a way to break the virtualization's jail,\nthey should not immediately gain root privileges.\n\nLuckily this is already covered by default (at least with Fedora 12 installs)\nin which libvirt will drop it's privileges to the qemu user for every guest\nthat it spawns. Each guest is also automatically given it's own SELinux label.\nThis will further isolate each guest.\n\n## Firewall Adjustments\n\nRegardless of the type of network that is chosen for guests all of their\ntraffic goes through the forwarding chain of iptables. While I really strongly\nadvocate against any default allow policies on anything, this is one place\nwhere I personally am willing to make an exception. To allow guests to talk to\nthe network make the following change to the default [IPTables][1] rules:\n\n```\n:FORWARD DROP [0:0]\n--- Replace with ---\n:FORWARD ACCEPT [0:0]\n```\n\n## CGroup Configuration\n\nAt a high level, CGroups are a generic mechanism the kernel provides for\ngrouping of processes and applying controls to those groups. Tunables within a\ncgroup are provided by what the kernel calls 'controllers', with each\ncontroller able to expose one or more tunable or control.\n\nWhen mounting the cgroups filesystem it is possible to indicate what\ncontrollers are to be activated. This makes it possible to mount the filesystem\nseveral times, with each mount point having a different set of\n(non-overlapping) controllers.\n\nThe provided controllers are:\n\n* memory: Memory controller - Allows for setting limits on RAM and swap usage\n  and querying cumulative usage of all processes in the group\n* cpuset: CPU set controller - Binding of processes within a group to a set of\n  CPUs and controlling migration between CPus\n* cpuacct: CPU accounting controller - Information about CPU usage for a group\n  of processes\n* cpu: CPU schedular controller - Controlling the priorization of processes in\n  the group. Think of it as a more advanced nice level\n* devices: Devices controller - Access control lists on character and block\n  devices\n* freezer: Freezer controller - Pause and resume execution of processes in the\n  group. Think of it as SIGSTOP for the whole group\n* net_cls: Network class controller - Control network utilization by associating\n  processes with a ‘tc’ network class \n\nUnder KVM all of these are not supported, however cpu, devices and memory are.\nWhich are in my humble opinion the most important for our task. \n\nhttp://berrange.com/posts/2009/12/03/using-cgroups-with-libvirt-and-lxckvm-guests-in-fedora-12/\n\n## Initial Setup\n\nBefore creating your first guest there are a few things that I like to do.\nThese include setting up additional storage pools and configuring networking.\nThis section covers anything that hasn't been done in the config files and I'll\nsave the majority of the networking information for it's own section as most of\nit is done outside of the virtualization console.\n\nSince I use bridge networking and that is the only way I want my guests to talk\nto each other and the rest of the world, I want to get rid of the default\nnetwork which performs NAT'ing so that the VMs don't suddenly get a second\nnetwork interface that I don't know about.\n\nTo do this I'll want to drop into a virsh shell and issue a few commands. You\ncan see an entire session of me removing the default network here:\n\n```\n[root@localhost ~]# virsh\nWelcome to virsh, the virtualization interactive terminal.\n\nType:  'help' for help with commands\n       'quit' to quit\n\nvirsh # net-destroy default\nNetwork default destroyed\n\nvirsh # net-undefine default\nNetwork default has been undefined\n```\n\nNow that our networking problem is taken care of, on to additional storage\npools. By default one storage pool (a directory) exists. This can be found at\n\"/var/lib/libvirt/images\". There is nothing wrong with just using the default\nfor disk images. The only reason I keep a second for disk images is that I like\nto keep all of my VMs on an encrypted partition all to themselves.\n\nSince I need one of my VMs at boot and won't be able to remotely provide a key\nuntil after that VM has booted it needs to be on an unencrypted partition which\nI still use the default for.\n\nThe rest of the VMs will live in a storage pool named \"secure\" mounted at\n\"/var/lib/libvirt/images/secure\". I'm using the the subdirectory of\n\"/var/lib/libvirt/images\" so that I won't have to add additional SELinux\npolicies which is potentially very messy and could present additional security\nrisks.\n\nFirst we need to create an XML file to define the storage pool. Create the\nfollowing in a file named \"secure.xml\":\n\n```xml\n\u003cpool type='dir'\u003e\n  \u003cname\u003esecure\u003c/name\u003e\n  \u003ctarget\u003e\n    \u003cpath\u003e/var/lib/libvirt/images/secure\u003c/path\u003e\n    \u003cpermissions\u003e\n      \u003cmode\u003e0700\u003c/mode\u003e\n      \u003cowner\u003e0\u003c/owner\u003e\n      \u003cgroup\u003e0\u003c/group\u003e\n    \u003c/permissions\u003e\n  \u003c/target\u003e\n\u003c/pool\u003e\n```\n\nNext you need to tell libvirt about the pool. You can do this with the\nfollowing series of commands:\n\n```\n[root@legba ~]# virsh\nWelcome to virsh, the virtualization interactive terminal.\n\nType:  'help' for help with commands\n       'quit' to quit\n\nvirsh # virsh pool-define secure.xml\nPool secure defined from secure.xml\n\nvirsh # pool-start secure\nPool secure started\n\nvirsh # pool-autostart secure\nPool secure marked as autostarted\n\nvirsh # pool-refresh secure\nPool secure refreshed\n```\n\n## Guest Networking\n\nMy network is a tad unusual in that I have VMs on the same box that need to be\non different network segments. Each of these network segments comes back to my\nvirtualization host through VLANs.\n\nWhile Linux has long supported tagged VLANs and been able to hop on different\nsegments as defined by the system administrator, libvirt's virtual network does\nnot support trunking. This means that the interfaces need to be setup on the\nhost and bridged through to the guest. I'm going to provide a working example\nthat passes VLAN20 into a guest as it's native network card.\n\nI'm assuming the trunked port on the host is eth1, please adjust the following\nconfiguration to match what your network has. This example is also specific to\nthe Red Hat architecture. The configuration will be different than it is here\n(and I have not documented it).\n\nYou will need to make sure that the package `bridge-utils` is installed on the\nserver. I think this is a dependency of libvirt but I could be wrong so it's\nbest to double check.\n\nFirst eth1 needs to be configured (or unconfigured in this case). This can be\ndone by editing `/etc/sysconfig/network-scripts/ifcfg-eth1`. The entirety of\nthe files contents should be replaced with the following:\n\n```\nDEVICE=eth1\nBOOTPROTO=none\nHWADDR=XX:XX:XX:XX:XX:XX\nONBOOT=yes\n```\n\nNext we need to tell the server about the VLAN. Create the file\n`/etc/sysconfig/network-scripts/ifcfg-vlan20` if it doesn't exist already, and\npopulate it with the following:\n\n```\nVLAN=yes\nVLAN_NAME_TYPE=VLAN_PLUS_VID_NO_PAD\nDEVICE=vlan20\nPHYSDEV=eth1\nBOOTPROTO=none\nONBOOT=yes\nTYPE=Ethernet\nBRIDGE=br20\n```\n\nNote above that I've specified which bridge the vlan device is going to be\nconnected to. The 20 in br20 can be changed to any other number you'd like,\nhowever for simplicity sake I like to have the bridge reflect what VLAN it is\nconnected to.\n\nThe last step is to configure the bridge interface. This is needed so that a\nguest may make a connection to it. Create the file\n`/etc/sysconfig/network-scripts/ifcfg-br20` if it doesn't already exist and\nreplace the contents with the following:\n\n```\n# br20 - vlan20 - Some non-existant subnet\nDEVICE=br20\nNM_CONTROLLED=no\nONBOOT=yes\nTYPE=Bridge\nBOOTPROTO=static\n\nIPADDR=\"10.13.37.30\"\nNETMASK=\"255.255.255.0\"\nGATEWAY=\"10.13.37.1\"\n\nDEFROUTE=yes\n\nDNS1=\"10.13.37.1\"\nDNS2=\"10.13.37.2\"\n\nIPV4_FAILURE_FATAL=yes\nIPV6INIT=no\n\nNAME=\"Some Bridged Network\"\n```\n\nCareful with this one, the value for TYPE is case sensitive. Putting 'bridge'\nor 'BRIDGE' will cause the scripts to ignore that this is a bridge and the\ninterface will not come up.\n\nIn my experience adding a VLAN (and it's the same for each one after) adds a\ncouple seconds to the boot process each time the server boots up. If\nuptime/downtime is important and you need servers coming back up as fast as\nthey can a large number of VLANs can significantly delay the process.\n\nWhen creating a guest you'll want to add the following line to the command to\ngive the guest an interface on the appropriate VLAN:\n\n```\n--network=bridge:br20\n```\n\nYou can also define a network in virsh so that it'll show up to utilities like\nvirt-manager by putting the following in an xml file and running `virsh\nnet-define bridge20.xml` assuming you name the file `bridge20.xml`, and `virsh\nnet-autostart LANBridge` which assumes you use the same name that I've chosen\nfor this bridge below:\n\n```xml\n\u003cnetwork\u003e\n  \u003cname\u003eLANBridge\u003c/name\u003e\n  \u003cuuid\u003e5ba71de3-c507-4512-93c3-65fd3a38e112\u003c/uuid\u003e\n  \u003cforward mode=\"bridge\" /\u003e\n  \u003cbridge name=\"br20\" /\u003e\n\u003c/network\u003e\n```\n\n### IPv6 Pass Through\n\nhttps://www.berrange.com/posts/2011/06/16/providing-ipv6-connectivity-to-virtual-guests-with-libvirt-and-kvm/\n\n## Devices\n\n### USB Devices\n\nhttp://david.wragg.org/blog/2009/03/using-usb-pass-through-under-libvirt.html\n\n### PCI Devices\n\n## Creating New Guests\n\nThings you'll need to determine before creating a new native KVM guest:\n\n* How much RAM do you want to allocate to it? (It's a good idea to consider how\n  much RAM is currently used by other guests before determining this, also you\n  should cross reference the amount used with what is configured in CGroups if\n  those are being implemented.)\n* How much disk space do you want to allocate to it? (If using a qcow2 image for\n  the disk image, only what is used within the image will be used on the host's\n  hard drive. Creating a 50GB image with nothing in it will use up 256Kb on the\n  host's drive.)\n* How many cores/CPUs should the guest have access to?\n* What networks should it belong to?\n* What OS will it be running?\n* Will it need a graphical interface or console?\n* What name would you like to refer to the guest by?\n\nI've found that headless linux guests don't need a whole lot of RAM to function\nwell. Depending on the function of the server I usually give my guests between\n128 and 384Mb of RAM.\n\nDisk space will have to be determined on a guest by guest basis, however I do\nlike to the qcow2 image for my disk images for a variety of reasons. Snapshots,\ncompression, encryption, and copy-on-write images are all handy little features\nof the qcow2 image format.\n\nThere is a small performance hit, I've measured that a qcow2 image is about 8%\nslower on reads, and about 12% slower on writes. The added bonuses however more\nthan make up for that for me. You'll have to decide on your own how you'd like\nto handle it.\n\nI have never used more than one core for a guest as they've never needed it,\nI've also heard some things that I've never checked into that some guests\nbecome unstable with more than one core defined. You'll have to figure that out\non your own.\n\nNetworks need to be created before the guests. If using bridging (like I always\ndo) then you need the bridge created and figure out which ones you want\navailable on your guest.\n\nThe OS is less important and can even be omitted, however libvirt has some\ninternal optimizations for certain OS's so if it's available its best to list\nwhat type of OS is going to be run in the guest during the initial creation.\n\nI don't normally need a graphical environment from my guests, preferring to\nadminister from the command line as it is more powerful and less resource\nhungry. In some cases it is unavoidable. You'll notice in the example I give\nthat the creation commands has the following in it:\n\n```\n--graphics none --serial pty --extra-args=\"console=ttyS0 serial\"\n```\n\nIf your going to be using graphics you will want to omit those two options and\nreplace them with:\n\n```\n--graphics vnc\n```\n\nThis will allow you to VNC into the guest (there is no authentication or\nencryption although these can be configured. These are not documented here.)\n\nFinally the name of the guest, this can not contain spaces and should not\ncontain any special characters as they may interfere with issuing commands. I\nlike the names to be short and descriptive referring to either the name of the\nserver or the primary purpose.\n\nThe following command creates a Fedora 16 guest with the name 'example', with 1\nCPU, 512Mb of RAM, no graphics, a network card on the LANBridge network, and a\n20Gb raw sparse hard disk image.\n\n```\nvirt-install --connect qemu:///system \\\n    --name example \\\n    --description=\"This is an example VM intended for the wiki\" \\\n    --ram 512 \\\n    --arch=x86_64 \\\n    --vcpus=1,maxvcpus=2 \\\n    --check-cpu \\\n    --os-type=linux \\\n    --os-variant=fedora16 \\\n    --hvm \\\n    --graphics none \\\n    --serial pty \\\n    --location=\"http://mirror.chpc.utah.edu/pub/fedora/linux/releases/17/Fedora/x86_64/os\" \\\n    --extra-args=\"console=ttyS0 serial ks=http://example.org/kickstarts/ks.cfg\" \\\n    --disk path=/var/lib/libvirt/images/example.img,format=raw,size=20,sparse=true \\\n    --network=network:LANBridge\n```\n\n## LXC\n\n* https://www.berrange.com/posts/2011/09/27/getting-started-with-lxc-using-libvirt/\n\n[1]: {{\u003c ref \"./iptables.md\" \u003e}}\n","created_at":-62135596800,"fuzzy_word_count":2300,"path":"/notes/libvirtd/","published_at":1540945455,"reading_time":11,"tags":null,"title":"libvirtd","type":"notes","updated_at":1540945455,"weight":0,"word_count":2207},{"cid":"7d8c6f9cc4766cb207aff8376afa4b3883552ddc","content":"\n***Note: This page is quite old and is likely out of date. My opinions may have\nalso changed dramatically since this was written. It is here as a reference\nuntil I get around to updating it.***\n\nPLEASE NOTE: This guide was developed for Red Had Based architectures,\nspecifically CentOS 5, and Fedora 16+. A lot of the information here is\ngenerally solid principles but you may need to adapt it to your distribution.\n\nHardening of all my servers is the largest piece IMHO to the [defence in\ndepth][1]. It brings to them each a strength to stand on their own. I want to\nbe able to plug each and everyone on a raw internet connection and feel safe\nknowing that they won't be compromised.\n\nIn the event that any section of my defence fails, I'll know that the rest of\nmy systems are strong enough to hopefully be protected from whatever the weak\nlink was.\n\nThis section of my documentation provides a quick overview of procedures to\nharden a system to make sure I never miss a step.\n\nThis hardening guide started out as a summary and a few updates to the [NSA\nRedHat 5 Operating System Security Guidelings][2]. Since then it has grown to\ninclude a lot of services that were not mentioned anywhere in there, and a few\nbest practices that I've developed myself over the years of managing Linux\nboxes.\n\nI've started including information from [National Vulnerability Database (NVD)\nNational Checklist Program Repository][3] hosted by NIST. They aggregate some\nuseful information from a few sites including the [Center for Internet Security\n(CIS) Security Benchmarks][4]. One more site I've got some information from is\nthe [SANS Information Security Reading Room][5].\n\nThese have all been listed here to give them some credit towards the following\nsecurity guidelines. If you doubt anything I've mentioned here I'd recommend\nyou go look through these sites.\n\n## General Principles\n\n* Encrypt Transmitted Data Whenever Possible\n* Minimize Software to Minimize Vulnerability\n* Run Different Network Services on Separate Systems\n* Configure Security Tools to Improve System Robustness\n* Least Privilege\n\n## Review of the Security Notes\n\nThis section is mostly a TODO for myself. To keep this as up to date and\nrelevant as possible I need to annually review the following information:\n\n* Any pre-bootloader changes include BIOS, EFI, RAID, etc.\n* OS install instructions\n* Applications to be installed, possibly sorted by machine function\n* Hardening guidelines starting with OS and including applications\n* Instructions for validating the configurables\n* Ensure exceptions are documented in the ticket for the device/machine\n* Regularly validate the configuration, this should be automated if possible\n\n## Hardware\n\nHardware and BIOS's change from machine to machine but there are some quick\nbest practices that should be followed. The BIOS should be hardened and\nphysical security should be assessed.\n\n## Installation Notes\n\n* Follow the [Partitioning][8] guidelines.\n* Ensure you set the date correctly so that the remote certificates can have\n  their expiration properly checked\n* Set a sufficiently strong boot loader password\n* Set a sufficiently strong root password\n* Install the absolute minimum required packages (Service specific packages can\n  be installed afterwards)\n* Include the updates repository, so there is never any initial out of date\n  packages\n\n## Firewall\n\nA strong firewall does not just protect against incoming attacks but also\nprevents a machine from connecting to things it shouldn't. In the event of a\npartially compromised system, an egress ruleset can help prevent an attacker\nfrom using the machine as a springboard to launch further attacks on the\ninternal networks.\n\nIPv4 and IPv6 traffic have separate firewalls. I have examples and a decent\nbase config on the [IPTables][9] page.\n\n## Networking\n\nFollow the [Network][11] guide to configure a static address and gateway.\n\nUpdate /etc/resolv.conf to include the proper domain and nameserver. It will\nlook something like this:\n\n```\nsearch internal.example.org\nnameserver 4.2.2.2\nnameserver 8.8.8.8\n```\n\nVerify hostname / domain in `/etc/sysconfig/network`. An example is given\nbelow:\n\n```\nNETWORKING=yes\nHOSTNAME=testing.internal.example.org\n```\n\nEdit `/etc/hosts`. Remove all the IPv6 loopback entries with the exception of\n`localhost6` and all of the IPv4 addresses with the exception of `localhost`.\n`127.0.0.1` should also have entries for the FQDN of this server and just the\nhostname. Other extraneous entries should be removed unless absolutely\nnecessary.\n\n```\n127.0.0.1       testing.internal.example.org testing localhost localhost4\n::1             testing.internal.example.org testing localhost localhost6\n```\n\n## Logging\n\nYou'll want to make sure you configure [RSyslog][12], [Logrotate][13], and\n[Logwatch][14] according to their appropriate guides. Logwatch is not installed\nin the minimum install and should be installed.\n\n### Configure Root's Mail Recipient\n\nAdd the line to following line to `/etc/aliases`, replacing\n`administrator@example.tld` with an administrator's email address. This is\nwhere logs will be emailed.\n\n```\nroot:     administrator@example.tld\n```\n\nAs root run the `newaliases` command which will update the binary database of\nmail recipients.\n\n## Services\n\nReview the list of services running on the machine with the following command:\n\n```\nsystemctl --type=service\n```\n\nPay special attention to any that is listed as \"running\", these are the always\non services that start up on boot. If it says \"exited\" it has started up on\nboot finished whatever part in the boot process it took care of and then\nproperly exited. The latter need to be audited as well, if they're not needed\nthey should not be executing on boot.\n\nWhenever you find a service that shouldn't be started on boot run the following\ncommand replacing \"rpcbind.service\" with the service you would like to disable.\n\n```\nsystemctl disable rpcbind.service\nsystemctl stop rpcbind.service\n```\n\nIf it's particularily pesky about not going away you may need to mask it. This\nwill essentially create a symlink of the service to `/dev/null`, allowing it to\nattempt to execute without actually accomplishing anything.\n\n```\nsystemctl mask rpcbind.service\n```\n\nTo re-enable a service you'll want to execute the following commands:\n\n```\nsystem enable rpcbind.service\nsystem start rpcbind.service\n```\n\nIf you masked the service you'll need to unmask it before re-enabling it.\n\n```\nsystemctl unmask rpcbind.service\n```\n\nYou need to be aware that the mask / unmask command is a stronger version of\n\"disable\", disable just prevents systemd from starting the service on boot.\n\"mask\" links the service file to `/dev/null` preventing manual activation as\nwell as other services attempting to start it as requirements.\n\nIf you mask a critical service that the machine need to boot it will fail and\nyou'll need to fix this by hand in the debug shell.\n\n### All Server Services\n\nOn all of my servers I configure:\n\n* [SSHd][15] (If you follow this guide, you won't be able to SSH into the box\n  until users and groups have been configured)\n* [NTPd][16] or [Chronyd][17] (pick your preference)\n* [RSyslog][12]\n\n## Updates\n\nUpdates regardless of whether KSplice is being used should be done as\nsoon as possible. For the most part there shouldn't be any reason not to update\nautomatically unless you are using untrusted repositories.\n\nThere is a daemon available yum-updatesd in the Fedora repositories which\ncan be configured to notify administrators of new packages and optionally\ninstall them automatically.\n\n### Updating Manually\n\nUpdating the system manually can be performed using the following command:\n\n```\nyum update\n```\n\n## Init Process\n\nEnsure that root logins are only permitted on appropriate terminals in\n`/etc/securetty`. An example used by me is the following:\n\n```\ntty1\ntty2\ntty3\n```\n\nIf I'm going to access the system of the serial console I'd want to add\n\"console\" to that as well.\n\nThe init process by default starts with that pretty plymouth graphical boot\nloader, to change it back to the default linux boot sequence that is actually\nverbose and useful you need to make a few changes to the `/etc/sysconfig/init`\nfile. I use the following:\n\n```\nBOOTUP=verbose\nAUTOSWAP=no\nACTIVE_CONSOLES=/dev/tty[1-3]\nSINGLE=/sbin/sulogin\n```\n\nThe changes actually made reduces the number of active TTYs to 3 from 6,\nrequires a root login to get into single user mode, and disables the pretty\ncolor status display when in text mode during boot up.\n\nThe other change that needs to be made is in the default grub options in\n`/etc/default/grub`. Remove `rhgb` and `quiet` from the `GRUB_CMDLINE_LINUX`\nvariable. Strictly speaking you don't need to remove quiet, however, it allows\nthe kernel to print out it's boot messages which can help debug some lower\nlevel issues and I generally find it useful. After making the change you'll\nneed to run `grub2-mkconfig -o /boot/grub2/grub.cfg` as root.\n\nOne thing that I have noticed is that as of grub2 if a boot password is defined\nyou need to add `--unrestricted` to the menu entry if you want to boot the menu\nentry without a super user password. I've done this in `/etc/grub.d/10_linux`\nby adding ` --unrestricted` to the `CLASS` environment variable around line 29\nand rebuilding the grub config.\n\n## Sudo\n\nSudo is one of those things that I need to go through and properly restrict.\nSince I'm usually the lone administrator or am very close with the few others\non servers and I trust the people given sudo permissions with global root like\npermissions.\n\nThis definitely has larger security implications but here is how to configure\nthat. This grants unlimited sudo privileges to any user in the sudoers group.\nThey still need to authenticate however.\n\nEdit `/etc/sudoers` adding the line:\n\n```\n%sudoers                ALL=(ALL)        ALL\n```\n\n## Users\n\n### Password Requirements\n\nUpdate the minimum user password to 10 characters in `/etc/login.defs`\n\n### Creation\n\nIf local authentication is used, create any users that will need access to the\nsystem using the following command:\n\n```\nuseradd \u003cusername\u003e\npasswd \u003cusername\u003e\n```\n\nIf the user isn't available locally to enter their own password, a hash from\nanother system can be extracted and dropped into place or alternatively a very\nstrong temporary password can be set and provided to the user through secure\nmeans. You can then run the following command to force the user to change their\npassword the next time they login:\n\n```\nchage -d 0 \u003cusername\u003e\n```\n\n### Securing System Users\n\nReview `/etc/passwd` to ensure all non-user accounts have their account shells\nset to `/sbin/nologin` with the exception of `halt`, `shutdown`, `sync` which\nshould be `/sbin/halt`, `/sbin/shutdown`, and `/bin/sync` respectively.\n\n[Mysql][20] creates it's user with `/bin/bash` as the shell which shouldn't be\n(this is just an example).\n\n### System Wide Settings\n\nEdit `/etc/profile`, find `HISTSIZE` replace that line with these couple of\nlines.\n\n```\nreadonly HISTSIZE=50\nexport readonly TMOUT=900\nexport readonly HISTFILE=\"$HOME/.bash_history\"\n```\n\n## Groups\n\n### Creation\n\nCreate the sudoers and sshers group\n\n```\ngroupadd sudoers -g 400\ngroupadd sshers -g 401\n```\n\n### Add Users\n\n```\nuseradd \u003cusername\u003e\npasswd \u003cusername\u003e\n```\n\nAny users that are going to be SSH'ing into the server should be added to the\n`sshers` group. This includes root if necessary (which by default is only\navailable to pubkey based authentication if you follow this guide).\n\n```\nusermod -a -G sshers \u003cusername\u003e\n```\n\nAny users that NEED sudo permission should be added to the `sudoers` group like\nso:\n\n```\nusermod -a -G sudoers \u003cusername\u003e\n```\n\nYou can do both at the same time like so:\n\n```\nusermod -a -G sshers,sudoers \u003cusername\u003e\n```\n\n## System Hardening\n\n* Change the permissions of the rpm binary `/bin/rpm` to `0700`\n* Remove suid privileges on `/bin/mount` and `/bin/umount` using the following\n  command: `chmod a-s /bin/mount /bin/umount`\n* Create `/etc/cron.allow` and `/etc/at.allow` putting `root` in both of them.\n  Set the permissions on the files to `600`.\n* Restrict permissions on all of the cron tasks using the following command:\n  `chmod -R go-rwx /etc/cron.* /etc/crontab`\n* Restrict boot configurations to only be readable / writable by root\n  `chmod 600 /etc/grub.conf`\n* Restrict access to the boot scripts `chmod -R go-rwx /etc/rc.d`\n* Change the owner and permission of the sudo binary to be owned by\n  `root:sudoers` and have permissions `750`\n\nYou'll want to verify that that there are not more SUID or GUID flags set than\nabsolutely necessary. You can find all of the files with these flags set with\nthe following command:\n\n```\n[root@localhost ~]# find / -type f \\( -perm -04000 -o -perm -02000 \\)\n```\n\nThe same goes for unowned files as there should never be any of these:\n\n```\n[root@localhost ~]# find / \\( -nouser -o -nogroup \\) -print\n```\n\nSomething that should be considered is security limits per user. This can not\nonly be used to prevent users from abusing the system but to prevent a user\naccidentally fork bombing a system or causing other kinds of havoc. These\nsettings can be found in `/etc/security/limits.conf`\n\n### Checklist\n\n* Network\n  * If DHCP is used, minimize accepted options `/etc/dhclient.conf`\n    * Accept only IP, Router, DNS\n    * Override domain name\n* Package updating\n  * Install additional packages\n    * aide\n* Disable USB Support\n  * Disable usb-storage kernel module in `/etc/modules.d/blacklist.conf`\n  * If USB devices are completely unecessary, disable USB completely with the\n    `nousb` option in kernel boot options\n* Set `kernel.randomize_va_space` to 2, 1 if that's not possible\n* Disable Rebooting When 'ctrl-alt-del' is Pressed\n  * `/etc/init/control-alt-delete.conf`: -exec /sbin/shutdown -r now\n    \"Control-Alt-Delete pressed\"\n\n### Updating Software\n\n* Repository Configuration\n  * yum install yum-plugin-fastestmirror yum-plugin-security yum-presto \\\n    yum-plugin-changelog yum-plugin-protectbase -y\n  * Configure local repository if available\n  * If using the btrfs filesystem install yum-plugin-fs-snapshot\n* Manual Software Update\n  * Manual verification of installed GPG key with remote resource\n  * Verify gpgcheck is enabled in all yum configuration files\n* Configuring Automatic Updates\n  * Fedora Update \u0026 Security Check Script\n\n### Permission Verification\n\n* Verify Permissions on passwd, shadow, group, and gshadow Files\n* Verify All World-Writeable Directories Have Sticky Bits Set\n* Verify All World Writeable Directories Have Proper Ownership\n* Find Unauthorized World-Writeable Files\n* Find Unauthorized SUID/SGID System Executables\n* Find and Repair Unowned Files\n* Restrict access to init files only to root `/etc/rc.d`\n* Restrict Permissions on Files Used by cron\n\n### Restrict Dangerous Execution Patterns\n\n* Set Daemon umask to `027`\n  * `/etc/init.d/functions`: `umask 022` -\u003e `umask 027`\n* Disable Core Dumps\n  * `/etc/security/limits.conf`: `+*                hard    core            0`\n  * `/etc/sysctl.conf`: `+fs.suid_dumpable = 0`\n* Disable Prelink (if turned on)\n  * `/etc/sysconfig/prelink`: `PRELINKING=yes` -\u003e `PRELINKING=no`\n  * `/usr/sbin/prelink -ua`\n  * `yum remove prelink -y`\n\n### Restrict Password Based Login\n\n* Limit `su` Access to `root` and the `wheel` Group\n  * `chgrp wheel /bin/su \u0026\u0026 chmod o-rx /bin/su`\n* Configure sudo to Improve Auditing of Root Access\n  * Create `sudoers` Group and Allow it to Use sudo\n* Verify Proper Storage and Existence of Password Hashes\n* Verify No Non-root Accounts Have UID 0\n\n### Account Security\n\n* Create a Unique Default Group for Each User\n* Create and Maintain a Group Containing All Human Users\n  * `users` Group\n* Set Password Quality Requirements\n  * Either pam_cracklib or pam_passwdqc\n* Set Lockouts for Failed Password Attempts\n  * pam_tally2\n* Set Password Hashing Algorithm to SHA-512\n  * `/etc/pam.d/system-auth`: Ensure: `password     sufficient    pam_unix.so\n    {sha512}`\n  * `/etc/login.defs`: Ensure: `ENCRYPT_METHOD SHA512`\n  * `/etc/libuser.conf`: Ensure: `crypt_style = sha512`\n* Limit Password Reuse\n  * `/etc/pam.d/system-auth`: `password    sufficient    pam_unix.so` +\n    \u003cexisting options\u003e remember=5\n* Ensure User Home Directories Are Not Group-Writable or World-Readable\n* Ensure User's Hidden Files Are Not World-Writeable\n* Configure User umask Values\n  * `/etc/profile`: `umask 022` -\u003e `umask 027`\n* Use a Centralized Authentication Service\n* Ensure there are no dangerous directories in root's path (realtive or\n  world-writable)\n\n### SELinux\n\n* Ensure SELinux is Properly Enabled\n* Disable and Remove SETroubleshoot if Possible\n* Disable MCS Translation Service (mcstrans) if Possible\n* Configure Restorecon Service (restorecond)\n* Check for Unconfined Daemons\n* Check for Unlabeled Device Files\n* Debug any SELinux Policy Errors\n* Strengthen the Default SELinux Boolean Configuration\n\n### Network/Firewall Configuration\n\n* Ensure System is Not Acting as a Network Sniffer\n* Remove Wireless Hardware if Possible\n* Disable Wireless Through Software Configuration if Possible\n\n### Logging and Auditing\n\n* Configure auditd's Data Retention\n* Enable Auditing for Processes Which Start Prior to the Audit Daemon\n* Configure auditd Rules for Comprehensive Auditing\n* Configure Audit Log aureport To Send Daily Emails\n\n### Misc\n\n* Add Local [Certificate Authority][25]'s Public Key to the Trusted Store\n* Harden sysctl.conf Values\n\n### Services\n\n* Configure SMART Disk Monitoring\n* Disable at \u0026 anacron if Possible\n* Configure an MTA if necessary\n\n[1]: https://en.wikipedia.org/wiki/Defence_in_depth\n[2]: http://www.nsa.gov/ia/_files/os/redhat/NSA_RHEL_5_GUIDE_v4.2.pdf\n[3]: http://web.nvd.nist.gov/view/ncp/repository\n[4]: http://benchmarks.cisecurity.org/en-us/\n[5]: http://www.sans.org/reading_room/\n[8]: {{\u003c ref \"./partitioning.md\" \u003e}}\n[9]: {{\u003c ref \"./iptables.md\" \u003e}}\n[11]: {{\u003c ref \"./network.md\" \u003e}}\n[12]: {{\u003c ref \"./rsyslog.md\" \u003e}}\n[13]: {{\u003c ref \"./logrotate.md\" \u003e}}\n[14]: {{\u003c ref \"./logwatch.md\" \u003e}}\n[15]: {{\u003c ref \"./sshd.md\" \u003e}}\n[16]: {{\u003c ref \"./ntpd.md\" \u003e}}\n[17]: {{\u003c ref \"./chronyd.md\" \u003e}}\n[20]: {{\u003c ref \"./mysql.md\" \u003e}}\n[25]: {{\u003c ref \"./certificate_authority.md\" \u003e}}\n","created_at":-62135596800,"fuzzy_word_count":2500,"path":"/notes/linux-hardening/","published_at":1540945455,"reading_time":12,"tags":null,"title":"Linux Hardening","type":"notes","updated_at":1540945455,"weight":0,"word_count":2450},{"cid":"f4458a7b46ab47f5c42ce05a94e083c5b9d49118","content":"\n***Note: This page is quite old and is likely out of date. My opinions may have\nalso changed dramatically since this was written. It is here as a reference\nuntil I get around to updating it.***\n\n## Keepalived\n\nMy VMs each have three interfaces, eth0 is the 'public' network network, eth1\nis the 'synchronization' network, and eth2 is the 'internal' network. This\nsample configuration will be making use of two directors and two \"real\nservers\".\n\nThe directors have hostnames 'director-01.i.0x378.net' and\n'director-02.i.0x378.net'. 'director-01' will have device/IP pairs of (eth0,\n192.168.122.141), (eth1, 10.10.10.10), and (eth2, 10.0.0.2). 'director-02' will\nhave device/IP pairs of (eth0, 192.168.122.142), (eth1, 10.10.10.11), and\n(eth2, 10.0.0.3).\n\nThere are two virtual IP addresses that will need to be failed over,\n'192.168.122.140' on the 'public' network and '10.0.0.1'  on the 'internal'\nnetwork.\n\nThese need to be failed over simulatenously.\n\nFirst we need to install the packages:\n\n```\nyum install ipvsadm keepalived -y\n```\n\nIf the directors are running within an LXC environment, keepalived won't be\nable to load the kernel module it needs for the services. From the host machine\nyou'll need to run the following command before attempting to start the\nkeepalived services.\n\n```\nmodprobe ip_vs\n```\n\nThis can be done automatically by creating `/etc/rc.d/rc.local` and marking it\nas executable with the following contents.\n\n```\n#!/bin/sh\n\nmodprobe ip_vs\n```\n\nEdit the `/etc/keepalived/keepalived.conf`:\n\n```\nglobal_defs {\n  notification_email_from failover@example.tld\n  notification_email {\n    admin+failover@example.tld\n  }\n\n  router_id LVS_DIRECTOR_01\n\n  smtp_connect_timeout 5\n  smtp_server          127.0.0.1\n}\n\nvrrp_sync_group gateway_group_1 {\n  group {\n    external_network_1\n    internal_network_1\n  }\n\n  smtp_alert\n}\n\nvrrp_instance external_network_1 {\n  interface eth0\n\n  state BACKUP\n\n  advert_int        1\n  garp_master_delay 1\n\n  priority          100\n  virtual_router_id 20\n\n  nopreempt\n\n  unicast_peer {\n    # IP address of the other director's eth0 interface\n    192.168.122.40\n  }\n\n  # You'll want to change this password...\n  authentication {\n    auth_type PASS\n    auth_pass password\n  }\n\n  virtual_ipaddress {\n    192.168.122.140/24 dev eth0\n    #2001:db8:15:7::100/64 dev eth0\n  }\n}\n\nvrrp_instance internal_network_1 {\n  interface eth2\n\n  state BACKUP\n\n  advert_int        1\n  garp_master_delay 1\n\n  priority          100\n  virtual_router_id 21\n\n  nopreempt\n\n  unicast_peer {\n    # IP address of the other director's eth2 interface\n    10.0.0.11\n  }\n\n  # You'll want to change this password...\n  authentication {\n    auth_type PASS\n    auth_pass password\n  }\n\n  virtual_ipaddress {\n    10.0.0.1/24 dev eth2\n    #2001:db8:16:10::100/64 dev eth2\n  }\n}\n```\n\nFor 'director-02' the `router_id` should be set to 'LVS_DIRECTOR_02'. You'll\nalso want to update the unicast peer addresses for the individual interfaces as\nwell. Since we've disabled `preempt` failover and are starting the servers in\nthe BACKUP state, adjusting the priority between the directors doesn't matter.\n\nWe now need to ensure we have the firewall rules in place to allow both LVS\ninstances to communicate their keepalive messages to each other. Add the\nfollowing firewall rules:\n\n```\n-A SERVICES -i eth0 -p vrrp -s 192.168.122.0/24 -j ACCEPT\n-A SERVICES -i eth2 -p vrrp -s 10.0.0.0/24 -j ACCEPT\n-A OUTPUT -o eth0 -p vrrp -d 192.168.122.0/24 -j ACCEPT\n-A OUTPUT -o eth2 -p vrrp -d 10.0.0.0/24 -j ACCEPT\n```\n\nAt this point we can start up keepalived on both directors and test their\nfailover. Run the following two commands on both directors.\n\n```\nsystemctl enable keepalived.service\nsystemctl start keepalived.service\n```\n\nYou can check the IP addresses of each nodes using `ip addr`. One of them will\nhave both of the virtual IP addresses. Shutdown the keepalive daemon on the\nother one and you should see it failover and grab the virtual IP addresses\nwithin a second.\n\n```\nsystemctl stop keepalived.service\nsleep 2\nsystemctl start keepalived.service\n```\n\nAs much as I tried I couldn't get keepalived to make use of the synchronization\ninterface to send it's VRRP packets. At the same time for my uses multicast\nisn't an option which I why I chose unicast synchronization. If you have more\nthan two directors unicast will increase the amount of traffic required for the\nsynchronization.\n\n## Conntrackd Tools\n\nTo allow us to use state based rules we'll also need to synchronize the known\nconnection states between the two machines for the event of failover. There is\na convenient system to do this. Enter `conntrackd`.\n\nFirst we'll need to install the package that has the synchronization daemon.\n\n```\nyum install conntrack-tools -y\n```\n\nWe'll need to throw a configuration into place at\n`/etc/conntrackd/conntrackd.conf`. The following is for director-01:\n\n```\nSync {\n  Mode FTFW {\n    DisableExternalCache Off\n    PurgeTimeout 5\n  }\n\n  Multicast {\n    IPv4_address 225.0.0.50\n    Group 3780\n    IPv4_interface 10.10.10.10\n    Interface eth1\n    SndSocketBuffer 1249280\n    RcvSocketBuffer 1249280\n    Checksum on\n  }\n}\n\nGeneral {\n  Nice -20\n\n  HashSize 32768\n  HashLimit 131072\n\n  Syslog on\n\n  LockFile /var/lock/conntrack.lock\n\n  UNIX {\n    Path /var/run/conntrackd.ctl\n    Backlog 20\n  }\n\n  NetlinkBufferSize 2097152\n  NetlinkBufferSizeMaxGrowth 8388608\n\n  Filter From Userspace {\n    Protocol Accept {\n      #DCCP\n      ICMP\n      IPv6-ICMP\n      #SCTP\n      TCP\n      UDP\n    }\n\n    Address Ignore {\n      # Loopback addresses\n      IPv4_address 127.0.0.1\n      IPv6_address ::1\n\n      # The director IP addresses\n      IPv4_address 10.0.0.10\n      IPv4_address 10.0.0.11\n      IPv4_address 192.168.122.40\n      IPv4_address 192.168.122.41\n\n      # The keepalive network\n      IPv4_address 10.10.10.0/24\n\n      # The virtual IP addresses\n      IPv4_address 192.168.122.140\n      IPv4_address 10.0.0.5\n    }\n  }\n}\n```\n\nThe only change for director-02 is to change the `IPv4_interface` to\n`10.10.10.11`.\n\nYou'll notice that I am using multicast here. There is an option in conntrackd\nto use unicast as well, and I'll need to come back and reconfigure it to make\nuse of it. For now though this should work in my environment.\n\nAdd the firewall rules needed for the synchronization process on both machines:\n\n```\n-A INPUT -i eth1 -m udp -p udp -s 10.10.10.0/24 -d 225.0.0.50 --dport 3780 -j ACCEPT\n-A OUTPUT -o eth1 -m udp -p udp -d 225.0.0.50 --dport 3780 -j ACCEPT\n```\n\nFor most services I would also include connection tracking state information,\nbut given the nature of this service I decided against it.\n\nEnable and start the service:\n\n```\nsystemctl enable conntrackd.service\nsystemctl start conntrackd.service\n```\n\nThere is some work that conntrackd will need to do whenever keepalive changes\nit's cluster state. A script is provided with the `conntrack-tools` package, we\ncan just copy it into the appropriate place with the following command:\n\n```\ncp /usr/share/doc/conntrack-tools-1.4.2/doc/sync/primary-backup.sh /etc/conntrackd/\n```\n\nNow we need to tell keepalived to call the script when it's state changes, if\nyou're following along with this page you'll need to add the following to the\nsection named `vrrp_sync_group gateway_group_1`.\n\n```\nnotify_backup \"/etc/conntrackd/primary-backup.sh backup\"\nnotify_fault  \"/etc/conntrackd/primary-backup.sh fault\"\nnotify_master \"/etc/conntrackd/primary-backup.sh primary\"\n```\n\nMake sure you restart keepalived on both machines.\n\nAt this point we have everything in place we need to perform our two target\ntasks. Having a gateway on the local network that can failover in the event the\nother machine dies, and provide highly availability to multiple services behind\nthem.\n","created_at":-62135596800,"fuzzy_word_count":1100,"path":"/notes/linux-virtual-servers/","published_at":1507586459,"reading_time":5,"tags":null,"title":"Linux Virtual Servers","type":"notes","updated_at":1507586459,"weight":0,"word_count":1024},{"cid":"14feb94f8fa058003ea5f59f2777d53cfbb83190","content":"\n***Note: This page is quite old and is likely out of date. My opinions may have\nalso changed dramatically since this was written. It is here as a reference\nuntil I get around to updating it.***\n\nUseful for monitoring system statistics such as CPU core temperatures. After\ninstalling the `lm_sensors` packages you can run the `sensors-detect` command\nthat will walk you through loading any kernel modules that may be necessary to\nread sensor data. Executing the `sensors` command will output the results of\nall of that nice information :)\n","created_at":-62135596800,"fuzzy_word_count":100,"path":"/notes/lm-sensors/","published_at":1507586459,"reading_time":1,"tags":null,"title":"LM Sensors","type":"notes","updated_at":1507586459,"weight":0,"word_count":89},{"cid":"39607a7ba31be3cd8873bcdb07150802f1962663","content":"\n***Note: This page is quite old and is likely out of date. My opinions may have\nalso changed dramatically since this was written. It is here as a reference\nuntil I get around to updating it.***\n\nLogrotate is a pretty simple and straight forward program. It's generally run\nas a nightly cron job testing the various configured file to see if they match\ntheir respective criteria for rotation.\n\nAfter installing logrotate defaults are configured that are generally 'good\nenough' for most systems and generally include the default system logs in\n`/var/log`.\n\nOther services can install their own logrotation configurations without\nstomping on other packages by placing their configuration in `/etc/logrotate.d`\nsuch as `yum` and [`nginx`][1]. Defaults for configurations are configured in\n`/etc/logrotate.conf`. I generally leave them alone and override them as needed\nfor specific log files. Where appropriate you'll find the rotation\nconfigurations I use on the relevant service pages.\n\n[1]: {{\u003c ref \"./nginx.md\" \u003e}}\n","created_at":-62135596800,"fuzzy_word_count":200,"path":"/notes/logrotate/","published_at":1540945455,"reading_time":1,"tags":null,"title":"Logrotate","type":"notes","updated_at":1540945455,"weight":0,"word_count":150},{"cid":"52ea616816d78cad990f084e432980de8f5dcfd1","content":"\n***Note: This page is quite old and is likely out of date. My opinions may have\nalso changed dramatically since this was written. It is here as a reference\nuntil I get around to updating it.***\n\n## Configuration\n\n### /etc/logwatch/conf/logwatch.conf\n\n```\nLogDir = /var/log\nTmpDir = /var/cache/logwatch\nOutput = stdout\nFormat = text\nEncode = none\n\nMailTo = administrator@example.net\nMailFrom = root@example.net\n\nArchives = No\nRange = yesterday\n\nDetail = Med\n\nService = All\n\nmailer = \"/usr/sbin/sendmail -t\"\nHostLimit = Yes\n```\n","created_at":-62135596800,"fuzzy_word_count":100,"path":"/notes/logwatch/","published_at":1507586090,"reading_time":1,"tags":null,"title":"Logwatch","type":"notes","updated_at":1507586090,"weight":0,"word_count":76},{"cid":"f34ba7638fddc623b22904178419cb65eff57c52","content":"\n***Note: This page is quite old and is likely out of date. My opinions may have\nalso changed dramatically since this was written. It is here as a reference\nuntil I get around to updating it.***\n\n[GNU Mailman][1] is a computer software application from the GNU project for\nmanaging electronic mailing lists.\n\nMailman is coded primarily in Python and currently maintained by Barry Warsaw.\nMailman is free software, distributed under the GNU General Public License.\n\nWhile I currently don't have a Mailman installation installed in the\nBedroomProgrammers.net, I have had to configure it on several occasions so\nkeeping my config guide handy in my documentation system could help me in the\nfuture.\n\n## Security Notes\n\nMailman in it's lowest form is a mail relay. Improperly configured, this means\nthat it could be used as a spam relay. This is not something that can be solved\nthrough crafty firewall trickery so care needs to be taken when configuring the\nservice.\n\n## Firewall Adjustments\n\nMailman needs to be able to both send and receive mail. These jobs are handled\nby the system's MTA, the only two of which I use are [Postfix][2] and\n[Sendmail][3] depending on the system. If I ever get around to it I'll also get\naround to writing a config guide for Exim. Refer to those for proper\nhardening and firewalling.\n\nThe other half of Mailman is the administrative interfaces. This is done\nthrough a web site hosted on an [Apache][5] installation. Please refer to that\nguide for firewall and configuration on that service.\n\n[1]: http://www.list.org/\n[2]: {{\u003c ref \"./postfix.md\" \u003e}}\n[3]: {{\u003c ref \"./sendmail.md\" \u003e}}\n[5]: {{\u003c ref \"./httpd.md\" \u003e}}\n","created_at":-62135596800,"fuzzy_word_count":300,"path":"/notes/mailman/","published_at":1540945455,"reading_time":2,"tags":null,"title":"Mailman","type":"notes","updated_at":1540945455,"weight":0,"word_count":251},{"cid":"e351bd0e9199d2b6815d4800f93a04c821241095","content":"\n***Note: This page is quite old and is likely out of date. My opinions may have\nalso changed dramatically since this was written. It is here as a reference\nuntil I get around to updating it.***\n\n## Installation\n\n```\nyum install memcached -y\n```\n\n## Configuration\n\nMemcached doesn't have a whole lot of configuration options. There is the user\nit's running under, what port it should listen on, what IP address it should\nbind too and how large it's cache is.\n\nThe default port is 11211 (TCP \u0026 UDP), and by default it will bind to\n`INADDR_ANY` (0.0.0.0 and ::). Multiple addresses CAN be specified to bind too\nwith the `-l` flag. By default memcached will use 64Mb of memory.\n\nThese options can be set in `/etc/sysconfig/memcached`. Here is mine which\nrestricts where it's bound too. For my general use 64Mb of storage is more than\nenough (roughly the equivalent of 17.5k pages of plain text and these are\nsimple key value text pairs).\n\n```\nPORT=\"11211\"\nUSER=\"memcached\"\nMAXCONN=\"1024\"\nCACHESIZE=\"64\"\nOPTIONS=\"-l 10.100.0.14\"\n```\n\n## Firewall\n\nBy default memcached listens on both UDP and TCP port `11211`\n","created_at":-62135596800,"fuzzy_word_count":200,"path":"/notes/memcached/","published_at":1507586459,"reading_time":1,"tags":null,"title":"Memcached","type":"notes","updated_at":1507586459,"weight":0,"word_count":173},{"cid":"303f69db28b4b20797f426a2b39127e6a9713515","content":"\n***Note: This page is quite old and is likely out of date. My opinions may have\nalso changed dramatically since this was written. It is here as a reference\nuntil I get around to updating it.***\n\n## Installation\n\n```\nyum install motion -y\n```\n\nMake sure all your camera's are on and connected.\n\n## Configuration\n\n### SELinux Woes\n\nSELinux by default block motion's access to it's logfile `/var/log/motion.log`,\nto allow this write the following out to a file `motion_log_file_access.log`:\n\n```\ntype=AVC msg=audit(1361649114.076:677): avc:  denied  { open } for  pid=25482 comm=\"motion\" path=\"/var/log/motion.log\" dev=\"dm-1\" ino=6293030 scontext=system_u:system_r:zoneminder_t:s0 tcontext=unconfined_u:object_r:var_log_t:s0 tclass=filetype=SYSCALL msg=audit(1361649114.076:677): arch=c000003e syscall=2 success=no exit=-13 a0=1dc3160 a1=441 a2=1b6 a3=238 items=0 ppid=25481 pid=25482 auid=4294967295 uid=0 gid=0 euid=0 suid=0 fsuid=0 egid=0 sgid=0 fsgid=0 ses=4294967295 tty=(none) comm=\"motion\" exe=\"/usr/bin/motion\" subj=system_u:system_r:zoneminder_t:s0 key=(null)\ntype=SERVICE_START msg=audit(1361649114.103:678): pid=1 uid=0 auid=4294967295 ses=4294967295 subj=system_u:system_r:init_t:s0 msg=' comm=\"motion\" exe=\"/usr/lib/systemd/systemd\" hostname=? addr=? terminal=? res=failed'\n```\n\nRun the following commands to generate and install the new policy:\n\n```\ncat motion_log_file_access.log | audit2allow -M motion_log_file_access\nsemodule -i motion_log_file_access.pp\n```\n\nIt can also help to restore the SELinux contexts on the entire `/var/motion`\ndirectory like so:\n\n```\nrestorecon -R /var/motion\n```\n\n### Disable Auto-Focus on a Camera\n\nI found the LifeCam was regularly refocusing itself much to my annoyance... It\nshows up as `/dev/video2` for me so this is how I turned it off:\n\n```\nuvcdynctrl -d /dev/video2 -s 'Focus, Auto' 0\n```\n","created_at":-62135596800,"fuzzy_word_count":300,"path":"/notes/motion/","published_at":1507586459,"reading_time":1,"tags":null,"title":"Motion","type":"notes","updated_at":1507586459,"weight":0,"word_count":201},{"cid":"81794274487c9f956bb7fca5e97598bdc3bcaae7","content":"\n***Note: This page is quite old and is likely out of date. My opinions may have\nalso changed dramatically since this was written. It is here as a reference\nuntil I get around to updating it.***\n\n## Server\n\n### Installation\n\nInstall the package.\n\n```\nyum install mysql-server -y\n```\n\nSet to start up auto-magically and get it going now.\n\n```\nsystemctl enable mysqld.service\nsystemctl start mysqld.service\n```\n\nRun the quick and dirty secure installation script that comes with the server.\nThe questions are pretty straight-forward just set a secure password when it\nasks for it.\n\n```\nmysql_secure_installation\n```\n\n### Configuration\n\nBefore applying the following configuration file in `/etc/my.cnf`. Safely\nshutdown the server and delete the logfiles `/var/lib/mysql/ib_logfile*`. This\nconfiguration file will change the size of the log buffer and it will cause an\nerror when starting the server backup that will look like:\n\n```\nInnoDB: Error: log file ./ib_logfile0 is of a different size 0 5242880 bytes\nInnoDB: than specified in the .cnf file 0 67108864 bytes!\n```\n\nThere is still one major security feature that I haven't added to this\nconfiguration yet. Running the mysql daemon in a chroot environment. Details on\nsetting up a chroot environment for mysql can be found, [here][1], and\n[here][2].\n\nDetailed information on all of the configuration options can be found\n[here][3].  There is a script that can assist in tuning larger production\ndatabases I've stored [here][4].\n\n```ini\n[mysqld]\n# Bind only to expected address. You can not bind to multiple addresses, it's\n# none, 1 or all (0.0.0.0)\nbind-address = 127.0.0.1\n\n# Always use UTF-8\ncharacter-set-server = utf8\n\n# Root path used to store MySQL's data\ndatadir = /var/lib/mysql\n\n# Use innodb by default (true anyways but I like to be explicit)\ndefault-storage-engine = innodb\n\n# 0 = Don't log warnings, 1 = log warnings, 2 = log warnings and access denied\n# errors.\nlog-warnings = 2\n\n# How many real world seconds pass before we consider a query to be long\n# running. Default is 10.\nlong_query_time = 5\n\n# Which port to listen on, default is 3306\nport = 3306\n\n# A user needs to have the INSERT privilege on the mysql.user table to use the\n# GRANT statement. This can be provided to non-root users with the following\n# command:\n# GRANT INSERT(user) ON mysql.user TO 'user_name'@'host_name';\nsafe_user_create\n\n# Prevent client connections from clients attempting to use insecure password\n# authentication\nsecure_auth\n\n# Only allow load data and outfile to be to/from the given directory. This\n# ensures that whatever user is trying to load or dump data from this server\n# instance already needs to be able to write to that directory. If an\n# unauthorized person can write to the mysql data directory anyways the data\n# can't be trusted anyway.\nsecure-file-priv = /var/lib/mysql\n\n# Require privileges to actually list/enumerate the databases\nskip-show-database\n\n# Prevent symbolic links as a security measureskip-symbolic-links\n# Log any \"slow\" queries from hosts, useful as a diagnostic measure for both\n# application development and production diagnostics. A slow query is defined by\n# the long_query_time variable.\n#\n# These logs will live in /var/lib/mysql/ and will be pre-pended with the\n# hostname that caused the slow query itself.\nslow_query_log\n\n# Path to the socket file used to access the server locally\nsocket = /var/lib/mysql/mysql.sock\n\n##### SSL Settings #####\n\n# All three options are required to enable SSL for the server. To force a user\n# to use an SSL connection when they are connecting to the server, while granting\n# them privileges add \"REQUIRE SSL\" to the end of the grant statement. \"REQUIRE\n# X509\" will require the user to present a valid certificate (it's unclear whether\n# this needs to be signed by a certificate in the server's CA file), \"REQUIRE\n# ISSUER\" and \"REQUIRE SUBJECT\" specify details required in the provided\n# certificate. The last three imply \"REQUIRE SSL\".\n\n# Define the certificate authority file, it should have been used to sign the\n# server's certificate, and will be used to validate client certificates that\n# have been presented.\n#ssl_ca = ca.crt\n\n# The name of the SSL certficate file in PEM format to use for establishing a\n# secure connection.\n#ssl_cert = mysql.crt\n\n# The name of the SSL key file in PEM format to use for establishing a secure\n# connection\n#ssl_key = mysql.key\n\n##### InnoDB specific settings #####\n\n# If there are lots of tables in the database you may need to increase this\n# value from it's default of 8Mb. MySQLd will log warning messages if this is\n# the case.\ninnodb_additional_mem_pool_size = 8M\n\n# Amount of data to cache, default is 8Mb, shouldn't ever be more than 80% of\n# server's physical memory size even on a dedicated box. Even that high may\n# cause other issues on the server such as competition from the OS, and the\n# space must be contiguous which isn't always available. This is only the table\n# and data cache, MySQLd may allocate additional memory for buffers and control\n# structures. On dedicated MySQL boxes it is recommended that this be between\n# 75% and 80%\ninnodb_buffer_pool_size = 32M\n\n# Validate the checksums of all pages read from disk to ensure fault tolerance\n# against broken hardware or data files. Enabled by default but I like to be\n# explicit...\ninnodb_checksums = 1\n\n# Breaks the database into smaller more managable pieces, and if careful can\n# allow for individual table backups and restoration while the server is\n# running. There is a special process to handle this and it should looked up\n# before attempting. The main benefit of using this option is the prevention\n# of main tablespace growth.\ninnodb_file_per_table\n\n# 0 = A most one second of transactions are lost in the event of an application\n# crash, 1 = ACID compliant and most reliable storage of data, 2 is somewhere in\n# between 0 and 1 losing at most 1 second of data only in the case of power\n# outage or server failure. 1 is the default. If you're not worried about losing\n# transactions for the last second or two a value of 2 can have a dramatic increase\n# especially when there are a lot of short write transactions.\ninnodb_flush_log_at_trx_commit = 2\n\n# An upper limit on the IO activity performed by InnoDB background tasks.\n# Default value is 200 with a minimum value of 100. This parameter should be set\n# approximately what the IOPS of the system disk the tables are stored on is\n# able to provide (100 for 5.2k \u0026 7.2k drives, 150 for 10k, 200 for 15k and\n# 250 for SSDs). It is recommended to 'keep this setting as low as practical, but\n# not so low that these background activities fall behind.' Too high and data will\n# be removed from the buffer pool too quickly to benefit from the caching.\ninnodb_io_capacity = 100\n\n# This will prevent too much log switching on write heavy databases (default is\n# 8Mb). Larger buffers all larger transactions to run without the need to write\n# the log to disk before the transactions commit. On larger databases this should\n# be set higher to something like 256Mb, there isn't a large benefit above 512Mb\ninnodb_log_file_size = 64M\n\n# The size of the buffer that InnoDB uses to write to the log files on disk.\n# Default is 8Mb. A larger log buffer allows larger transactions to run without\n# a need to write the log to disk before the transactions commit.\ninnodb_log_buffer_size = 8M\n\n# The size of each transaction log (default is 5Mb). The larger the value the\n# less checkpoint flush activity is needed in the buffer pool saving disk IO at\n# the expense of crash recovery time. Recommended is 1/nth of the\n# innodb_buffer_pool_size where N is the value of innodb_log_files_in_group. In\n# this file that's 1/2 of 32Mb.\ninnodb_log_file_size = 16M\n\n# The number of transaction logs to make use of, the default, recommended and\n# minimum is 2. These will written to in a circular fashion.\ninnodb_log_files_in_group = 2\n\n[mysqld_safe]\n\n# Where to store the MySQLd process ID\npid-file = /var/run/mysqld/mysqld.pid\n\n# Enable logging to syslog\nsyslog\n\n[mysql]\n\n# Adjust the prompt to be more useful/verbose.\nprompt=(\\\\u@\\\\h) [\\\\d]\u003e\\\\_\n```\n\nThere is some additional configuration in the systemd init script, specifically\nwhich user/group the service will fork too (default is mysql/mysql). If you\nneed to change this you'll need to make the adjustment in\n`/etc/systemd/system/multi-user.target.wants/mysqld.service`.\n\n### Firewall\n\n```\n-A SERVICES -m tcp -p tcp --dport 3306 -j ACCEPT\n```\n\n### Resetting the Root Password\n\nStop the MySQL daemon, and disable remote access to the server (block in\nfirewall etc). Start up MySQL with privilege checking disabled (In this mode\nany credential will be accepted for any username and that user will have full\nroot database privileges (thus blocking remote access).\n\n```\nmysqld_safe --skip-grant-tables\n```\n\nIn a separate terminal window connect to the database:\n\n```\nmysql -u root\n```\n\nRefer to the section on changing a user's password at this point. When done be\nsure to kill the `mysqld_safe` instance, start up the daemon normally and\nrestore access to the server.\n\n### Quick Self Signed SSL Cert w/ Client\n\nThis should be used for testing and development only, it requires a bit more\nthan a simple SSL cert as MySQL requires a CA cert as well so we'll generate a\nself signed CA, a server certificate, and then a client certificate that can be\nused for user authentication.\n\nFirst the CA:\n\n```\nopenssl req -new -x509 -newkey rsa:4096 -keyout ca.key -nodes -days 365 \\\n  -out ca.crt\n```\n\nThen the server cert:\n\n```\nopenssl req -newkey rsa:4096 -keyout server.key -nodes -days 365 \\\n  -out server.csr\nopenssl x509 -req -in server.csr -days 365 -CA ca.crt -CAkey ca.key \\\n  -set_serial 01 -out server.crt\nrm server.csr\n```\n\nThen the client cert:\n\n```\nopenssl req -newkey rsa:4096 -keyout client.key -nodes -days 365 \\\n  -out client.csr\nopenssl x509 -req -in client.csr -days 365 -CA ca.crt -CAkey ca.key \\\n  -set_serial 02 -out client.crt\nrm client.csr\n```\n\n## Client\n\n### Remotely Calculate the Size of a Database\n\nPaste the following query into an interactive console session to collect the\nsizes of all databases the current user has access to:\n\n```\nSELECT table_schema \"Database Name\", SUM(data_length + index_length) / 1024\n  / 1024 \"Size in Mb\" FROM information_schema.TABLES GROUP BY table_schema;\n```\n\n### Drop Tables\n\nWithout being able to drop a database and create a new one it can be kind of\nfrustrating to empty a database of all it's tables so I've written a quick\nscript to drop all the tables from a specified database.\n\n```\n#!/bin/bash\n\nDBUSER=\"$1\"\nDBPASS=\"$2\"\nDBNAME=\"$3\"\nDBHOST=\"$4\"\n\nif [ $# -ne 4 ]; then\n  echo \"Usage: $0 {username} {password} {database} {hostname}\"\n  echo \"Drops all tables from a MySQL\"\n  exit 1\nfi\n\nTABLES=$(mysql -u $DBUSER -p$DBPASS -h$DBHOST $DBNAME -e 'show tables;' |\n  awk '{ print $1}' | grep -v '^Tables' )\n\nfor t in $TABLES; do\n  echo \"Deleting $t table from $MDB database...\"\n  mysql -u $DBUSER -p$DBPASS -h$DBHOST $DBNAME -e \"drop table $t;\"\ndone\n```\n\n### New Database and User\n\nNote: For security reasons user creation should be performed over an encrypted\nchannel. See the section on Using SSL.\n\n```\nCREATE DATABASE example;\nGRANT ALL ON example.* TO 'some-example-user'@'%.home-network.net' IDENTIFIED\n  BY 'mYsUperSt0ngpassWordD0ntcoPyM3!';\nFLUSH PRIVILEGES;\n```\n\n### Change User Password\n\nNote: For security reasons this password update should be performed over an\nencrypted channel. See the section on Using SSL.\n\n```\nUPDATE mysql.user SET password=PASSWORD(\"my-new-super-secure-password\") WHERE\n  user='some-user-name';\n```\n\n### Configuration\n\nQuick and simple change I make to my client configuration to make the prompt a\nlittle more verbose. Create the following file in `~/.my.cnf`:\n\n```\n[mysql]prompt=(\\\\u@\\\\h) [\\\\d]\u003e\\\\_\n```\n\nThis can also be set system wide in `/etc/my.cnf` for all users by appending\nthe above (which I do in all my server configurations).\n\n### Using SSL\n\nPretty straight forward, one off can be done like the following:\n\n```\nmysql -u user -ppassword -h host --ssl-ca=./ca.crt database\n```\n\nOnce connected you can verify the session is encrypted with the following\nquery:\n\n```\n(root@127.0.0.1) [(none)]\u003e show status like 'ssl_cipher';\n```\n\nIf the second column returned is non-empty then your session is encrypted.\n\n### See Remote Version\n\n```\n(user@example-mysql) [(none)]\u003e SELECT VERSION();\n```\n\n### Useful Diagnostic / Testing / Maintenance Scripts\n\n* http://www.percona.com/doc/percona-toolkit/2.1/\n\n### TCMalloc\n\nDue to high thread contention in some versions of MySQL a huge performance\nboost can be gained by using a third party malloc, TCMalloc has been proven to\nreduce thread contention in MySQL and provide as much as a 30% boost in all\nquery timing.\n\n* http://goog-perftools.sourceforge.net/doc/tcmalloc.html\n* https://code.google.com/p/gperftools/\n\nMORE TODO\n\n[1]: http://www.symantec.com/connect/articles/securing-mysql-step-step\n[2]: http://www.webhostingskills.com/articles/mysql_in_a_chrooted_environment\n[3]: http://dev.mysql.com/doc/refman/5.5/en/dynamic-system-variables.html\n[4]: {{\u003c ref \"./mysql_tuning.md\" \u003e}}\n","created_at":-62135596800,"fuzzy_word_count":2200,"path":"/notes/mysql/","published_at":1540945455,"reading_time":11,"tags":null,"title":"MySQL","type":"notes","updated_at":1540945455,"weight":0,"word_count":2179},{"cid":"eb3cdd70011412a854079e874ec2b9123b7de2f3","content":"\n***Note: This page is quite old and is likely out of date. My opinions may have\nalso changed dramatically since this was written. It is here as a reference\nuntil I get around to updating it.***\n\n```sh\n#!/bin/sh\n\n# MySQL performance tuning primer script\n#\n# Writen by:      Matthew Montgomery\n# Modified by:    Sam Stelfox\n# Report bugs to: https://bugs.launchpad.net/mysql-tuning-primer\n# Inspired by:    MySQLARd (http://gert.sos.be/demo/mysqlar/)\n# Version:        1.6-r1\n# Released:       2011-08-06\n# Updated:        2013-05-23\n#\n# Licenced under GPLv2.\n#\n# Usage: ./tuning-primer.sh [mode]\n#\n# Available Modes:\n#   all:         perform all checks (default)\n#   prompt:      prompt for login credintials and socket and execution mode.\n#   mem, memory: run checks for tunable options which effect memory usage.\n#   disk, file:  run checks for options which effect i/o performance or file\n#                handle limits\n#   innodb:      run InnoDB checks, to be improved\n#   misc:        run checks for that don't categorise well slow queries, binary\n#                logs, used connections and worker threads.\n\n# Set this socket variable ONLY if you have multiple instances running or we\n# are unable to find your socket, and you don't want to to be prompted for\n# input each time you run this script.\nsocket=\n\n# Nice and easy names for the various colors\nexport black='\\033[0m'\nexport boldblack='\\033[1;0m'\nexport red='\\033[31m'\nexport boldred='\\033[1;31m'\nexport green='\\033[32m'\nexport boldgreen='\\033[1;32m'\nexport yellow='\\033[33m'\nexport boldyellow='\\033[1;33m'\nexport blue='\\033[34m'\nexport boldblue='\\033[1;34m'\nexport magenta='\\033[35m'\nexport boldmagenta='\\033[1;35m'\nexport cyan='\\033[36m'\nexport boldcyan='\\033[1;36m'\nexport white='\\033[37m'\nexport boldwhite='\\033[1;37m'\n\n# Function to easliy print colored text\n#   Argument $1 = message\n#   Argument $2 = color\ncecho() {\n  local default_msg=\"No message passed.\"\n  message=${1:-$default_msg}\n\n  # Change it for fun, we use pure names. Defaults to black, if not specified.\n  color=${2:-black}\n\n  case $color in\n    black)\n      printf \"$black\";;\n    boldblack)\n      printf \"$boldblack\";;\n    red)\n      printf \"$red\";;\n    boldred)\n      printf \"$boldred\";;\n    green)\n      printf \"$green\";;\n    boldgreen)\n      printf \"$boldgreen\";;\n    yellow)\n      printf \"$yellow\";;\n    boldyellow)\n      printf \"$boldyellow\";;\n    blue)\n      printf \"$blue\";;\n    boldblue)\n      printf \"$boldblue\";;\n    magenta)\n      printf \"$magenta\";;\n    boldmagenta)\n      printf \"$boldmagenta\";;\n    cyan)\n      printf \"$cyan\";;\n    boldcyan)\n      printf \"$boldcyan\";;\n    white)\n      printf \"$white\";;\n    boldwhite)\n      printf \"$boldwhite\";;\n  esac\n\n  printf \"%s\" \"$message\"\n  tput sgr0 # Reset all the colors back to normal.\n  printf \"$black\"\n\n  return\n}\n\n# Function to easliy print colored text without a newline\n#   Argument $1 = message\n#   Argument $2 = color\ncechon() {\n  local default_msg=\"No message passed.\"\n  message=${1:-$default_msg}\n\n  color=${2:-black}\n\n  case $color in\n    black)\n      printf \"$black\";;\n    boldblack)\n      printf \"$boldblack\";;\n    red)\n      printf \"$red\";;\n    boldred)\n      printf \"$boldred\";;\n    green)\n      printf \"$green\";;\n    boldgreen)\n      printf \"$boldgreen\";;\n    yellow)\n      printf \"$yellow\";;\n    boldyellow)\n      printf \"$boldyellow\";;\n    blue)\n      printf \"$blue\";;\n    boldblue)\n      printf \"$boldblue\";;\n    magenta)\n      printf \"$magenta\";;\n    boldmagenta)\n      printf \"$boldmagenta\";;\n    cyan)\n      printf \"$cyan\";;\n    boldcyan)\n      printf \"$boldcyan\";;\n    white)\n      printf \"$white\";;\n    boldwhite)\n      printf \"$boldwhite\";;\n  esac\n\n  printf \"%s\" \"$message\"\n  tput sgr0 # Reset all the colors back to normal.\n  printf \"$black\"\n\n  return\n}\n\n# Banner\nprint_banner() {\n  cecho \" -- MYSQL PERFORMANCE TUNING PRIMER --\" boldblue\n  cecho \"      - By: Matthew Montgomery -\" black\n  cecho \"   - Modifications By: Sam Stelfox -\" black\n}\n\n# Find the location of the mysql.sock file\ncheck_for_socket() {\n  if [ -z \"$socket\" ]; then\n    # Use ~/my.cnf version\n    if [ -f ~/.my.cnf ]; then\n      cnf_socket=$(grep ^socket ~/.my.cnf | awk -F \\= '{ print $2 }' | head -1)\n    fi\n\n    if [ -S \"$cnf_socket\" ]; then\n      socket=$cnf_socket\n    elif [ -S /var/lib/mysql/mysql.sock ]; then\n      socket=/var/lib/mysql/mysql.sock\n    elif [ -S /var/run/mysqld/mysqld.sock ]; then\n      socket=/var/run/mysqld/mysqld.sock\n    elif [ -S /tmp/mysql.sock ]; then\n      socket=/tmp/mysql.sock\n    else\n      if [ -S \"$ps_socket\" ]; then\n        socket=$ps_socket\n      fi\n    fi\n  fi\n\n  if [ -S \"$socket\" ]; then\n    echo UP \u003e /dev/null\n  else\n    cecho 'No valid socket file \"$socket\" found!' boldred\n    cecho 'The mysqld process is not running or it is installed in a custom location.' red\n    cecho 'If you are sure mysqld is running, execute script in \"prompt\" mode or set' red\n    cecho 'the socket= variable at the top of this script' red\n    exit 1\n  fi\n}\n\n# Check for the existance of plesk and login using it's credentials\ncheck_for_plesk_passwords() {\n  if [ -f /etc/psa/.psa.shadow ]; then\n    mysql=\"mysql -S $socket -u admin -p$(cat /etc/psa/.psa.shadow)\"\n    mysqladmin=\"mysqladmin -S $socket -u admin -p$(cat /etc/psa/.psa.shadow)\"\n  else\n    mysql=\"mysql\"\n    mysqladmin=\"mysqladmin\"\n  fi\n}\n\n# Test for running mysql\ncheck_mysql_login() {\n  is_up=$($mysqladmin ping 2\u003e\u00261)\n\n  if [ \"$is_up\" = \"mysqld is alive\" ]; then\n    echo UP \u003e /dev/null\n  elif [ \"$is_up\" != \"mysqld is alive\" ]; then\n    printf \"\\n\"\n    cecho \"Using login values from ~/.my.cnf\"\n    cecho \"- INITIAL LOGIN ATTEMPT FAILED -\" boldred\n\n    if [ -z $prompted ]; then\n      find_webmin_passwords\n    else\n      return 1\n    fi\n  else\n    cecho \"Unknow exit status\" red\n    exit -1\n  fi\n}\n\nfinal_login_attempt() {\n  is_up=$($mysqladmin ping 2\u003e\u00261)\n\n  if [ \"$is_up\" = \"mysqld is alive\" ]; then\n    echo UP \u003e /dev/null\n  elif [ \"$is_up\" != \"mysqld is alive\" ]; then\n    cecho \"- FINAL LOGIN ATTEMPT FAILED -\" boldred\n    cecho \"Unable to log into socket: $socket\" boldred\n\n    exit 1\n  fi\n}\n\n# Create a ~/.my.cnf and exit when all else fails\nsecond_login_failed() {\n  cecho \"Could not auto detect login info!\"\n  cecho \"Found potential sockets: $found_socks\"\n  cecho \"Using: $socket\" red\n\n  read -p \"Would you like to provide a different socket?: [y/N] \" REPLY\n\n  case $REPLY in\n    yes|y|Y|YES)\n      read -p \"Socket: \" socket ;;\n  esac\n\n  read -p \"Do you have your login handy?: [y/N] \" REPLY\n\n  case $REPLY in\n    yes|y|Y|YES)\n      answer1='yes'\n      read -p \"User: \" user\n      read -rp \"Password: \" pass\n\n      if [ -z $pass ] ; then\n        export mysql=\"$mysql -S$socket -u$user\"\n        export mysqladmin=\"$mysqladmin -S$socket -u$user\"\n      else\n        export mysql=\"$mysql -S$socket -u$user -p$pass\"\n        export mysqladmin=\"$mysqladmin -S$socket -u$user -p$pass\"\n      fi\n      ;;\n    *)\n      cecho \"Please create a valid login to MySQL\"\n      cecho \"Or, set correct values for  'user=' and 'password=' in ~/.my.cnf\"\n      ;;\n  esac\n\n  cecho \" \"\n  read -p \"Would you like me to create a ~/.my.cnf file for you?: [y/N] \" REPLY\n\n  case $REPLY in\n    yes|y|Y|YES)\n      answer2='yes'\n\n      if [ ! -f ~/.my.cnf ] ; then\n        umask 077\n        printf \"[client]\\nuser=$user\\npassword=$pass\\nsocket=$socket\" \u003e ~/.my.cnf\n\n        if [ \"$answer1\" != 'yes' ] ; then\n          exit 1\n        else\n          final_login_attempt\n          return 0\n        fi\n      else\n        printf \"\\n\"\n        cecho \"~/.my.cnf already exists!\" boldred\n        printf \"\\n\"\n        read -p \"Replace?: [y/N] \" REPLY\n\n        if [ \"$REPLY\" = 'y' ] || [ \"$REPLY\" = 'Y' ] ; then\n          printf \"[client]\\nuser=$user\\npassword=$pass\\socket=$socket\" \u003e ~/.my.cnf\n\n          if [ \"$answer1\" != 'yes' ] ; then\n            exit 1\n          else\n            final_login_attempt\n            return 0\n          fi\n        else\n          cecho \"Please set the 'user=' and 'password=' and 'socket=' values in ~/.my.cnf\"\n          exit 1\n        fi\n      fi\n      ;;\n    *)\n      if [ \"$answer1\" != 'yes' ] ; then\n        exit 1\n      else\n        final_login_attempt\n        return 0\n      fi\n      ;;\n  esac\n}\n\n# Populate the .my.cnf file using values harvested from Webmin\nfind_webmin_passwords() {\n  cecho \"Testing for stored webmin passwords:\"\n\n  if [ -f /etc/webmin/mysql/config ]; then\n    user=$(grep ^login= /etc/webmin/mysql/config | cut -d \"=\" -f 2)\n    pass=$(grep ^pass= /etc/webmin/mysql/config | cut -d \"=\" -f 2)\n\n    if [  $user ] \u0026\u0026 [ $pass ] \u0026\u0026 [ ! -f ~/.my.cnf  ]; then\n      cecho \"Setting login info as User: $user Password: $pass\"\n      touch ~/.my.cnf\n      chmod 600 ~/.my.cnf\n\n      printf \"[client]\\nuser=$user\\npassword=$pass\" \u003e ~/.my.cnf \n      cecho \"Retrying login\"\n      is_up=$($mysqladmin ping 2\u003e\u00261)\n\n      if [ \"$is_up\" = \"mysqld is alive\" ]; then\n        echo UP \u003e /dev/null\n      else\n        second_login_failed\n      fi\n\n      echo\n    else\n      second_login_failed\n      echo\n    fi\n  else\n    cecho \" None Found\" boldred\n    second_login_failed\n  fi\n}\n\n# Function to pull MySQL status variable\n#\n# Ex:\n#     mysql_status 'Mysql_status_variable' bash_dest_variable\nmysql_status() {\n  local status=$($mysql -Bse \"show /*!50000 global */ status like $1\" | awk '{ print $2 }')\n  export \"$2\"=$status\n}\n\n# Function to pull MySQL server runtime variable\n#\n# Ex:\n#     mysql_variable 'Mysql_server_variable' bash_dest_variable\n#     mysql_variableTSV 'Mysql_server_variable' bash_dest_variable\nmysql_variable() {\n  local variable=$($mysql -Bse \"show /*!50000 global */ variables like $1\" | awk '{ print $2 }')\n  export \"$2\"=$variable\n}\n\nmysql_variableTSV() {\n  local variable=$($mysql -Bse \"show /*!50000 global */ variables like $1\" | awk -F \\t '{ print $2 }')\n  export \"$2\"=$variable\n}\n\nfloat2int() {\n  local variable=$(echo \"$1 / 1\" | bc -l)\n  export \"$2\"=$variable\n}\n\n# Divide two integers\ndivide() {\n  usage=\"$0 dividend divisor '$variable' scale\"\n\n  if [ $1 -ge 1 ]; then\n    dividend=$1\n  else\n    cecho \"Invalid Dividend\" red\n    echo $usage\n    exit 1\n  fi\n\n  if [ $2 -ge 1 ]; then\n    divisor=$2\n  else\n    cecho \"Invalid Divisor\" red\n    echo $usage\n    exit 1\n  fi\n\n  if [ ! -n $3 ]; then\n    cecho \"Invalid variable name\" red\n    echo $usage\n    exit 1\n  fi\n\n  if [ -z $4 ]; then\n    scale=2\n  elif [ $4 -ge 0 ]; then\n    scale=$4\n  else\n    cecho \"Invalid scale\" red\n    echo $usage\n    exit 1\n  fi\n\n  export $3=$(echo \"scale=$scale; $dividend / $divisor\" | bc -l)\n}\n\n# Convert a value in to human readable size and populate a variable with the\n# result.\n#\n# value=$1\n# variable=$2\n#\n# Ex:\n#     human_readable $value 'variable name' [places of precision]\nhuman_readable() {\n  scale=$3\n\n  if [ $1 -ge 1073741824 ]; then\n    if [ -z $3 ]; then\n      scale=2\n    fi\n\n    divide $1 1073741824 \"$2\" $scale\n    unit=\"G\"\n  elif [ $1 -ge 1048576 ]; then\n    if [ -z $3 ]; then\n      scale=0\n    fi\n\n    divide $1 1048576 \"$2\" $scale\n    unit=\"M\"\n  elif [ $1 -ge 1024 ]; then\n    if [ -z $3 ]; then\n      scale=0\n    fi\n\n    divide $1 1024 \"$2\" $scale\n    unit=\"K\"\n  else\n    export \"$2\"=$1\n    unit=\"bytes\"\n  fi\n}\n\n# Function to produce human readable time\nhuman_readable_time() {\n  usage=\"$0 seconds 'variable'\"\n\n  if [ -z $1 ] || [ -z $2 ] ; then\n    cecho $usage red\n    exit 1\n  fi\n\n  days=$(echo \"scale=0 ; $1 / 86400\" | bc -l)\n  remainder=$(echo \"scale=0 ; $1 % 86400\" | bc -l)\n  hours=$(echo \"scale=0 ; $remainder / 3600\" | bc -l)\n  remainder=$(echo \"scale=0 ; $remainder % 3600\" | bc -l)\n  minutes=$(echo \"scale=0 ; $remainder / 60\" | bc -l)\n  seconds=$(echo \"scale=0 ; $remainder % 60\" | bc -l)\n\n  export $2=\"$days days $hours hrs $minutes min $seconds sec\"\n}\n\n# Print version info\ncheck_mysql_version() {\n  mysql_variable \\'version\\' mysql_version\n  mysql_variable \\'version_compile_machine\\' mysql_version_compile_machine\n\n  if [ \"$mysql_version_num\" -lt 050000 ]; then\n    cecho \"MySQL Version $mysql_version $mysql_version_compile_machine is EOL please upgrade to MySQL 4.1 or later\" boldred\n  else\n    cecho \"MySQL Version $mysql_version $mysql_version_compile_machine\"\n  fi\n}\n\n# Present a reminder that mysql must run for a couple of days to build up good\n# numbers in server status variables before these tuning suggestions should be\n# used.\npost_uptime_warning() {\n  mysql_status \\'Uptime\\' uptime\n  mysql_status \\'Threads_connected\\' threads\n  queries_per_sec=$(($questions/$uptime))\n  human_readable_time $uptime uptimeHR\n\n  cecho \"Uptime = $uptimeHR\"\n  cecho \"Avg. qps = $queries_per_sec\"\n  cecho \"Total Questions = $questions\"\n  cecho \"Threads Connected = $threads\"\n  echo\n\n  if [ $uptime -gt 172800 ] ; then\n    cecho \"Server has been running for over 48hrs.\"\n    cecho \"It should be safe to follow these recommendations\"\n  else\n    cechon \"Warning: \" boldred\n    cecho \"Server has not been running for at least 48hrs.\" boldred\n    cecho \"It may not be safe to use these recommendations\" boldred\n  fi\n\n  echo \"\"\n  cecho \"To find out more information on how each of these\" red\n  cecho \"runtime variables effects performance visit:\" red\n\n  if [ \"$major_version\" = '3.23' ] || [ \"$major_version\" = '4.0' ] || [ \"$major_version\" = '4.1' ] ; then\n    cecho \"http://dev.mysql.com/doc/refman/4.1/en/server-system-variables.html\" boldblue\n  elif [ \"$major_version\" = '5.0' ] || [ \"$mysql_version_num\" -gt '050100' ]; then\n    cecho \"http://dev.mysql.com/doc/refman/$major_version/en/server-system-variables.html\" boldblue\n  else\n    cecho \"UNSUPPORTED MYSQL VERSION\" boldred\n    exit 1\n  fi\n\n  cecho \"Visit http://www.mysql.com/products/enterprise/advisors.html\" boldblue\n  cecho \"for info about MySQL's Enterprise Monitoring and Advisory Service\" boldblue\n}\n\n# Slow Queries\ncheck_slow_queries () {\n  cecho \"SLOW QUERIES\" boldblue\n\n  mysql_status \\'Slow_queries\\' slow_queries\n  mysql_variable \\'long_query_time\\' long_query_time\n  mysql_variable \\'log%queries\\' log_slow_queries\n\n  prefered_query_time=5\n\n  if [ -e /etc/my.cnf ]; then\n    if [ -z $log_slow_queries ]; then\n      log_slow_queries=$(grep log-slow-queries /etc/my.cnf)\n    fi\n  fi\n\n  if [ \"$log_slow_queries\" = 'ON' ]; then\n    cecho \"The slow query log is enabled.\"\n  elif [ \"$log_slow_queries\" = 'OFF' ]; then\n    cechon \"The slow query log is \"\n    cechon \"NOT\" boldred\n    cecho \" enabled.\"\n  elif [ -z $log_slow_queries ]; then\n    cechon \"The slow query log is \"\n    cechon \"NOT\" boldred\n    cecho \" enabled.\"\n  else\n    cecho \"Error: $log_slow_queries\" boldred\n  fi\n\n  cecho \"Current long_query_time = $long_query_time sec.\"\n  cechon \"You have \"\n  cechon \"$slow_queries\" boldred \n  cechon \" out of \"\n  cechon \"$questions\" boldred\n  cecho \" that take longer than $long_query_time sec. to complete\"\n\n  float2int long_query_time long_query_timeInt\n\n  if [ $long_query_timeInt -gt $prefered_query_time ]; then\n    cecho \"Your long_query_time may be too high, I typically set this under $prefered_query_time sec.\" red\n  else\n    cecho \"Your long_query_time seems to be fine\" green\n  fi\n}\n\n# Binary Log\ncheck_binary_log() {\n  cecho \"BINARY UPDATE LOG\" boldblue\n\n  mysql_variable \\'log_bin\\' log_bin\n  mysql_variable \\'max_binlog_size\\' max_binlog_size\n  mysql_variable \\'expire_logs_days\\' expire_logs_days\n  mysql_variable \\'sync_binlog\\' sync_binlog\n  #mysql_variable \\'max_binlog_cache_size\\' max_binlog_cache_size\n\n  if [ \"$log_bin\" = 'ON' ]; then\n    cecho \"The binary update log is enabled\"\n\n    if [ -z \"$max_binlog_size\" ]; then\n      cecho \"The max_binlog_size is not set. The binary log will rotate when it reaches 1GB.\" red\n    fi\n\n    if [ \"$expire_logs_days\" -eq 0 ]; then\n      cecho \"The expire_logs_days is not set.\" boldred\n      cechon \"The mysqld will retain the entire binary log until \" red\n      cecho \"RESET MASTER or PURGE MASTER LOGS commands are run manually\" red\n      cecho \"Setting expire_logs_days will allow you to remove old binary logs automatically\"  yellow\n      cecho \"See http://dev.mysql.com/doc/refman/$major_version/en/purge-master-logs.html\" yellow\n    fi\n\n    if [ \"$sync_binlog\" = 0 ] ; then\n      cecho \"Binlog sync is not enabled, you could loose binlog records during a server crash\" red\n    fi\n  else\n    cechon \"The binary update log is \"\n    cechon \"NOT \" boldred\n    cecho \"enabled.\"\n    cecho \"You will not be able to do point in time recovery\" red\n    cecho \"See http://dev.mysql.com/doc/refman/$major_version/en/point-in-time-recovery.html\" yellow\n  fi\n}\n\n# Used Connections\ncheck_used_connections() {\n  mysql_variable \\'max_connections\\' max_connections\n  mysql_status \\'Max_used_connections\\' max_used_connections\n  mysql_status \\'Threads_connected\\' threads_connected\n\n  connections_ratio=$(($max_used_connections*100/$max_connections))\n\n  cecho \"MAX CONNECTIONS\" boldblue\n  cecho \"Current max_connections = $max_connections\"\n  cecho \"Current threads_connected = $threads_connected\"\n  cecho \"Historic max_used_connections = $max_used_connections\"\n  cechon \"The number of used connections is \"\n\n  if [ $connections_ratio -ge 85 ]; then\n    txt_color=red\n    error=1\n  elif [ $connections_ratio -le 10 ]; then\n    txt_color=red\n    error=2\n  else\n    txt_color=green\n    error=0\n  fi\n\n  cechon \"$connections_ratio% \" $txt_color\n  cecho \"of the configured maximum.\"\n\n  if [ $error -eq 1 ]; then\n    cecho \"You should raise max_connections\" $txt_color\n  elif [ $error -eq 2 ]; then\n    cecho \"You are using less than 10% of your configured max_connections.\" $txt_color\n    cecho \"Lowering max_connections could help to avoid an over-allocation of memory\" $txt_color\n    cecho \"See \\\"MEMORY USAGE\\\" section to make sure you are not over-allocating\" $txt_color\n  else\n    cecho \"Your max_connections variable seems to be fine.\" $txt_color\n  fi\n\n  unset txt_color\n}\n\n# Worker Threads\ncheck_threads() {\n  cecho \"WORKER THREADS\" boldblue\n\n  mysql_status \\'Threads_created\\' threads_created1\n  sleep 1\n  mysql_status \\'Threads_created\\' threads_created2\n\n  mysql_status \\'Threads_cached\\' threads_cached\n  mysql_status \\'Uptime\\' uptime\n  mysql_variable \\'thread_cache_size\\' thread_cache_size\n\n  historic_threads_per_sec=$(($threads_created1/$uptime))\n  current_threads_per_sec=$(($threads_created2-$threads_created1))\n\n  cecho \"Current thread_cache_size = $thread_cache_size\"\n  cecho \"Current threads_cached = $threads_cached\"\n  cecho \"Current threads_per_sec = $current_threads_per_sec\"\n  cecho \"Historic threads_per_sec = $historic_threads_per_sec\"\n\n  if [ $historic_threads_per_sec -ge 2 ] \u0026\u0026 [ $threads_cached -le 1 ] ; then\n    cecho \"Threads created per/sec are overrunning threads cached\" red\n    cecho \"You should raise thread_cache_size\" red\n  elif [ $current_threads_per_sec -ge 2 ] ; then\n    cecho \"Threads created per/sec are overrunning threads cached\" red\n    cecho \"You should raise thread_cache_size\" red\n  else\n    cecho \"Your thread_cache_size is fine\" green\n  fi\n}\n\n# Key buffer Size\ncheck_key_buffer_size() {\n  cecho \"KEY BUFFER\" boldblue\n\n  mysql_status \\'Key_read_requests\\' key_read_requests\n  mysql_status \\'Key_reads\\' key_reads\n  mysql_status \\'Key_blocks_used\\' key_blocks_used\n  mysql_status \\'Key_blocks_unused\\' key_blocks_unused\n  mysql_variable \\'key_cache_block_size\\' key_cache_block_size\n  mysql_variable \\'key_buffer_size\\' key_buffer_size\n  mysql_variable \\'datadir\\' datadir\n  mysql_variable \\'version_compile_machine\\' mysql_version_compile_machine\n\n  myisam_indexes=$($mysql -Bse \"/*!50000 SELECT IFNULL(SUM(INDEX_LENGTH),0) from information_schema.TABLES where ENGINE='MyISAM' */\")\n\n  if [ -z $myisam_indexes ]; then\n    myisam_indexes=$(find $datadir -name '*.MYI' -exec du $duflags '{}' \\; 2\u003e\u00261 | awk '{ s += $1 } END { printf(\"%.0f\\n\", s )}')\n  fi\n\n  if [ $key_reads -eq 0 ]; then\n    cecho \"No key reads?!\" boldred\n    cecho \"Seriously look into using some indexes\" red\n\n    key_cache_miss_rate=0\n    key_buffer_free=$(echo \"$key_blocks_unused * $key_cache_block_size / $key_buffer_size * 100\" | bc -l )\n    key_buffer_freeRND=$(echo \"scale=0; $key_buffer_free / 1\" | bc -l)\n  else\n    key_cache_miss_rate=$(($key_read_requests/$key_reads))\n\n    if [ ! -z $key_blocks_unused ] ; then\n      key_buffer_free=$(echo \"$key_blocks_unused * $key_cache_block_size / $key_buffer_size * 100\" | bc -l )\n      key_buffer_freeRND=$(echo \"scale=0; $key_buffer_free / 1\" | bc -l)\n    else\n      key_buffer_free='Unknown'\n      key_buffer_freeRND=75\n    fi\n  fi\n\n  human_readable $myisam_indexes myisam_indexesHR\n  cecho \"Current MyISAM index space = $myisam_indexesHR $unit\" \n\n  human_readable  $key_buffer_size key_buffer_sizeHR\n  cecho \"Current key_buffer_size = $key_buffer_sizeHR $unit\"\n  cecho \"Key cache miss rate is 1 : $key_cache_miss_rate\"\n  cecho \"Key buffer free ratio = $key_buffer_freeRND %\" \n\n  if [ \"$major_version\" = '5.1' ] \u0026\u0026 [ $mysql_version_num -lt 050123 ]; then\n    if [ $key_buffer_size -ge 4294967296 ] \u0026\u0026 ( echo \"x86_64 ppc64 ia64 sparc64 i686\" | grep -q $mysql_version_compile_machine ) ; then\n      cecho \"Using key_buffer_size \u003e 4GB will cause instability in versions prior to 5.1.23 \" boldred\n      cecho \"See Bug#5731, Bug#29419, Bug#29446\" boldred\n    fi\n  fi\n\n  if [ \"$major_version\" = '5.0' ] \u0026\u0026 [ $mysql_version_num -lt 050052 ] ; then\n    if [ $key_buffer_size -ge 4294967296 ] \u0026\u0026 ( echo \"x86_64 ppc64 ia64 sparc64 i686\" | grep -q $mysql_version_compile_machine ) ; then\n      cecho \"Using key_buffer_size \u003e 4GB will cause instability in versions prior to 5.0.52 \" boldred\n      cecho \"See Bug#5731, Bug#29419, Bug#29446\" boldred\n    fi\n  fi\n\n  if [ \"$major_version\" = '4.1' -o \"$major_version\" = '4.0' ] \u0026\u0026 [ $key_buffer_size -ge 4294967296 ] \u0026\u0026 ( echo \"x86_64 ppc64 ia64 sparc64 i686\" | grep -q $mysql_version_compile_machine ) ; then\n    cecho \"Using key_buffer_size \u003e 4GB will cause instability in versions prior to 5.0.52 \" boldred\n    cecho \"Reduce key_buffer_size to a safe value\" boldred\n    cecho \"See Bug#5731, Bug#29419, Bug#29446\" boldred\n  fi\n\n  if [ $key_cache_miss_rate -le 100 ] \u0026\u0026 [ $key_cache_miss_rate -gt 0 ] \u0026\u0026 [ $key_buffer_freeRND -le 20 ]; then\n    cecho \"You could increase key_buffer_size\" boldred\n    cecho \"It is safe to raise this up to 1/4 of total system memory;\"\n    cecho \"assuming this is a dedicated database server.\"\n  elif [ $key_buffer_freeRND -le 20 ] \u0026\u0026 [ $key_buffer_size -le $myisam_indexes ] ; then\n    cecho \"You could increase key_buffer_size\" boldred\n    cecho \"It is safe to raise this up to 1/4 of total system memory;\"\n    cecho \"assuming this is a dedicated database server.\"\n  elif [ $key_cache_miss_rate -ge 10000 ] || [ $key_buffer_freeRND -le 50  ] ; then\n    cecho \"Your key_buffer_size seems to be too high.\" red \n    cecho \"Perhaps you can use these resources elsewhere\" red\n  else\n    cecho \"Your key_buffer_size seems to be fine\" green\n  fi\n}\n\n# Query Cache\ncheck_query_cache() {\n  cecho \"QUERY CACHE\" boldblue\n\n  mysql_variable \\'version\\' mysql_version\n  mysql_variable \\'query_cache_size\\' query_cache_size\n  mysql_variable \\'query_cache_limit\\' query_cache_limit\n  mysql_variable \\'query_cache_min_res_unit\\' query_cache_min_res_unit\n  mysql_status \\'Qcache_free_memory\\' qcache_free_memory\n  mysql_status \\'Qcache_total_blocks\\' qcache_total_blocks\n  mysql_status \\'Qcache_free_blocks\\' qcache_free_blocks\n  mysql_status \\'Qcache_lowmem_prunes\\' qcache_lowmem_prunes\n\n  if [ -z $query_cache_size ] ; then\n    cecho \"You are using MySQL $mysql_version, no query cache is supported.\" red\n    cecho \"I recommend an upgrade to MySQL 4.1 or better\" red\n  elif [ $query_cache_size -eq 0 ] ; then\n    cecho \"Query cache is supported but not enabled\" red\n    cecho \"Perhaps you should set the query_cache_size\" red\n  else\n    qcache_used_memory=$(($query_cache_size-$qcache_free_memory))\n    qcache_mem_fill_ratio=$(echo \"scale=2; $qcache_used_memory * 100 / $query_cache_size\" | bc -l)\n    qcache_mem_fill_ratioHR=$(echo \"scale=0; $qcache_mem_fill_ratio / 1\" | bc -l)\n\n    cecho \"Query cache is enabled\" green\n    human_readable $query_cache_size query_cache_sizeHR\n    cecho \"Current query_cache_size = $query_cache_sizeHR $unit\"\n    human_readable $qcache_used_memory qcache_used_memoryHR\n    cecho \"Current query_cache_used = $qcache_used_memoryHR $unit\"\n    human_readable $query_cache_limit query_cache_limitHR\n    cecho \"Current query_cache_limit = $query_cache_limitHR $unit\"\n    cecho \"Current Query cache Memory fill ratio = $qcache_mem_fill_ratio %\"\n\n    if [ -z $query_cache_min_res_unit ] ; then\n      cecho \"No query_cache_min_res_unit is defined.  Using MySQL \u003c 4.1 cache fragmentation can be inpredictable\" %yellow\n    else\n      human_readable $query_cache_min_res_unit query_cache_min_res_unitHR \n      cecho \"Current query_cache_min_res_unit = $query_cache_min_res_unitHR $unit\"\n    fi\n\n    if [ $qcache_free_blocks -gt 2 ] \u0026\u0026 [ $qcache_total_blocks -gt 0 ]; then\n      qcache_percent_fragmented=$(echo \"scale=2; $qcache_free_blocks * 100 / $qcache_total_blocks\" | bc -l)\n      qcache_percent_fragmentedHR=$(echo \"scale=0; $qcache_percent_fragmented / 1\" | bc -l)\n\n      if [ $qcache_percent_fragmentedHR -gt 20 ] ; then\n        cecho \"Query Cache is $qcache_percent_fragmentedHR % fragmented\" red\n        cecho \"Run \\\"FLUSH QUERY CACHE\\\" periodically to defragment the query cache memory\" red \n        cecho \"If you have many small queries lower 'query_cache_min_res_unit' to reduce fragmentation.\" red\n      fi\n    fi\n\n    if [ $qcache_mem_fill_ratioHR -le 25 ]; then\n      cecho \"Your query_cache_size seems to be too high.\" red\n      cecho \"Perhaps you can use these resources elsewhere\" red\n    fi\n\n    if [ $qcache_lowmem_prunes -ge 50 ] \u0026\u0026 [ $qcache_mem_fill_ratioHR -ge 80 ]; then\n      cechon \"However, \"\n      cechon \"$qcache_lowmem_prunes \" boldred\n      cecho \"queries have been removed from the query cache due to lack of memory\"\n      cecho \"Perhaps you should raise query_cache_size\" boldred\n    fi\n\n    cecho \"MySQL won't cache query results that are larger than query_cache_limit in size\" yellow\n  fi\n}\n\n# Sort Operations\ncheck_sort_operations() {\n  cecho \"SORT OPERATIONS\" boldblue\n\n  mysql_status \\'Sort_merge_passes\\' sort_merge_passes\n  mysql_status \\'Sort_scan\\' sort_scan\n  mysql_status \\'Sort_range\\' sort_range\n  mysql_variable \\'sort_buffer%\\' sort_buffer_size \n  mysql_variable \\'read_rnd_buffer_size\\' read_rnd_buffer_size \n\n  total_sorts=$(($sort_scan+$sort_range))\n  if [ -z $read_rnd_buffer_size ]; then\n    mysql_variable \\'record_buffer\\' read_rnd_buffer_size\n  fi\n\n  ## Correct for rounding error in mysqld where 512K != 524288 ##\n  sort_buffer_size=$(($sort_buffer_size+8))\n  read_rnd_buffer_size=$(($read_rnd_buffer_size+8))\n\n  human_readable $sort_buffer_size sort_buffer_sizeHR\n  cecho \"Current sort_buffer_size = $sort_buffer_sizeHR $unit\"\n\n  human_readable $read_rnd_buffer_size read_rnd_buffer_sizeHR\n  cechon \"Current \"\n\n  if [ \"$major_version\" = '3.23' ]; then\n    cechon \"record_rnd_buffer \"\n  else\n    cechon \"read_rnd_buffer_size \"\n  fi\n\n  cecho \"= $read_rnd_buffer_sizeHR $unit\"\n  if [ $total_sorts -eq 0 ] ; then\n    cecho \"No sort operations have been performed\"\n    passes_per_sort=0\n  fi\n\n  if [ $sort_merge_passes -ne 0 ]; then\n    passes_per_sort=$(($sort_merge_passes/$total_sorts))\n  else\n    passes_per_sort=0\n  fi\n\n  if [ $passes_per_sort -ge 2 ]; then\n    cechon \"On average \"\n    cechon \"$passes_per_sort \" boldred\n    cecho \"sort merge passes are made per sort operation\"\n    cecho \"You should raise your sort_buffer_size\"\n    cechon \"You should also raise your \"\n\n    if [ \"$major_version\" = '3.23' ]; then\n      cecho \"record_rnd_buffer_size\"\n    else\n      cecho \"read_rnd_buffer_size\"\n    fi\n  else\n    cecho \"Sort buffer seems to be fine\" green\n  fi\n}\n\n# Joins\ncheck_join_operations() {\n  cecho \"JOINS\" boldblue\n\n  mysql_status \\'Select_full_join\\' select_full_join\n  mysql_status \\'Select_range_check\\' select_range_check\n  mysql_variable \\'join_buffer%\\' join_buffer_size\n\n  # Some 4K is dropped from join_buffer_size adding it back to make sane\n  # handling of human-readable conversion\n\n  join_buffer_size=$(($join_buffer_size+4096))\n  human_readable $join_buffer_size join_buffer_sizeHR 2\n\n  cecho \"Current join_buffer_size = $join_buffer_sizeHR $unit\"\n  cecho \"You have had $select_full_join queries where a join could not use an index properly\"\n\n  if [ $select_range_check -eq 0 ] \u0026\u0026 [ $select_full_join -eq 0 ]; then\n    cecho \"Your joins seem to be using indexes properly\" green\n  fi\n\n  if [ $select_full_join -gt 0 ]; then\n    print_error='true'\n    raise_buffer='true'\n  fi\n\n  if [ $select_range_check -gt 0 ]; then\n    cecho \"You have had $select_range_check joins without keys that check for key usage after each row\" red\n    print_error='true'\n    raise_buffer='true'\n  fi\n\n  if [ $join_buffer_size -ge 4194304 ]; then\n    cecho \"join_buffer_size \u003e= 4 M\" boldred\n    cecho \"This is not advised\" boldred\n    raise_buffer=\n  fi\n\n  if [ $print_error ]; then\n    if [ \"$major_version\" = '3.23' ] || [ \"$major_version\" = '4.0' ]; then\n      cecho \"You should enable \\\"log-long-format\\\" \"\n    elif [ \"$mysql_version_num\" -gt 040100 ]; then\n      cecho \"You should enable \\\"log-queries-not-using-indexes\\\"\"\n    fi\n\n    cecho \"Then look for non indexed joins in the slow query log.\"\n    if [ $raise_buffer ]; then\n      cecho \"If you are unable to optimize your queries you may want to increase your\"\n      cecho \"join_buffer_size to accommodate larger joins in one pass.\"\n      printf \"\\n\"\n      cecho \"Note! This script will still suggest raising the join_buffer_size when\" boldred\n      cecho \"ANY joins not using indexes are found.\" boldred\n    fi\n  fi\n}\n\n# Temp Tables\ncheck_tmp_tables() {\n  cecho \"TEMP TABLES\" boldblue\n\n  mysql_status \\'Created_tmp_tables\\' created_tmp_tables \n  mysql_status \\'Created_tmp_disk_tables\\' created_tmp_disk_tables\n  mysql_variable \\'tmp_table_size\\' tmp_table_size\n  mysql_variable \\'max_heap_table_size\\' max_heap_table_size\n\n  if [ $created_tmp_tables -eq 0 ]; then\n    tmp_disk_tables=0\n  else\n    tmp_disk_tables=$((created_tmp_disk_tables*100/(created_tmp_tables+created_tmp_disk_tables)))\n  fi\n\n  human_readable $max_heap_table_size max_heap_table_sizeHR\n  cecho \"Current max_heap_table_size = $max_heap_table_sizeHR $unit\"\n\n  human_readable $tmp_table_size tmp_table_sizeHR \n  cecho \"Current tmp_table_size = $tmp_table_sizeHR $unit\"\n\n  cecho \"Of $created_tmp_tables temp tables, $tmp_disk_tables% were created on disk\"\n  if [ $tmp_table_size -gt $max_heap_table_size ]; then\n    cecho \"Effective in-memory tmp_table_size is limited to max_heap_table_size.\" yellow\n  fi\n\n  if [ $tmp_disk_tables -ge 25 ]; then\n    cecho \"Perhaps you should increase your tmp_table_size and/or max_heap_table_size\" boldred\n    cecho \"to reduce the number of disk-based temporary tables\" boldred\n    cecho \"Note! BLOB and TEXT columns are not allow in memory tables.\" yellow\n    cecho \"If you are using these columns raising these values might not impact your \" yellow\n    cecho  \"ratio of on disk temp tables.\" yellow\n  else\n    cecho \"Created disk tmp tables ratio seems fine\" green\n  fi\n}\n\n# Open Files Limit\ncheck_open_files() {\n  cecho \"OPEN FILES LIMIT\" boldblue\n\n  mysql_variable \\'open_files_limit\\' open_files_limit\n  mysql_status   \\'Open_files\\' open_files\n\n  if [ -z $open_files_limit ] || [ $open_files_limit -eq 0 ]; then\n    open_files_limit=$(ulimit -n)\n    cant_override=1\n  else\n    cant_override=0\n  fi\n\n  cecho \"Current open_files_limit = $open_files_limit files\"\n\n  open_files_ratio=$(($open_files*100/$open_files_limit))\n\n  cecho \"The open_files_limit should typically be set to at least 2x-3x\" yellow\n  cecho \"that of table_cache if you have heavy MyISAM usage.\" yellow\n  if [ $open_files_ratio -ge 75 ]; then\n    cecho \"You currently have open more than 75% of your open_files_limit\" boldred\n\n    if [ $cant_override -eq 1 ]; then\n      cecho \"You should set a higer value for ulimit -u in the mysql startup script then restart mysqld\" boldred\n      cecho \"MySQL 3.23 users : This is just a guess based upon the current shell's ulimit -u value\" yellow\n    elif [ $cant_override -eq 0 ]; then\n      cecho \"You should set a higher value for open_files_limit in my.cnf\" boldred\n    else\n      cecho \"ERROR can't determine if mysqld override of ulimit is allowed\" boldred\n      exit 1\n    fi\n  else\n    cecho \"Your open_files_limit value seems to be fine\" green\n  fi\n}\n\n# Table Cache\ncheck_table_cache() {\n  cecho \"TABLE CACHE\" boldblue\n\n  mysql_variable \\'datadir\\' datadir\n  mysql_variable \\'table_cache\\' table_cache\n\n  # MySQL +5.1 version of table_cache\n  mysql_variable \\'table_open_cache\\' table_open_cache\n  mysql_variable \\'table_definition_cache\\' table_definition_cache\n\n  mysql_status \\'Open_tables\\' open_tables\n  mysql_status \\'Opened_tables\\' opened_tables\n  mysql_status \\'Open_table_definitions\\' open_table_definitions\n\n  table_count=$($mysql -Bse \"/*!50000 SELECT COUNT(*) FROM INFORMATION_SCHEMA.TABLES WHERE TABLE_TYPE='BASE TABLE' */\")\n\n  if [ -z \"$table_count\" ]; then\n    if [ \"$UID\" != \"$socket_owner\" ] \u0026\u0026 [ \"$UID\" != \"0\" ] ; then\n      cecho \"You are not '$socket_owner' or 'root'\" red\n      cecho \"I am unable to determine the table_count!\" red\n    else\n      table_count=$(find $datadir 2\u003e\u00261 | grep -c .frm$)\n    fi\n  fi\n\n  if [ $table_open_cache ]; then\n    table_cache=$table_open_cache\n  fi\n\n  if [ $opened_tables -ne 0 ] \u0026\u0026 [ $table_cache -ne 0 ]; then\n    table_cache_hit_rate=$(($open_tables*100/$opened_tables))\n    table_cache_fill=$(($open_tables*100/$table_cache))\n  elif [ $opened_tables -eq 0 ] \u0026\u0026 [ $table_cache -ne 0 ]; then\n    table_cache_hit_rate=100\n    table_cache_fill=$(($open_tables*100/$table_cache))\n  else\n    cecho \"ERROR no table_cache ?!\" boldred\n    exit 1\n  fi\n\n  if [ $table_cache ] \u0026\u0026 [ ! $table_open_cache ]; then\n    cecho \"Current table_cache value = $table_cache tables\"\n  fi\n\n  if [ $table_open_cache ]; then\n    cecho \"Current table_open_cache = $table_open_cache tables\"\n    cecho \"Current table_definition_cache = $table_definition_cache tables\"\n  fi\n\n  if [ $table_count ]; then\n    cecho \"You have a total of $table_count tables\"\n  fi\n\n  if  [ $table_cache_fill -lt 95 ]; then\n    cechon \"You have \"\n    cechon \"$open_tables \" green\n    cecho \"open tables.\" \n    cecho \"The table_cache value seems to be fine\" green\n  elif [ $table_cache_hit_rate -le 85 -o  $table_cache_fill -ge 95 ]; then\n    cechon \"You have \"\n    cechon \"$open_tables \" boldred\n    cecho \"open tables.\"\n    cechon \"Current table_cache hit rate is \" \n    cecho \"$table_cache_hit_rate%\" boldred\n    cechon \", while \"\n    cechon \"$table_cache_fill% \" boldred\n    cecho \"of your table cache is in use\"\n    cecho \"You should probably increase your table_cache\" red\n  else\n    cechon \"Current table_cache hit rate is \"\n    cechon \"$table_cache_hit_rate%\" green\n    cechon \", while \"\n    cechon \"$table_cache_fill% \" green\n    cecho \"of your table cache is in use\"\n    cecho \"The table cache value seems to be fine\" green\n  fi\n\n  if [ $table_definition_cache ] \u0026\u0026 [ $table_definition_cache -le $table_count ] \u0026\u0026 [ $table_count -ge 100 ]; then\n    cecho \"You should probably increase your table_definition_cache value.\" red\n  fi\n}\n\n# Table Locking\ncheck_table_locking() {\n  cecho \"TABLE LOCKING\" boldblue\n\n  mysql_status \\'Table_locks_waited\\' table_locks_waited\n  mysql_status \\'Table_locks_immediate\\' table_locks_immediate\n  mysql_variable \\'concurrent_insert\\' concurrent_insert\n  mysql_variable \\'low_priority_updates\\' low_priority_updates\n\n  if [ \"$concurrent_insert\" = 'ON' ]; then\n    concurrent_insert=1\n  elif [ \"$concurrent_insert\" = 'OFF' ]; then\n    concurrent_insert=0\n  fi\n\n  cechon \"Current Lock Wait ratio = \"\n  if [ $table_locks_waited -gt 0 ]; then\n    immediate_locks_miss_rate=$(($table_locks_immediate/$table_locks_waited))\n    cecho \"1 : $immediate_locks_miss_rate\" red\n  else\n    immediate_locks_miss_rate=99999 # perfect\n    cecho \"0 : $questions\"\n  fi\n\n  if [ $immediate_locks_miss_rate -lt 5000 ]; then\n    cecho \"You may benefit from selective use of InnoDB.\"\n\n    if [ \"$low_priority_updates\" = 'OFF' ]; then\n      cecho \"If you have long running SELECT's against MyISAM tables and perform\"\n      cecho \"frequent updates consider setting 'low_priority_updates=1'\"\n    fi\n\n    if [ \"$mysql_version_num\" -gt 050000 ] \u0026\u0026 [ \"$mysql_version_num\" -lt 050500 ]; then\n      if [ $concurrent_insert -le 1 ]; then\n        cecho \"If you have a high concurrency of inserts on Dynamic row-length tables\"\n        cecho \"consider setting 'concurrent_insert=2'.\"\n      fi\n    elif [ \"$mysql_version_num\" -gt 050500 ]; then\n      if [ \"$concurrent_insert\" = 'AUTO' ] || [ \"$concurrent_insert\" = 'NEVER' ]; then\n        cecho \"If you have a high concurrency of inserts on Dynamic row-length tables\"\n        cecho \"consider setting 'concurrent_insert=ALWAYS'.\"\n      fi\n    fi\n  else\n    cecho \"Your table locking seems to be fine\" green\n  fi\n}\n\n# Table Scans\ncheck_table_scans() {\n  cecho \"TABLE SCANS\" boldblue\n\n  mysql_status \\'Com_select\\' com_select\n  mysql_status \\'Handler_read_rnd_next\\' read_rnd_next\n  mysql_variable \\'read_buffer_size\\' read_buffer_size\n\n  if [ -z $read_buffer_size ]; then\n    mysql_variable \\'record_buffer\\' read_buffer_size\n  fi\n\n  human_readable $read_buffer_size read_buffer_sizeHR\n  cecho \"Current read_buffer_size = $read_buffer_sizeHR $unit\"\n\n  if [ $com_select -gt 0 ]; then\n    full_table_scans=$(($read_rnd_next/$com_select))\n\n    cecho \"Current table scan ratio = $full_table_scans : 1\"\n    if [ $full_table_scans -ge 4000 ] \u0026\u0026 [ $read_buffer_size -le 2097152 ]; then\n      cecho \"You have a high ratio of sequential access requests to SELECTs\" red\n      cechon \"You may benefit from raising \" red\n\n      if [ \"$major_version\" = '3.23' ] ; then\n        cechon \"record_buffer \" red\n      else\n        cechon \"read_buffer_size \" red\n      fi\n\n      cecho \"and/or improving your use of indexes.\" red\n    elif [ $read_buffer_size -gt 8388608 ] ; then\n      cechon \"read_buffer_size is over 8 MB \" red\n      cecho \"there is probably no need for such a large read_buffer\" red\n    else\n      cecho \"read_buffer_size seems to be fine\" green\n    fi\n  else\n    cecho \"read_buffer_size seems to be fine\" green\n  fi\n}\n\n# InnoDB\ncheck_innodb_status () {\n  ## See http://bugs.mysql.com/59393\n\n  if [ \"$mysql_version_num\" -lt 050603 ]; then\n    mysql_variable \\'have_innodb\\' have_innodb\n  fi\n\n  if [ \"$mysql_version_num\" -lt 050500 ] \u0026\u0026 [ \"$have_innodb\" = \"YES\" ]; then\n    innodb_enabled=1\n  fi\n\n  if [ \"$mysql_version_num\" -ge 050500 ] \u0026\u0026 [ \"$mysql_version_num\" -lt 050512 ]; then\n    mysql_variable \\'ignore_builtin_innodb\\' ignore_builtin_innodb\n\n    if [ \"$ignore_builtin_innodb\" = \"ON\" ] || [ $have_innodb = \"NO\" ]; then\n      innodb_enabled=0\n    else\n      innodb_enabled=1\n    fi\n  elif [ \"$major_version\"  = '5.5' ] \u0026\u0026 [ \"$mysql_version_num\" -ge 050512 ]; then\n    mysql_variable \\'ignore_builtin_innodb\\' ignore_builtin_innodb\n    if [ \"$ignore_builtin_innodb\" = \"ON\" ]; then\n      innodb_enabled=0\n    else\n      innodb_enabled=1\n    fi\n  elif [ \"$mysql_version_num\" -ge 050600 ] \u0026\u0026 [ \"$mysql_version_num\" -lt 050603 ]; then\n    mysql_variable \\'ignore_builtin_innodb\\' ignore_builtin_innodb\n\n    if [ \"$ignore_builtin_innodb\" = \"ON\" ] || [ $have_innodb = \"NO\" ]; then\n      innodb_enabled=0\n    else\n      innodb_enabled=1\n    fi\n  elif [ \"$major_version\" = '5.6' ] \u0026\u0026 [ \"$mysql_version_num\" -ge 050603 ]; then\n    mysql_variable \\'ignore_builtin_innodb\\' ignore_builtin_innodb\n\n    if [ \"$ignore_builtin_innodb\" = \"ON\" ]; then\n      innodb_enabled=0\n    else\n      innodb_enabled=1\n    fi\n  fi\n  if [ \"$innodb_enabled\" = 1 ]; then\n    mysql_variable \\'innodb_buffer_pool_size\\' innodb_buffer_pool_size\n    mysql_variable \\'innodb_additional_mem_pool_size\\' innodb_additional_mem_pool_size\n    mysql_variable \\'innodb_fast_shutdown\\' innodb_fast_shutdown\n    mysql_variable \\'innodb_flush_log_at_trx_commit\\' innodb_flush_log_at_trx_commit\n    mysql_variable \\'innodb_locks_unsafe_for_binlog\\' innodb_locks_unsafe_for_binlog\n    mysql_variable \\'innodb_log_buffer_size\\' innodb_log_buffer_size\n    mysql_variable \\'innodb_log_file_size\\' innodb_log_file_size\n    mysql_variable \\'innodb_log_files_in_group\\' innodb_log_files_in_group\n    mysql_variable \\'innodb_safe_binlog\\' innodb_safe_binlog\n    mysql_variable \\'innodb_thread_concurrency\\' innodb_thread_concurrency\n\n    cecho \"INNODB STATUS\" boldblue\n    innodb_indexes=$($mysql -Bse \"/*!50000 SELECT IFNULL(SUM(INDEX_LENGTH),0) from information_schema.TABLES where ENGINE='InnoDB' */\")\n    innodb_data=$($mysql -Bse \"/*!50000 SELECT IFNULL(SUM(DATA_LENGTH),0) from information_schema.TABLES where ENGINE='InnoDB' */\")\n\n    if [ ! -z \"$innodb_indexes\" ]; then\n      mysql_status \\'Innodb_buffer_pool_pages_data\\' innodb_buffer_pool_pages_data\n      mysql_status \\'Innodb_buffer_pool_pages_misc\\' innodb_buffer_pool_pages_misc\n      mysql_status \\'Innodb_buffer_pool_pages_free\\' innodb_buffer_pool_pages_free\n      mysql_status \\'Innodb_buffer_pool_pages_total\\' innodb_buffer_pool_pages_total\n\n      mysql_status \\'Innodb_buffer_pool_read_ahead_seq\\' innodb_buffer_pool_read_ahead_seq\n      mysql_status \\'Innodb_buffer_pool_read_requests\\' innodb_buffer_pool_read_requests\n\n      mysql_status \\'Innodb_os_log_pending_fsyncs\\' innodb_os_log_pending_fsyncs\n      mysql_status \\'Innodb_os_log_pending_writes\\' innodb_os_log_pending_writes\n      mysql_status \\'Innodb_log_waits\\' innodb_log_waits\n\n      mysql_status \\'Innodb_row_lock_time\\' innodb_row_lock_time\n      mysql_status \\'Innodb_row_lock_waits\\' innodb_row_lock_waits\n\n      human_readable $innodb_indexes innodb_indexesHR\n      cecho \"Current InnoDB index space = $innodb_indexesHR $unit\"\n      human_readable $innodb_data innodb_dataHR\n      cecho \"Current InnoDB data space = $innodb_dataHR $unit\"\n      percent_innodb_buffer_pool_free=$(($innodb_buffer_pool_pages_free*100/$innodb_buffer_pool_pages_total))\n      cecho \"Current InnoDB buffer pool free = \"$percent_innodb_buffer_pool_free\" %\"\n    else\n      cecho \"Cannot parse InnoDB stats prior to 5.0.x\" red\n      $mysql -s -e \"SHOW /*!50000 ENGINE */ INNODB STATUS\\G\"\n    fi\n\n    human_readable $innodb_buffer_pool_size innodb_buffer_pool_sizeHR\n    cecho \"Current innodb_buffer_pool_size = $innodb_buffer_pool_sizeHR $unit\"\n    cecho \"Depending on how much space your innodb indexes take up it may be safe\"\n    cecho \"to increase this value to up to 2 / 3 of total system memory\"\n  else\n    cecho \"No InnoDB Support Enabled!\" boldred\n  fi\n}\n\n# Total Memory Usage\ntotal_memory_used() {\n  cecho \"MEMORY USAGE\" boldblue\n\n  mysql_variable \\'read_buffer_size\\' read_buffer_size\n  mysql_variable \\'read_rnd_buffer_size\\' read_rnd_buffer_size\n  mysql_variable \\'sort_buffer_size\\' sort_buffer_size\n  mysql_variable \\'thread_stack\\' thread_stack\n  mysql_variable \\'max_connections\\' max_connections\n  mysql_variable \\'join_buffer_size\\' join_buffer_size\n  mysql_variable \\'tmp_table_size\\' tmp_table_size\n  mysql_variable \\'max_heap_table_size\\' max_heap_table_size\n  mysql_variable \\'log_bin\\' log_bin\n  mysql_status \\'Max_used_connections\\' max_used_connections\n\n  if [ \"$major_version\" = \"3.23\" ]; then\n    mysql_variable \\'record_buffer\\' read_buffer_size\n    mysql_variable \\'record_rnd_buffer\\' read_rnd_buffer_size\n    mysql_variable \\'sort_buffer\\' sort_buffer_size\n  fi\n\n  if [ \"$log_bin\" = \"ON\" ]; then\n    mysql_variable \\'binlog_cache_size\\' binlog_cache_size\n  else\n    binlog_cache_size=0\n  fi\n\n  if [ $max_heap_table_size -le $tmp_table_size ]; then\n    effective_tmp_table_size=$max_heap_table_size\n  else\n    effective_tmp_table_size=$tmp_table_size\n  fi\n\n  per_thread_buffers=$(echo \"($read_buffer_size+$read_rnd_buffer_size+$sort_buffer_size+$thread_stack+$join_buffer_size+$binlog_cache_size)*$max_connections\" | bc -l)\n  per_thread_max_buffers=$(echo \"($read_buffer_size+$read_rnd_buffer_size+$sort_buffer_size+$thread_stack+$join_buffer_size+$binlog_cache_size)*$max_used_connections\" | bc -l)\n\n  mysql_variable \\'innodb_buffer_pool_size\\' innodb_buffer_pool_size\n  if [ -z $innodb_buffer_pool_size ]; then\n    innodb_buffer_pool_size=0\n  fi\n\n  mysql_variable \\'innodb_additional_mem_pool_size\\' innodb_additional_mem_pool_size\n  if [ -z $innodb_additional_mem_pool_size ]; then\n    innodb_additional_mem_pool_size=0\n  fi\n\n  mysql_variable \\'innodb_log_buffer_size\\' innodb_log_buffer_size\n  if [ -z $innodb_log_buffer_size ]; then\n    innodb_log_buffer_size=0\n  fi\n\n  mysql_variable \\'key_buffer_size\\' key_buffer_size\n  mysql_variable \\'query_cache_size\\' query_cache_size\n  if [ -z $query_cache_size ]; then\n    query_cache_size=0\n  fi\n\n  global_buffers=$(echo \"$innodb_buffer_pool_size+$innodb_additional_mem_pool_size+$innodb_log_buffer_size+$key_buffer_size+$query_cache_size\" | bc -l)\n\n  max_memory=$(echo \"$global_buffers+$per_thread_max_buffers\" | bc -l)\n  total_memory=$(echo \"$global_buffers+$per_thread_buffers\" | bc -l)\n\n  pct_of_sys_mem=$(echo \"scale=0; $total_memory*100/$physical_memory\" | bc -l)\n\n  if [ $pct_of_sys_mem -gt 90 ]; then\n    txt_color=boldred\n    error=1\n  else\n    txt_color=\n    error=0\n  fi\n\n  human_readable $max_memory max_memoryHR\n  cecho \"Max Memory Ever Allocated : $max_memoryHR $unit\" $txt_color\n\n  human_readable $per_thread_buffers per_thread_buffersHR\n  cecho \"Configured Max Per-thread Buffers : $per_thread_buffersHR $unit\" $txt_color\n\n  human_readable $global_buffers global_buffersHR\n  cecho \"Configured Max Global Buffers : $global_buffersHR $unit\" $txt_color\n\n  human_readable $total_memory total_memoryHR\n  cecho \"Configured Max Memory Limit : $total_memoryHR $unit\" $txt_color\n\n  human_readable $physical_memory physical_memoryHR\n  cecho \"Physical Memory : $physical_memoryHR $unit\" $txt_color\n\n  if [ $error -eq 1 ]; then\n    printf \"\\n\"\n    cecho \"Max memory limit exceeds 90% of physical memory\" $txt_color\n  else\n    cecho \"Max memory limit seem to be within acceptable norms\" green\n  fi\n  unset txt_color\n}\n\n# Required Functions\nlogin_validation() {\n  check_for_socket          # determine the socket location -- 1st login\n  check_for_plesk_passwords # determine the login method -- 2nd login\n  check_mysql_login         # determine if mysql is accepting login -- 3rd login\n  export major_version=$($mysql -Bse \"SELECT SUBSTRING_INDEX(VERSION(), '.', +2)\")\n  export mysql_version_num=$($mysql -Bse \"SELECT VERSION()\" |\n    awk -F \\. '{ printf \"%02d\", $1; printf \"%02d\", $2; printf \"%02d\", $3 }')\n}\n\nshared_info() {\n  export major_version=$($mysql -Bse \"SELECT SUBSTRING_INDEX(VERSION(), '.', +2)\")\n  export mysql_version_num=$($mysql -Bse \"SELECT VERSION()\" |\n    awk -F \\. '{ printf \"%02d\", $1; printf \"%02d\", $2; printf \"%02d\", $3 }')\n  mysql_status \\'Questions\\' questions\n  socket_owner=$(ls -nH $socket | awk '{ print $3 }')\n}\n\nget_system_info() {\n  export OS=$(uname)\n\n  # Get information for various UNIXes\n  if [ \"$OS\" = 'Darwin' ]; then\n    ps_socket=$(netstat -ln | awk '/mysql(.*)?\\.sock/ { print $9 }' | head -1)\n    found_socks=$(netstat -ln | awk '/mysql(.*)?\\.sock/ { print $9 }')\n    export physical_memory=$(sysctl -n hw.memsize)\n    export duflags=''\n  elif [ \"$OS\" = 'FreeBSD' ] || [ \"$OS\" = 'OpenBSD' ]; then\n    ## On FreeBSD must be root to locate sockets.\n    ps_socket=$(netstat -ln | awk '/mysql(.*)?\\.sock/ { print $9 }' | head -1)\n    found_socks=$(netstat -ln | awk '/mysql(.*)?\\.sock/ { print $9 }')\n    export physical_memory=$(sysctl -n hw.realmem)\n    export duflags=''\n  elif [ \"$OS\" = 'Linux' ]; then\n    ## Includes SWAP\n    ## export physical_memory=$(free -b | grep -v buffers |  awk '{ s += $2 } END { printf(\"%.0f\\n\", s ) }')\n    ps_socket=$(netstat -ln | awk '/mysql(.*)?\\.sock/ { print $9 }' | head -1)\n    found_socks=$(netstat -ln | awk '/mysql(.*)?\\.sock/ { print $9 }')\n    export physical_memory=$(awk '/^MemTotal/ { printf(\"%.0f\", $2*1024 ) }' \u003c /proc/meminfo)\n    export duflags='-b'\n  elif [ \"$OS\" = 'SunOS' ]; then\n    ps_socket=$(netstat -an | awk '/mysql(.*)?.sock/ { print $5 }' | head -1)\n    found_socks=$(netstat -an | awk '/mysql(.*)?.sock/ { print $5 }')\n    export physical_memory=$(prtconf | awk '/^Memory\\ size:/ { print $3*1048576 }')\n  fi\n\n  if [ -z $(which bc) ]; then\n    echo \"Error: Command line calculator 'bc' not found!\"\n    exit\n  fi\n}\n\n## Optional Components Groups\nbanner_info() {\n  shared_info\n  print_banner; echo\n  check_mysql_version; echo\n  post_uptime_warning; echo\n}\n\nmisc() {\n  shared_info\n  check_slow_queries; echo\n  check_binary_log; echo\n  check_threads; echo\n  check_used_connections; echo\n  check_innodb_status; echo\n}\n\nmemory() {\n  shared_info\n  total_memory_used; echo\n  check_key_buffer_size; echo\n  check_query_cache; echo\n  check_sort_operations; echo\n  check_join_operations; echo\n}\n\nfile() {\n  shared_info\n  check_open_files; echo\n  check_table_cache; echo\n  check_tmp_tables; echo\n  check_table_scans; echo\n  check_table_locking; echo\n}\n\nall() {\n  banner_info\n  misc\n  memory\n  file\n}\n\nprompt() {\n  prompted='true'\n  read -p \"Username [anonymous] : \" user\n  read -rp \"Password [\u003cnone\u003e] : \" pass\n  cecho \" \"\n  read -p \"Socket [ /var/lib/mysql/mysql.sock ] : \" socket\n\n  if [ -z $socket ]; then\n    export socket='/var/lib/mysql/mysql.sock'\n  fi\n\n  if [ -z $pass ]; then\n    export mysql=\"mysql -S $socket -u$user\"\n    export mysqladmin=\"mysqladmin -S $socket -u$user\"\n  else\n    export mysql=\"mysql -S $socket -u$user -p$pass\"\n    export mysqladmin=\"mysqladmin -S $socket -u$user -p$pass\"\n  fi\n\n  check_for_socket\n  check_mysql_login\n\n  if [ $? = 1 ]; then\n    exit 1\n  fi\n\n  read -p \"Mode to test - banner, file, misc, mem, innodb, [all] : \" REPLY\n\n  if [ -z $REPLY ]; then\n    REPLY='all'\n  fi\n\n  case $REPLY in\n    banner|BANNER|header|HEADER|head|HEAD)\n      banner_info\n      ;;\n    misc|MISC|miscelaneous)\n      misc\n      ;;\n    mem|memory|MEM|MEMORY)\n      memory\n      ;;\n    file|FILE|disk|DISK)\n      file\n      ;;\n    innodb|INNODB)\n      innodb\n      ;;\n    all|ALL)\n      cecho \" \"\n      all\n      ;;\n    *)\n      cecho \"Invalid Mode!  Valid options are 'banner', 'misc', 'memory', 'file', 'innodb' or 'all'\" boldred\n      exit 1\n      ;;\n  esac\n}\n\n# Address environmental differences\nget_system_info\n\nif [ -z \"$1\" ]; then\n  login_validation\n  mode='ALL'\nelif [ \"$1\" = \"prompt\" ] || [ \"$1\" = \"PROMPT\" ]; then\n  mode=$1\nelif [ \"$1\" != \"prompt\" ] || [ \"$1\" != \"PROMPT\" ]; then\n  login_validation\n  mode=$1\nfi\n\ncase $mode in\n  all|ALL)\n    cecho \" \"\n    all\n    ;;\n  mem|memory|MEM|MEMORY)\n    cecho \" \"\n    memory\n    ;;\n  file|FILE|disk|DISK)\n    cecho \" \"\n    file\n    ;;\n  banner|BANNER|header|HEADER|head|HEAD)\n    banner_info\n    ;;\n  misc|MISC|miscelaneous)\n    cecho \" \"\n    misc\n    ;;\n  innodb|INNOD )\n    banner_info\n    check_innodb_status; echo\n    ;;\n  prompt|PROMPT)\n    prompt\n    ;;\n  *)\n    cecho \"usage: $0 [ all | banner | file | innodb | memory | misc | prompt ]\" boldred\n    exit 1\n    ;;\nesac\n```\n","created_at":-62135596800,"fuzzy_word_count":7600,"path":"/notes/mysql-tuning/","published_at":1507587004,"reading_time":36,"tags":null,"title":"MySQL Tuning","type":"notes","updated_at":1507587004,"weight":0,"word_count":7528},{"cid":"71bf9b4eb77eeb21f0a3488f155c5048e29bfe1f","content":"\n***Note: This page is quite old and is likely out of date. My opinions may have\nalso changed dramatically since this was written. It is here as a reference\nuntil I get around to updating it.***\n\n```\nyum install nagios nagios-plugins httpd nagios-plugins-load \\\n  nagios-plugins-users nagios-plugins-http nagios-plugins-ping \\\n  nagios-plugins-ssh nagios-plugins-procs nagios-plugins-disk \\\n  nagios-plugins-smtp nagios-plugins-dns nagios-plugins-tcp -y\n```\n\n```\nrm -f /etc/httpd/conf.d/welcome.conf\n```\n\nThere is also an undeclared depedency:\n\n```\nyum install perl-Text-ParseWords -y\n```\n\n```\nhtpasswd -c /etc/nagios/passwd nagiosadmin\n```\n\nUpdates are going to be handled by yum exclusively, so the first change is\nadjusting `check_for_updates=1` to `0`, and for good measure\n`bare_update_check=0` to `1` in `/etc/nagios/nagios.cfg`.\n\n```\nsystemctl enable httpd.service\nsystemctl enable nagios.service\nsystemctl start httpd.service\nsystemctl start nagios.service\n```\n\nI had a strange issue where the ping plugin wasn't working with an error like\n\"CRITICAL - Could not interpret output from ping command\" the solution was to\ngive the setuid bit to the ping program like so:\n\n```\nchmod u+s /usr/bin/ping\n```\n\nNext steps:\n\n* Investigate using nginx instead of apache\n* Setup nagios to be the root of apache\n* Add SSL to apache\n* Add a port forward through to nagios\n\n## Permission Fixing\n\nIn the event all the editing goo breaks nagios's ability to access files I\nwrote the following quick little script to fix the various ownerships and\npermissions.\n\n```sh\n#!/bin/bash\n\n# Fix the ownership of the various directories\nchown root:root /etc/nagios\nchown -R root:nagios /etc/nagios/*\nchown root:apache /etc/nagios/passwd\n\n# And fix the permissions on the various files and directories\nchmod -R u=rwX,g=rX,o=rX /etc/nagios\nchmod 0640 /etc/nagios/private/resource.cfg /etc/nagios/passwd\n\n# And restore any selinux attributes on the files\nrestorecon -R /etc/nagios\n```\n","created_at":-62135596800,"fuzzy_word_count":300,"path":"/notes/nagios/","published_at":1507587004,"reading_time":2,"tags":null,"title":"Nagios","type":"notes","updated_at":1507587004,"weight":0,"word_count":264},{"cid":"e4ce5b7ae400352f3ca8fbdf18b54a9be893e56e","content":"\n***Note: This page is quite old and is likely out of date. My opinions may have\nalso changed dramatically since this was written. It is here as a reference\nuntil I get around to updating it.***\n\n## Red Hat Based Systems\n\n### Simple Static IP Address\n\nYou'll need to find the interface name of your primary ethernet card. Generally\nthis can be done with the `ipaddr show` command. The device name will more\nlikely than not be either `eth0` or `em1`, though occasionally I've seen\nethernet addresses along the lines of `p12p1` (which is a terrible naming\nconvention that bothers the hell out of me but I understand why the linux\ndevelopers switched to this naming convention).\n\nThis is going to assume that you want to set a static address on `eth0`, it's\ngoing to be the primary interface (in that it will have the default gateway).\nYou'll want to edit the file `/etc/sysconfig/network-scripts/ifcfg-eth0` in\nthis case. Make sure that the device name at the end of the file AND the one in\nthe config match.\n\nYou'll also need to make sure the mac address of the card matches the `HWADDR`\nin the file otherwise the network script will throw a fit and will either fail\nto bring up the interface or just complain and use the real hardware address\nanyway. You can not use this script to \"spoof\" mac addresses.\n\nThe contents of the following file set the static IP address `10.13.37.200` on\nthe interface with `10.13.37.1` as the default gateway, and two nameservers\n`8.8.8.8` and `4.2.2.2`.\n\nThis also tells the network that we're on a `/24` network the interface should\nstart up when the machine does and that if this fails to come up the rest of\nthe boot scripts that rely on a network should be skipped (That's the\n`IPV4_FAILURE_FATAL` command).\n\n```\n# /etc/sysconfig/network-scripts/ifcfg-eth0\nDEVICE=\"eth0\"\nNM_CONTROLLED=\"yes\"\nONBOOT=\"yes\"\nHWADDR=\"12:34:56:78:90:AB\"\nBOOTPROTO=\"static\"\n\nIPADDR=\"10.13.37.200\"\nNETMASK=\"255.255.255.0\"\nGATEWAY=\"10.13.37.1\"\n\nDNS1=\"8.8.8.8\"\nDNS2=\"4.2.2.2\"\nDOMAIN=\"internal.example.org\"\n\nDEFROUTE=\"yes\"\n\nIPV4_FAILURE_FATAL=\"yes\"\nIPV6INIT=\"yes\"\n\nNAME=\"Standard LAN\"\n```\n\nYou'll need to restart the NetworkManager for this to take effect like so:\n\n```\nsystemctl restart NetworkManager.service\n```\n\n### Additional IP Addresses\n\nAdditional IP addresses are one of the simplest things to perform. In the\nfollowing example I'm going to be created an additional IP on the eth0\ninterface assuming we're using the same eth0 described in the last section.\n\nToo add an IP address to the eth0 interface we simply create a new one by\ncopying that file into a new one and adding a `:1` to the name additional\naddresses would need to increment this number. So like so:\n\n```\ncp /etc/sysconfig/network-scripts/ifcfg-eth0 /etc/sysconfig/network-scripts/ifcfg-eth0:1\n```\n\nYou'll need to open up the new file and change a few things. Specifically you\nwant to get rid of the \"HWADDR\" variable, change the IP to whatever the second\nIP will be and change the device name to reflect the `:1`. The new file will\nlook like this:\n\n```\n# /etc/sysconfig/network-scripts/ifcfg-eth0:1\nDEVICE=\"eth0:1\"\nNM_CONTROLLED=\"no\"\nONBOOT=\"yes\"\nBOOTPROTO=\"static\"\n\nIPADDR=\"10.13.37.201\"\nNETMASK=\"255.255.255.0\"\nDEFROUTE=\"no\"\n\nIPV4_FAILURE_FATAL=\"yes\"\nIPV6INIT=\"yes\"\n\nNAME=\"Standard LAN\"\n```\n\nBring it up like so:\n\n```\nifup eth0:1\n```\n\nBam. Done.\n\n```\n[root@localhost ~]# ifconfig eth0:1\neth0:1    Link encap:Ethernet  HWaddr 12:34:56:78:90:AB  \n          inet addr:10.13.37.201  Bcast:10.13.37.255  Mask:255.255.255.0\n          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1\n```\n\n### Bridges\n\nTODO\n\n#### Spanning Tree\n\nTODO\n\n### NIC Bonding\n\nTODO\n\n### Routing\n\nThis section describes how to turn a linux system into a basic router. This\nassumes that the user is competent enough to know a few things about routing\nalready including what needs to be routed.\n\nNote: I noticed that my port forwarding issue seemed to go away after\ninstalling the iptstate package. This may be because it installed\n`libnetfilter_conntrack` as a dependency. Based on the `yum info` description I\ndon't believe this to be the case. Further testing may be required.\n\n#### Configuration\n\n##### /etc/sysctl.conf\n\nThe following settings need to be changed/added to securely handle the routing.\nDetailed information about the individual options is [available][1] as well as\n[Routing RFC information][2].\n\n```\n# Controls IP packet forwarding\nnet.ipv4.ip_forward = 1\n\n# Enable dynamic-ip address hacking for use with a DHCP WAN address and Masquerading\nnet.ipv4.ip_dynaddr = 1\n\n# Ignore ICMP broadcast packets\nnet.ipv4.icmp_echo_ignore_broadcasts = 1\n\n# Log spoofed packets, source routed packets, redirect packets\nnet.ipv4.conf.default.log_martians = 1\n\n# Do not accept source routing\nnet.ipv4.conf.default.accept_source_route = 0\n\n# Controls source route verification, this would drop all packets going out a different interface\nnet.ipv4.conf.default.rp_filter = 0\n\n# Since we're only going to have one route going out (the WAN interface, we have no need to adjust our routing table\nnet.ipv4.conf.default.accept_redirects = 0\n\n# Decrease the time default value for tcp_fin_timeout connection\nnet.ipv4.tcp_fin_timeout = 15\n\n# Decrease the time default value for tcp_keepalive_time connection\nnet.ipv4.tcp_keepalive_time = 1800\n```\n\nActive One:\n\n```\n# Kernel sysctl configuration file for Red Hat Linux\n#\n# For binary values, 0 is disabled, 1 is enabled.  See sysctl(8) and\n# sysctl.conf(5) for more details.\n\n# Controls IP packet forwarding\nnet.ipv4.ip_forward = 1\n\n# We use a dynamic address for our WAN\nnet.ipv4.ip_dynaddr = 1\n\n# Controls source route verification\nnet.ipv4.conf.default.rp_filter = 1\nnet.ipv4.conf.all.rp_filter = 1\n\n# Do not accept source routing\nnet.ipv4.conf.default.accept_source_route = 0\nnet.ipv4.conf.all.accept_source_route = 0\n\n# Disable protocol features with few legitimate uses\nnet.ipv4.conf.default.accept_redirects = 0\nnet.ipv4.conf.all.accept_redirects = 0\nnet.ipv4.conf.default.secure_redirects = 0\nnet.ipv4.conf.all.secure_redirects = 0\n\n# Log all suspcious packets\nnet.ipv4.conf.all.log_martians = 1\n\n# Thwart some forms of ICMP attacks\nnet.ipv4.icmp_echo_ignore_broadcasts = 1\nnet.ipv4.icmp_ignore_bogus_error_messages = 1\n\nnet.ipv4.tcp_syncookies - 1\n\n# Controls the System Request debugging functionality of the kernel\nkernel.sysrq = 0\n\n# Controls whether core dumps will append the PID to the core filename.\n# Useful for debugging multi-threaded applications.\nkernel.core_uses_pid = 1\n\n# Disable netfilter on bridges.\nnet.bridge.bridge-nf-call-ip6tables = 0\nnet.bridge.bridge-nf-call-iptables = 0\nnet.bridge.bridge-nf-call-arptables = 0\n```\n\n##### /etc/sysconfig/network\n\n```\nNETWORKING=yes\nHOSTNAME=talos.home.bedroomprogrammers.net\nFORWARD_IPV4=yes\nNETWORKING_IPV6=yes\n```\n\n##### /etc/sysconfig/network-scripts/ifcfg-eth1\n\n```\n# eth1 - br1 - vlan100 - WAN\nDEVICE=eth1\nNM_CONTROLLED=no\nONBOOT=yes\nHWADDR=52:54:00:10:B6:10\nTYPE=Ethernet\nBOOTPROTO=dhcp\n\nDEFROUTE=yes\nPEERROUTES=yes\n\nPEERDNS=no\nDNS1=\"208.67.222.222\"\nDNS2=\"208.67.220.220\"\n\nIPV4_FAILURE_FATAL=yes\nIPV6INIT=yes\nIPV6_PRIVACY=\"rfc3041\"\n\nNAME=\"WAN\"\n```\n\n##### /etc/sysconfig/network-scripts/ifcfg-eth{0,2,3,4,5}\n\nThe only thing that changes in this among the five interfaces are the IP\naddress options, the comment at the top and the name at the bottom. All of that\ninformation is documented elsewhere on this wiki.\n\n```\n# eth0 - br0 - vlan37 - Public\nDEVICE=\"eth0\"\nNM_CONTROLLED=\"no\"\nONBOOT=\"yes\"\nHWADDR=\"52:54:00:40:97:25\"\nTYPE=Ethernet\nBOOTPROTO=\"static\"\n\nIPADDR=\"10.13.37.1\"\nNETMASK=\"255.255.255.192\"\n#NETWORK=\"10.13.37.0\"\n#BROADCAST=\"10.13.37.63\"\n\nIPV4_FAILURE_FATAL=yes\nIPV6INIT=yes\nIPV6_PRIVACY=\"rfc3041\"\n\nNAME=\"LAN/Trusted\"\n```\n\n##### Firewall\n\n```\n####################### NAT Table ######################\n\n*nat\n:PREROUTING ACCEPT [0:0]\n:POSTROUTING ACCEPT [0:0]\n:OUTPUT ACCEPT [0:0]\n:PRUPNP - [0:0]\n\n# Forwarding packets\n-A PREROUTING -m tcp -p tcp -i eth0 --dport 2200 -j DNAT --to 10.13.37.70:22\n\n# This is a precursor to the UPnP rules to prevent ports from \n-A PREROUTING -m multiport -m tcp -p tcp -i eth0 --dports 1:1024 -j ACCEPT\n\n# Run through the UPnP rules after the rest of the forward rules to prevent\n# any of them from overuling our manually generated rules\n-A PREROUTING -i eth0 -j PRUPNP\n\n# Many-to-One NAT Masquerading\n-A POSTROUTING -s 10.13.37.0/24 -o eth0 -j MASQUERADE\n-A POSTROUTING -s 192.168.100.0/24 -o eth0 -j MASQUERADE\n\n# Weird DMZ shit going on here\n-A POSTROUTING -s 10.13.37.0/24 -o eth3 -j ACCEPT\n-A POSTROUTING -s 192.168.100.0/24 -o eth3 -j ACCEPT\n\nCOMMIT\n\n##################### Filter Table #####################\n\n*filter\n:INPUT DROP [0:0]\n:FORWARD DROP [0:0]\n:OUTPUT ACCEPT [0:0]\n:FWUPNP - [0:0]\n\n# I trust the loopback device\n-A INPUT -i lo -j ACCEPT\n-A OUTPUT -o lo -j ACCEPT\n\n# Any connections that are already established may continue unheeded\n-A INPUT -m state --state ESTABLISHED,RELATED -j ACCEPT\n-A OUTPUT -m state --state ESTABLISHED,RELATED -j ACCEPT\n-A FORWARD -m state --state ESTABLISHED,RELATED -j ACCEPT\n\n#################### Sanity Checks #####################\n\n# Drop any new connections that aren't advertising themselves as new\n-A INPUT -m tcp -p tcp ! --syn -m state --state NEW -j LOG --log-prefix \"Invalid new connection in\"\n-A INPUT -m tcp -p tcp ! --syn -m state --state NEW -j DROP\n-A FORWARD -m tcp -p tcp ! --syn -m state --state NEW -j LOG --log-prefix \"Invalid new connection fw\"\n-A FORWARD -m tcp -p tcp ! --syn -m state --state NEW -j DROP\n\n#################### Local Traffic #####################\n\n# Allow ping from local subnets\n-A INPUT -s 192.168.100.0/16 -p icmp -j ACCEPT\n-A INPUT -s 10.13.37.0/24 -p icmp -j ACCEPT\n\n# Allow access to the standard SSH from our local subnets\n-A INPUT -s 10.13.37.64/26 -m tcp -p tcp --dport 22 -j ACCEPT\n-A INPUT -s 10.13.37.128/26 -m tcp -p tcp --dport 22 -j ACCEPT\n-A INPUT -s 10.13.37.192/26 -m tcp -p tcp --dport 22 -j ACCEPT\n\n# Accept DHCP requests\n-A INPUT -i eth1 -m udp -p udp --dport 67 --sport 68 -j ACCEPT\n-A INPUT -i eth2 -m udp -p udp --dport 67 --sport 68 -j ACCEPT\n-A INPUT -i eth3 -m udp -p udp --dport 67 --sport 68 -j ACCEPT\n-A INPUT -i eth4 -m udp -p udp --dport 67 --sport 68 -j ACCEPT\n-A INPUT -i eth5 -m udp -p udp --dport 67 --sport 68 -j ACCEPT\n\n################ Inter-Subnet Traffic ##################\n\n# Public net\n-A FORWARD -i eth4 -j DROP\n\n# DMZ - This needs to be restricted further\n-A FORWARD -i eth3 -o eth0 -s 10.13.37.64/26 -j ACCEPT\n\n# Trusted - This needs to be restricted further\n-A FORWARD -i eth1 -o eth0 -s 10.13.37.128/26 -j ACCEPT\n\n# Servers - This needs to be restricted further\n-A FORWARD -i eth2 -o eth0 -s 10.13.37.192/26 -j ACCEPT\n\n# Room mates\n-A FORWARD -i eth5 -o eth0 -s 192.168.100.0/24 -j DROP\n\n# Forward traffic from the trusted subnet to the server subnet and vice versa\n-A FORWARD -i eth1 -o eth2 -s 10.13.37.128/26 -d 10.13.37.192/26 -j ACCEPT\n-A FORWARD -i eth2 -o eth1 -s 10.13.37.192/26 -d 10.13.37.128/26 -j ACCEPT\n\n# Forward traffic from the DMZ subnet and the server subnet, this will need to\n# be restricted more later\n-A FORWARD -i eth3 -o eth2 -s 10.13.37.64/26 -d 10.13.37.192/26 -j ACCEPT\n-A FORWARD -i eth2 -o eth3 -s 10.13.37.192/26 -d 10.13.37.64/26 -j ACCEPT\n\n# Forward traffic from the trusted subnet to the DMZ, but not vice versa\n-A FORWARD -i eth1 -o eth3 -s 10.13.37.128/26 -d 10.13.37.64/26 -j ACCEPT\n\n# Allow the public and room mate networks to access the DMZ but not vice versa\n-A FORWARD -i eth4 -o eth3 -s 10.13.37.0/26 -d 10.13.37.64/26 -j ACCEPT\n-A FORWARD -i eth5 -o eth3 -s 192.168.100.0/24 -d 10.13.37.64/26 -j ACCEPT\n\n# Allow port forwarded traffic, only valid to the DMZ subnet (.66-126)\n-A FORWARD -m tcp -p tcp -d 10.13.37.70 --dport 22 -o eth3 -j ACCEPT\n\n# Jump into the UPnP chain if on an interface that is allowed to use UPnP\n-A FORWARD -i eth0 -o eth1 -d 10.13.37.128/26 -j FWUPNP\n-A FORWARD -i eth0 -o eth5 -d 192.168.100.0/24 -j FWUPNP\n\nCOMMIT\n```\n\n##### Useful Tools\n\n* netstat-nat\n* conntrack-tools\n* iptstate\n* dstat\n\n### VLANs\n\n[1]: http://www.frozentux.net/ipsysctl-tutorial/ipsysctl-tutorial.html\n[2]: http://www.faqs.org/rfcs/rfc1812.html\n","created_at":-62135596800,"fuzzy_word_count":1800,"path":"/notes/network/","published_at":1508771245,"reading_time":9,"tags":null,"title":"Network","type":"notes","updated_at":1508771245,"weight":0,"word_count":1716},{"cid":"7ab760a2d1d232d0275f8a265cf7b1fa4a00195b","content":"\n***Note: This page is quite old and is likely out of date. My opinions may have\nalso changed dramatically since this was written. It is here as a reference\nuntil I get around to updating it.***\n\nInstall packages `nfs-utils` and `nfs4-acl-tools`\n\nThis installation sets `rpcbind`, `rpcgssd`, `rpcidmapd` and `nfslock` to all\nautomatically start on boot... That pisses me off properly, so I set them all\ntoo disabled for testing... I'll turn them back on as needed (hopefully I won't\nhave too).\n\nI started `nfs` and `rpcbind`... lets see if thats enough.\n\n/etc/exports on server:\n\n```\n/media/storage/Media           10.13.37.51(rw,sync)  \n/media/storage/HomeDrives      10.13.37.51(rw,sync)\n/media/storage/SharedDocuments 10.13.37.51(rw,sync)\n```\n\nRun the following\n\n```\nexportfs -rv\n```\n\nApparently the client needs the `nfs-utils` installed as well, if the client\ndoesn't have it they'll receive this error:\n\n```\nmount: wrong fs type, bad option, bad superblock on 10.1.1.1:/home/nfs,\nmissing codepage or helper program, or other error\n(for several filesystems (e.g. nfs, cifs) you might\nneed a /sbin/mount. helper program)\nIn some cases useful info is found in syslog - try\ndmesg | tail  or so\n```\n\n* http://www.cyberciti.biz/faq/centos-fedora-rhel-nfs-v4-configuration/\n* http://www.server-world.info/en/note?os=Fedora_15\u0026p=nfs\n* http://tldp.org/HOWTO/NFS-HOWTO/security.html\n* http://www.linuxjournal.com/article/4880\n* `http://www.copiouscom.com/2010/05/open-directory-kerberos-single-sign-on-sso-and-centos-with-ssh-and-kerberized-nfs-home-directories/` (Dead link)\n","created_at":-62135596800,"fuzzy_word_count":200,"path":"/notes/nfs/","published_at":1508540507,"reading_time":1,"tags":null,"title":"NFS","type":"notes","updated_at":1508540507,"weight":0,"word_count":176},{"cid":"f66aceb34b340cc3d2480b32c7c79aad40306752","content":"\n***Note: This page is quite old and is likely out of date. My opinions may have\nalso changed dramatically since this was written. It is here as a reference\nuntil I get around to updating it.***\n\n## Dynamic Backend w/ Fallback\n\nSources:\n\n* http://wiki.nginx.org/HttpRedis2Module\n* http://spin.atomicobject.com/2013/07/08/nignx-load-balancing-reverse-proxy-updated/\n\n```\nupstream redis {\n  # TODO Properly configure this\n  server redis-01.i.0x378.net:6379;\n  keepalive 1024 single;\n\n  redis2_connect_timeout 100ms;\n  redis2_send_timeout 100ms;\n  redis2_read_timeout 100ms;\n  redis2_pass redis;\n}\n\nserver {\n  listen 443;\n\n  # Include SSL stuff;\n\n  # Log stuff\n  access_log /var/log/nginx/access.log;\n\n  # Local/Internal DNS resolver\n  resolver 192.168.122.1;\n\n  location / {\n    # Don't do any magic to static files\n    root '/var/www/assets.i.0x378.net';\n    if (-f $request_filename) {\n      break;\n      # This probably needs some work\n    }\n\n    # TODO: Get this from Redis\n    set $server 'app-01.i.0x378.net';\n    set $port '80';\n\n    proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n    proxy_set_header Host $http_host;\n    proxy_set_header X-Real-IP $remote_addr;\n    proxy_set_header X-Forwarded-Proto $scheme; # Should always be https\n\n    # Intercept all errors, if one matches the error block sent the request to\n    # a different backend\n    proxy_intercept_errors on;\n    error_page 502 = @fallback;\n\n    proxy_pass http://$server:$port;\n  }\n\n  location @fallback {\n    # Look this up, but pretty sure it means don't allow clients / HTTP\n    # requests to access this.\n    internal;\n    proxy_pass http://fall-back-host.i.0x378.net:80/;\n  }\n}\n```\n\n## Cert\n\n```\necho -e \"AC\\n\\n \\n \\n\\n*.i.0x378.net\\n\\n\" | openssl req -new -x509 \\\n  -newkey rsa:2048 -keyout server.key -nodes -days 365 -out server.crt\n```\n\nAnd include some DH parameters.\n\n```\nopenssl dhparam -rand - 2048 \u003e server_dh.pem\n```\n\nI just dropped `server.{key,cert}` into `/etc/nginx/`, and a quick SSL\nconfiguration to dump into the nginx configuration.\n\n```\ncat \u003c\u003c EOF \u003e /etc/nginx/conf.d/ssl.conf\nserver {\n  listen 443;\n\n  ssl on;\n\n  ssl_certificate     /etc/nginx/server.crt;\n  ssl_certificate_key /etc/nginx/server.key;\n\n  ssl_dhparam /etc/nginx/server_dh.pem;\n\n  ssl_session_timeout 5m;\n  ssl_protocols TLSv1.2 TLSv1.1 TLSv1;\n\n  ssl_ciphers 'ECDHE-RSA-AES128-GCM-SHA256:ECDHE-ECDSA-AES128-GCM-SHA256:ECDHE-RSA-AES256-GCM-SHA384:ECDHE-ECDSA-AES256-GCM-SHA384:kEDH+AESGCM:ECDHE-RSA-AES128-SHA256:ECDHE-ECDSA-AES128-SHA256:ECDHE-RSA-AES128-SHA:ECDHE-ECDSA-AES128-SHA:ECDHE-RSA-AES256-SHA384:ECDHE-ECDSA-AES256-SHA384:ECDHE-RSA-AES256-SHA:ECDHE-ECDSA-AES256-SHA:DHE-RSA-AES128-SHA256:DHE-RSA-AES128-SHA:DHE-RSA-AES256-SHA256:DHE-DSS-AES256-SHA:AES128-GCM-SHA256:AES256-GCM-SHA384:ECDHE-RSA-RC4-SHA:ECDHE-ECDSA-RC4-SHA:RC4-SHA:HIGH:!aNULL:!eNULL:!EXPORT:!DES:!3DES:!MD5:!PSK';\n  ssl_prefer_server_ciphers on;\n\n  add_header Strict-Transport-Security max-age=15768000;\n\n  access_log /var/log/nginx/access.log;\n\n  resolver 192.168.122.1;\n\n  location / {\n    # Don't do any magic to static files\n    root '/var/www/default';\n    index index.html;\n  }\n}\nEOF\n```\n","created_at":-62135596800,"fuzzy_word_count":300,"path":"/notes/nginx/","published_at":1508540507,"reading_time":2,"tags":null,"title":"NGinx","type":"notes","updated_at":1508540507,"weight":0,"word_count":289},{"cid":"a2c4d707ecbcafbc585aacdd73b6d818f25fa400","content":"\n***Note: This page is quite old and is likely out of date. My opinions may have\nalso changed dramatically since this was written. It is here as a reference\nuntil I get around to updating it.***\n\nThe Network Time Protocol (NTP) is a protocol for synchronizing the clocks of\ncomputer systems over packet-switched, variable-latency data networks. NTP uses\nUDP on port 123 as its transport layer. It is designed particularly to resist\nthe effects of variable latency by using a jitter buffer.\n\nNTP also refers to a reference software implementation that is distributed by\nthe NTP Public Services Project.\n\n## Security Notes\n\nSeveral authentication schemes and miscellaneous programs running on the\nservers rely heavily on closely synced clocks. NTP provides a clock\nsynchronized to within 10 milliseconds of the other clocks over the internet\nand as close as 200 microseconds.\n\nThis runs as an unprivileged user with no shell when run as a server as such\nit's compromise could at the very worst make it hand out incorrect time. While\nthis would mean the authentication schemes and programs wouldn't function\ncorrectly this is something that would be noticed when day to day activity\nceased to function.\n\nWhen run as a client, it has to be run as root to update the system's clock. If\nan attacker was to compromise the data stream and was able to execute arbitrary\ncommands in a response packet there would be issues. This is a highly unlikely\nscenario. None-the-less I prefer to have them synchronize against a trusted\nlocally run server.\n\n## Firewall Adjustments\n\nNTP synchronizations are allowed through my default [IPTables][1] firewall\nscript. By default it is unrestricted and should be configured to be more\nrestrictive by replacing it with the one below. You should replace the address\nwith your time server's address.\n\n```\n# Allow time sync updates, once an ntp server is setup this can be further\n# restricted to only update from there.\n-A OUTPUT -m udp -p udp --dport 123 -j ACCEPT\n```\n\nFor the server the opposite should be true. The service should only accept\nconnections on the local subnets. The following firewall rules should be added\nto the firewall.\n\n```\n# Allow other servers and clients on the local subnet to synchronize their\n# clocks to this server using ntp\n-A SERVICES -m udp -p udp -s 10.13.37.0/24 --dport 123 -j ACCEPT\n-A SERVICES -m udp -p udp -s 10.13.38.0/24 --dport 123 -j ACCEPT\n```\n\n## Configuration\n\n### /etc/ntp.conf\n\nA standard configuration of ntp is shown below, there isn't a whole lot to\nexplain so I'll leave it at that.\n\n```\n# For more information about this file, see the man pages\n# ntp.conf(5), ntp_acc(5), ntp_auth(5), ntp_clock(5), ntp_misc(5), ntp_mon(5).\n\ndriftfile /var/lib/ntp/drift\n\n# Permit time synchronization with our time source, but do not\n# permit the source to query or modify the service on this system.\nrestrict default kod nomodify notrap nopeer noquery\nrestrict -6 default kod nomodify notrap nopeer noquery\n\nrestrict 127.0.0.1 \nrestrict -6 ::1\n\n# Hosts on local network are less restricted.\nrestrict 10.13.37.64 mask 255.255.255.192 nomodify notrap nopeer\nrestrict 10.13.37.128 mask 255.255.255.192 nomodify notrap nopeer\nrestrict 10.13.37.192 mask 255.255.255.192 nomodify notrap nopeer\n\n# Stratum 1 time servers - diverse group\nserver clock.nyc.he.net iburst # IPv6 enabled\nserver clock.sjc.he.net iburst # IPv6 enabled\nserver time.keneli.org iburst\nserver bonehed.lcs.mit.edu iburst\nserver gnomon.cc.columbia.edu iburst\n\n# Enable public key cryptography.\n#crypto\nincludefile /etc/ntp/crypto/pw\nkeys /etc/ntp/keys\n\n# Specify the key identifiers which are trusted.\ntrustedkey 1\n\n# Specify the key identifier to use with the ntpdc utility.\nrequestkey 1\n\n# Specify the key identifier to use with the ntpq utility.\ncontrolkey 1\n\n# Enable writing of statistics records.\nstatistics clockstats cryptostats loopstats peerstats\n```\n\nThe server will need to be set to start automatically on boot. Use the\nfollowing command to do this:\n\n```\nchkconfig --level 345 ntpd on\n```\n\n### /etc/ntp/keys\n\nA keyfile needs to be created for local and remote administration of the\nservice. To do this generate a random 8 character alphanumeric password and\nreplace the 'xxxxxxxx' in the file `/etc/ntp/keys`.\n\nThis file should also be protected from read / write from unauthorized users\n`600` with chmod. The contents are below:\n\n```\n1      M       xxxxxxxx\n```\n\n## Checking Server Status\n\nOn the server running `ntpd` you can use the `ntpq` utility to check the\ncurrent status of the ntpd service like so:\n\n```\n[root@localhost ~]# ntpq -p 127.0.0.1\n     remote           refid      st t when poll reach   delay   offset  jitter\n==============================================================================\n*clock.nyc.he.ne .CDMA.           1 u    1   64    1   52.015  -16.888   2.421\n clock.sjc.he.ne .GPS.            1 u    2   64    1  110.573    3.201   0.366\n time.keneli.org .PPS.            1 u    1   64    1   29.378   -4.100   1.737\n bonehed.lcs.mit .PPS.            1 u    2   64    1   16.254   -1.620   1.396\n gnomon.cc.colum .USNO.           1 u    1   64    1   51.110  -15.607   2.697\n```\n\n[1]: {{\u003c ref \"./iptables.md\" \u003e}}\n","created_at":-62135596800,"fuzzy_word_count":800,"path":"/notes/ntpd/","published_at":1540945455,"reading_time":4,"tags":null,"title":"NTPd","type":"notes","updated_at":1540945455,"weight":0,"word_count":763},{"cid":"70787560915ae76231972712c32206d537cb5c80","content":"\n***Note: This page is quite old and is likely out of date. My opinions may have\nalso changed dramatically since this was written. It is here as a reference\nuntil I get around to updating it.***\n\nThere are two different means of client configuration. Fedora has a package\n`ntpdate` that should be installed instead of [ntpd][1]. Whenever it is started\nit will synchronize the server's clock to any configured time servers.  It\nneeds to be told about time servers it should be synchronizing to in the\n`/etc/ntp/step-tickers`. It then needs to be turned on using the following\ncommand:\n\n```\n[root@localhost ~]# chkconfig --level 345 ntpdate on\n```\n\nIf the server doesn't have the ntpdate service (ntpdate seems like it's being\ndeprecated though I can't imagine why) than the same thing can be accomplished\nby installing [ntpd][1] and adding a cron job to synchronize the time.\n\nThe following command will add a cron entry that will get run hourly and\nsynchronize the time against the time server configured for the ntp daemon\n(You'll want to reference [ntpd][1] for that configuration.\n\n```\necho '0 * * * * root /usr/sbin/ntpd -q -u ntp:ntp' \u003e /etc/cron.d/ntpd\n```\n\n## /etc/ntp/step-tickers\n\nBy default this file is empty and ntpdate will have no servers to get it's time\nfrom. This file should include one server per line that are either a FQDN or an\nIP address. If there is one or more local time servers they should be the only\nones listed. If there aren't then use the following for the config file:\n\n```\n0.pool.ntp.org\n1.pool.ntp.org\n```\n\n[1]: {{\u003c ref \"./ntpd.md\" \u003e}}\n","created_at":-62135596800,"fuzzy_word_count":300,"path":"/notes/ntpdate/","published_at":1540945455,"reading_time":2,"tags":null,"title":"NTPDate","type":"notes","updated_at":1540945455,"weight":0,"word_count":251},{"cid":"eba4b63e72785fcbdd574a10d7790c6f334ef2c6","content":"\n***Note: This page is quite old and is likely out of date. My opinions may have\nalso changed dramatically since this was written. It is here as a reference\nuntil I get around to updating it.***\n\nThis is a mostly (95%) script-less installation and setup of OpenStack from\nscratch on a single host, though done in a way that the services could each be\nvery easily broken out on to their own machines or multiple machines.\n\nA few references that I've used generally for assistance figuring out what I\nneed:\n\n* http://fedoraproject.org/wiki/Getting_started_with_OpenStack_on_Fedora_17\n* http://wiki.openstack.org/QpidSupport\n* https://github.com/mseknibilel/OpenStack-Folsom-Install-guide/blob/master/OpenStack_Folsom_Install_Guide_WebVersion.rst\n* https://github.com/openstack/keystone/blob/master/etc/keystone.conf.sample\n* http://docs.openstack.org/developer/keystone/configuration.html\n\n## Installation\n\nThis guide and all the other OpenStack sections on this wiki were written using\nthe pre-packaged version of OpenStack that comes with Fedora (version 2012.1.3\nEssex) any other version may have different configuration options, and\ndifferent command line flags.\n\nThe various services should be configured in the order of the sections listed\nhere.\n\n### Network Setup\n\nFor the sake of knowing exactly what is going, what connections are being made\nand between what services I've elected to put every individual service on it's\nown IP address. This separation also makes it easy to break individual services\noff on to other machines if they become a bottleneck and need a more\ndistributed setup.\n\nSince this was initially done on a development machine I also created a bridge\nto act as a virtual ethernet adapter that was only available locally named\n`br-mgmt` which also hosts all of the service's. The address range chosen for\nthis development test was `10.100.0.0/24` as it generally doesn't interfere\nwith any networks I normally encounter.\n\nThe main bridge was setup using the following configuration file which was\nwritten to `/etc/sysconfig/network-scripts/ifcfg-br-mgmt`.\n\n```\nNM_CONTROLLED=no\nONBOOT=yes\nTYPE=Bridge\nBOOTPROTO=static\n\nIPADDR=10.100.0.10\nNETMASK=255.255.255.0\nNETWORK=10.100.0.0\nBROADCAST=10.100.0.255\n\nIPV6INIT=yes\nIPV6_PRIVACY=rfc3041\n\nNAME=ManagementNet\n```\n\nSubsequent IP addresses were added to the bridge by creating files at\n`/etc/sysconfig/network-scripts/ifcfg-br-mgmt:{id}` replacing {id} with 1\nthrough 9 with the following contents:\n\n```\nDEVICE={Interface Name}\nNM_CONTROLLED=no\nONBOOT=yes\nBOOTPROTO=static\n\nIPADDR={IP Address}\nNETMASK=255.255.255.0\n\nIPV6INIT=yes\nIPV6_PRIVACY=rfc3041\n\nNAME={Service}\n```\n\nThe following table show the address allocations I used, and what I intend the\naddress be used for:\n\n| Interface Name | IP Address  | Service           |\n| -------------- | ----------- | ----------------- |\n| br-mgmt:1      | 10.100.0.11 | MySQLd            |\n| br-mgmt:2      | 10.100.0.12 | Qpidd             |\n| br-mgmt:3      | 10.100.0.13 | Keystone/Identity |\n| br-mgmt:4      | 10.100.0.14 | Memcached         |\n| br-mgmt:5      | 10.100.0.15 | Nova/Compute      |\n| br-mgmt:6      | 10.100.0.16 | Cinder/Volume     |\n| br-mgmt:7      | 10.100.0.17 | Glance/Image      |\n| br-mgmt:8      | 10.100.0.18 | EC2               |\n| br-mgmt:9      | 10.100.0.19 | Quantum/Network   |\n\n### Service Pre-Requisites\n\nWhen configuring the following service be sure to bind them to the address\nlisted in the table above in the Service/Management network section.\n\n* Install [Mysql Server][1]\n* Install [Qpid Message Broker][2]\n* Install [Chrony NTP Server][3]\n* Install [Memcache][4]\n\n### Keystone/Identity\n\nKeystone is the OpenStack Identity service and it's configuration is documented\non [it's wiki page][5].\n\n### Glance/Image Service\n\nGlance is the OpenStack Image service and it's configuration is documented on\n[it's wiki page][6]\n\n### Horizon/Dashboard\n\nNotes for when I get to this:\n\nI should make use of the Cached Database session backend:\n\n* http://docs.openstack.org/trunk/openstack-compute/install/apt/content/dashboard-session-cached-database.html\n\nIt will need to have memcache configured as if it was going to be the session\nstorage:\n\n* http://docs.openstack.org/trunk/openstack-compute/install/apt/content/dashboard-session-memcache.html\n\nAdditional notes:\n\n* `http://docs.openstack.org/trunk/openstack-object-storage/admin/content/memcached-considerations.html` (Dead link)\n* http://www.cybera.ca/tech-radar/using-memcached-openstack-nova\n\n[1]: {{\u003c ref \"./mysql.md\" \u003e}}\n[2]: {{\u003c ref \"./qpid.md\" \u003e}}\n[3]: {{\u003c ref \"./chronyd.md\" \u003e}}\n[4]: {{\u003c ref \"./memcached.md\" \u003e}}\n[5]: {{\u003c ref \"./openstack_keystone.md\" \u003e}}\n[6]: {{\u003c ref \"./openstack_glance.md\" \u003e}}\n","created_at":-62135596800,"fuzzy_word_count":500,"path":"/notes/openstack/","published_at":1540945455,"reading_time":3,"tags":null,"title":"Openstack","type":"notes","updated_at":1540945455,"weight":0,"word_count":479},{"cid":"28099406628ba5eca952fa49e8262a4721bc3ac3","content":"\n***Note: This page is quite old and is likely out of date. My opinions may have\nalso changed dramatically since this was written. It is here as a reference\nuntil I get around to updating it.***\n\n## Installation/Configuration\n\nInstall the required packages with the following command:\n\n```\nyum install openstack-utils openstack-glance python-glanceclient -y\n```\n\nGenerate a long, strong unique password for glance's [MySQL][1] user (MySQL\nshould already be setup at this point and bound to `10.100.0.11` if you're\nfollowing along from the [Openstack][2] page. Open up a MySQL console as the\nroot user and run the following commands:\n\n```\nCREATE DATABASE glance;\nGRANT ALL ON glance.* TO 'glance'@'10.100.0.%' IDENTIFIED BY 'LongStrongUniquePasswordYouGenerated';\nFLUSH PRIVILEGES;\n```\n\n[1]: {{\u003c ref \"./mysql.md\" \u003e}}\n[2]: {{\u003c ref \"./openstack.md\" \u003e}}\n","created_at":-62135596800,"fuzzy_word_count":200,"path":"/notes/openstack-glance/","published_at":1540945455,"reading_time":1,"tags":null,"title":"OpenStack Glance","type":"notes","updated_at":1540945455,"weight":0,"word_count":107},{"cid":"801182bc6d215396e5e65679b04a6624049ea8b0","content":"\n## Installation/Configuration\n\nInstall the required packages with the following command:\n\n```\nyum install openstack-utils openstack-keystone python-keystoneclient -y\n```\n\nGenerate a long, strong unique password for keystone's [MySQL][1] user (MySQL\nshould already be setup at this point and bound to `10.100.0.11` if you're\nfollowing along from the [Openstack][2] page. Open up a MySQL console as the\nroot user and run the following commands:\n\nImportant Note: Openstack has a convenient command `openstack-db` that does the\nfollowing for you, however, when it creates the user it uses `keystone` for a\npassword and doesn't restrict the hosts that can use that account (globally\navailable). It seems a bit excessive to me to have a script for those three\nMySQL commands:\n\n```\nCREATE DATABASE keystone;\nGRANT ALL ON keystone.* TO 'keystone'@'10.100.0.%' IDENTIFIED BY\n  'LongStrongUniquePasswordYouGenerated';\nFLUSH PRIVILEGES;\n```\n\nGenerate a token to be used as the 'admin' global token. When used this will\ngive you full admin credentials on the keystone service regardless of whether\nor not an admin exists.\n\nThis is needed for the initial administration of the keystone service once we\nget it up and running. I save it directly to the root user's bashrc file as it\nit'll be useful to have later on:\n\n```\necho export ADMIN_TOKEN=$(openssl rand -hex 32) \u003e /root/.bashrc\n```\n\nAfter logging off and back on make sure you have the token and copy this for\nuse in the configuration file later on:\n\n```\necho $ADMIN_TOKEN\nf35fb1205820907c2ddd1eb046f6ba2de1e5b729d6b7aedc5b0959156013f4a3\n```\n\nOpen up the keystone configuration file at: `/etc/keystone/keystone.conf` and\nreplace it with the following (changing appropriate variables for your\ninstallation such as `admin_token` and the MySQL password:\n\n```ini\n[DEFAULT]\nonready = keystone.common.systemd\nadmin_port = 35357\nadmin_token = ADMIN\nbind_host = 0.0.0.0\ncompute_port = 8774\npublic_port = 5000\n\n# Logging Options\ndebug = False\nsyslog_log_facility = LOG_USER\nuse_syslog = True\nverbose = False\n\n[sql]\nconnection = mysql://keystone:keystone@127.0.0.1/keystone\nidle_timeout = 200\n\n[identity]\ndriver = keystone.identity.backends.sql.Identity\n\n[catalog]\ndriver = keystone.catalog.backends.sql.Catalog\ntemplate_file = /etc/keystone/default_catalog.templates\n\n[token]\ndriver = keystone.token.backends.sql.Token\nexpiration = 7200\n\n[policy]\ndriver = keystone.policy.backends.rules.Policy\n\n[ec2]\ndriver = keystone.contrib.ec2.backends.sql.Ec2\n\n[ssl]\n#enable = True\n#certfile = /etc/keystone/ssl/certs/keystone.pem\n#keyfile = /etc/keystone/ssl/private/keystonekey.pem\n#ca_certs = /etc/keystone/ssl/certs/ca.pem\n#cert_required = True\n\n[signing]\ncertfile = /etc/keystone/ssl/certs/signing_cert.pem\nca_certs = /etc/keystone/ssl/certs/ca.pem\nca_password = SuperSecretCAPass\nkeyfile = /etc/keystone/ssl/private/signing_key.pem\nkey_size = 4096\nvalid_days = 3650\ntoken_format = PKI\n\n[ldap]\n# url = ldap://localhost\n# user = dc=Manager,dc=example,dc=com\n# password = None\n# suffix = cn=example,cn=com\n# use_dumb_member = False\n\n# user_tree_dn = ou=Users,dc=example,dc=com\n# user_objectclass = inetOrgPerson\n# user_id_attribute = cn\n# user_name_attribute = sn\n\n# tenant_tree_dn = ou=Groups,dc=example,dc=com\n# tenant_objectclass = groupOfNames\n# tenant_id_attribute = cn\n# tenant_member_attribute = member\n# tenant_name_attribute = ou\n\n# role_tree_dn = ou=Roles,dc=example,dc=com\n# role_objectclass = organizationalRole\n# role_id_attribute = cn\n# role_member_attribute = roleOccupant\n\n[filter:debug]\npaste.filter_factory = keystone.common.wsgi:Debug.factory\n\n[filter:token_auth]\npaste.filter_factory = keystone.middleware:TokenAuthMiddleware.factory\n\n[filter:admin_token_auth]\npaste.filter_factory = keystone.middleware:AdminTokenAuthMiddleware.factory\n\n[filter:xml_body]\npaste.filter_factory = keystone.middleware:XmlBodyMiddleware.factory\n\n[filter:json_body]\npaste.filter_factory = keystone.middleware:JsonBodyMiddleware.factory\n\n[filter:user_crud_extension]\npaste.filter_factory = keystone.contrib.user_crud:CrudExtension.factory\n\n[filter:crud_extension]\npaste.filter_factory = keystone.contrib.admin_crud:CrudExtension.factory\n\n[filter:ec2_extension]\npaste.filter_factory = keystone.contrib.ec2:Ec2Extension.factory\n\n[filter:s3_extension]\npaste.filter_factory = keystone.contrib.s3:S3Extension.factory\n\n[filter:url_normalize]\npaste.filter_factory = keystone.middleware:NormalizingFilter.factory\n\n[filter:stats_monitoring]\npaste.filter_factory = keystone.contrib.stats:StatsMiddleware.factory\n\n[filter:stats_reporting]\npaste.filter_factory = keystone.contrib.stats:StatsExtension.factory\n\n[app:public_service]\npaste.app_factory = keystone.service:public_app_factory\n\n[app:admin_service]\npaste.app_factory = keystone.service:admin_app_factory\n\n[pipeline:public_api]\npipeline = stats_monitoring url_normalize token_auth admin_token_auth xml_body json_body debug ec2_extension user_crud_extension public_service\n\n[pipeline:admin_api]\npipeline = stats_monitoring url_normalize token_auth admin_token_auth xml_body json_body debug stats_reporting ec2_extension s3_extension crud_extension admin_service\n\n[app:public_version_service]\npaste.app_factory = keystone.service:public_version_app_factory\n\n[app:admin_version_service]\npaste.app_factory = keystone.service:admin_version_app_factory\n\n[pipeline:public_version_api]\npipeline = stats_monitoring url_normalize xml_body public_version_service\n\n[pipeline:admin_version_api]\npipeline = stats_monitoring url_normalize xml_body admin_version_service\n\n[composite:main]\nuse = egg:Paste#urlmap\n/v2.0 = public_api\n/ = public_version_api\n\n[composite:admin]\nuse = egg:Paste#urlmap\n/v2.0 = admin_api\n/ = admin_version_api\n```\n\nPlease note that I haven't yet delved into the Compositions, Filters, Apps or\nPipelines and don't have a handle on what exactly they're doing. As far as\nconfiguration documentation goes there is much to be desired in Keystone...\n\nWe need to create a full [certificate authority][3] in\n`/etc/keystone/ssl/certs`. Ideally the CA itself would be signed by a higher CA\nof our control. A keystone SSL cert and key will also need to be created (and\nsigned by the CA).\n\nThe cert and key file should be placed at\n`/etc/keystone/ssl/certs/signing_cert.pem` and\n`/etc/keystone/ssl/private/signing_key.pem` respectively. You'll also want to\nensure the `[signing]` portion of the `keystone.conf` file reflects the actual\nsize of the key.\n\n```\nkeystone-manage db_sync\n```\n\nNow that we're configured let's get the service going...\n\n```\nsystemctl enable openstack-keystone.service\nsystemctl start openstack-keystone.service\n```\n\nYou should now have a running (though empty) keystone service. Let's test and\nmake sure it's responding nicely:\n\n```\nkeystone --token $ADMIN_TOKEN --endpoint http://10.100.0.13:35357/v2.0/ tenant-list\n```\n\n## Initial Setup\n\nNow that we have the service up and running we need to add users, roles,\ntenants, services and endpoints. A quick note on a couple of things here.\n'Tenant' is used as a means of logically grouping a bunch of users. A user can\nexist in multiple tenants and will only be aware of the resources within\ntenants that it has been granted access too unless that user is an admin of the\nspecial admin tenant which will automatically grant them full access to all\ntenants and their contents regardless of whether that user is in that tenant\nand as such care should be taken when granting that level of permissions.\n\nFirst since we'll be running a lot of commands and it can quickly become\nburdensome to repeatedly provide the endpoint and the token we're going to set\ntwo environment variables to make the access easier, then test to make sure we\ncan still access the service:\n\n```\nexport SERVICE_ENDPOINT=http://10.100.0.13:35357/v2.0/\nexport SERVICE_TOKEN=$ADMIN_TOKEN\nkeystone tenant-list\n```\n\nThere is another slightly annoying thing that can be easily worked around in\nOpenstack; Whenever you refer to a user, role, tenant, service, endpoint or\nother object you need to do so by it's ID and not it's friendly name. As such\nI've adapted a few helper methods to easily grab an object ID based on it's\nfriendly name. Add these too root's bash file and re-source it by either\nlogging out and back in or by running \". ~/.bashrc\".\n\nWe now need to create an admin user, role, tenant, and combine them all:\n\n```\nkeystone user-create --name admin --pass secret --email admin@email.net\n+----------+-------------------------------------------------------------------------------------------------------------------------+\n| Property |                                                          Value                                                          |\n+----------+-------------------------------------------------------------------------------------------------------------------------+\n|  email   |                                                     sam@stelfox.net                                                     |\n| enabled  |                                                           True                                                          |\n|    id    |                                             2e981959c1d54789a3ae6a88611cc0db                                            |\n|   name   |                                                          admin                                                          |\n| password | $6$rounds=40000$ZXGfdZeIcwVWGWvA$VLgWIRk7tM5OdEjeYZEnpANbjVO0SydvXZgK7UAlh0VED4S1dbCMWYxFNWgz4p.7Ni6Nmzw3FUtLx6MLYVEm21 |\n| tenantId |                                                                                                                         |\n+----------+-------------------------------------------------------------------------------------------------------------------------+\nkeystone role-create --name admin\n+----------+----------------------------------+\n| Property |              Value               |\n+----------+----------------------------------+\n|    id    | df6b8c5b5a0847259e1c78a34aae09d1 |\n|   name   |              admin               |\n+----------+----------------------------------+\nkeystone tenant-create --name admin\n+-------------+----------------------------------+\n|   Property  |              Value               |\n+-------------+----------------------------------+\n| description |                                  |\n|   enabled   |               True               |\n|      id     | adb579d87cc7438da8ddb6147ca69717 |\n|     name    |              admin               |\n+-------------+----------------------------------+\nkeystone user-role-add --user-id 2e981959c1d54789a3ae6a88611cc0db --role-id df6b8c5b5a0847259e1c78a34aae09d1 --tenant-id adb579d87cc7438da8ddb6147ca69717\n```\n\n## Potentially Old Information\n\n```\nget_tenant_id() {\n  echo `keystone tenant-list | grep $@ | awk '{ print $2 }'`\n}\n\nget_user_id() {\n  echo `keystone user-list | grep $@ | awk '{ print $2 }'`\n}\n\nget_role_id() {\n  echo `keystone role-list | grep $@ | awk '{ print $2 }'`\n}\n\nget_service_id() {\n  echo `keystone service-list | grep $@ | awk '{ print $2 }'`\n}\n```\n\n### Tenant Creation\n\n```\nkeystone tenant-create --name=admin\nkeystone tenant-create --name=service\n```\n\n### Role Creation\n\n```\nkeystone role-create --name admin\nkeystone role-create --name KeystoneAdmin\nkeystone role-create --name KeystoneServiceAdmin\nkeystone role-create --name Member\n```\n\n### User Creation\n\nGenerate a five long strong unique passwords for the admin user and each of the\nfour service accounts here. For this example I'm going to use password{1-5}. If\nyou use the same ones... you probably shouldn't be playing at this level... You\nwill need these later when setting up each of the services so note them down\nsomewhere secure.\n\n```\nkeystone user-create --name=admin --pass=password1 --email=admin@your-admin.cn\nkeystone user-create --name=nova --pass=password2 --tenant_id $(get_tenant_id service)\nkeystone user-create --name=glance --pass=password3 --tenant_id $(get_tenant_id service)\nkeystone user-create --name=quantum --pass=password4 --tenant_id $(get_tenant_id service)\nkeystone user-create --name=cinder --pass=password5 --tenant_id $(get_tenant_id service)\n```\n\n### Granting Roles\n\nGrant the admin user the `admin`, `KeystoneAdmin`, and `KeystoneServiceAdmin`\nroles within the admin tenant and the service accounts the admin role within\nthe service tenant.\n\n```\nkeystone user-role-add --user $(get_user_id admin) --role $(get_role_id admin) --tenant_id $(get_tenant_id admin)\nkeystone user-role-add --user $(get_user_id admin) --role $(get_role_id KeystoneAdmin) --tenant_id $(get_tenant_id admin)\nkeystone user-role-add --user $(get_user_id admin) --role $(get_role_id KeystoneServiceAdmin) --tenant_id $(get_tenant_id admin)\n\nkeystone user-role-add --tenant_id $(get_tenant_id service) --user $(get_user_id nova) --role $(get_role_id admin)\nkeystone user-role-add --tenant_id $(get_tenant_id service) --user $(get_user_id glance) --role $(get_role_id admin)\nkeystone user-role-add --tenant_id $(get_tenant_id service) --user $(get_user_id quantum) --role $(get_role_id admin)\nkeystone user-role-add --tenant_id $(get_tenant_id service) --user $(get_user_id cinder) --role $(get_role_id admin)\n```\n\n### Service Creation\n\n```\nkeystone service-create --name nova --type compute --description 'OpenStack Compute Service'\nkeystone service-create --name cinder --type volume --description 'OpenStack Volume Service'\nkeystone service-create --name glance --type image --description 'OpenStack Image Service'\nkeystone service-create --name keystone --type identity --description 'OpenStack Identity'\nkeystone service-create --name ec2 --type ec2 --description 'OpenStack EC2 service'\nkeystone service-create --name quantum --type network --description 'OpenStack Networking service'\n```\n\n### Endpoint Creation\n\nEnd points are the next thing to be defined, they require that a region be\nspecified. This is an arbitrary string that can be used to delinieate between\nservice regions that could be data centers, portions of a data-center or what\nhave you.\n\nSince I'm not working on an installation large enough to really need multiple\nregions this name is more or less useless for me so I chose to just use Primary\nas the region name.\n\n```\nkeystone endpoint-create --region Primary --service_id $(get_service_id compute) \\\n  --publicurl 'http://10.100.0.15:8774/v2/$(tenant_id)s' \\\n  --adminurl 'http://10.100.0.15:8774/v2/$(tenant_id)s' \\\n  --internalurl 'http://10.100.0.15:8774/v2/$(tenant_id)s'\n\nkeystone endpoint-create --region Primary --service_id $(get_service_id volume) \\\n  --publicurl 'http://10.100.0.16:8776/v1/$(tenant_id)s' \\\n  --adminurl 'http://10.100.0.16:8776/v1/$(tenant_id)s' \\\n  --internalurl 'http://10.100.0.16:8776/v1/$(tenant_id)s'\n\nkeystone endpoint-create --region Primary --service_id $(get_service_id image) \\\n  --publicurl 'http://10.100.0.17:9292/v2' \\\n  --adminurl 'http://10.100.0.17:9292/v2' \\\n  --internalurl 'http://10.100.0.17:9292/v2'\n\nkeystone endpoint-create --region Primary --service_id $(get_service_id identity) \\\n  --publicurl 'http://10.100.0.13:5000/v2.0' \\\n  --adminurl 'http://10.100.0.13:35357/v2.0' \\\n  --internalurl 'http://10.100.0.13:5000/v2.0'\n\nkeystone endpoint-create --region Primary --service_id $(get_service_id ec2) \\\n  --publicurl 'http://10.100.0.18:8773/services/Cloud' \\\n  --adminurl 'http://10.100.0.18:8773/services/Admin' \\\n  --internalurl 'http://10.100.0.18:8773/services/Cloud'\n\nkeystone endpoint-create --region Primary --service_id $(get_service_id network) \\\n  --publicurl 'http://10.100.0.19:9696/' \\\n  --adminurl 'http://10.100.0.19:9696/' \\\n  --internalurl 'http://10.100.0.19:9696/'\n```\n\n### Keystone Management\n\nWith keystone setup, the admin user in place, and endpoints defined you no\nlonger need to use the admin token to authenticate and manage the service. As a\nregular user export the following information into your bash shell, unset the\nadmin token and try to access the keystone user-list again:\n\n```\nexport OS_TENANT_NAME=admin\nexport OS_USERNAME=admin\nexport OS_PASSWORD=password1\nexport OS_AUTH_URL=http://10.100.0.13:5000/v2.0/\nunset ADMIN_TOKEN\nkeystone tenant-list\n+----------------------------------+---------+---------+\n|                id                |   name  | enabled |\n+----------------------------------+---------+---------+\n| bb9509fd9dea40f5b58d720aaaa15044 | service | True    |\n| cbabea52a4be417998b266e43280ef35 | admin   | True    |\n+----------------------------------+---------+---------+\n```\n\nYou can also specify those options via the command line if you don't want to\nset the  environment variables like so:\n\n```\nkeystone --os_username admin --os_password password1 --os_tenant_name admin \\\n  --os_auth_url http://10.100.0.13:5000/v2.0/ tenant-list\n+----------------------------------+---------+---------+\n|                id                |   name  | enabled |\n+----------------------------------+---------+---------+\n| bb9509fd9dea40f5b58d720aaaa15044 | service | True    |\n| cbabea52a4be417998b266e43280ef35 | admin   | True    |\n+----------------------------------+---------+---------+\n```\n\n## Future Steps\n\nThere are some features that I'd really like to take a look at getting as I\nfeel they will improve the quality, security or reliability of the service in\ngeneral. These features that I haven't documented here yet are as follows:\n\n* LDAP Authentication Backend\n  * With Memcached queries\n* SSL\n* Detailed policy.json evaluation\n  * Breaking out permissions into more explicit roles\n  * What can I actually accomplish with this file?\n\n[1]: {{\u003c ref \"./mysql.md\" \u003e}}\n[2]: {{\u003c ref \"./openstack.md\" \u003e}}\n[3]: {{\u003c ref \"./certificate_authority.md\" \u003e}}\n","created_at":-62135596800,"fuzzy_word_count":2e3,"path":"/notes/openstack-keystone/","published_at":1540945455,"reading_time":10,"tags":null,"title":"Openstack Keystone","type":"notes","updated_at":1540945455,"weight":0,"word_count":1927},{"cid":"8e54c6dde28be4dee5c59553c841a162a67b7cb2","content":"\n***Note: This page is quite old and is likely out of date. My opinions may have\nalso changed dramatically since this was written. It is here as a reference\nuntil I get around to updating it.***\n\n## Setup\n\nInstall the packages `nikto`, `openvas-scanner`, `openvas-manager`, and\n`openvas-client`.\n\n## Scanner\n\nAs root run openvas-mkcert like the following:\n\n```\n[root@localhost ~]# openvas-mkcert\n-------------------------------------------------------------------------------\n                        Creation of the OpenVAS SSL Certificate\n-------------------------------------------------------------------------------\n\nThis script will now ask you the relevant information to create the SSL certificate of OpenVAS.\nNote that this information will *NOT* be sent to anybody (everything stays local), but anyone with the ability to connect to your OpenVAS daemon will be able to retrieve this information.\n\nCA certificate life time in days [1460]: 3650\nServer certificate life time in days [365]: \nYour country (two letter code) [DE]: US\nYour state or province name [none]: Some State\nYour location (e.g. town) [Berlin]: Some City\nYour organization [OpenVAS Users United]: Example.net\n\n-------------------------------------------------------------------------------\n                        Creation of the OpenVAS SSL Certificate\n-------------------------------------------------------------------------------\n\nCongratulations. Your server certificate was properly created.\n\nThe following files were created:\n\nCertification authority:\n   Certificate = /etc/pki/openvas/CA/cacert.pem\n   Private key = /etc/pki/openvas/private/CA/cakey.pem\n\nOpenVAS Server :\n    Certificate = /etc/pki/openvas/CA/servercert.pem\n    Private key = /etc/pki/openvas/private/CA/serverkey.pem\n\nPress [ENTER] to exit\n\n[root@localhost ~]#\n```\n\nAs root get the NVT feed by running the command `openvas-nvt-sync`.\n\nCreate a certificate for the OpenVAS-Manager to connect to the scanner like so:\n\n```\n[root@localhost ~]# openvas-mkcert-client -i\nThis script will now ask you the relevant information to create the SSL client certificates for OpenVAS.\n\nClient certificates life time in days [365]: 3650\nYour country (two letter code) [DE]: US\nYour state or province name [none]: Some State\nYour location (e.g. town) [Berlin]: Some City\nYour organization [OpenVAS Users United]: Example.net\nYour organizational unit [none]: \n\n**********\nWe are going to ask you some question for each client certificate. \n\nIf some question has a default answer, you can force an empty answer by entering a single dot '.'\n\n*********\nOpenVAS username for the new user: om\nClient certificates life time in days [3650]: \nYour country (two letter code) [DE]: US\nYour state or province name [none]: Some State\nYour location (e.g. town) [Berlin]: Some City\nYour organization [OpenVAS Users United]: Example.net\nOrganization unit []: \ne-Mail []: \nGenerating RSA private key, 1024 bit long modulus\n...................................++++++\n.................................................++++++\ne is 65537 (0x10001)\nYou are about to be asked to enter information that will be incorporated\ninto your certificate request.\nWhat you are about to enter is what is called a Distinguished Name or a DN.\nThere are quite a few fields but you can leave some blank\nFor some fields there will be a default value,\nIf you enter '.', the field will be left blank.\n-----\nCountry Name (2 letter code) [DE]:\nState or Province Name (full name) [Some-State]:\nLocality Name (eg, city) []:\nOrganization Name (eg, company) [Internet Widgits Pty Ltd]:\nOrganizational Unit Name (eg, section) []:\nCommon Name (eg, your name or your server's hostname) []:\nEmail Address []:\nUsing configuration from /tmp/openvas-mkcert-client.26805/stdC.cnf\nCheck that the request matches the signature\nSignature ok\nThe Subject's Distinguished Name is as follows\ncountryName           :PRINTABLE:'US'\nstateOrProvinceName   :PRINTABLE:'Some State'\nlocalityName          :PRINTABLE:'Some City'\norganizationName      :PRINTABLE:'Example.net'\ncommonName            :PRINTABLE:'om'\nCertificate is to be certified until Aug 20 17:24:00 2021 GMT (3650 days)\n\nWrite out database with 1 new entries\nData Base Updated\n\nUser rules\n\n----------\nopenvassd has a rules system which allows you to restrict the hosts that  has the right to test.\nFor instance, you may want him to be able to scan his own host only.\nPlease see the openvas-adduser(8) man page for the rules syntax.\nEnter the rules for this user, and hit ctrl-D once you are done:\n(the user can have an empty rules set)\nUser om added to OpenVAS.\n[root@localhost ~]#\n```\n\nSetup GPG trust of the nvt feed. You'll need to create a GPG key for openvas,\ndownload and verify the transfer integrity key, import \u0026 sign the transfer\nintegrity key and turn on plugin signature checking. This can be done with the\nfollowing set of commands:\n\n```\n[root@localhost ~]# mkdir /etc/openvas/gnupg\n[root@localhost ~]# gpg --homedir=/etc/openvas/gnupg --gen-key\ngpg: WARNING: unsafe permissions on homedir `/etc/openvas/gnupg'\ngpg (GnuPG) 1.4.11; Copyright (C) 2010 Free Software Foundation, Inc.\nThis is free software: you are free to change and redistribute it.\nThere is NO WARRANTY, to the extent permitted by law.\n\ngpg: keyring `/etc/openvas/gnupg/secring.gpg' created\ngpg: keyring `/etc/openvas/gnupg/pubring.gpg' created\nPlease select what kind of key you want:\n   (1) RSA and RSA (default)\n   (2) DSA and Elgamal\n   (3) DSA (sign only)\n   (4) RSA (sign only)\nYour selection? 1\nRSA keys may be between 1024 and 4096 bits long.\nWhat keysize do you want? (2048) \nRequested keysize is 2048 bits\nPlease specify how long the key should be valid.\n         0 = key does not expire\n      \u003cn\u003e  = key expires in n days\n      \u003cn\u003ew = key expires in n weeks\n      \u003cn\u003em = key expires in n months\n      \u003cn\u003ey = key expires in n years\nKey is valid for? (0) \nKey does not expire at all\nIs this correct? (y/N) y\n\nYou need a user ID to identify your key; the software constructs the user ID\nfrom the Real Name, Comment and Email Address in this form:\n    \"Heinrich Heine (Der Dichter) \u003cheinrichh@duesseldorf.de\u003e\"\n\nReal name: OpenVAS Plugin Verifier\nEmail address: \nComment: \nYou selected this USER-ID:\n    \"OpenVAS Plugin Verifier\"\n\nChange (N)ame, (C)omment, (E)mail or (O)kay/(Q)uit? o\nYou need a Passphrase to protect your secret key.\n\nYou don't want a passphrase - this is probably a *bad* idea!\nI will do it anyway.  You can change your passphrase at any time,\nusing this program with the option \"--edit-key\".\n\nWe need to generate a lot of random bytes. It is a good idea to perform\nsome other action (type on the keyboard, move the mouse, utilize the\ndisks) during the prime generation; this gives the random number\ngenerator a better chance to gain enough entropy.\n+++++\n....+++++\nWe need to generate a lot of random bytes. It is a good idea to perform\nsome other action (type on the keyboard, move the mouse, utilize the\ndisks) during the prime generation; this gives the random number\ngenerator a better chance to gain enough entropy.\n\nNot enough random bytes available.  Please do some other work to give\nthe OS a chance to collect more entropy! (Need 67 more bytes)\n....+++++\n.+++++\ngpg: /etc/openvas/gnupg/trustdb.gpg: trustdb created\ngpg: key ######## marked as ultimately trusted\npublic and secret key created and signed.\n\ngpg: checking the trustdb\ngpg: 3 marginal(s) needed, 1 complete(s) needed, PGP trust model\ngpg: depth: 0  valid:   1  signed:   0  trust: 0-, 0q, 0n, 0m, 0f, 1u\npub   2048R/######## 2011-08-24\n      Key fingerprint = #### #### #### #### ####  #### #### #### #### ####\nuid                  OpenVAS Plugin Verifier\nsub   2048R/######## 2011-08-24\n\n[root@localhost ~]# wget http://www.openvas.org/OpenVAS_TI.asc\n--2011-08-23 22:38:34--  http://www.openvas.org/OpenVAS_TI.asc\nResolving www.openvas.org... 78.47.251.62\nConnecting to www.openvas.org|78.47.251.62|:80... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 1673 (1.6K) [text/plain]\nSaving to: “OpenVAS_TI.asc”\n\n100%[==============================================================\u003e] 1,673       --.-K/s   in 0s      \n\n####-##-## ##:##:## (152 MB/s) - “OpenVAS_TI.asc” saved [1673/1673]\n\n[root@localhost ~]# gpg --homedir=/etc/openvas/gnupg --import OpenVAS_TI.asc\ngpg: WARNING: unsafe permissions on homedir `/etc/openvas/gnupg'\ngpg: key 48DB4530: public key \"OpenVAS Transfer Integrity\" imported\ngpg: Total number processed: 1\ngpg:               imported: 1\n[root@localhost ~]# gpg --homedir=/etc/openvas/gnupg --lsign-key 48DB4530\ngpg: WARNING: unsafe permissions on homedir `/etc/openvas/gnupg'\n\npub  1024D/48DB4530  created: 2007-11-05  expires: never       usage: SC  \n                     trust: unknown       validity: unknown\nsub  2048g/70610CFB  created: 2007-11-05  expires: never       usage: E   \n[ unknown] (1). OpenVAS Transfer Integrity\npub  1024D/48DB4530  created: 2007-11-05  expires: never       usage: SC  \n                     trust: unknown       validity: unknown\n Primary key fingerprint: C3B4 68D2 288C 68B9 D526  4522 4847 9FF6 48DB 4530\n\n     OpenVAS Transfer Integrity\n\nAre you sure that you want to sign this key with your\nkey \"OpenVAS Plugin Verifier\" (########)\n\nThe signature will be marked as non-exportable.\n\nReally sign? (y/N) y\n```\n\nAlright now we need to start up the scanner service for the first time. It will\ntake a LONG time the first time and a significant amount of time everytime\nthere are updates to the NVT feed, though not nearly as long for the updates.\n\nIt takes so long that the service script will think that it failed. It HASN'T,\nit's still doing it magic in the background, it's easiest to just watch it go\nwith top. When it finally settles down it'll be ready for the next step.\n\n## OpenVAS Manager\n\nNow we need to build the OpenVAS manager database. I'm not entirely sure what\nthe manager gets us as right now we have a working scanner (albeit without\nusers). I know it provides alternative connection methods such as an XML based\nAPI for web services. Hey it might be useful it might not and there is some\nfunnyness but it's part of the full installation so I'm going to include\ndirections.\n\nThat touch at the beginning? Yeah if you don't create that file you'll get a\n\"Aborted (core dumped)\" message when you try and rebuild the database. Having\nit in existence though allows it to just run smoothly. Pretty big bug if you\nask me but one with an easy work around.\n\n```\n[root@localhost ~]# touch /var/lib/openvas/mgr/tasks.db\n[root@localhost ~]# openvasmd --rebuild\n```\n\n## OpenVAS Administrator ##\n\nThis is getting it's own special section as it isn't available via the standard\nFedora package repositories annoyingly enough. For the time being I'm going to\nleave this alone.\n\n## Creating Users ##\n\nI personally prefer using certificate for authentication against the server.\nEasier than typing in a password every time and more secure in some ways. To\ncreate a user that authenticates with a certificate you need to use the\n`openvas-mkcert-client` command like so:\n\n```\n[root@localhost ~]# openvas-mkcert-client\nThis script will now ask you the relevant information to create the SSL client certificates for OpenVAS.\n\nClient certificates life time in days [365]:\nYour country (two letter code) [DE]: US\nYour state or province name [none]: Some State\nYour location (e.g. town) [Berlin]: Some City\nYour organization [none]: Example.net\nYour organizational unit [none]:\n**********\nWe are going to ask you some question for each client certificate.\n\nIf some question has a default answer, you can force an empty answer by entering a single dot '.'\n\n*********\nOpenVAS username for the new user: \u003cusername\u003e\nClient certificates life time in days [365]:\nCountry (two letter code) [US]:\nState or province name [Some State]:\nLocation (e.g. town) [Some City]:\nOrganization [Example.net]:\nOrganization unit []:\ne-Mail []:\nGenerating RSA private key, 1024 bit long modulus\n....++++++\n............................................++++++\ne is 65537 (0x10001)\nYou are about to be asked to enter information that will be incorporated\ninto your certificate request.\nWhat you are about to enter is what is called a Distinguished Name or a DN.\nThere are quite a few fields but you can leave some blank\nFor some fields there will be a default value,\nIf you enter '.', the field will be left blank.\n-----\nCountry Name (2 letter code) [DE]:\nState or Province Name (full name) [Some-State]:\nLocality Name (eg, city) []:\nOrganization Name (eg, company) [Internet Widgits Pty Ltd]:\nOrganizational Unit Name (eg, section) []:\nCommon Name (eg, your name or your server's hostname) []:\nEmail Address []:\nUsing configuration from /tmp/openvas-mkcert-client.15980/stdC.cnf\nCheck that the request matches the signature\nSignature ok\nThe Subject's Distinguished Name is as follows\ncountryName           :PRINTABLE:'US'\nstateOrProvinceName   :PRINTABLE:'Some State'\nlocalityName          :PRINTABLE:'Some City'\norganizationName      :PRINTABLE:'Example.net'\ncommonName            :PRINTABLE:'\u003cusername\u003e'\nCertificate is to be certified until Aug 23 18:10:38 2012 GMT (365 days)\n\nWrite out database with 1 new entries\nData Base Updated\n\nUser rules\n\n----------\nopenvassd has a rules system which allows you to restrict the hosts that  has the right to test.\nFor instance, you may want him to be able to scan his own host only.\nPlease see the openvas-adduser(8) man page for the rules syntax.\nEnter the rules for this user, and hit ctrl-D once you are done:\n(the user can have an empty rules set)\nUser sstelfox added to OpenVAS.\nYour client certificates are in /tmp/openvas-mkcert-client.15980 .\nYou will have to copy them by hand.\n[root@localhost ~]# chown -R \u003cusername\u003e:\u003cusername\u003e /tmp/openvas-mkcert-client.15980/\n[root@localhost ~]# mv /tmp/openvas-mkcert-client.15980/ /home/\u003cusername\u003e/\n[root@localhost ~]# cp /etc/pki/openvas/CA/cacert.pem /home/\u003cusername\u003e/openvas-mkcert-client.15980/\n```\n\n## Verification\n\nSave the following script and make it executable.\n\n```sh\n#!/bin/sh\n\n# OpenVAS\n# $Id$\n# Description: Script for checking completeness and readiness\n# of OpenVAS.\n#\n# Authors:\n# Jan-Oliver Wagner \u003cjan-oliver.wagner@greenbone.net\u003e\n# Michael Wiegand \u003cmichael.wiegand@greenbone.net\u003e\n#\n# Copyright:\n# Copyright (C) 2011 Greenbone Networks GmbH\n#\n# This program is free software; you can redistribute it and/or modify\n# it under the terms of the GNU General Public License version 2,\n# or at your option any later version, as published by the\n# Free Software Foundation\n#\n# This program is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with this program; if not, write to the Free Software\n# Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n\nLOG=/tmp/openvas-check-setup.log\nCHECKVERSION=2.1.3\n\nif [ \"$1\" = \"--server\" -o \"$2\" = \"--server\" ]\nthen\n  MODE=\"server\"\nelse\n  MODE=\"desktop\"\nfi\n\nif [ \"$1\" = \"--v5\" -o \"$2\" = \"--v5\" ]\nthen\n  VER=\"5\"\n  SCANNER_MAJOR=\"3\"\n  SCANNER_MINOR=\"3\"\n  MANAGER_MAJOR=\"3\"\n  MANAGER_MINOR=\"0\"\n  ADMINISTRATOR_MAJOR=\"1\"\n  ADMINISTRATOR_MINOR=\"1\"\n  GSA_MAJOR=\"3\"\n  GSA_MINOR=\"0\"\n  CLI_MAJOR=\"1\"\n  CLI_MINOR=\"1\"\n  GSD_MAJOR=\"1\"\n  GSD_MINOR=\"2\"\nelse\n  VER=\"4\"\n  SCANNER_MAJOR=\"3\"\n  SCANNER_MINOR=\"2\"\n  MANAGER_MAJOR=\"2\"\n  MANAGER_MINOR=\"0\"\n  ADMINISTRATOR_MAJOR=\"1\"\n  ADMINISTRATOR_MINOR=\"1\"\n  GSA_MAJOR=\"2\"\n  GSA_MINOR=\"0\"\n  CLI_MAJOR=\"1\"\n  CLI_MINOR=\"1\"\n  GSD_MAJOR=\"1\"\n  GSD_MINOR=\"2\"\nfi\n\necho \"openvas-check-setup $CHECKVERSION\"\necho \"  Test completeness and readiness of OpenVAS-$VER\"\necho \"\"\necho \"  Please report us any non-detected problems and\"\necho \"  help us to improve this check routine:\"\necho \"  http://lists.wald.intevation.org/mailman/listinfo/openvas-discuss\"\necho \"\"\necho \"  Send us the log-file ($LOG) to help analyze the problem.\"\necho \"\"\n\nif [ \"$MODE\" = \"desktop\" ]\nthen\n  echo \"  Use the parameter --server to skip checks for client tools\"\n  echo \"  like GSD and OpenVAS-CLI.\"\n  echo \"\"\nfi\n\nlog_and_print ()\n{\n  echo \"       \" $1\n  echo \"       \" $1 \u003e\u003e $LOG\n}\n\ncheck_failed ()\n{\n  echo \"\"\n  echo \" ERROR: Your OpenVAS-$VER installation is not yet complete!\"\n  echo \"\"\n  echo \"Please follow the instructions marked with FIX above and run this\"\n  echo \"script again.\"\n  echo \"\"\n  echo \"If you think this result is wrong, please report your observation\"\n  echo \"and help us to improve this check routine:\"\n  echo \"http://lists.wald.intevation.org/mailman/listinfo/openvas-discuss\"\n  echo \"Please attach the log-file ($LOG) to help us analyze the problem.\"\n  echo \"\"\n  exit 1\n}\n\n# LOG start\necho \"openvas-check-setup $CHECKVERSION\" \u003e $LOG\necho \"  Mode:  $MODE\" \u003e\u003e $LOG\necho \"  Date: \" `date -R` \u003e\u003e $LOG\necho \"\" \u003e\u003e $LOG\n\necho \"Step 1: Checking OpenVAS Scanner ... \"\n\necho \"Checking for old OpenVAS Scanner \u003c= 2.0 ...\" \u003e\u003e $LOG\nopenvasd -V \u003e\u003e $LOG 2\u003e\u00261\nif [ $? -eq 0 ]: then\n  log_and_print \"ERROR: Old version of OpenVAS Scanner detected.\"\n  log_and_print \"FIX: Please remove the installation of the old OpenVAS Scanner (openvasd).\"\n  check_failed\nfi\necho \"\" \u003e\u003e $LOG\n\necho \"Checking presence of OpenVAS Scanner ...\" \u003e\u003e $LOG\nopenvassd --version \u003e\u003e $LOG 2\u003e\u00261\nif [ $? -ne 0 ]; then\n  log_and_print \"ERROR: No OpenVAS Scanner (openvassd) found.\"\n  log_and_print \"FIX: Please install OpenVAS Scanner.\"\n  check_failed\nfi\necho \"\" \u003e\u003e $LOG\n\necho \"Checking OpenVAS Scanner version ...\" \u003e\u003e $LOG\n\nVERSION=`openvassd --version | head -1 | sed -e \"s/OpenVAS Scanner //\"`\n\nif [ `echo $VERSION | grep \"^$SCANNER_MAJOR\\.$SCANNER_MINOR\" | wc -l` -ne \"1\" ]; then\n  log_and_print \"ERROR: OpenVAS Scanner too old or too new: $VERSION\"\n  log_and_print \"FIX: Please install OpenVAS Scanner $SCANNER_MAJOR.$SCANNER_MINOR.\"\n  check_failed\nfi\necho \"\" \u003e\u003e $LOG\n\nlog_and_print \"OK: OpenVAS Scanner is present in version $VERSION.\"\n\nopenvassd -s \u003e\u003e $LOG\n\necho \"Checking OpenVAS Scanner CA cert ...\" \u003e\u003e $LOG\nCAFILE=`openvassd -s | grep ca_file | sed -e \"s/^ca_file = //\"`\nif [ ! -e $CAFILE ]; then\n  log_and_print \"ERROR: No CA certificate file of OpenVAS Scanner found.\"\n  log_and_print \"FIX: Run 'openvas-mkcert'.\"\n  check_failed\nfi\necho \"\" \u003e\u003e $LOG\n\nlog_and_print \"OK: OpenVAS Scanner CA Certificate is present as $CAFILE.\"\n\necho \"Checking NVT collection ...\" \u003e\u003e $LOG\n\nPLUGINSFOLDER=`openvassd -s | grep plugins_folder | sed -e \"s/^plugins_folder = //\"`\nif [ ! -d $PLUGINSFOLDER ]; then\n  log_and_print \"ERROR: Directory containing the NVT collection not found.\"\n  log_and_print \"FIX: Run a synchronization script like openvas-nvt-sync or greenbone-nvt-sync.\"\n  check_failed\nfi\n\nOLDPLUGINSFOLDER=`echo \"$PLUGINSFOLDER\" | grep -q -v \"/var/\" 2\u003e\u00261`\nif [ $? -eq 0 ]: then\n  CONFFILE=`openvassd -s | grep config_file | sed -e \"s/^config_file = //\"`\n  log_and_print \"ERROR: Your OpenVAS Scanner configuration seems to be from a pre-OpenVAS-4 installation and contains non-FHS compliant paths.\"\n  log_and_print \"FIX: Delete your OpenVAS Scanner Configuration file ($CONFFILE).\"\n  check_failed\nfi\n\nNVTCOUNT=`find $PLUGINSFOLDER -name \"*nasl\" | wc -l`\nif [ $NVTCOUNT -lt 10 ]; then\n  log_and_print \"ERROR: The NVT collection is very small.\"\n  log_and_print \"FIX: Run a synchronization script like openvas-nvt-sync or greenbone-nvt-sync.\"\n  check_failed\nfi\necho \"\" \u003e\u003e $LOG\n\nlog_and_print \"OK: NVT collection in $PLUGINSFOLDER contains $NVTCOUNT NVTs.\"\n\necho \"Checking status of signature checking in OpenVAS Scanner ...\" \u003e\u003e $LOG\nNOSIGCHECK=`openvassd -s | grep nasl_no_signature_check | sed -e \"s/^nasl_no_signature_check = //\"`\n\nif [ $NOSIGCHECK != \"no\" ]; then\n  log_and_print \"WARNING: Signature checking of NVTs is not enabled in OpenVAS Scanner.\"\n  log_and_print \"SUGGEST: Enable signature checking (see http://www.openvas.org/trusted-nvts.html).\"\nelse\n  log_and_print \"OK: Signature checking of NVTs is enabled in OpenVAS Scanner.\"\nfi\n\necho \"\" \u003e\u003e $LOG\necho \"Step 2: Checking OpenVAS Manager ... \"\necho \"Checking presence of OpenVAS Manager ...\" \u003e\u003e $LOG\nopenvasmd --version \u003e\u003e $LOG 2\u003e\u00261\n\nif [ $? -ne 0 ]; then\n  log_and_print \"ERROR: No OpenVAS Manager (openvasmd) found.\"\n  log_and_print \"FIX: Please install OpenVAS Manager.\"\n  check_failed\nfi\necho \"\" \u003e\u003e $LOG\n\nVERSION=`openvasmd --version | head -1 | sed -e \"s/OpenVAS Manager //\"`\n\nif [ `echo $VERSION | grep \"^$MANAGER_MAJOR\\.$MANAGER_MINOR\" | wc -l` -ne \"1\" ]; then\n  log_and_print \"ERROR: OpenVAS Manager too old or too new: $VERSION\"\n  log_and_print \"FIX: Please install OpenVAS Manager $MANAGER_MAJOR.$MANAGER_MINOR.\"\n  check_failed\nfi\necho \"\" \u003e\u003e $LOG\n\nlog_and_print \"OK: OpenVAS Manager is present in version $VERSION.\"\n\necho \"Checking OpenVAS Manager client certificate ...\" \u003e\u003e $LOG\nCERTDIR=`dirname $CAFILE`\nCLIENTCERTFILE=\"$CERTDIR/clientcert.pem\"\n\nif [ ! -e $CLIENTCERTFILE ]: then\n  log_and_print \"ERROR: No client certificate file of OpenVAS Manager found.\"\n  log_and_print \"FIX: Run 'openvas-mkcert-client -n om -i'\"\n  check_failed\nfi\necho \"\" \u003e\u003e $LOG\n\nlog_and_print \"OK: OpenVAS Manager client certificate is present as $CLIENTCERTFILE.\"\n\necho \"Checking OpenVAS Manager database ...\" \u003e\u003e $LOG\n# Guess openvas state dir from $PLUGINSFOLDER\nSTATEDIR=`dirname $PLUGINSFOLDER`\nTASKSDB=\"$STATEDIR/mgr/tasks.db\"\n\nif [ ! -e $TASKSDB ]; then\n  log_and_print \"ERROR: No OpenVAS Manager database found. (Tried: $TASKSDB)\"\n  log_and_print \"FIX: Run 'openvasmd --rebuild' while OpenVAS Scanner is running.\"\n  check_failed\nfi\necho \"\" \u003e\u003e $LOG\n\nlog_and_print \"OK: OpenVAS Manager database found in $TASKSDB.\"\n\necho \"Checking access rights of OpenVAS Manager database ...\" \u003e\u003e $LOG\nTASKSDBPERMS=`stat -c \"%a\" \"$TASKSDB\"`\nif [ \"$TASKSDBPERMS\" != \"600\" ]; then\n  log_and_print \"ERROR: The access rights of the OpenVAS Manager database are incorrect.\"\n  log_and_print \"FIX: Run 'chmod 600 $TASKSDB'.\"\n  check_failed\nfi\necho \"\" \u003e\u003e $LOG\n\nlog_and_print \"OK: Access rights for the OpenVAS Manager database are correct.\"\n\necho \"Checking sqlite3 presence ...\" \u003e\u003e $LOG\nSQLITE3=`type sqlite3 2\u003e /dev/null`\nif [ $? -ne 0 ]: then\n  log_and_print \"WARNING: Could not find sqlite3 binary, extended manager checks of the OpenVAS Manager installation are disabled.\"\n  log_and_print \"SUGGEST: Install sqlite3.\"\n  HAVE_SQLITE=0\nelse\n  log_and_print \"OK: sqlite3 found, extended checks of the OpenVAS Manager installation enabled.\"\n  HAVE_SQLITE=1\nfi\necho \"\" \u003e\u003e $LOG\n\nif [ $HAVE_SQLITE -eq 1 ]; then\n  echo \"Checking OpenVAS Manager database revision ...\" \u003e\u003e $LOG\n  TASKSDBREV=`sqlite3 $TASKSDB \"select value from meta where name='database_version';\"`\n\n  if [ -z $TASKSDBREV ]; then\n    log_and_print \"ERROR: Could not determine database revision, database corrupt or in invalid format.\"\n    log_and_print \"FIX: Delete database at $TASKSDB and rebuild it.\"\n    check_failed\n  else\n    log_and_print \"OK: OpenVAS Manager database is at revision $TASKSDBREV.\"\n  fi\n\n  echo \"Checking database revision expected by OpenVAS Manager ...\" \u003e\u003e $LOG\n  MANAGERDBREV=`openvasmd --version | grep \"Manager DB revision\" | sed -e \"s/.*\\ //\"`\n\n  if [ -z $MANAGERDBREV ]; then\n    log_and_print \"ERROR: Could not determine database revision expected by OpenVAS Manager.\"\n    log_and_print \"FIX: Ensure OpenVAS Manager is installed correctly.\"\n    check_failed\n  else\n    log_and_print \"OK: OpenVAS Manager expects database at revision $MANAGERDBREV.\"\n  fi\n\n  if [ $TASKSDBREV -lt $MANAGERDBREV ]; then\n    log_and_print \"ERROR: Database schema is out of date.\"\n    log_and_print \"FIX: Run 'openvasmd --migrate'.\"\n    check_failed\n  else\n    log_and_print \"OK: Database schema is up to date.\"\n  fi\n\n  echo \"Checking OpenVAS Manager database (NVT data) ...\" \u003e\u003e $LOG\n  DBNVTCOUNT=`sqlite3 $TASKSDB \"select count(*) from nvts;\"`\n\n  if [ $DBNVTCOUNT -lt 20000 ]; then\n    log_and_print \"ERROR: The number of NVTs in the OpenVAS Manager database is too low.\"\n    log_and_print \"FIX: Make sure OpenVAS Scanner is running with an up-to-date NVT collection and run 'openvasmd --rebuild'.\"\n    check_failed\n  else\n    log_and_print \"OK: OpenVAS Manager database contains information about $DBNVTCOUNT NVTs.\"\n  fi\nfi\n\necho \"Checking xsltproc presence ...\" \u003e\u003e $LOG\nXSLTPROC=`type xsltproc 2\u003e /dev/null`\n\nif [ $? -ne 0 ]; then\n  log_and_print \"WARNING: Could not find xsltproc binary, most report formats will not work.\"\n  log_and_print \"SUGGEST: Install xsltproc.\"\nelse\n  log_and_print \"OK: xsltproc found.\"\nfi\n\necho \"\" \u003e\u003e $LOG\necho \"Step 3: Checking OpenVAS Administrator ... \"\necho \"Checking presence of OpenVAS Administrator ...\" \u003e\u003e $LOG\nopenvasad --version \u003e\u003e $LOG 2\u003e\u00261\n\nif [ $? -ne 0 ]; then\n  log_and_print \"ERROR: No OpenVAS Administrator (openvasad) found.\"\n  log_and_print \"FIX: Please install OpenVAS Administrator.\"\n  check_failed\nfi\necho \"\" \u003e\u003e $LOG\n\nVERSION=`openvasad --version | head -1 | sed -e \"s/OpenVAS Administrator //\"`\n\nif [ `echo $VERSION | grep \"^$ADMINISTRATOR_MAJOR\\.$ADMINISTRATOR_MINOR\" | wc -l` -ne \"1\" ]; then\n  log_and_print \"ERROR: OpenVAS Administrator too old or too new: $VERSION\"\n  log_and_print \"FIX: Please install OpenVAS Administrator $ADMINISTRATOR_MAJOR.$ADMINISTRATOR_MINOR.\"\n  check_failed\nfi\necho \"\" \u003e\u003e $LOG\n\nlog_and_print \"OK: OpenVAS Administrator is present in version $VERSION.\"\n\necho \"Checking if users exist ...\" \u003e\u003e $LOG\nUSERCOUNT=`openvasad -c \"list_users\" | sed -e \"/^$/d\" | wc -l`\nif [ $USERCOUNT -eq 0 ]; then\n  log_and_print \"ERROR: No users found. You need to create at least one user to log in.\"\n  log_and_print \"FIX: Create a user using 'openvasad -c 'add_user' -n \u003cname\u003e'\"\n  check_failed\nelse\n  log_and_print \"OK: At least one user exists.\"\nfi\necho \"\" \u003e\u003e $LOG\n\necho \"Checking if at least one admin user exists ...\" \u003e\u003e $LOG\nADMINEXISTS=`ls $STATEDIR/users/*/isadmin 2\u003e /dev/null`\nif [ $? -ne 0 ]; then\n  log_and_print \"ERROR: No admin user found. You need to create at least one admin user to log in.\"\n  log_and_print \"FIX: Create a user using 'openvasad -c 'add_user' -n \u003cname\u003e -r Admin'\"\n  check_failed\nelse\n  log_and_print \"OK: At least one admin user exists.\"\nfi\n\necho \"\" \u003e\u003e $LOG\necho \"Step 4: Checking Greenbone Security Assistant (GSA) ... \"\necho \"Checking presence of Greenbone Security Assistant ...\" \u003e\u003e $LOG\ngsad --version \u003e\u003e $LOG 2\u003e\u00261\n\nif [ $? -ne 0 ]; then\n  log_and_print \"ERROR: No Greenbone Security Assistant (gsad) found.\"\n  log_and_print \"FIX: Please install Greenbone Security Assistant.\"\n  check_failed\nfi\necho \"\" \u003e\u003e $LOG\n\nVERSION=`gsad --version | head -1 | sed -e \"s/Greenbone Security Assistant //\"`\n\nif [ `echo $VERSION | grep \"^$GSA_MAJOR\\.$GSA_MINOR\" | wc -l` -ne \"1\" ]; then\n  log_and_print \"ERROR: Greenbone Security Assistant too old or too new: $VERSION\"\n  log_and_print \"FIX: Please install Greenbone Security Assistant $GSA_MAJOR.$GSA_MINOR.\"\n  check_failed\nfi\necho \"\" \u003e\u003e $LOG\n\nlog_and_print \"OK: Greenbone Security Assistant is present in version $VERSION.\"\n\necho \"Step 5: Checking OpenVAS CLI ... \"\n\nif [ \"$MODE\" != \"server\" ]; then\n  echo \"Checking presence of OpenVAS CLI ...\" \u003e\u003e $LOG\n  omp --version \u003e\u003e $LOG 2\u003e\u00261\n\n  if [ $? -ne 0 ]; then\n    log_and_print \"ERROR: No OpenVAS CLI (omp) found.\"\n    log_and_print \"FIX: Please install OpenVAS CLI.\"\n    check_failed\n  fi\n  echo \"\" \u003e\u003e $LOG\n\n  VERSION=`omp --version | head -1 | sed -e \"s/OMP Command Line Interface //\"`\n\n  if [ `echo $VERSION | grep \"^$CLI_MAJOR\\.$CLI_MINOR\" | wc -l` -ne \"1\" ]; then\n    log_and_print \"ERROR: OpenVAS CLI too old or too new: $VERSION\"\n    log_and_print \"FIX: Please install OpenVAS CLI $CLI_MAJOR.$CLI_MINOR.\"\n    check_failed\n  fi\n  echo \"\" \u003e\u003e $LOG\n\n  log_and_print \"OK: OpenVAS CLI version $VERSION.\"\nelse\n  log_and_print \"SKIP: Skipping check for OpenVAS CLI.\"\nfi\n\necho \"Step 6: Checking Greenbone Security Desktop (GSD) ... \"\n\nif [ \"$MODE\" != \"server\" ]; then\n  echo \"Checking presence of Greenbone Security Desktop ...\" \u003e\u003e $LOG\n  DISPLAY=fake gsd --version \u003e\u003e $LOG 2\u003e\u00261\n\n  if [ $? -ne 0 ]; then\n    log_and_print \"ERROR: No Greenbone Security Desktop (gsd) found or too old.\"\n    log_and_print \"FIX: Please install Greenbone Security Desktop 1.1.0.\"\n    check_failed\n  fi\n  echo \"\" \u003e\u003e $LOG\n\n  VERSION=`gsd --version | head -1 | sed -e \"s/Greenbone Security Desktop //\"`\n\n  if [ `echo $VERSION | grep \"^$GSD_MAJOR\\.$GSD_MINOR\" | wc -l` -ne \"1\" ]; then\n    # a special exception rule for v4 where also another release is OK\n    if [ $VER -eq \"4\" -a `echo $VERSION | grep \"^1\\.1\" | wc -l` -ne \"1\" ]; then\n      log_and_print \"ERROR: Greenbone Security Desktop too old or too new: $VERSION\"\n      log_and_print \"FIX: Please install Greenbone Security Desktop $GSD_MAJOR.$GSD_MINOR.\"\n      check_failed\n    fi\n  fi\n  echo \"\" \u003e\u003e $LOG\n\n  log_and_print \"OK: Greenbone Security Desktop is present in Version $VERSION.\"\nelse\n  log_and_print \"SKIP: Skipping check for Greenbone Security Assistant.\"\nfi\n\necho \"Step 7: Checking if OpenVAS services are up and running ... \"\necho \"Checking netstat presence ...\" \u003e\u003e $LOG\n\nNETSTAT=`type netstat 2\u003e /dev/null`\nif [ $? -ne 0 ]; then\n  log_and_print \"WARNING: Could not find netstat binary, checks of the OpenVAS services are disabled.\"\n  log_and_print \"SUGGEST: Install netstat.\"\n  HAVE_NETSTAT=0\nelse\n  log_and_print \"OK: netstat found, extended checks of the OpenVAS services enabled.\"\n  HAVE_NETSTAT=1\nfi\necho \"\" \u003e\u003e $LOG\n\nif [ $HAVE_NETSTAT -eq 1 ]; then\n  netstat -A inet -ntlp 2\u003e /dev/null \u003e\u003e $LOG\n  OPENVASSD_HOST=`netstat -A inet -ntlp 2\u003e /dev/null | grep openvassd | awk -F\\  '{print $4}' | awk -F: '{print $1}'`\n  OPENVASSD_PORT=`netstat -A inet -ntlp 2\u003e /dev/null | grep openvassd | awk -F\\  '{print $4}' | awk -F: '{print $2}'`\n  OPENVASMD_HOST=`netstat -A inet -ntlp 2\u003e /dev/null | grep openvasmd | awk -F\\  '{print $4}' | awk -F: '{print $1}'`\n  OPENVASMD_PORT=`netstat -A inet -ntlp 2\u003e /dev/null | grep openvasmd | awk -F\\  '{print $4}' | awk -F: '{print $2}'`\n  OPENVASAD_HOST=`netstat -A inet -ntlp 2\u003e /dev/null | grep openvasad | awk -F\\  '{print $4}' | awk -F: '{print $1}'`\n  OPENVASAD_PORT=`netstat -A inet -ntlp 2\u003e /dev/null | grep openvasad | awk -F\\  '{print $4}' | awk -F: '{print $2}'`\n  GSAD_HOST=`netstat -A inet -ntlp 2\u003e /dev/null | grep gsad | awk -F\\  '{print $4}' | awk -F: '{print $1}'`\n  GSAD_PORT=`netstat -A inet -ntlp 2\u003e /dev/null | grep gsad | awk -F\\  '{print $4}' | awk -F: '{print $2}'`\n\n  case \"$OPENVASSD_HOST\" in\n    \"0.0.0.0\") log_and_print \"OK: OpenVAS Scanner is running and listening on all interfaces.\" ;;\n    \"127.0.0.1\") log_and_print \"OK: OpenVAS Scanner is running and listening only on the local interface.\" ;;\n    \"\") log_and_print \"ERROR: OpenVAS Scanner is NOT running!\" ; log_and_print \"FIX: Start OpenVAS Scanner (openvassd).\" ; OPENVASSD_PORT=-1 ;;\n  esac\n\n  case $OPENVASSD_PORT in\n    -1) ;;\n    9391) log_and_print \"OK: OpenVAS Scanner is listening on port 9391, which is the default port.\" ;;\n    *) log_and_print \"WARNING: OpenVAS Scanner is listening on port $OPENVASSD_PORT, which is NOT the default port!\"\n       log_and_print \"SUGGEST: Ensure OpenVAS Scanner is listening on port 9391.\" ;;\n  esac\n\n  case \"$OPENVASMD_HOST\" in\n    \"0.0.0.0\") log_and_print \"OK: OpenVAS Manager is running and listening on all interfaces.\" ;;\n    \"127.0.0.1\") log_and_print \"WARNING: OpenVAS Manager is running and listening only on the local interface. This means that you will not be able to access the OpenVAS Manager from the outside using GSD or OpenVAS CLI.\"\n                 log_and_print \"SUGGEST: Ensure that OpenVAS Manager listens on all interfaces.\" ;;\n    \"\") log_and_print \"ERROR: OpenVAS Manager is NOT running!\" ; log_and_print \"FIX: Start OpenVAS Manager (openvasmd).\" ; OPENVASMD_PORT=-1 ;;\n  esac\n\n  case $OPENVASMD_PORT in\n    -1)\n      ;;\n    9390)\n      log_and_print \"OK: OpenVAS Manager is listening on port 9390, which is the default port.\"\n      ;;\n    *)\n      log_and_print \"WARNING: OpenVAS Manager is listening on port $OPENVASMD_PORT, which is NOT the default port!\"\n      log_and_print \"SUGGEST: Ensure OpenVAS Manager is listening on port 9390.\"\n      ;;\n  esac\n\n  case \"$OPENVASAD_HOST\" in\n    \"0.0.0.0\") log_and_print \"OK: OpenVAS Administrator is running and listening on all interfaces.\" ;;\n    \"127.0.0.1\") log_and_print \"OK: OpenVAS Administrator is running and listening only on the local interface.\" ;;\n    \"\") log_and_print \"ERROR: OpenVAS Administrator is NOT running!\" ; log_and_print \"FIX: Start OpenVAS Administrator (openvasad).\" ; OPENVASAD_PORT=-1 ;;\n  esac\n\n  case $OPENVASAD_PORT in\n    -1) ;;\n    9393) log_and_print \"OK: OpenVAS Administrator is listening on port 9393, which is the default port.\" ;;\n    *) log_and_print \"WARNING: OpenVAS Administrator is listening on port $OPENVASAD_PORT, which is NOT the default port!\"\n       log_and_print \"SUGGEST: Ensure OpenVAS Administrator is listening on port 9393.\" ;;\n  esac\n\n  case \"$GSAD_HOST\" in\n    \"0.0.0.0\") log_and_print \"OK: Greenbone Security Assistant is running and listening on all interfaces.\" ;;\n    \"127.0.0.1\") log_and_print \"WARNING: Greenbone Security Assistant is running and listening only on the local interface. This means that you will not be able to access the Greenbone Security Assistant from the outside using a web browser.\"\n                 log_and_print \"SUGGEST: Ensure that Greenbone Security Assistant listens on all interfaces.\" ;;\n    \"\") log_and_print \"ERROR: Greenbone Security Assistant is NOT running!\" ; log_and_print \"FIX: Start Greenbone Security Assistant (gsad).\" ; GSAD_PORT=-1 ;;\n  esac\n\n  case $GSAD_PORT in\n    -1) ;;\n    80|443|9392) log_and_print \"OK: Greenbone Security Assistant is listening on port $GSAD_PORT, which is the default port.\" ;;\n    *) log_and_print \"WARNING: Greenbone Security Assistant is listening on port $GSAD_PORT, which is NOT the default port!\"\n       log_and_print \"SUGGEST: Ensure Greenbone Security Assistant is listening on one of the following ports: 80, 443, 9392.\" ;;\n  esac\n\n  if [ $OPENVASSD_PORT -eq -1 ] || [ $OPENVASMD_PORT -eq -1 ] || [ $OPENVASAD_PORT -eq -1 ] || [ $GSAD_PORT -eq -1 ]; then\n    check_failed\n  fi\nfi\n\necho \"Step 8: Checking nmap installation ...\"\necho \"Checking presence of nmap ...\" \u003e\u003e $LOG\nVERSION=`nmap --version | awk '/Nmap version/ { print $3 }'`\n\nif [ $? -ne 0 ]; then\n  log_and_print \"WARNING: No nmap installation found.\"\n  log_and_print \"SUGGEST: You should install nmap for comprehensive network scanning (see http://nmap.org)\"\nelse\n  if [ `echo $VERSION | grep \"5\\.51\" | wc -l` -ne \"1\" ]; then\n    log_and_print \"WARNING: Your version of nmap is not fully supported: $VERSION\"\n    log_and_print \"SUGGEST: You should install nmap 5.51.\"\n  else\n    log_and_print \"OK: nmap is present in version $VERSION.\"\n  fi\nfi\n\necho \"\" \u003e\u003e $LOG\n\necho \"Step 9: Checking presence of optional tools ...\"\necho \"Checking presence of pdflatex ...\" \u003e\u003e $LOG\nPDFLATEX=`type pdflatex 2\u003e /dev/null`\n\nif [ $? -ne 0 ]; then\n  log_and_print \"WARNING: Could not find pdflatex binary, the PDF report format will not work.\"\n  log_and_print \"SUGGEST: Install pdflatex.\"\n  HAVE_PDFLATEX=0\nelse\n  log_and_print \"OK: pdflatex found.\"\n  HAVE_PDFLATEX=1\nfi\necho \"\" \u003e\u003e $LOG\n\nif [ $HAVE_PDFLATEX -eq 1 ]; then\n  echo \"Checking presence of LaTeX packages required for PDF report generation ...\" \u003e\u003e $LOG\n  PDFTMPDIR=`mktemp -d -t openvas-check-setup-tmp.XXXXXXXXXX`\n  TEXFILE=\"$PDFTMPDIR/test.tex\"\n  cat \u003c\u003cEOT \u003e $TEXFILE\n\\documentclass{article}\n\\pagestyle{empty}\n\n%\\usepackage{color}\n\\usepackage{tabularx}\n\\usepackage{geometry}\n\\usepackage{comment}\n\\usepackage{longtable}\n\\usepackage{titlesec}\n\\usepackage{chngpage}\n\\usepackage{calc}\n\\usepackage{url}\n\\usepackage[utf8x]{inputenc}\n\n\\DeclareUnicodeCharacter {135}{{\\textascii ?}}\n\\DeclareUnicodeCharacter {129}{{\\textascii ?}}\n\\DeclareUnicodeCharacter {128}{{\\textascii ?}}\n\n\\usepackage{colortbl}\n\n% must come last\n\\usepackage{hyperref}\n\\definecolor{linkblue}{rgb}{0.11,0.56,1}\n\\definecolor{inactive}{rgb}{0.56,0.56,0.56}\n\\definecolor{openvas_debug}{rgb}{0.78,0.78,0.78}\n\\definecolor{openvas_false_positive}{rgb}{0.2275,0.2275,0.2275}\n\\definecolor{openvas_log}{rgb}{0.2275,0.2275,0.2275}\n\\definecolor{openvas_hole}{rgb}{0.7960,0.1137,0.0902}\n\\definecolor{openvas_note}{rgb}{0.3255,0.6157,0.7961}\n\\definecolor{openvas_report}{rgb}{0.68,0.74,0.88}\n\\definecolor{openvas_user_note}{rgb}{1.0,1.0,0.5625}\n\\definecolor{openvas_user_override}{rgb}{1.0,1.0,0.5625}\n\\definecolor{openvas_warning}{rgb}{0.9764,0.6235,0.1922}\n\\hypersetup{colorlinks=true,linkcolor=linkblue,urlcolor=blue,bookmarks=true,bookmarksopen=true}\n\\usepackage[all]{hypcap}\n\n%\\geometry{verbose,a4paper,tmargin=24mm,bottom=24mm}\n\\geometry{verbose,a4paper}\n\\setlength{\\parskip}{\\smallskipamount}\n\\setlength{\\parindent}{0pt}\n\n\\title{PDF Report Test}\n\\pagestyle{headings}\n\\pagenumbering{arabic}\n\\begin{document}\nThis is a test of the PDF generation capabilities of your OpenVAS installation. Please ignore.\n\\end{document}\nEOT\n\n  pdflatex -interaction batchmode -output-directory $PDFTMPDIR $TEXFILE \u003e /dev/null 2\u003e\u00261\n  if [ ! -f \"$PDFTMPDIR/test.pdf\" ]; then\n    log_and_print \"WARNING: PDF generation failed, most likely due to missing LaTeX packages. The PDF report format will not work.\"\n    log_and_print \"SUGGEST: Install required LaTeX packages.\"\n  else\n    log_and_print \"OK: PDF generation successful. The PDF report format is likely to work.\"\n  fi\n\n  if [ -f \"$PDFTMPDIR/test.log\" ]; then\n    cat $PDFTMPDIR/test.log \u003e\u003e $LOG\n  fi\n  rm -rf $PDFTMPDIR\nfi\n\necho \"Checking presence of ssh-keygen ...\" \u003e\u003e $LOG\nSSHKEYGEN=`type ssh-keygen 2\u003e /dev/null`\nif [ $? -ne 0 ]; then\n  log_and_print \"WARNING: Could not find ssh-keygen binary, LSC credential generation for GNU/Linux targets will not work.\"\n  log_and_print \"SUGGEST: Install ssh-keygen.\"\n  HAVE_SSHKEYGEN=0\nelse\n  log_and_print \"OK: ssh-keygen found, LSC credential generation for GNU/Linux targets is likely to work.\"\n  HAVE_SSHKEYGEN=1\nfi\necho \"\" \u003e\u003e $LOG\n\nif [ $HAVE_SSHKEYGEN -eq 1 ]; then\n  echo \"Checking presence of rpm ...\" \u003e\u003e $LOG\n  RPM=`type rpm 2\u003e /dev/null`\n  if [ $? -ne 0 ]; then\n    log_and_print \"WARNING: Could not find rpm binary, LSC credential package generation for RPM and DEB based targets will not work.\"\n    log_and_print \"SUGGEST: Install rpm.\"\n    HAVE_RPM=0\n  else\n    log_and_print \"OK: rpm found, LSC credential package generation for RPM based targets is likely to work.\"\n    HAVE_RPM=1\n  fi\n  echo \"\" \u003e\u003e $LOG\n\n  if [ $HAVE_RPM -eq 1 ]; then\n    echo \"Checking presence of alien ...\" \u003e\u003e $LOG\n    ALIEN=`type alien 2\u003e /dev/null`\n\n    if [ $? -ne 0 ]; then\n      log_and_print \"WARNING: Could not find alien binary, LSC credential package generation for DEB based targets will not work.\"\n      log_and_print \"SUGGEST: Install alien.\"\n      HAVE_ALIEN=0\n    else\n      log_and_print \"OK: alien found, LSC credential package generation for DEB based targets is likely to work.\"\n      HAVE_ALIEN=1\n    fi\n    echo \"\" \u003e\u003e $LOG\n  fi\nfi\n\necho \"Checking presence of nsis ...\" \u003e\u003e $LOG\nNSIS=`type makensis 2\u003e /dev/null`\nif [ $? -ne 0 ]; then\n  log_and_print \"WARNING: Could not find makensis binary, LSC credential package generation for Microsoft Windows targets will not work.\"\n  log_and_print \"SUGGEST: Install nsis.\"\n  HAVE_NSIS=0\nelse\n  log_and_print \"OK: nsis found, LSC credential package generation for Microsoft Windows targets is likely to work.\"\n  HAVE_NSIS=1\nfi\necho \"\" \u003e\u003e $LOG\n\necho \"\"\necho \"It seems like your OpenVAS-$VER installation is OK.\"\necho \"\"\necho \"If you think it is not OK, please report your observation\"\necho \"and help us to improve this check routine:\"\necho \"http://lists.wald.intevation.org/mailman/listinfo/openvas-discuss\"\necho \"Please attach the log-file ($LOG) to help us analyze the problem.\"\necho \"\"\n```\n\nMake the script executable and run it, it should find any errors with your\ninstallation or steps you skipped.\n\n## Scanning ##\n\nI'm going to assume that you're using the openvas-client as the scanning\nclient. Here are a few quick notes for setting up a decent scan setup\n\n* Global Settings\n  * General\n    * Range: default\n    * 20 concurrent hosts\n    * 5 concurrent checks\n    * Do a reverse lookup\n    * Optimize the test\n    * Safe checks\n    * Scanners\n      * OpenVAS TCP Scanner\n      * Ping Host\n      * Nmap (NASL wrapper)\n  * Plugins\n    * Enable All\n    * Enable at runtime\n    * Not Silent\n    * Automatically enable new plugins\n  * Credentials\n    * (Optional but a more thorough scan is done with them)\n    * SSH credentials need to be managed through the Extras-\u003eLSC Credentials Manager in the tool bar. It annoyingly does not support simple username/password SSH combos very well requiring you to generate a key pair, set a comment on the keypair (without spaces), and a password on the key pair EVEN IF YOU DON'T INTEND TO USE IT.\n  * Leave target selection as localhost only, these are the global configuration options and you should be forced to set them for each set of scans\n  * KB\n    * Enable KB Saving\n\nWhen you're ready to create a scan create a new \"Task\" I usually create a Task\nper organization that I intend to scan and allows me to organize both different\nkinds of scans for that organization as well as multiple scanners.\n\nCreate a \"Scope\" inside the Task, this will request logging into the server\nthat you'll be running the scan from and will inherit all of the global\nsettings. Adjust the target selection and make scan specific adjustments then\nLaunch!\n\n## Configuration\n\n### /etc/openvas/openvassd.conf\n\n```ini\n# Configuration file of the OpenVAS Security Scanner\n\n# Every line starting with a '#' is a comment\n\n[Misc]\n\n# Path to the security checks folder:\nplugins_folder = /var/lib/openvas/plugins\n\n# Path to OpenVAS caching folder:\ncache_folder = /var/cache/openvas\n\n# Path to OpenVAS include directories:\n# (multiple entries are separated with colon ':')\ninclude_folders = /var/lib/openvas/plugins\n\n# Maximum number of simultaneous hosts tested :\nmax_hosts = 30\n\n# Maximum number of simultaneous checks against each host tested :\nmax_checks = 10\n\n# Niceness. If set to 'yes', openvassd will renice itself to 10.\nbe_nice = yes\n\n# Log file (or 'syslog') :\nlogfile = /var/log/openvas/openvassd.log\n\n# Shall we log every details of the attack ? (disk intensive)\nlog_whole_attack = no\n\n# Log the name of the plugins that are loaded by the server ?\nlog_plugins_name_at_load = no\n\n# Dump file for debugging output, use `-' for stdout\ndumpfile = /var/log/openvas/openvassd.dump\n\n# Rules file :\nrules = /etc/openvas/openvassd.rules\n\n# CGI paths to check for (cgi-bin:/cgi-aws:/ can do)\ncgi_path = /cgi-bin:/scripts\n\n# Range of the ports the port scanners will scan :\n# 'default' means that OpenVAS will scan ports found in its\n# services file.\nport_range = default\n\n# Optimize the test (recommended) :\noptimize_test = yes\n\n# Optimization :\n# Read timeout for the sockets of the tests :\nchecks_read_timeout = 5\n\n# Ports against which two plugins should not be run simultaneously :\n# non_simult_ports = Services/www, 139, Services/finger\nnon_simult_ports = 139, 445\n\n# Maximum lifetime of a plugin (in seconds) :\nplugins_timeout = 320\n\n# Safe checks rely on banner grabbing :\nsafe_checks = yes\n\n# Automatically activate the plugins that are depended on\nauto_enable_dependencies = yes\n\n# Do not echo data from plugins which have been automatically enabled\nsilent_dependencies = no\n\n# Designate hosts by MAC address, not IP address (useful for DHCP networks)\nuse_mac_addr = no\n\n#--- Knowledge base saving (can be configured by the client) :\n# Save the knowledge base on disk :\nsave_knowledge_base = yes\n\n# Restore the KB for each test :\nkb_restore = yes\n\n# Only test hosts whose KB we do not have :\nonly_test_hosts_whose_kb_we_dont_have = no\n\n# Only test hosts whose KB we already have :\nonly_test_hosts_whose_kb_we_have = no\n\n# KB test replay :\nkb_dont_replay_scanners = no\nkb_dont_replay_info_gathering = no\nkb_dont_replay_attacks = no\nkb_dont_replay_denials = no\nkb_max_age = 864000\n#--- end of the KB section\n\n# If this option is set, OpenVAS will not scan a network incrementally\n# (10.0.0.1, then 10.0.0.2, 10.0.0.3 and so on..) but will attempt to\n# slice the workload throughout the whole network (ie: it will scan\n# 10.0.0.1, then 10.0.0.127, then 10.0.0.2, then 10.0.0.128 and so on...\nslice_network_addresses = no\n\n# Should consider all the NASL scripts as being signed ? (unsafe if set to 'yes')\nnasl_no_signature_check = no\n\n#Certificates\ncert_file=/etc/pki/openvas/CA/servercert.pem\nkey_file=/etc/pki/openvas/private/CA/serverkey.pem\nca_file=/etc/pki/openvas/CA/cacert.pem\n\n# If you decide to protect your private key with a password,\n# uncomment and change next line\n# pem_password=password\n# If you want to force the use of a client certificate, uncomment next line\n# force_pubkey_auth = yes\n\n#end.\n```\n","created_at":-62135596800,"fuzzy_word_count":7100,"path":"/notes/openvas/","published_at":1507587004,"reading_time":34,"tags":null,"title":"OpenVAS","type":"notes","updated_at":1507587004,"weight":0,"word_count":7033},{"cid":"fdf42a311bad2ff7c5ed799f4a0da8ec0951d97d","content":"\n***Note: This page is quite old and is likely out of date. My opinions may have\nalso changed dramatically since this was written. It is here as a reference\nuntil I get around to updating it.***\n\nIf doing this from within an LXC container you'll need to perform the following\ntwo steps (this didnt work):\n\n```\nmkdir /var/lib/libvirt/lxc/${VMNAME}/dev/net\nmknod /var/lib/libvirt/lxc/${VMNAME}/dev/net/tun c 10 200\n```\n\n```\nyum install openssl openvpn -y\nmkdir -p /etc/openvpn/keys\ncd /etc/openvpn/keys\nopenssl dhparam -rand - 1024 \u003e dh1024.pem\n\n# Create the CA\ntouch /etc/pki/CA/index.txt\necho -e 'US\\nVermont\\nBurlington\\nstelfox.net\\n\\nca.stelfox.net\\n\\n' \\\n  | openssl req -new -x509 -newkey rsa:2048 -keyout ca.key -nodes -days 365 \\\n  -out ca.crt \u0026\u003e /dev/null\n\n# Create the server key \u0026 csr\necho -e 'US\\nVermont\\nBurlington\\nstelfox.net\\n\\nwild-spring-cobweb.stelfox.net\\n\\n\\n\\n' \\\n  | openssl req -newkey rsa:2048 -keyout server.key -nodes -days 365 \\\n  -out server.csr \u0026\u003e /dev/null\n\n# Sign the server cert\nopenssl x509 -req -in server.csr -days 365 -CA ca.crt -CAkey ca.key \\\n  -set_serial 01 -out server.crt\n\n# Create the client key \u0026 csr\necho -e 'US\\nVermont\\nBurlington\\nstelfox.net\\n\\nclient01.stelfox.net\\n\\n\\n\\n' \\\n  | openssl req -newkey rsa:2048 -keyout client.key -nodes -days 365 \\\n    -out client.csr \u0026\u003e /dev/null\n\n# Sign the client cert\nopenssl x509 -req -in client.csr -days 365 -CA ca.crt -CAkey ca.key \\\n  -set_serial 01 -out client.crt\n```\n\nCreate the server configuration file:\n\n```\ncat \u003c\u003c EOF \u003e /etc/openvpn/server.conf\nport 1194\ndev tun\nmode server\n\ntls-server\nca keys/ca.crt\ncert keys/server.crt\nkey keys/server.key\ndh keys/dh1024.pem\n\nifconfig 10.8.0.1 10.8.0.2\nifconfig-pool 10.8.0.4 10.8.0.255\nifconfig-pool-persist ipp.txt\n\npush \"route 10.8.0.1 255.255.255.255\"\nroute 10.8.0.0 255.255.255.0\n\ncomp-lzo\n\nkeepalive 10 60\ninactive 600\n\nuser openvpn\ngroup openvpn\n\npersist-tun\npersist-key\n\nverb 4\nstatus openvpn-status.log\nlog-append openvpn.log\nEOF\n```\n\n```\niptables -t filter -A FORWARD -m state --state RELATED,ESTABLISHED -j ACCEPT\niptables -t filter -A FORWARD -s 10.8.0.0/16 -i tun0 -o eth0 -j ACCEPT\niptables -t nat -A POSTROUTING -s 10.8.0.0/16 -o eth0 -j MASQUERADE\necho 1 \u003e /proc/sys/net/ipv4/ip_forward\n```\n","created_at":-62135596800,"fuzzy_word_count":300,"path":"/notes/openvpn/","published_at":1507587004,"reading_time":2,"tags":null,"title":"OpenVPN","type":"notes","updated_at":1507587004,"weight":0,"word_count":295},{"cid":"6606f0cda02f670bd3453b4bfc81f001dde5fa6b","content":"\n***Note: This page is quite old and is likely out of date. My opinions may have\nalso changed dramatically since this was written. It is here as a reference\nuntil I get around to updating it.***\n\n## Overview\n\n`/etc/pam.d/` directory contains the PAM configuration files for each PAM-aware\napplication. Each pam aware configuration file has lines in the format of:\n\n```\n\u003cmodule interface\u003e \u003ccontrol flag\u003e \u003cmodule\u003e \u003cmodule arguments\u003e\n```\n\n## Module Interfaces\n\nThere are only four module interfaces:\n\n* account - Verify access is allowed. Check if account has expired, time of day\n  checks, etc\n* auth - Authenticates use, verifies the validity of a password, set credentials\n  \u0026 group memberships \u0026 kerberos tickets\n* password - Used for updating credentials\n* session - Additional tasks performed after access has been granted, stuff like\n  mounting user directories and making users mailbox available.\n\nA module can provide any or all module interfaces.\n\nModule interface directives can be 'stacked' so that multiple modules are used\ntogether for one purpose. The order of these directives. Commented out lines\nstart with '#'.\n\n## Control flags\n\n* include - The module name that is provided will not load a module, but rather\n  include another pam configuration file at this point in the config.\n* optional - The module result is ignored, this is only becomes necessary for\n  successful authentication when no other modules reference the interface.\n* required - The module result must be successful for authentication to\n  continue. If the test fails at this point, the user is not notified until the\n  results of all modules tests that reference that interface are complete.\n* requisite - The module result must be successful for authentication to\n  continue. If a test fails at this point, the uesr is notified immediately with\n  a message reflecting the first failed required or requisite module test.\n* sufficient - The module result is ignored if it fails. However, if the result\n  of a module flagged sufficient is successful and no previous modules flagged\n  required have failed, then no other results are required and the request\n  succeeds.\n\n## Module\n\nAvailable modules can be found in `/lib/security` or `/lib64/security`\ndepending on the system architecture.\n\n### Module Arguments\n\nThese are module dependent refer to the man page of the module.\n\n### Confusing Lines I Don't Understand\n\n```\naccount [default=bad success=ok user_unknown=ignore] pam_krb5.so\n```\n\n## pam_cracklib\n\n`pam_cracklib` allows for quick quality control settings of passwords. It\nallows minimum requirements of the number of different types of characters that\nneed to be used in a password before it can be used. It also checks the\npassword against a dictionary.\n\nBy default it checks against a dictionary but that really isn't enough for good\npassword security. Further password control can be accomplished using\n`pam_passwdqc`.\n\nDefault:\n\n```\npassword  requisite  pam_cracklib.so try_first_pass retry=3 type=\n```\n\nRecommended (require one of each type, minimum length 14):\n\n```\npassword  requisite  pam_cracklib.so try_first_pass retry=3 minlen=14 dcredit=-1 ucredit=-1 ocredit=-1 lcredit=-1\n```\n\nOther (credit based):\n\n```\npassword  requisite  pam_cracklib.so try_first_pass retry=3 minlen=22 dcredit=2 ocredit=4\n```\n\nThe credit based system requires a bit of explaining, it requires a 22\ncharacter password and treats numbers as 2 characters and symbols as 4\ncharacters. This can be dangerous as you can have an 11 digit number as a\npassword which isn't secure...\n\n## pam_passwdqc\n\n`pam_passwdqc` replaces `pam_cracklib` in checking the quality of the password.\nTo use it you need to comment out `pam_cracklib` in `/etc/pam.d/system-auth`.\n\nYou'll need to add this line to the `/etc/pam.d/system-auth`:\n\n```\npassword  requisite  pam_passwdqc.so retry=3 min=disabled,disabled,22,16,12 passphrase=4 similar=deny enforce=users\n```\n\nWhat this does is it prevent any passwords with one or two character types.\nRequires a password with three character types to be a minimum of 16 characters\nand one with four character types to be a minimum of 12 characters.\n\nThe middle option (22) is the minimum length of a passphrase (grouping of words\nfound in the dictionary file). The minimum number of words is controlled by the\npassphrase option.\n\nIt denies similar passwords as previous ones and only enforces quality control\nfor root passwords (since they're sha512 hashes they only have two character\n  classes and would thus be denied).\n\nThis is untested but to the best of my knowledge passphrases still need to be\nat least three character classes (since one and two are disabled).\n\n## pam_tally2\n\n`pam_tally2` comes with Fedora's pam package so it will be installed. It's very\nuseful to prevent bruteforce attempts BUT it can also used to lock access out\nto a legitimate user. It is recommended to not turn this on for the entire\nsystem but just remote login services that use pam (for example SSH).\n\nTo use it you need to add the following line to the services pam file. So for\nSSH the file would be `/etc/pam.d/sshd`. The following is what needs to be\nadded:\n\n```\nauth    required     pam_tally2.so deny=5 onerr=fail audit silent\naccount     required     pam_tally2.so\n```\n\nYou'll also need to change:\n\n```\nauth     sufficient     pam_unix.so nullok try_first_pass\n```\n\nTo:\n\n```\nauth     required     pam_unix.so nullok try_first_pass\n```\n\nAnd get rid of the following two lines as they will interfere with the rest of\nthe changes:\n\n```\nauth     requisite    pam_succeed_if.so uid \u003e= 500 quiet\nauth     required     pam_deny.so\n```\n\n### Resetting a User's Account\n\nInformation about users password tally can be found in `/var/log/tallylog`.\n\n```\n[root@localhost ~]# /sbin/pam_tally2 --user username --reset\n```\n\n### Automatic Unlocking\n\nAdditionally adding `unlock_time=1800`. This allows the user to log back in\nhalf an hour after the account has been locked.\n","created_at":-62135596800,"fuzzy_word_count":900,"path":"/notes/pam/","published_at":1507587004,"reading_time":4,"tags":null,"title":"PAM","type":"notes","updated_at":1507587004,"weight":0,"word_count":846},{"cid":"07ee32ef1008ac57b9be9cfaa3bd706f35d7bc6c","content":"\n***Note: This page is quite old and is likely out of date. My opinions may have\nalso changed dramatically since this was written. It is here as a reference\nuntil I get around to updating it.***\n\n## Recommended / Best Practices\n\n* /boot 200Mb\n* swap 1.5x times the amount of RAM\n* /\n* /home\n* /tmp\n* /var\n* /var/log\n* /var/log/audit\n* Encrypt all partitions except /boot\n\n## Creating an LVM partition\n\n### Create a Physical Partition\n\n* Open the drive in fdisk, create a new primary partition\n* Mark the new partition as type '8e' or Linux LVM\n* Write the partition table\n\n### Create Physical LVM Volume\n\n```\npvcreate /dev/sdc1 (assuming /dev/sdc1 was the partition just created)\n```\n\n### Volume Group Creation\n\nThis point needs a bit of thinking before doing. There is a limit on the number\nof physical extents that can be accessed. This in turn limits the size of the\nvolume group. This limit is 65,536 extents.\n\nThe default size is 4MB/extent. This by default leads to a limit of 256GB per\nvolume group. 8MB/extent gets you 512GB. The maximum size I see needing is\n32MB/extent for a size of 2TB. The following example uses an extent size of\n16MB and assumes the partition is `/dev/sdc1`.\n\n```\nvgcreate -s 16M vg_test /dev/sdc1\n```\n\n### Logical Volume Creation\n\nThe following creates a 1GB logical volume group, there does not seem to be a\nway to create a logical volume with all the remaining space in the volume\ngroup.\n\n```\nlvcreate -L 1G -n lv_test vg_test\n```\n\n## Securing Partitions\n\n* Add `nodev` to all non-root local ext{2,3,4} partitions in `/etc/fstab`\n* Add `nosuid` and `noexec` to `/tmp` partition\n* Bind mount /tmp to /var/tmp with `rw,nodev,nosuid,noexec,bind` as options\n* If removable storage is enabled add `nodev`, `nosuid`, `noexec` to their\n  partitions\n* Disable auto mounter (autofs)\n* Dynamically [encrypt the swap partition][1] (not for any machine that will be\n  hibernated)\n\n## Encrypting Additional Partitions\n\n```\ncryptsetup luksFormat -c aes-xts-plain -s 256 -h sha256 /dev/vg_test/lv_test\ncryptsetup luksOpen -c aes-xts-plain -s 256 -h sha256 /dev/vg_test/lv_test \\\n  cryptTest\nmkfs.ext4 /dev/mapper/cryptTest\ncryptsetup luksClose /dev/mapper/cryptTest\n```\n\n[1]: {{\u003c ref \"./swap.md\" \u003e}}\n","created_at":-62135596800,"fuzzy_word_count":400,"path":"/notes/partitioning/","published_at":1540945455,"reading_time":2,"tags":null,"title":"Partitioning","type":"notes","updated_at":1540945455,"weight":0,"word_count":316},{"cid":"f132d9d159717e7d09d87e46de24aa1957e69cc6","content":"\n***Note: This page is quite old and is likely out of date. My opinions may have\nalso changed dramatically since this was written. It is here as a reference\nuntil I get around to updating it.***\n\n## Random Password Generation\n\nThis little ruby snippet generates a random 31 character string consisting of\nuppercase letters, lowercase letters, and numbers.\n\n```ruby\n#!/usr/bin/env ruby\n\ncharacter_set =  [('a'..'z'),('A'..'Z'),(0..9)].map do |item|\n  item.to_a\nend\ncharacter_set.flatten!\n\npassword = \"\"\n31.times do\n  password += character_set[rand(character_set.length)].to_s\nend\n\nputs password\n```\n\n## Shamir's Secret Sharing Scheme\n\nCiting from the Wikipedia article about Secret Sharing:\n\n```\nIn cryptography, a secret sharing scheme is a method for distributing a secret\namongst a group of participants, each of which is allocated a share of the\nsecret. The secret can only be reconstructed when the shares are combined\ntogether; individual shares are of no use on their own.\n\nMore formally, in a secret sharing scheme there is one dealer and n players.\nThe dealer gives a secret to the players, but only when specific conditions are\nfulfilled. The dealer accomplishes this by giving each player a share in such a\nway that any group of t (for threshold) or more players can together\nreconstruct the secret but no group of less than t players can. Such a system\nis called a (t,n)-threshold scheme.\n\nA popular technique to implement threshold schemes uses polynomial\ninterpolation (\"Lagrange interpolation\"). This method was invented by Adi\nShamir in 1979.\n\nNote that Shamir's scheme is provable secure, that means: in a (t,n) scheme one\ncan prove that it makes no difference whether an attacker has t-1 valid shares\nat his disposal or none at all; as long as he has less than t shares, there is\nno better option than guessing to find out the secret.\n```\n\nThere is a fedora package named `ssss` that can split a string up to 128\ncharacters long into X pieces, requiring at least Y pieces to recover it.\nAssuming the pieces are broken up over several different locations and stored\nin different manners it's a fairly strong way to store passwords or other\ninformation that needs to be written once and recovered rarely.  The following\nexample uses X = 5, and Y = 3:\n\n```\n[root@localhost ~]# ssss-split -t 3 -n 5\nGenerating shares using a (3,5) scheme with dynamic security level.\nEnter the secret, at most 128 ASCII characters: my secret root password\nUsing a 184 bit security level.\n1-1c41ef496eccfbeba439714085df8437236298da8dd824\n2-fbc74a03a50e14ab406c225afb5f45c40ae11976d2b665\n3-fa1c3a9c6df8af0779c36de6c33f6e36e989d0e0b91309\n4-468de7d6eb36674c9cf008c8e8fc8c566537ad6301eb9e\n5-4756974923c0dce0a55f4774d09ca7a4865f64f56a4ee0\n```\n\nYou make get the following error message as well which is safe to ignore as\nlong as you're on a fully trusted system, and are the only user of such system.\n\n```\nWARNING: couldn't get memory lock (ENOMEM, try to adjust RLIMIT_MEMLOCK!).\n```\n\nTo recover the example above, we need to tell it that we need 3 (being our Y)\nof the pieces to recover the password and it can be accomplished like so:\n\n```\n[root@localhost ~]# ssss-combine -t 3\nEnter 3 shares separated by newlines:\nShare [1/3]: 3-fa1c3a9c6df8af0779c36de6c33f6e36e989d0e0b91309\nShare [2/3]: 5-4756974923c0dce0a55f4774d09ca7a4865f64f56a4ee0\nShare [3/3]: 2-fbc74a03a50e14ab406c225afb5f45c40ae11976d2b665\nResulting secret: my secret root password\n```\n","created_at":-62135596800,"fuzzy_word_count":600,"path":"/notes/password-security/","published_at":1507587428,"reading_time":3,"tags":null,"title":"Password Security","type":"notes","updated_at":1507587428,"weight":0,"word_count":511},{"cid":"8526fee03250ec2f75e4de667a814dd291188146","content":"\n***Note: This page is quite old and is likely out of date. My opinions may have\nalso changed dramatically since this was written. It is here as a reference\nuntil I get around to updating it.***\n\nBy default Fedora/CentOS come with sendmail. Postfix is preferable as it was\nwritten with security in mind.\n\n[Something that would be good][1] to port over to Fedora/Postfix with a nice\nStelfox spin, and it's [follow up][2].\n\n[This][3] is what originally turned me on to that.\n\n[This][4] will help setup a mail gateway.\n\nI can use [this][5] to centralize my email.\n\nThis [site][6] has some diagnostic information that may be useful.\n\nIf I have rate limit issues sending too Yahoo, [this article][11] can help\nsolve it.\n\nAs for Dovecot, while not being Postfix, is still an important piece of this\npuzzle. The [Mail Location][7] for Dovecot will need to be considered, so far\nI'm leaning towards [sdbox][8], however, it is not compatible with mutt which\nis my ideal client. My next alternative would be the [Maildir][9].\n\n## General Host MTA\n\n## Local Network Mail Relay\n\n## Generic Mail Relay\n\n### Configuration\n\n#### /etc/postfix/main.cf\n\nThe following configuration was used as a temporary mail relay. It does not\nfilter spam and was done fast so there are probably problems in the\nconfiguration that were missed. Proceed with caution. A tool that was\ninvaluable to me in diagnosing queueing issues was `qshape` which is part of\nthe package `postfix-perl-scripts` in Fedora 15.\n\n```ini\nmyhostname = mailrelay.example.org\n\ncommand_directory = /usr/sbin\ndaemon_directory = /usr/libexec/postfix\ndata_directory = /var/lib/postfix\nqueue_directory = /var/spool/postfix\n\nmail_owner = postfix\ndefault_privs = nobody\nsetgid_group = postdrop\n\nallow_percent_hack = no\ndisable_vrfy_command = yes\nsmtpd_helo_required = yes\n\ninet_interfaces = all\ninet_protocols = all\n\nmydestination = $myhostname\nunknown_local_recipient_reject_code = 550\n\nmynetworks_style = subnet\nmynetworks = 10.15.0.0/16, 127.0.0.0/8\n\nrelay_domains = example.org, example.com\nrelayhost = [10.15.1.65]\n\nin_flow_delay = 0\n\nalias_maps = hash:/etc/aliases\nalias_database = hash:/etc/aliases\n\nfast_flush_domains = $relay_domains\n\nsmtpd_banner = $myhostname ESMTP\n\nlocal_destination_concurrency_limit = 2\ndefault_destination_concurrency_limit = 20\n\nsendmail_path = /usr/sbin/sendmail.postfix\nnewaliases_path = /usr/bin/newaliases.postfix\nmailq_path = /usr/bin/mailq.postfix\n```\n\n## Full Local Client MTA w/ Relaying\n\n```ini\nqueue_directory = /var/spool/postfix\ncommand_directory = /usr/sbin\ndaemon_directory = /usr/libexec/postfix\ndata_directory = /var/lib/postfix\nmail_owner = postfix\ninet_interfaces = localhost\ninet_protocols = all\nmydestination = $myhostname, localhost\nlocal_recipient_maps = unix:passwd.byname $alias_maps\nunknown_local_recipient_reject_code = 550\nmynetworks_style = host\nmynetworks = 127.0.0.0/8\nrelayhost = 0x378.net\n#relayhost = [mail-01.i.0x378.net]\nalias_maps = hash:/etc/aliases\nalias_database = hash:/etc/aliases\ndebug_peer_level = 2\n\ndebugger_command =\n         PATH=/bin:/usr/bin:/usr/local/bin:/usr/X11R6/bin\n         ddd $daemon_directory/$process_name $process_id \u0026 sleep 5\n\nsendmail_path = /usr/sbin/sendmail.postfix\nnewaliases_path = /usr/bin/newaliases.postfix\nmailq_path = /usr/bin/mailq.postfix\n\nsetgid_group = postdrop\nhtml_directory = no\nmanpage_directory = /usr/share/man\nsample_directory = /usr/share/doc/postfix-2.10.2/samples\nreadme_directory = /usr/share/doc/postfix-2.10.2/README_FILES\n```\n\nYou'll also want to append an administrator's email to the aliases table:\n\n```\necho \"root:  admin+$(hostname -s)@i.0x378.net\" \u003e\u003e /etc/aliases\nnewaliases\n```\n\nAnd enable/start the service:\n\n```\nsystemctl enable postfix.service\nsystemctl start postfix.service\n```\n\n## Creating a catch-all\n\n```\nyum install postfix -y\n```\n\nEdit `/etc/postfix/main.cf`\n\nCreated lines:\n\n```\ndisable_vrfy_command = yes\nmyhostname = mail-01.i.0x378.net\nhome_mailbox = Maildir/\nlocal_recipient_maps = unix:passwd.byname $alias_maps\nvirtual_alias_maps = regexp:/etc/postfix/virtual_aliases\n```\n\nChanged lines:\n\n```\ninet_interfaces = all\n```\n\nMake sure `mydestination` is unset\n\nCreated the file `/etc/postfix/virtual_aliases` with the following contents:\n\n```\n/.*/  root\n```\n\nOpened port `25/tcp` on the firewall.\n\nSince the root account is going to be receiving mail for all accounts and all\ndomains make sure that root is not being redirected to another email address in\nthe `/etc/aliases` file. Make sure to run `newaliases` if you make a change to\nthe file.\n\nGreat success! The local root account is now receiving emails to literally any\naddress. In production this probably shouldn't be the root user....\n\n## Notes on Building the App\n\n```\nyum install ruby git rubygem-bundler ruby-devel make gcc openssl-devel \\\n  openssh-server -y\n\nsystemctl enable sshd.service\nsystemctl start sshd.service\n```\n\nTemporarily allow HTTPS for gem installation...\n\nOn development box...\n\n```\nrails new parcel_pot --skip-test-unit --skip-bundle\ncd parcel_pot\n\ncat \u003e Gemfile \u003c\u003c-EOS\nsource 'https://rubygems.org'\n\ngem 'rails', '4.0.0'\n\ngem 'jquery-rails'\ngem 'turbolinks'\ngem 'jbuilder', '~\u003e 1.2'\n\n# Use ActiveModel has_secure_password\n# gem 'bcrypt-ruby', '~\u003e 3.0.0'\n\ngem 'puma'\n\ngroup :development do\n  gem 'capistrano'\n  gem 'sqlite3'\nend\n\ngem 'debugger', group: [:development, :test]\nEOS\n\nbundle\ngit init\ngit add .\ngit commit -m \"Initial commit\"\n\ncap install\n\ncat \u003e Capfile \u003c\u003c-EOS\nrequire 'capistrano/setup'\nrequire 'capistrano/deploy'\nrequire 'capistrano/bundler'\nrequire 'capistrano/rails/assets'\nrequire 'capistrano/rails/migrations'\n\nDir.glob('lib/capistrano/tasks/*.cap').each { |r| import r } \nEOS\n```\n\n## Potentially Useful Links\n\n* http://library.edgecase.com/configuring-postfix-to-deliver-email-to-ruby\n\n## Diagnosing Mail Issues\n\nTo check the current state of the mail queue on a server you can run the\nfollowing command:\n\n```\npostqueue -p\n```\n\nor:\n\n```\nmailq\n```\n\nThe error messages often indicate what the problem is allowing you to find mail\nrouting issues.\n\nTo immediately flush the queue (attempt to resent all the mail:\n\n```\npostqueue -f\n```\n\nAlternatively you can just delete all the deferred mail in the queue or all\nmail in the queue with one of the following two commands:\n\n```\npostsuper -d ALL deferred\npostsuper -d ALL\n```\n\n## Authenticated Mail Relay\n\n* http://www.anthonyldechiaro.com/blog/2008/10/17/postfix-authenticated-smtp-relayhost/\n\n## Greylisting\n\n* milter-greylist\n* sqlgrey\n\n## SSL\n\n```\nyum install cyrus-sasl -y\n```\n\nCreate the SSL certificate\n\n```\ncd /etc/postfix\nopenssl req -new -x509 -newkey rsa:4096 -keyout key.pem -nodes -days 365 -out cert.pem\nchmod 0600 *.pem\n```\n\nEnable TLS on the SMTP server by adding the following set of commands:\n\n```\nsmtpd_use_tls = yes\ntls_random_source = dev:/dev/urandom\nsmtpd_tls_cert_file = /etc/postfix/cert.pem\nsmtpd_tls_key_file = /etc/postfix/key.pem\n```\n\n## Anti-Virus \u0026 Spam Filtering\n\n```\nyum install amavisd-new spamassassin clamav clamav-update \\\n  clamav-server-sysvinit -y\n```\n\nI opened up `/etc/sysconfig/freshclam` and commented out the last line.\n\nEdited `$mydomain` variable in `/etc/amavisd/amavisd.conf` to be `0x378.net`\nand finally enable and start the services.\n\n```\nsystemctl enable amavisd.service\nsystemctl enable clamd.amavisd.service\nsystemctl enable spamassassin.service\nsystemctl start amavisd.service\nsystemctl start clamd.amavisd.service\nsystemctl start spamassassin.service\n```\n\nAdditional things to consider:\n\n* bogofilter\n* spambayes\n* spamprobe\n* dspam\n\n## DKIM\n\nThere are two postfix DKIM milters available, `opendkim`, and `dkim-milter`.\nThe former is a fork of the latter, has fewer bugs, and is under a much tighter\nrelease schedule.\n\n```\nyum install opendkim -y\n```\n\n## DMARC\n\n\n\n## SPF Validation\n\nThere are two postfix SPF filters available as well. The first one is\n`perl-Mail-SPF` and the other is `pypolicyd-spf`. From what I can tell the\nlatter is significantly more sophisticated and it provides a saner set of\ndefaults so thats the choice I'm going with.\n\nThe following is a set of the valid SPF keywords and their modifiers.\n\n|          |                                                                                                                                                                                                                                                                                            |\n| -------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |\n| v=spf1   | Specifies this is an SPF record, and we're using version 1 of it.                                                                                                                                                                                                                          |\n| all      | Matches everything not specifically matched.                                                                                                                                                                                                                                               |\n| a        | Match the A record of the domain by default, additionally an other domain can be specified, and a CIDR network prefix can be applied to the resolved address. (ex: a a:other.tld a/24 a:test.tld/24)                                                                                       |\n| mx       | Match the MX records of the domain by default, additional domains and CIDR suffixes can be applied like the a record. (ex: mx mx:other.tld mx/28 mx:something.tld/24)                                                                                                                      |\n| ip4      | IPv4 address or network (CIDR format) (ex: ip4:192.168.0.0/24)                                                                                                                                                                                                                             |\n| ip6      | IPv6 address or network (CIDR format) (ex: ip6:2001:51b2:200C::/64)                                                                                                                                                                                                                        |\n| ptr      | ptr, without an argument this validates that at least one of the domains resolved by looking up the client's IP address resolves back to the current domain. When a domain is provided it validates the reverse lookups match that domain instead.                                         |\n| exists   | Matches successfully if the provided domain successfully resolves to an address (it doesn't matter what address). This is valuable for use with macros, allowing RBL-style reversed-IP lookups.                                                                                            |\n| include  | Additionally add the SPF record from another domain. If that other domain doesn't include an SPF record the result will be a PermError. (ex: include:example.com).                                                                                                                         |\n| redirect | ignore this SPF record and use the SPF record at the provided domain instead. If the provided domain doesn't have an spf record the result is unknown. (ex: redirect=example.com)                                                                                                          |\n| exp      | If an SMTP receiver rejects a message, it can direct non-conforming users to a web page that provides further instructions. When the domain is expanded; a TXT lookup is performed, the result of the TXT query is then macro expanded and shown to the sender (ex: exp=explain._spf.%{d}) |\n\nExpressions:\n\n|   |                             |\n|---|-----------------------------|\n| - | Fail                        |\n| ~ | Soft-fail (accept but mark) |\n| + | Pass                        |\n| ? | Neutral                     |\n\nInstall SPF stuff:\n\n```\nyum install pypolicyd-spf -y\n```\n\n## SenderID\n\nThis is the Microsoft specific version of SPF. It's confusing as they have\nsimilar syntax. SenderID is obsolete but occasionally still causes issues when\nbroken SenderID implementations attempt to validate a SPF record. There was\nsome mention in the articles I read that SPF and SenderID operate at different\nlayers, but they never went into details about it.\n\nLooking into the 'layers' thing a little bit deeper, it seems that SPF use the\nSMTP protocol's \"MAIL FROM\" envelope address to perform the validation while\nSenderID uses the headers in the body content. Definitely smarter on the SPF\nfront.\n\n## Spam Trapping\n\nThis is a very simple message of seeding the website of a domain with fake\nemail addresses in a way that isn't visible too users but would get picked up\nby any scrapers that are using simple regular expressions (such as using HTML\ncomments too hide the email).\n\nThis email address should be a real one that can receive mail. How the\nadministrator uses these mails is up too them. Some ideas could be:\n\n* Auto-training spam filters\n* Immediately blacklisting senders\n* Watching for potential phishing attacks\n\nThese all have their own pros and cons and are not exclusive from each other.\nThere is a high probability that anything sent too this address is going too be\na spammer, especially so if the name is obvious too a human reader.\n\n## Bounce Message Handling\n\n## Monitoring Sent Bounce Messages\n\nYou can configure postfix in a way that when it sends a bounce message to a\nremote mail server, it additionally sends a copy to static address. This is\nuseful for monitoring for abuse from mail clients that are compromised (large\nnumber of bounce messages probably indicate a spam blast).\n\nTo send these too the address 'sent-bounces@example.tld' you'll want to add the\nfollowing lines too your postfix configuration:\n\n```\nnotify_classes = resource, software, bounce\nbounce_notice_recipient = sent-bounces@example.tld\n```\n\nThe first line adds the 'bounce' class of messages to the standard messages\nsent too the postmaster for delivery issues related too the software or server.\nThe second line redirects those specific messages to the email address shown\nabove.\n\n### VERP\n\nThis is a pretty nifty trick that has [been standardized][10] too a certain\nextent. The key too understanding how this works is knowing the difference\nbetween the various forms of the FROM address used by mail servers. The three\nare:\n\n1. Return-Path (sometimes referred too as the Reverse-Path, or the\n   Envelope-FROM) is the value submitted in the `MAIL FROM` part of the SMTP\n   session. This does not need to be the same value found in the headers of the\n   email sent after the DATA portion of the session.\n\n   This is added as a header by the recipient's SMTP server. If one already\n   exists it is replaced.\n\n   All email bounces that get automatically generated should go too the\n   Return-Path value. Not all mail servers obey this rule, some will bounce the\n   email back to the From address.\n2. The From address is the value found in the From header. This is supposed to\n   be who the message is actually From (i.e. The user or software that actually\n   sent the message). This is what is generally shown in mail clients as who\n   sent the mail. This is also the email address that will be used for all\n   human (mail client) responses if the email doesn't have a Reply-To header.\n3. The Reply-To header is added by the sender (or the sender's software). This\n   header is used to direct human responses to another address. It should be\n   the first thing looked for when a user clicks on the 'Reply' button in their\n   client and should be used to populate the `To:` field on the new message.\n\nVERP takes advantage of the 'Return-Path' header to help direct bounces too an\nautomated system. While this can be used by things like mailing list systems\ntoo automatically unsubscribe bad or no longer active email addresses, it can\nalso be used too detect compromised accounts and abuse of your mail server.\n\nBy combining a recipient delimiter in the 'Return-Path', you could add unique\nbounce message processing detection based on any individual metric you want.\nUsually a unique value for the receiver of the original message is the most\nrelevant.\n\nAs it stands Postfix supports VERP both for receiving, and for sending. Sending\nis trickier as the client that is initiating the SMTP session for the mail to\nget relayed has too explicitely enable VERP for that message by appending\n'VERP' too the 'MAIL FROM:' component. This is not ideal for general tracking\nand I haven't found a solution for it yet.\n\nFor receipt of these VERP bounces I prefer setting up a dedicated address that\nI can pass too a script for processing such as 'bounce@example.tld'. Make sure\nthis address exists if before taking the following actions.\n\nThis method makes use of postfix transports too handle bounces speciality.\nAfter the address has been created ensure you have the recipient delimiter\noption configured in your `main.cf` file like so:\n\n```\nrecipient_delimiter = +\n```\n\nWe'll need to define a new transport, which will send the mail through the\nscript or software that will be handling the bounce messages. This will receive\nthe messages via STDIN, and can take any arguments you want too provide. I use\nthe bounce extension as the only argument too my script which results in an\naddition to my 'master.cf' file that looks like the following:\n\n```\nbounce   unix  -       n       n       -       -       pipe\n  flags=RX user=nobody argv=/srv/scripts/mail_bounce_handler ${extension}\n```\n\nThe 'R' flag ensures that a 'Return-Path' message header is added too the\nmessage before passing it into the script. The 'X' flag is used specifically by\nme for my script, this flag indicates that this transport performs final\ndelivery of the message. If you want to additionally have these bounce messages\nend up in a mailbox you'll want too leave this out.\n\nThe transport additionally indicates the script specified by the argv argument\nwill be executed as the nobody user with the extension passed as the sole\nparameter too the script. You'll want too change the path of argv too the path\nof your script.\n\nIf you don't already have a transports map (otherwise just add this tranport\nand remap it if it's a hash type), create the file '/etc/postfix/transport' and\nadd the following contents:\n\n```\nbounce@example.tld    bounce:\n```\n\nAnd build the hash map:\n\n```\npostmap /etc/postfix/transport\n```\n\nFinally make sure postfix is aware of your transport map (the following line\nshould be in your 'main.cf' file).\n\n```\ntransport_maps = hash:/etc/postfix/transport\n```\n\nReload your postfix configuration and your script should now be handling all\nmail sent too 'bounce@example.tld'.\n\n[1]: https://grepular.com/Automatically_Encrypting_all_Incoming_Email\n[2]: https://grepular.com/Automatically_Encrypting_all_Incoming_Email_Part_2\n[3]: https://grepular.com/Protecting_a_Laptop_from_Simple_and_Sophisticated_Attacks\n[4]: http://library.linode.com/email/postfix/dovecot-mysql-centos-5\n[5]: http://library.linode.com/email/fetchmail\n[6]: http://rimuhosting.com/support/settingupemail.jsp?mta=postfix\n[7]: http://wiki.dovecot.org/MailLocation/\n[8]: http://wiki2.dovecot.org/MailboxFormat/dbox\n[9]: http://wiki.dovecot.org/MailboxFormat/Maildir\n[10]: http://cr.yp.to/proto/verp.txt\n[11]: http://stevejenkins.com/blog/2012/08/how-to-postfix-configuration-to-reduce-yahoo-deferrals-using-transport-maps/\n","created_at":-62135596800,"fuzzy_word_count":2500,"path":"/notes/postfix/","published_at":1507586459,"reading_time":12,"tags":null,"title":"Postfix","type":"notes","updated_at":1507586459,"weight":0,"word_count":2410},{"cid":"e5b3e11defac586304e4866b31a58834e6cf606d","content":"\n***Note: This page is quite old and is likely out of date. My opinions may have\nalso changed dramatically since this was written. It is here as a reference\nuntil I get around to updating it.***\n\n## Puppet Master / Server\n### Installation\n\n```\nyum install puppet puppet-server -y\n```\n\nAnd configure the puppet master like so:\n\nThe following file is `/etc/puppet/puppet.conf`:\n\n```ini\n[master]\n  confdir = /etc/puppet\n  vardir = /var/lib/puppet\n  logdir = /var/log/puppet\n\n  # Whether to print stack traces on some errors\n  trace = false\n\n  # Whether log files should always flush to disk.\n  autoflush = true\n\n  # What syslog facility to use when logging to syslog.\n  syslogfacility = daemon\n\n  # The directory where Puppet state is stored.  Generally, this directory can\n  # be removed without causing harm (although it might result in spurious\n  # service restarts).\n  statedir = /var/lib/puppet/state\n\n  # Where Puppet PID files are kept.\n  rundir = /var/run/puppet\n\n  # Whether to just print a manifest to stdout and exit. Only makes sense when\n  # used interactively. Takes into account arguments specified on the CLI.\n  #genmanifest = false\n\n  # Whether to use colors when logging to the console.  Valid values are\n  # `ansi` (equivalent to `true`), `html`, and `false`, which produces no color.\n  color = ansi\n\n  # Whether to create the necessary user and group that puppet agent will run\n  # as.\n  #mkusers = false\n\n  # Whether Puppet should manage the owner, group, and mode of files it uses\n  # internally\n  manage_internal_file_permissions = true\n\n  # Run the configuration once, rather than as a long-running daemon. This is\n  # useful for interactively running puppetd.\n  #onetime = false\n\n  # The shell search path.  Defaults to whatever is inherited from the parent\n  # process.\n  #path = none\n\n  # An extra search path for Puppet. This is only useful for those files that\n  # Puppet will load on demand, and is only guaranteed to work for those\n  # cases. In fact, the autoload mechanism is responsible for making sure this\n  # directory is in Ruby's search path\n  libdir = /var/lib/puppet/lib\n\n  # If true, allows the parser to continue without requiring all files\n  # referenced with `import` statements to exist. This setting was primarily\n  # designed for use with commit hooks for parse-checking.\n  #ignoreimport = false\n\n  # The configuration file that defines the rights to the different namespaces\n  # and methods. This can be used as a coarse-grained authorization system for\n  # both `puppet agent` and `puppet master`.\n  authconfig = /etc/puppet/namespaceauth.conf\n\n  # The environment Puppet is running in. For clients (e.g., `puppet agent`)\n  # this determines the environment itself, which is used to find modules and\n  # much more. For servers (i.e., `puppet master`) this provides the default\n  # environment for nodes we know nothing about.\n  environment = production\n\n  # Which arguments to pass to the diff command when printing differences between\n  # files. The command to use can be chosen with the `diff` setting.\n  #diff_args = -u\n\n  # Which diff command to use when printing differences between files. This\n  # setting has no default value on Windows, as standard `diff` is not\n  # available, but Puppet can use many third-party diff tools.\n  diff = diff\n\n  # Whether to log and report a contextual diff when files are being replaced.\n  # This causes partial file contents to pass through Puppet's normal logging\n  # and reporting system, so this setting should be used with caution if you\n  # are sending Puppet's reports to an insecure destination. This feature\n  # currently requires the `diff/lcs` Ruby library.\n  show_diff = false\n\n  # Whether to send the process into the background. This defaults to true on\n  # POSIX systems, and to false on Windows (where Puppet currently cannot\n  # daemonize).\n  #daemonize = true\n\n  # The maximum allowed UID. Some platforms use negative UIDs but then ship\n  # with tools that do not know how to handle signed ints, so the UIDs show up\n  # as huge numbers that can then not be fed back into the system. This is a\n  # hackish way to fail in a slightly more useful way when that happens.\n  #maximum_uid = 4294967290\n\n  # The YAML file containing indirector route configuration.\n  route_file = /etc/puppet/routes.yaml\n\n  # Where to find information about nodes.\n  #node_terminus = plain\n\n  # Where to get node catalogs. This is useful to change if, for instance,\n  # you'd like to pre-compile catalogs and store them in memcached or some\n  # other easily-accessed store.\n  catalog_terminus = compiler\n\n  # The node facts terminus.\n  facts_terminus = yaml\n\n  # Should usually be the same as the facts terminus\n  inventory_terminus = yaml\n\n  # Where the puppet agent web server logs.\n  httplog = /var/log/puppet/http.log\n\n  # The HTTP proxy host to use for outgoing connections. Note: You may need to\n  # use a FQDN for the server hostname when using a proxy.\n  http_proxy_host = none\n\n  # The HTTP proxy port to use for outgoing connections\n  #http_proxy_port = 3128\n\n  # The minimum time to wait (in seconds) between checking for updates in\n  # configuration files. This timeout determines how quickly Puppet checks\n  # whether a file (such as manifests or templates) has changed on disk.\n  filetimeout = 15\n\n  # Which type of queue to use for asynchronous processing.\n  #queue_type = stomp\n\n  # Which type of queue to use for asynchronous processing. If your stomp\n  # server requires authentication, you can include it in the URI as long as\n  # your stomp client library is at least 1.1.1\n  #queue_source = stomp://localhost:61613/\n\n  # Whether to use a queueing system to provide asynchronous database\n  # integration. Requires that `puppetqd` be running and that 'PSON' support\n  # for ruby be installed.\n  #async_storeconfigs = false\n\n  # Whether storeconfigs store in the database only the facts and exported\n  # resources. If true, then storeconfigs performance will be higher and still\n  # allow exported/collected resources, but other usage external to Puppet\n  # might not work.\n  #thin_storeconfigs = false\n\n  # How to determine the configuration version. By default, it will be the\n  # time that the configuration is parsed, but you can provide a shell script\n  # to override how the version is determined. The output of this script will\n  # be added to every log message in the reports, allowing you to correlate\n  # changes on your hosts to the source version on the server.\n  #config_version = \n\n  # Whether to use the zlib library\n  zlib = true\n\n  # A command to run before every agent run. If this command returns a\n  # non-zero return code, the entire Puppet run will fail.\n  #prerun_command = \n\n  # A command to run after every agent run. If this command returns a non-zero\n  # return code, the entire Puppet run will be considered to have failed, even\n  # though it might have performed work during the normal run.\n  #postrun_command = \n\n  # Freezes the 'main' class, disallowing any code to be added to it. This\n  # essentially means that you can't have any code outside of a node, class,\n  # or definition other than in the site manifest.\n  #freeze_main = false\n\n  # The name to use when handling certificates.\n  certname = balum.internal.bedroomprogrammers.net\n\n  # The certificate directory.\n  certdir = /var/lib/puppet/ssl/certs\n\n  # Where SSL certificates are kept.\n  ssldir = /var/lib/puppet/ssl\n\n  # The public key directory.\n  publickeydir = /var/lib/puppet/ssl/public_keys\n\n  # Where host certificate requests are stored.\n  requestdir = /var/lib/puppet/ssl/certificate_requests\n\n  # The private key directory.\n  privatekeydir = /var/lib/puppet/ssl/private_keys\n\n  # Where the client stores private certificate information.\n  privatedir = /var/lib/puppet/ssl/private\n\n  # Where puppet agent stores the password for its private key.\n  passfile = /var/lib/puppet/ssl/private/password\n\n  # Where individual hosts store and look for their certificate information.\n  hostcsr = /var/lib/puppet/ssl/csr_balum.internal.bedroomprogrammers.net.pem\n  hostcert = /var/lib/puppet/ssl/certs/balum.internal.bedroomprogrammers.net.pem\n  hostprivkey = /var/lib/puppet/ssl/private_keys/balum.internal.bedroomprogrammers.net.pem\n  hostpubkey = /var/lib/puppet/ssl/public_keys/balum.internal.bedroomprogrammers.net.pem\n\n  localcacert = /var/lib/puppet/ssl/certs/ca.pem\n\n  # Where the host's certificate revocation list can be found. This is\n  # distinct from the certificate authority's CRL.\n  hostcrl = /var/lib/puppet/ssl/crl.pem\n\n  # Whether certificate revocation should be supported by downloading a\n  # Certificate Revocation List (CRL) to all clients. If enabled, CA chaining\n  # will almost definitely not work.\n  certificate_revocation = true\n\n  # Where Puppet should store plugins that it pulls down from the central\n  # server.\n  plugindest = /var/lib/puppet/lib\n\n  # From where to retrieve plugins. The standard Puppet `file` type is used\n  # for retrieval, so anything that is a valid file source can be used here.\n  pluginsource = puppet://puppet/plugins\n\n  # Whether plugins should be synced with the central server.\n  pluginsync = true\n\n  # What files to ignore when pulling down plugins.\n  pluginsignore = .git\n\n  # Where Puppet should look for facts. Multiple directories should be\n  # separated by the system path separator character. (The POSIX path\n  # separator is ':', and the Windows path separator is ';'.)\n  factpath = /var/lib/puppet/lib/facter:/var/lib/puppet/facts\n\n  # Where Puppet should store facts that it pulls down from the central\n  # server.\n  factdest = /var/lib/puppet/facts/\n\n  # From where to retrieve facts. The standard Puppet `file` type is used for\n  # retrieval, so anything that is a valid file source can be used here.\n  factsource = puppet://puppet/facts/\n\n  # Whether facts should be synced with the central server.\n  factsync = true\n\n  # What files to ignore when pulling down facts.\n  factsignore = .git\n\n  # An external command that can produce node information. The command's\n  # output must be a YAML dump of a hash, and that hash must have a `classes`\n  # key and/or a `parameters` key, where `classes` is an array or hash and\n  # `parameters` is a hash. For unknown nodes, the command should exit with a\n  # non-zero exit code. This command makes it straightforward to store your\n  # node mapping information in other data sources like databases.\n  #external_nodes = none\n\n  # The module repository\n  #module_repository = http://forge.puppetlabs.com\n\n  # The directory into which module tool data is stored\n  module_working_dir = /var/lib/puppet/puppet-module\n\n  # Certificate authority configuration\n  ca_name = Puppet CA: balum.internal.bedroomprogrammers.net\n  cadir = /var/lib/puppet/ssl/ca\n  cacert = /var/lib/puppet/ssl/ca/ca_crt.pem\n  cakey = /var/lib/puppet/ssl/ca/ca_key.pem\n  capub = /var/lib/puppet/ssl/ca/ca_pub.pem\n  cacrl = /var/lib/puppet/ssl/ca/ca_crl.pem\n  caprivatedir = /var/lib/puppet/ssl/ca/private\n  csrdir = /var/lib/puppet/ssl/ca/requests\n\n  # Where the CA stores signed certificates.\n  signeddir = /var/lib/puppet/ssl/ca/signed\n\n  # Where the CA stores the password for the private key\n  capass = /var/lib/puppet/ssl/ca/private/ca.pass\n\n  # Where the serial number for certificates is stored.\n  serial = /var/lib/puppet/ssl/ca/serial\n\n  # Whether to enable autosign. Valid values are true (which autosigns any key\n  # request, and is a very bad idea), false (which never autosigns any key\n  # request), and the path to a file, which uses that configuration file to\n  # determine which keys to sign.\n  autosign = /etc/puppet/autosign.conf\n\n  # Whether to allow a new certificate request to overwrite an existing\n  # certificate.\n  allow_duplicate_certs = false\n\n  # The default TTL for new certificates; valid values must be an integer,\n  # optionally followed by one of the units 'y' (years of 365 days), 'd'\n  # (days), 'h' (hours), or 's' (seconds). The unit defaults to seconds. If\n  # this setting is set, ca_days is ignored. Examples are '3600' (one hour)\n  # and '1825d', which is the same as '5y' (5 years) \n  ca_ttl = 3y\n\n  # The type of hash used in certificates.\n  ca_md = sha256\n\n  # The bit length of the certificates.\n  req_bits = 4096\n\n  # The bit length of keys.\n  keylength = 4096\n\n  # A Complete listing of all certificates\n  cert_inventory = /var/lib/puppet/ssl/ca/inventory.txt\n\n  # The configuration file for master.\n  config = /etc/puppet/puppet.conf\n\n  # The pid file\n  pidfile = /var/run/puppet/master.pid\n\n  # The address a listening server should bind to. Mongrel servers default to\n  # 127.0.0.1 and WEBrick defaults to 0.0.0.0.\n  bindaddress = 0.0.0.0\n\n  # The type of server to use. Currently supported options are webrick and\n  # mongrel. If you use mongrel, you will need a proxy in front of the process\n  # or processes, since Mongrel cannot speak SSL.\n  servertype = webrick\n\n  # The user puppet master should run as.\n  user = puppet\n\n  # The group puppet master should run as.\n  group = puppet\n\n  # Where puppet master looks for its manifests.\n  manifestdir = /etc/puppet/manifests\n\n  # The entry-point manifest for puppet master.\n  manifest = /etc/puppet/manifests/site.pp\n\n  # Code to parse directly. This is essentially only used by `puppet`, and\n  # should only be set if you're writing your own Puppet executable.\n  #code = \n\n  # Where puppet master logs. This is generally not used, since syslog is the\n  # default log destination.\n  masterlog = /var/log/puppet/puppetmaster.log\n\n  # Where the puppet master web server logs.\n  masterhttplog = /var/log/puppet/masterhttp.log\n\n  # Which port puppet master listens on.\n\n  # How the puppet master determines the client's identity and sets the\n  # 'hostname', 'fqdn' and 'domain' facts for use in the manifest, in\n  # particular for determining which 'node' statement applies to the client.\n  # \n  # Possible values are 'cert' (use the subject's CN in the client's\n  # certificate) and 'facter' (use the hostname that the client reported in\n  # its facts)\n  node_name = cert\n\n  # Where FileBucket files are stored.\n  bucketdir = /var/lib/puppet/bucket\n\n  # The configuration file that defines the rights to the different rest\n  # indirections. This can be used as a fine-grained authorization system for\n  # `puppet master`.\n  rest_authconfig = /etc/puppet/auth.conf\n\n  # Wether the master should function as a certificate authority.\n  ca = true\n\n  # The search path for modules, as a list of directories separated by the\n  # system path separator character. (The POSIX path separator is ':', and the\n  # Windows path separator is ';'.)\n  modulepath = /etc/puppet/modules:/usr/share/puppet/modules\n\n  # The directory in which YAML data is stored, usually in a subdirectory.\n  yamldir = /var/lib/puppet/yaml\n\n  # The directory in which serialized data is stored, usually in a\n  # subdirectory.\n  server_datadir = /var/lib/puppet/server_data\n\n  # The list of reports to generate. All reports are looked for in\n  # `puppet/reports/name.rb`, and multiple report names should be\n  # comma-separated (whitespace is okay).\n  #reports = store\n\n  # The directory in which to store reports received from the client. Each\n  # client gets a separate subdirectory.\n  reportdir = /var/lib/puppet/reports\n\n  # The URL used by the http reports processor to send reports\n  #reporturl = http://localhost:3000/reports/upload\n\n  # Where the fileserver configuration is stored.\n  fileserverconfig = /etc/puppet/fileserver.conf\n\n  # Whether to only search for the complete hostname as it is in the\n  # certificate when searching for node information in the catalogs.\n  #\n  # TODO: Probably for the best to set this to true\n  #strict_hostname_checking = false\n\n  # Whether to store each client's configuration, including catalogs, facts,\n  # and related data. This also enables the import and export of resources in\n  # the Puppet language - a mechanism for exchange resources between nodes.\n  # \n  # By default this uses ActiveRecord and an SQL database to store and query\n  # the data; this, in turn, will depend on Rails being available. You can\n  # adjust the backend using the storeconfigs_backend setting.\n  #\n  # TODO: This would probably be useful\n  #storeconfigs = false\n\n  # Configure the backend terminus used for StoreConfigs. By default, this\n  # uses the ActiveRecord store, which directly talks to the database from\n  # within the Puppet Master process.\n  #storeconfigs_backend = active_record\n\n  # The directory where RRD database files are stored. Directories for each\n  # reporting host will be created under this directory.\n  rrddir = /var/lib/puppet/rrd\n\n  # How often RRD should expect data. This should match how often the hosts\n  # report back to the server.\n  rrdinterval = 1800\n\n  # The root directory of devices' $vardir\n  devicedir = /var/lib/puppet/devices\n\n  # Path to the device config file for puppet device\n  deviceconfig = /etc/puppet/device.conf\n\n  # The explicit value used for the node name for all requests the agent makes\n  # to the master. WARNING: This setting is mutually exclusive with\n  # node_name_fact. Changing this setting also requires changes to the default\n  # auth.conf configuration on the Puppet Master. Please see\n  # http://links.puppetlabs.com/node_name_value for more information.\n  node_name_value = balum.internal.bedroomprogrammers.net\n\n  # Where puppet agent caches the local configuration. An extension indicating\n  # the cache format is added automatically.\n  localconfig = /var/lib/puppet/state/localconfig\n\n  # Where puppet agent and puppet master store state associated with the\n  # running configuration. In the case of puppet master, this file reflects\n  # the state discovered through interacting with clients.\n  statefile = /var/lib/puppet/state/state.yaml\n\n  # The directory in which client-side YAML data is stored.\n  clientyamldir = /var/lib/puppet/client_yaml\n\n  # The directory in which serialized data is stored on the client.\n  client_datadir = /var/lib/puppet/client_data\n\n  # The file in which puppet agent stores a list of the classes associated\n  # with the retrieved configuration. Can be loaded in the separate `puppet`\n  # executable using the `--loadclasses` option.\n  classfile = /var/lib/puppet/state/classes.txt\n\n  # The file in which puppet agent stores a list of the resources associated\n  # with the retrieved configuration.\n  resourcefile = /var/lib/puppet/state/resources.txt\n\n  # The log file for puppet agent.  This is generally not used.\n  # The default value is '$logdir/puppetd.log'.\n  puppetdlog = /var/log/puppet/puppetd.log\n\n  # The server to which server puppet agent should connect\n  server = balum.internal.bedroomprogrammers.net\n\n  # Whether puppet agent should ignore schedules. This is useful for initial\n  # puppet agent runs.\n  ignoreschedules = false\n\n  # Which port puppet agent listens on.\n  puppetport = 8139\n\n  # Whether puppet agent should be run in noop mode.\n  noop = false\n\n  # How often puppet agent applies the client configuration; in seconds. Note\n  # that a runinterval of 0 means \"run continuously\" rather than \"never run\".\n  # If you want puppet agent to never run, you should start it with the\n  # `--no-client` option.\n  runinterval = 1800\n\n  # Whether puppet agent should listen for connections. If this is true, then\n  # puppet agent will accept incoming REST API requests, subject to the\n  # default ACLs and the ACLs set in the `rest_authconfig` file. Puppet agent\n  # can respond usefully to requests on the `run`, `facts`, `certificate`,\n  # and `resource` endpoints.\n  #\n  # TODO: This may be valuable\n  #listen = false\n\n  # The server to use for certificate authority requests. It's a separate\n  # server because it cannot and does not need to horizontally scale.\n  ca_server = balum.internal.bedroomprogrammers.net\n\n  # The port to use for the certificate authority.\n  ca_port = 8140\n\n  # The preferred means of serializing ruby instances for passing over the\n  # wire. This won't guarantee that all instances will be serialized using\n  # this method, since not all classes can be guaranteed to support this\n  # format, but it will be used for all classes that support it.\n  preferred_serialization_format = pson\n\n  # A lock file to temporarily stop puppet agent from doing anything.\n  puppetdlockfile = /var/lib/puppet/state/puppetdlock\n\n  # Whether to use the cached configuration when the remote configuration will\n  # not compile. This option is useful for testing new configurations, where\n  # you want to fix the broken configuration rather than reverting to a\n  # known-good one.\n  usecacheonfailure = true\n\n  # Whether to only use the cached catalog rather than compiling a new catalog\n  # on every run. Puppet can be run with this enabled by default and then\n  # selectively disabled when a recompile is desired.\n  use_cached_catalog = false\n\n  # Ignore cache and always recompile the configuration. This is useful for\n  # testing new configurations, where the local cache may in fact be stale\n  # even if the timestamps are up to date - if the facts change or if the\n  # server changes.\n  #ignorecache = false\n\n  # Whether facts should be made all lowercase when sent to the server.\n  #downcasefacts = false\n\n  # Facts that are dynamic; these facts will be ignored when deciding whether\n  # changed facts should result in a recompile. Multiple facts should be\n  # comma-separated.\n  #dynamicfacts = memorysize,memoryfree,swapsize,swapfree\n\n  # The maximum time to delay before runs. Defaults to being the same as the\n  # run interval.\n  splaylimit = 1800\n\n  # Whether to sleep for a pseudo-random (but consistent) amount of time\n  # before a run.\n  splay = true\n\n  # Where FileBucket files are stored locally.\n  clientbucketdir = /var/lib/puppet/clientbucket\n\n  # How long the client should wait for the configuration to be retrieved\n  # before considering it a failure. This can help reduce flapping if too many\n  # clients contact the server at one time.\n  configtimeout = 60\n\n  # The server to send transaction reports to.\n  report_server = balum.internal.bedroomprogrammers.net\n\n  # The port to communicate with the report_server.\n  report_port = 8140\n\n  # The server to send facts to.\n  inventory_server = balum.internal.bedroomprogrammers.net\n\n  # The port to communicate with the inventory_server.\n  inventory_port = 8140\n\n  # Whether to send reports after every transaction.\n  report = true\n\n  # Where puppet agent stores the last run report summary in yaml format.\n  lastrunfile = /var/lib/puppet/state/last_run_summary.yaml\n\n  # Where puppet agent stores the last run report in yaml format.\n  lastrunreport = /var/lib/puppet/state/last_run_report.yaml\n\n  # Whether to create dot graph files for the different configuration graphs.\n  # These dot files can be interpreted by tools like OmniGraffle or dot (which\n  # is part of ImageMagick).\n  graph = true\n\n  # Where to store dot-outputted graphs.\n  graphdir = /var/lib/puppet/state/graphs\n\n  # Allow http compression in REST communication with the master. This setting\n  # might improve performance for agent -\u003e master communications over slow\n  # WANs.\n  #\n  # Your puppet master needs to support compression (usually by activating\n  # some settings in a reverse-proxy in front of the puppet master, which\n  # rules out webrick).\n  #\n  # It is harmless to activate this settings if your master doesn't support\n  # compression, but if it supports it, this setting might reduce performance\n  # on high-speed LANs.\n  http_compression = false\n\n  # During an inspect run, whether to archive files whose contents are audited\n  # to a file bucket.\n  archive_files = true\n\n  # During an inspect run, the file bucket server to archive files to if\n  # archive_files is set.\n  archive_file_server = balum.internal.bedroomprogrammers.net\n\n  # The mapping between reporting tags and email addresses.\n  tagmap = /etc/puppet/tagmail.conf\n\n  # Where to find the sendmail binary with which to send email.\n  sendmail = /usr/sbin/sendmail\n\n  # The 'from' email address for the reports.\n  reportfrom = puppet-master@balum.internal.bedroomprogrammers.net\n\n  # The server through which to send email reports.\n  smtpserver = none\n\n  # The database cache for client configurations. Used for querying within the\n  # language.\n  dblocation = /var/lib/puppet/state/clientconfigs.sqlite3\n\n  # The type of database to use.\n  dbadapter = sqlite3\n\n  # Whether to automatically migrate the database.\n  dbmigrate = true\n\n\n  # The database server for caching.\n  #dbserver = localhost\n  #dbport = \n  #dbname = puppet\n  #dbuser = puppet\n  #dbpassword = puppet\n\n  # The number of database connections for networked databases.\n  #dbconnections = \n\n  # The database socket location. Only used when networked databases are used.\n  # Will be ignored if the value is an empty string.\n  #dbsocket = \n\n  # Where Rails-specific logs are sent\n  railslog = /var/log/puppet/rails.log\n\n  # The log level for Rails connections. The value must be a valid log level\n  # within Rails. Production environments normally use `info` and other\n  # environments normally use `debug`.\n  rails_loglevel = info\n\n  # The url where the puppet couchdb database will be created\n  #couchdb_url = http://127.0.0.1:5984/puppet\n\n  # Tags to use to find resources. If this is set, then only resources tagged\n  # with the specified tags will be applied. Values must be comma-separated.\n  #tags = \n\n  # Whether each resource should log when it is being evaluated. This allows\n  # you to interactively see exactly what is being done.\n  #evaltrace = false\n\n  # Whether to print a transaction summary.\n  #summarize = false\n\n  # Whether to search for node configurations in LDAP. See\n  # http://projects.puppetlabs.com/projects/puppet/wiki/LDAP_Nodes for more\n  # information.\n  #ldapnodes = false\n\n  # Whether SSL should be used when searching for nodes. Defaults to false\n  # because SSL usually requires certificates to be set up on the client\n  # side.\n  #ldapssl = false\n\n  # Whether TLS should be used when searching for nodes. Defaults to false\n  # because TLS usually requires certificates to be set up on the client\n  # side.\n  #ldaptls = false\n\n  # The LDAP server. Only used if `ldapnodes` is enabled.\n  #ldapserver = ldap.example.org\n\n  # The LDAP port.  Only used if `ldapnodes` is enabled.\n  #ldapport = 389\n\n  # The search string used to find an LDAP node.\n  #ldapstring = (\u0026(objectclass=puppetClient)(cn=%s))\n\n  # The LDAP attributes to use to define Puppet classes. Values should be\n  # comma-separated.\n  #ldapclassattrs = puppetclass\n\n  # The LDAP attributes that should be stacked to arrays by adding the values\n  # in all hierarchy elements of the tree. Values should be comma-separated.\n  #ldapstackedattrs = puppetvar\n\n  # The LDAP attributes to include when querying LDAP for nodes. All returned\n  # attributes are set as variables in the top-level scope. Multiple values\n  # should be comma-separated. The value 'all' returns all attributes.\n  #ldapattrs = all\n\n  # The attribute to use to define the parent node.\n  #ldapparentattr = parentnode\n\n  # The user to use to connect to LDAP. Must be specified as a full DN.\n  #ldapuser = \n\n  # The password to use to connect to LDAP.\n  #ldappassword = \n\n  # The search base for LDAP searches. It's impossible to provide a meaningful\n  # default here, although the LDAP libraries might have one already set.\n  # Generally, it should be the 'ou=Hosts' branch under your main directory.\n  #ldapbase = \n\n  # Whether to use lexical scoping (vs. dynamic).\n  #lexical = false\n\n  # Where Puppet looks for template files. Can be a list of colon-separated\n  # directories.\n  templatedir = /var/lib/puppet/templates\n\n  # Document all resources\n  #document_all = false\n```\n\n## Puppet Client\n\n### Installation\n\n```\nyum install puppet -y\n```\n","created_at":-62135596800,"fuzzy_word_count":4900,"path":"/notes/puppet/","published_at":1507587428,"reading_time":23,"tags":null,"title":"Puppet","type":"notes","updated_at":1507587428,"weight":0,"word_count":4825},{"cid":"669512b0bd8e0a1cab27c8d895f8dd2f096b491c","content":"\n***Note: This page is quite old and is likely out of date. My opinions may have\nalso changed dramatically since this was written. It is here as a reference\nuntil I get around to updating it.***\n\nQpid is an open source AMQP broker, providing transaction management, queuing,\ndistribution, security, management, clustering, and federation.\n\n* http://wiki.openstack.org/QpidSupport\n* http://www.linuxjournal.com/magazine/advanced-message-queuing-protocol-amqp?page=0,1\n\n## Installation\n\nFedora provides a package for qpid called `qpid-cpp-server` which can be\ninstalled like so:\n\n```\nyum install qpid-cpp-server qpid-cpp-server-ssl qpid-cpp-server-store \\\n  qpid-cpp-server-ha -y\n```\n\nAdditional packages that may be of use:\n\n* qpid-tools\n* qpid-qmf\n\n## Configuration\n\nThe following configuration file assumes the rest of the configuration on this\npage. You'll want to replace the existing configuration at `/etc/qpidd.conf`\nwith the following:\n\n```ini\n##### General Configuration #####\n\n# Directory to contain persistent data\ndata-dir=/var/lib/qpidd/\n\nworker-threads=3\n\n##### Queue Configuration #####\n\n# Set queue events async, used for services like replication\nasync-queue-events=no\n\n# How often to attempt purging expired messages from the queues\nqueue-purge-interval=600\n\n# Default storage limit of any given queue\ndefault-queue-limit=104857600\n\n# The ratio of any specified queue limit at which an event will be raised\ndefault-event-threshold-ratio=80\n\n# Percent of queues maximum capacity at which flow control is activated and\n# deactivated.\ndefault-flow-stop-threshold=80\ndefault-flow-resume-threshold=70\n\n# Group identifier to assign to messages delivered to a message group queue that\n# do not contain an identifier.\ndefault-message-group=qpid.no-group\n\n# Add current time to each received message\nenable-timestamp=yes\n\n##### Management Options #####\n\n# Enable management, publishing data every 10 seconds, with QMF protocol 1 and\n# 2\nmgmt-enable=yes\nmgmt-publish=yes\nmgmt-pub-interval=10\nmgmt-qmf1=yes\nmgmt-qmf2=yes\n\n##### Networking Configuration #####\n\nconnection-backlog=10\n\nlink-heartbeat-interval=120\nlink-maintenace-interval=2\n\n# Maximum time a connection can take to send the initial protocol negotiation\n# (in milliseconds).\nmax-negotiate-time=2000\n\n# Port to listen on for unencrypted connections\nport=5672\n\n# Set TCP_NODELAY on TCP connections\ntcp-nodelay=yes\n\n##### Logging Options #####\n\nlog-enable=info+\n\nlog-category=yes\nlog-level=yes\nlog-time=no\n\nlog-to-stderr=no\nlog-to-stdout=no\nlog-to-syslog=yes\n\n##### Persistent Storage Options #####\n\n# Number of pages in each journal (1 page is 64Kb)\njfile-size-pgs=24\n\n# Default number of files for each journal instance (queue)\nnum-jfiles=8\n\ntruncate=no\n\n# Size of the pages in the write page cache in Kb, allowable values are powers\n# of 2\nwcache-page-size=32\n\n##### Authentication \u0026 Authorization Configurations #####\n\n# Enable authentication, and configured to use the QPID realm\nauth=yes\nrealm=QPID\n\n# Get SASL configuration from standard redhat location\nsasl-config=/etc/sasl2/\n\n# The policy file to load from, loaded from the data dir\nacl-file=qpid.acl\n\n# Maximum combined number of connections allowed (0 is no limit)\nmax-connections=500\nmax-connections-per-user=0\nmax-connections-per-ip=0\n\n##### SSL Settings #####\n\nssl-port=5671\n\n# File containing password to use for accessing the certificate database\nssl-cert-password-file=/var/lib/qpidd/ssl-db-pass\n\n# Where the cert database is stored, the data directory is as good as any\nssl-cert-db=/var/lib/qpidd\n\n# Name of the cerificate to use (usually the hostname)\nssl-cert-name=agni\n\n# Whether or not the server will accept unencrypted connections, there is of\n# course overhead to encrypted connections and if the only services that will\n# be talking to it will be the localhost then there is no need to require it...\n# External connections should of course be encrypted wherever possible.\nrequire-encryption=no\n```\n\nYou'll want to enable and start the service once the configuration is complete.\n\n```\nsystemctl enable qpidd.service\nsystemctl start qpidd.service\n```\n\n### Enabling User/Pass Authentication\n\nBy default only ANONYMOUS authentication is enabled which isn't good for any\nproduction systems... Qpid uses SASL for user authentication and that is how we\nneed to configure what system to make use of. Open up the file\n`/etc/sasl2/qpidd.conf` and replace it's contents with the following:\n\n```\nauxprop_plugin: sasldb\nmech_list: DIGEST-MD5 PLAIN+SSL\npwcheck_method: auxprop\nsasldb_path: /var/lib/qpidd/qpidd.sasldb\nsql_select: dummy select\n```\n\n### Creating Users\n\nI've named the SASL authentication realm after the service that uses it, in\nthis case Qpid (makes sense no?). You'll need to create at least one user to\nmake use of authentication, I chose to make one user for administration tasks\n(admin), and one per server named after the server (in this case the server is\nnamed openstack after the service I mainly use Qpid for).\n\nCreate the user's as root:\n\n```\nsaslpasswd2 -f /var/lib/qpidd/qpidd.sasldb -u QPID admin\nsaslpasswd2 -f /var/lib/qpidd/qpidd.sasldb -u QPID openstack\n```\n\nSet STRONG passwords for both as arbitrary instructions can be provided to\nvarious openstack services through this service.\n\nThe saslpasswd2 command will additionally create the password file, but with\nincorrect permissions. Set the ownership and strengthen the permissions like\nso:\n\n```\nchown qpidd:qpidd /var/lib/qpidd/qpidd.sasldb\nchmod 0640 /var/lib/qpidd/qpidd.sasldb\n```\n\nIf you have started Qpid with `auth=yes` configured before creating the account\nit will automatically create this file and add a user with the username and\npassword 'guest'.\n\n### Listing Users\n\nYou can list all the configured realm / username combinations with the\nfollowing command:\n\n```\nsasldblistusers2 -f /var/lib/qpidd/qpidd.sasldb\n```\n\n### User ACLs\n\nThe ACL files are pretty straight-forward and plain text. The file lives at\n`/var/lib/qpidd/qpid.acl` which doesn't exist initially and will need to be\ncreated. This is a solid initial ACL allowing the admin and openstack access to\nany permissions they need while preventing anything else.\n\nIn the future I'll need to make this more fine-grained. This is more useful\nwhen using Kerberos as the back end which would have more users you wouldn't\nwant to have access.\n\n```\n# Define the admins and let them do whatever they want\ngroup admins admin@Qpid\nacl allow admins all all\n\n# Define the server accounts and set their permissions\ngroup servers openstack@Qpid\nacl allow servers all all\n\n# Default deny with logging on any other attempts\nacl deny-log all all\n```\n\nEnsure the ownership and permissions on the file are appropriate:\n\n```\nchown qpidd:qpidd /var/lib/qpidd/qpid.acl\nchmod 0640 /var/lib/qpidd/qpid.acl\n```\n\n#### ACL BML Syntax\n\nACLs are case-insensitve, all white space is essentially ignored except when\ndelimiting between syntax types. Lines can be wrapped by ending the line with a\nbackslash.\n\n```\nuser = username[/domain[@realm]]\nuser-list = user1 user2 user3 ...\ngroup-name-list = group1 group2 group3 ...\n\ngroup \u003cgroup-name\u003e = [user-list] [group-name-list]\n\npermission = [allow|allow-log|deny|deny-log]\naction = [consume|publish|create|access|bind|unbind|delete|purge|update]\nobject = [virtualhost|queue|exchange|broker|link|route|method]\nproperty = [name|durable|owner|routingkey|autodelete|exclusive|\n            type|alternate|queuename|schemapackage|schemaclass|\n            queuemaxsizelowerlimit|queuemaxsizeupperlimit|\n            queuemaxcountlowerlimit|queuemaxcountupperlimit]\n\nacl permission {\u003cgroup-name\u003e|\u003cuser-name\u003e|\"all\"} {action|\"all\"} [object|\"all\" \n            [property=\u003cproperty-value\u003e ...]]\n```\n\n#### Action Listing\n\n| Action  | Description                                                                                        |\n| ------- | -------------------------------------------------------------------------------------------------- |\n| consume | Applied when subscriptions are created                                                             |\n| publish | Applied on a per message basis on publish message transfers, this rule consumes the most resources |\n| create  | Applied when an object is created, such as bindings, queues, exchanges, links                      |\n| access  | Applied when an object is read or accessed                                                         |\n| bind    | Applied when objects are bound together                                                            |\n| unbind  | Applied when objects are unbound                                                                   |\n| delete  | Applied when objects are deleted                                                                   |\n| purge   | Similar to delete but the action is performed on more than one object                              |\n| update  | Applied when an object is updated                                                                  |\n\n#### Object Listing\n\n| Object   | Description                          |\n| -------- | ------------------------------------ |\n| queue    | A queue                              |\n| exchange | An exchange                          |\n| broker   | The broker                           |\n| link     | A federation or inter-broker link    |\n| method   | Management or agent or broker method |\n\n#### Property Listing\n\n| Property                | Type    | Description                                                                    | Usage                                           |\n| ----------------------- | ------- | ------------------------------------------------------------------------------ | ----------------------------------------------- |\n| name                    | String  | Object name, such as a queue name or exchange name.                            |                                                 |\n| durable                 | Boolean | Indicates the object is durable                                                | CREATE QUEUE, CREATE EXCHANGE                   |\n| routingkey              | String  | Specifies routing key                                                          | BIND EXCHANGE, UNBIND EXCHANGE, ACCESS EXCHANGE |\n| autodelete              | Boolean | Indicates whether or not the object gets deleted when the connection is closed | CREATE QUEUE                                    |\n| exclusive               | Boolean | Indicates the presence of an exclusive flag                                     | CREATE QUEUE                                    |\n| type                    | String  | Type of exchange, such as topic, fanout, or xml                                | CREATE EXCHANGE                                 |\n| alternate               | String  | Name of the alternate exchange                                                 | CREATE EXCHANGE, CREATE QUEUE                   |\n| queuename               | String  | Name of the queue                                                              | ACCESS EXCHANGE                                 |\n| schemapackage           | String  | QMF schema package name                                                        | ACCESS METHOD                                   |\n| schemaclass             | String  | QMF schema class name                                                          | ACCESS METHOD                                   |\n| queuemaxsizelowerlimit  | Integer | Minimum value for queue.max_size                                               | CREATE QUEUE                                    |\n| queuemaxsizeupperlimit  | Integer | Maximum value for queue.max_size                                               | CREATE QUEUE                                    |\n| queuemaxcountlowerlimit | Integer | Minimum value for queue.max_count                                              | CREATE QUEUE                                    |\n| queuemaxcountupperlimit | Integer | Maximum value for queue.max_count                                              | CREATE QUEUE                                    |\n\n### SSL\n\nUnfortunately SSL for Qpid isn't as easy as generating PEM certificates and\npointing the config at them. Qpid makes use of a Mozilla Network Security\nServices database. These databases can be created using certutil.\n\nFirst we'll need to initialize the database:\n\n```\ncertutil -N -d /var/lib/qpidd/\n```\n\nProvide a strong password to the database and put a copy in the file\n`/var/lib/qpidd/ssl-db-pass` on it's own with no newline.\n\nI already have PKI in place and a trusted [Certificate Authority][1], so I\njust have to import my trusted certificate authority chain. Generate a\ncertificate for this server and import it's certificate and chain.\n\nImport the CA cert:\n\n```\ncertutil -A -n cert-authority -t \"TC,,\" -i ca.crt -d /var/lib/qpidd\n```\n\nThe server certificate is a bit trickier, before we can import an OpenSSL\ngenerated PEM format key set we'll need to convert it to a pkcs12 file, luckily\nOpenSSL plays well with others:\n\n```\nopenssl pkcs12 -export -out qpid.pfx -inkey qpid.key -in qpid.crt -certfile ca.crt\n```\n\nImport the newly generate pkcs12 file, it will firstly ask for the password for\nthe database, and then the password for the pkcs12 file:\n\n```\npk12util -d /var/lib/qpidd/ -i qpid.pfx\n```\n\nAnd be sure to clean up after yourself:\n\n```\nrm qpid.pfx\n```\n\nSet the permissions on all of the files we just created:\n\n```\nchown qpidd:qpidd /var/lib/qpidd/{cert8.db,key3.db,ssl-db-pass}\nchmod 0640 /var/lib/qpidd/{cert8.db,key3.db,ssl-db-pass}\n```\n\n### Firewall\n\n```\n# Encrypted Qpid connections (Unencrypted are 5672 but those shouldn't be\n# accessed remotely)\n-A SERVICE -m tcp -p tcp --dport 5671 -j ACCEPT\n```\n\n[1]: {{\u003c ref \"./certificate_authority.md\" \u003e}}\n","created_at":-62135596800,"fuzzy_word_count":1600,"path":"/notes/qpid/","published_at":1540945455,"reading_time":8,"tags":null,"title":"Qpid","type":"notes","updated_at":1540945455,"weight":0,"word_count":1573},{"cid":"9677abc0c2f8d86d8abd5614310b5f156c3e50f1","content":"\n***Note: This page is quite old and is likely out of date. My opinions may have\nalso changed dramatically since this was written. It is here as a reference\nuntil I get around to updating it.***\n\n## Installation\n\n```\nyum install redis -y\n```\n\n## Configuration\n\nDefault configuration:\n\n```\n############### GENERAL ###############\n\n# Daemonize the Redis process and write out the specified PID file\n#\ndaemonize no\npidfile /var/run/redis/redis.pid\n\n# TCP Listen mode, if bind is not specified it will listen on all available\n# interfaces\n#\nbind 127.0.0.1\nport 6379\n\n# Specify the path for the unix socket that will be used to listen for incoming\n# connections. There is no default, so Redis will not listen on a unix socket\n# when not specified.\n#\n#unixsocket /tmp/redis.sock\n#unixsocketperm 755\n\n# Close the connection after a client is idle for N seconds (0 to disable)\n#\ntimeout 30\ntcp-keepalive 5\n\n# Set the log output:\n#   debug (a lot of information, useful for development/testing)\n#   verbose (many rarely useful info, but not a mess like the debug level)\n#   notice (moderately verbose, what you want in production probably)\n#   warning (only very important / critical messages are logged)\n#\nloglevel notice\n\n# Specify the log file name. Also 'stdout' can be used to force Redis to log on\n# the standard output. Note that if you use standard output for logging but\n# daemonize, logs will be sent to /dev/null\n#\nlogfile /var/log/redis/redis.log\n\n# To enable logging to the system logger, just set 'syslog-enabled' to yes, and\n# optionally update the other syslog parameters to suit your needs.\n#\nsyslog-enabled yes\n\n# Specify the syslog identity.\n#\nsyslog-ident redis\n\n# Specify the syslog facility.  Must be USER or between LOCAL0-LOCAL7.\n#\nsyslog-facility local0\n\n# Set the number of databases. The default database is DB 0, you can select a\n# different one on a per-connection basis using SELECT \u003cdbid\u003e where dbid is a\n# number between 0 and 'databases' - 1\n#\ndatabases 16\n\n############### SNAPSHOTTING ###############\n\n# The name of the file to sync the in memory database too.\n#\ndbfilename database.rdb\n\n# The working directory.\n#\n# The DB will be written inside this directory, with the filename specified\n# above using the 'dbfilename' configuration directive. Also the Append Only\n# File will be created inside this directory. Note that you must specify a\n# directory here, not a file name.\n#\ndir /var/lib/redis/\n\n# Compress string objects using LZF when dump .rdb databases? For default that's\n# set to 'yes' as it's almost always a win. If you want to save some CPU in the\n# saving child set it to 'no' but the dataset will likely be bigger if you have\n# compressible values or keys.\n#\nrdbcompression yes\n\n# Save the DB on disk:\n#   save \u003cseconds\u003e \u003cchanges\u003e\n#\n# Will save the DB if both the given number of seconds and the given number of\n# write operations against the DB occurred. In the configuration below, the\n# database will be saved when any of the following rules are matched:\n#\n#   after 900 sec (15 min) if at least 1 key changed\n#   after 300 sec (5 min) if at least 10 keys changed\n#   after 60 sec if at least 10000 keys changed\n#\n# You can disable saving to disk at all commenting all the \"save\" lines.\n#\nsave 900 1\nsave 300 10\nsave  60 10000\n\n############### REPLICATION ###############\n\n# Master-Slave replication. Use slaveof to make a Redis instance a copy of\n# another Redis server. Note that the configuration is local to the slave so for\n# example it is possible to configure the slave to save the DB with a different\n# interval, or to listen to another port, and so on.\n#\n#slaveof \u003cmasterip\u003e \u003cmasterport\u003e\n\n# If the master is password protected (using the \"requirepass\" configuration\n# directive below) it is possible to tell the slave to authenticate before\n# starting the replication synchronization process, otherwise the master will\n# refuse the slave request.\n#\n#masterauth \u003cmaster-password\u003e\n\n# When a slave lost the connection with the master, or when the replication is\n# still in progress, the slave can act in two different ways:\n#\n# 1) if slave-serve-stale-data is set to 'yes' (the default) the slave will\n#    still reply to client requests, possibly with out of data data, or the data\n#    set may just be empty if this is the first synchronization.\n#\n# 2) if slave-serve-stale data is set to 'no' the slave will reply with an error\n#    \"SYNC with master in progress\" to all the kind of commands but to INFO and\n#    SLAVEOF.\n#\nslave-serve-stale-data yes\n\n# Slaves send PINGs to server in a predefined interval. It's possible to change\n# this interval with the repl_ping_slave_period option. The default value is 10\n# seconds.\n#\nrepl-ping-slave-period 10\n\n# The following option sets a timeout for both Bulk transfer I/O timeout and\n# master data or ping response timeout. The default value is 60 seconds.\n#\n# It is important to make sure that this value is greater than the value\n# specified for repl-ping-slave-period otherwise a timeout will be detected every\n# time there is low traffic between the master and the slave.\n#\nrepl-timeout 60\n\n############### SECURITY ###############\n\n# Require clients to issue AUTH \u003cPASSWORD\u003e before processing any other commands.\n# This might be useful in environments in which you do not trust others with\n# access to the host running redis-server.\n#\n# This should stay commented out for backward compatibility and because most\n# people do not need auth (e.g. they run their own servers).\n#\n# Warning: since Redis is pretty fast an outside user can try up to 150k\n# passwords per second against a good box. This means that you should use a very\n# strong password otherwise it will be very easy to break.\n#\nrequirepass xRDdPoEtcB6DtrRhuMDi1B1TXa0h3hNhxmaFWzruZdFd9hBJnNqzkY0pRY7UVxf5\n\n# Command renaming.\n#\n# It is possilbe to change the name of dangerous commands in a shared\n# environment. For instance the CONFIG command may be renamed into something of\n# hard to guess so that it will be still available for internal-use tools but\n# not available for general clients.\n#\n#   rename-command CONFIG b840fc02d524045429941cc15f59e41cb7be6c52\n#\n# It is also possilbe to completely kill a command renaming it into an empty\n# string:\n#\n#   rename-command CONFIG \"\"\n#\n# The following commands are potentially dangerous and they've either been\n# renamed if they seem legitimately useful or disabled completely otherwise.\n# Most of the commands that have been disabled are configuration options that\n# should be specified in this file.\n\nrename-command CLIENT \"\"\nrename-command CONFIG \"\"\nrename-command SLAVEOF \"\"\n\nrename-command MONITOR \"\"\nrename-command OBJECT \"\"\n\nrename-command FLUSHALL \"\"\nrename-command FLUSHDB \"\"\n\nrename-command EVAL \"\"\nrename-command EVALSHA \"\"\nrename-command SCRIPT \"\"\n\n############### LIMITS ###############\n\n# Set the max number of connected clients at the same time. By default there is\n# no limit, and it's up to the number of file descriptors the Redis process is\n# able to open. The special value '0' means no limits. Once the limit is reached\n# Redis will close all the new connections sending an error 'max number of\n# clients reached'.\n#\nmaxclients 128\n\n# Don't use more memory than the specified amount of bytes. When the memory\n# limit is reached Redis will try to remove keys accordingly to the eviction\n# policy selected (see maxmemmory-policy).\n#\n# If Redis can't remove keys according to the policy, or if the policy is set to\n# 'noeviction', Redis will start to reply with errors to commands that would use\n# more memory, like SET, LPUSH, and so on, and will continue to reply to\n# read-only commands like GET.\n#\n# This option is usually useful when using Redis as an LRU cache, or to set a\n# hard memory limit for an instance (using the 'noeviction' policy).\n#\n# WARNING: If you have slaves attached to an instance with maxmemory on, the\n# size of the output buffers needed to feed the slaves are subtracted from the\n# used memory count, so that network problems / resyncs will not trigger a loop\n# where keys are evicted, and in turn the output buffer of slaves is full with\n# DELs of keys evicted triggering the deletion of more keys, and so forth until\n# the database is completely emptied.\n#\n# In short... if you have slaves attached it is suggested that you set a lower\n# limit for maxmemory so that there is some free RAM on the system for slave\n# output buffers (but this is not needed if the policy is 'noeviction').\n#\n#maxmemory 512Mb\n\n# MAXMEMORY POLICY: how Redis will select what to remove when maxmemory is\n# reached? You can select among five behavior:\n#\n# volatile-lru: remove keys with an expire set using an LRU algorithm (default)\n# allkeys-lru: remove any key accordingly to the LRU algorithm\n# volatile-random: remove a random key with an expire set\n# allkeys-\u003erandom: remove a random key, any key\n# volatile-ttl: remove the key with the nearest expire time (minor TTL)\n# noeviction: don't expire at all, just return an error on write operations\n#\n# Note: with all the kind of policies, Redis will return an error on write\n# operations, when there are not suitable keys for eviction.\n#\n# At the date of writing this commands are: set setnx setex append incr decr\n# rpush lpush rpushx lpushx linsert lset rpoplpush sadd sinter sinterstore\n# sunion sunionstore sdiff sdiffstore zadd zincrby zunionstore zinterstore hset\n# hsetnx hmset hincrby incrby decrby getset mset msetnx exec sort\n#\n#maxmemory-policy volatile-lru\n\n# LRU and minimal TTL algorithms are not precise algorithms but approximated\n# algorithms (in order to save memory), so you can select as well the sample\n# size to check. For instance for default Redis will check three keys and\n# pick the one that was used less recently, you can change the sample size\n# using the following configuration directive.\n#\n#maxmemory-samples 3\n\n############### APPEND ONLY MODE ###############\n\n# By default Redis asynchronously dumps the dataset on disk. If you can live\n# with the idea that the latest records will be lost if something like a crash\n# happens this is the preferred way to run Redis. If instead you care a lot\n# about your data and don't want to that a single record can get lost you should\n# enable the append only mode: when this mode is enabled Redis will append every\n# write operation received in the file appendonly.aof. This file will be read on\n# startup in order to rebuild the full dataset in memory.\n#\n# Note that you can have both the async dumps and the append only file if you\n# like (you have to comment the \"save\" statements above to disable the dumps).\n# Still if append only mode is enabled Redis will load the data from the log\n# file at startup ignoring the dump.rdb file.\n#\n# IMPORTANT: Check the BGREWRITEAOF to check how to rewrite the append log file\n# in background when it gets too big.\n#\nappendonly no\n\n# The name of the append only file (default: \"appendonly.aof\")\n#\nappendfilename appendonly.aof\n\n# The fsync() call tells the Operating System to actually write data on disk\n# instead to wait for more data in the output buffer. Some OS will really flush\n# data on disk, some other OS will just try to do it ASAP.\n#\n# Redis supports three different modes:\n#\n# no: don't fsync, just let the OS flush the data when it wants. Faster.\n# always: fsync after every write to the append only log. Slow, Safest.\n# everysec: fsync only if one second passed since the last fsync. Compromise.\n#\n# The default is \"everysec\" that's usually the right compromise between speed\n# and data safety. It's up to you to understand if you can relax this to \"no\"\n# that will will let the operating system flush the output buffer when it wants,\n# for better performances (but if you can live with the idea of some data loss\n# consider the default persistence mode that's snapshotting), or on the\n# contrary, use \"always\" that's very slow but a bit safer than everysec.\n#\n# If unsure, use \"everysec\".\n#\nappendfsync everysec\n\n# When the AOF fsync policy is set to always or everysec, and a background\n# saving process (a background save or AOF log background rewriting) is\n# performing a lot of I/O against the disk, in some Linux configurations Redis\n# may block too long on the fsync() call. Note that there is no fix for this\n# currently, as even performing fsync in a different thread will block our\n# synchronous write(2) call.\n#\n# In order to mitigate this problem it's possible to use the following option\n# that will prevent fsync() from being called in the main process while a BGSAVE\n# or BGREWRITEAOF is in progress.\n#\n# This means that while another child is saving the durability of Redis is the\n# same as \"appendfsync none\", that in pratical terms means that it is possible\n# to lost up to 30 seconds of log in the worst scenario (with the default Linux\n# settings).\n#\n# If you have latency problems turn this to \"yes\". Otherwise leave it as \"no\"\n# that is the safest pick from the point of view of durability.\n#\nno-appendfsync-on-rewrite no\n\n# Automatic rewrite of the append only file. Redis is able to automatically\n# rewrite the log file implicitly calling BGREWRITEAOF when the AOF log size\n# will growth by the specified percentage.\n#\n# This is how it works: Redis remembers the size of the AOF file after the\n# latest rewrite (or if no rewrite happened since the restart, the size of the\n# AOF at startup is used).\n#\n# This base size is compared to the current size. If the current size is bigger\n# than the specified percentage, the rewrite is triggered. Also you need to\n# specify a minimal size for the AOF file to be rewritten, this is useful to\n# avoid rewriting the AOF file even if the percentage increase is reached but it\n# is still pretty small.\n#\n# Specify a precentage of zero in order to disable the automatic AOF rewrite\n# feature.\n#\nauto-aof-rewrite-min-size 64mb\nauto-aof-rewrite-percentage 100\n\n############### SLOW LOG ###############\n\n# The Redis Slow Log is a system to log queries that exceeded a specified\n# execution time. The execution time does not include the I/O operations like\n# talking with the client, sending the reply and so forth, but just the time\n# needed to actually execute the command (this is the only stage of command\n# execution where the thread is blocked and can not serve other requests in the\n# meantime).\n#\n# You can configure the slow log with two parameters: one tells Redis what is\n# the execution time, in microseconds, to exceed in order for the command to get\n# logged, and the other parameter is the length of the slow log. When a new\n# command is logged the oldest one is removed from the queue of logged commands.\n#\n# The following time is expressed in microseconds, so 1000000 is equivalent to\n# one second. Note that a negative number disables the slow log, while a value\n# of zero forces the logging of every command.\n#\nslowlog-log-slower-than 10000\n\n# There is no limit to this length. Just be aware that it will consume memory.\n# You can reclaim memory used by the slow log with SLOWLOG RESET.\n#\nslowlog-max-len 1024\n\n############### VIRTUAL MEMORY ###############\n\n### WARNING! Virtual Memory is deprecated in Redis 2.4\n### The use of Virtual Memory is strongly discouraged.\n\n# Virtual Memory allows Redis to work with datasets bigger than the actual\n# amount of RAM needed to hold the whole dataset in memory. In order to do so\n# very used keys are taken in memory while the other keys are swapped into a\n# swap file, similarly to what operating systems do with memory pages.\n#\n# To enable VM just set 'vm-enabled' to yes, and set the following three VM\n# parameters accordingly to your needs.\n#\nvm-enabled no\n\n# This is the path of the Redis swap file. As you can guess, swap files can't be\n# shared by different Redis instances, so make sure to use a swap file for every\n# redis process you are running. Redis will complain if the swap file is already\n# in use.\n#\n# The best kind of storage for the Redis swap file (that's accessed at random)\n# is a Solid State Disk (SSD).\n#\n# *** WARNING *** if you are using a shared hosting the default of putting the\n# swap file under /tmp is not secure. Create a dir with access granted only to\n# Redis user and configure Redis to create the swap file there.\n#\nvm-swap-file /tmp/redis.swap\n\n# vm-max-memory configures the VM to use at max the specified amount of RAM.\n# Everything that deos not fit will be swapped on disk *if* possible, that is,\n# if there is still enough contiguous space in the swap file.\n#\n# With vm-max-memory 0 the system will swap everything it can. Not a good\n# default, just specify the max amount of RAM you can in bytes, but it's better\n# to leave some margin. For instance specify an amount of RAM that's more or\n# less between 60 and 80% of your free RAM.\n#\nvm-max-memory 0\n\n# Redis swap files is split into pages. An object can be saved using multiple\n# contiguous pages, but pages can't be shared between different objects. So if\n# your page is too big, small objects swapped out on disk will waste a lot of\n# space. If you page is too small, there is less space in the swap file\n# (assuming you configured the same number of total swap file pages).\n#\n# If you use a lot of small objects, use a page size of 64 or 32 bytes.\n# If you use a lot of big objects, use a bigger page size.\n# If unsure, use the default :)\n#\nvm-page-size 32\n\n# Number of total memory pages in the swap file. Given that the page table (a\n# bitmap of free/used pages) is taken in memory, every 8 pages on disk will\n# consume 1 byte of RAM.\n#\n# The total swap size is vm-page-size * vm-pages\n#\n# With the default of 32-bytes memory pages and 134217728 pages Redis will use a\n# 4 GB swap file, that will use 16 MB of RAM for the page table.\n#\n# It's better to use the smallest acceptable value for your application, but the\n# default is large in order to work in most conditions.\n#\nvm-pages 134217728\n\n# Max number of VM I/O threads running at the same time. This threads are used\n# to read/write data from/to swap file, since they also encode and decode\n# objects from disk to memory or the reverse, a bigger number of threads can\n# help with big objects even if they can't help with I/O itself as the physical\n# device may not be able to couple with many reads/writes operations at the same\n# time.\n#\n# The special value of 0 turn off threaded I/O and enables the blocking Virtual\n# Memory implementation.\n#\nvm-max-threads 4\n\n############### ADVANCED CONFIG ###############\n\n# Hashes are encoded in a special way (much more memory efficient) when they\n# have at max a given numer of elements, and the biggest element does not exceed\n# a given threshold. You can configure this limits with the following\n# configuration directives.\n#\nhash-max-zipmap-entries 512\nhash-max-zipmap-value 64\n\n# Similarly to hashes, small lists are also encoded in a special way in order to\n# save a lot of space. The special representation is only used when you are under\n# the following limits:\n#\nlist-max-ziplist-entries 512\nlist-max-ziplist-value 64\n\n# Sets have a special encoding in just one case: when a set is composed of just\n# strings that happens to be integers in radix 10 in the range of 64 bit signed\n# integers. The following configuration setting sets the limit in the size of\n# the set in order to use this special memory saving encoding.\n#\nset-max-intset-entries 512\n\n# Similarly to hashes and lists, sorted sets are also specially encoded in order\n# to save a lot of space. This encoding is only used when the length and\n# elements of a sorted set are below the following limits:\n#\nzset-max-ziplist-entries 128\nzset-max-ziplist-value 64\n\n# Active rehashing uses 1 millisecond every 100 milliseconds of CPU time in\n# order to help rehashing the main Redis hash table (the one mapping top-level\n# keys to values). The hash table implementation redis uses (see dict.c)\n# performs a lazy rehashing: the more operation you run into an hash table that\n# is rhashing, the more rehashing \"steps\" are performed, so if the server is\n# idle the rehashing is never complete and some more memory is used by the hash\n# table.\n#\n# The default is to use this millisecond 10 times every second in order to\n# active rehashing the main dictionaries, freeing memory when possible.\n#\n# Use \"activerehashing no\". If you have hard latency requirements and it is not\n# a good thing in your environment that Redis can reply form time to time to\n# queries with 2 milliseconds delay.\n#\n# use \"activerehashing yes\" if you don't have such hard requirements but want to\n# free memory asap when possible.\n#\nactiverehashing yes\n\n############### INCLUDES ###############\n# Include one or more other config files here. This is useful if you have a\n# standard template that goes to all redis server but also need to customize a\n# few per-server settings. Include files can include other files, so use this\n# wisely.\n#\n# include /path/to/local.conf\n# include /path/to/other.conf\n```\n\n## Master\n\nOpen up incoming connections from the app servers (or the app server network)\nto port 6379.\n\nUse the default 'redis.conf' file in this repository, but change the\nrequirepass to something random and difficult and set masterauth to match the\nvalue.\n\n## Slaves\n\nOpen up incoming connections from the app servers (or the app server network)\nto port tcp/6379. As well as outgoing connections to the master server on port\ntcp/6379.\n\nUse the identical config from the master but append the following line to it:\n\n```\nslaveof \u003cmaster-ip\u003e 6379\n```\n","created_at":-62135596800,"fuzzy_word_count":3900,"path":"/notes/redis/","published_at":1507587428,"reading_time":18,"tags":null,"title":"Redis","type":"notes","updated_at":1507587428,"weight":0,"word_count":3807},{"cid":"f1267f8f1b7fabd120cc1b0af51bcf30de0e1c35","content":"\n***Note: This page is quite old and is likely out of date. My opinions may have\nalso changed dramatically since this was written. It is here as a reference\nuntil I get around to updating it.***\n\nResources:\n\n* http://www.fclose.com/816/port-forwarding-using-iptables/\n\nThis was done as a quick and dirty iptables NAT/gateway for a private LXC\nnetwork. Create the LXC container using our base templating trick:\n\n```\ncp -ar /var/lib/libvirt/lxc/fedora-19-x86_64-template /var/lib/libvirt/lxc/gateway-01\nvirt-install --connect lxc:// --name gateway-01 --ram 512 --vcpus 1 \\\n  --filesystem \"/var/lib/libvirt/lxc/gateway-01,/\" --network=\"network=br0\" \\\n  --network=\"network=isolated\" --console \"pty\" --check-cpu\n```\n\n```\necho 1 \u003e /proc/sys/net/ipv4/ip_forward\necho 'net.ipv4.ip_forward = 1' \u003e /etc/sysctl.d/70-routing.conf\n```\n\ncat \u003c\u003c EOF \u003e /etc/sysconfig/network-scripts/ifcfg-eth1\nDEVICE=\"eth1\"\nNM_CONTROLLED=\"no\"\nONBOOT=\"yes\"\nTYPE=\"Ethernet\"\nBOOTPROTO=\"static\"\n\nIPADDR=\"10.0.0.1\"\nNETMASK=\"255.255.255.0\"\nDEFROUTE=\"no\"\n\nIPV4_FAILURE_FATAL=\"no\"\nIPV6INIT=\"yes\"\n\nNAME=\"LAN\"\nEOF\n\nGoing to ignore IPv6 for now :(\n\nFor this eth0 is the external network connection and eth1 is the internal\nnetwork. These will have 192.168.122.10 as the external IP address and 10.0.0.1\nfor the internal IP address.\n\n## /etc/sysconfig/iptables\n\n```\n*filter\n:INPUT DROP [0:0]\n:FORWARD DROP [0:0]\n:OUTPUT DROP [0:0]\n\n-A INPUT -m conntrack --ctstate ESTABLISHED,RELATED -j ACCEPT\n-A INPUT -p icmp -j ACCEPT\n-A INPUT -i lo -j ACCEPT\n\n# Allow the internal network to connect to this SSH server (harden to airlock)\n-A INPUT -m tcp -p tcp --dport 22 -i eth1 -m conntrack --ctstate NEW -j ACCEPT\n\n-A OUTPUT -m conntrack --ctstate ESTABLISHED,RELATED -j ACCEPT\n-A OUTPUT -p icmp -j ACCEPT\n-A OUTPUT -o lo -j ACCEPT\n\n# DNS requests (harden to internal server)\n-A OUTPUT -m udp -p udp --dport 53 -m conntrack --ctstate NEW -j ACCEPT\n-A OUTPUT -m tcp -p tcp --dport 53 -m conntrack --ctstate NEW -j ACCEPT\n\n# For updates (harden to squid server)\n-A OUTPUT -m tcp -p tcp --dport 80 -m conntrack --ctstate NEW -j ACCEPT\n-A OUTPUT -m tcp -p tcp --dport 443 -m conntrack --ctstate NEW -j ACCEPT\n\n# Log any attempts that try to slip through\n-A OUTPUT -m limit --limit 2/s --limit-burst 5 -j LOG --log-prefix \"Outgoing attempt: \"\n-A OUTPUT -j REJECT\n\n# Allow existing connections in both directions\n-A FORWARD -i eth0 -o eth1 -m conntrack --ctstate RELATED,ESTABLISHED -j ACCEPT\n-A FORWARD -i eth1 -o eth0 -m conntrack --ctstate RELATED,ESTABLISHED -j ACCEPT\n\n# Always allow pinging out from the internal network\n-A FORWARD -i eth1 -o eth0 -s 10.0.0.0/24 -p icmp -j ACCEPT\n\n# Allow us to forward to the airlock's SSH server\n-A FORWARD -m tcp -p tcp -d 10.0.0.100 --dport 22 -j ACCEPT\n\n# Temporarily allow routing all other traffic from the inside out\n-A FORWARD -i eth1 -s 10.0.0.0/24 -o eth0 -j ACCEPT\n\nCOMMIT\n\n*nat\n:PREROUTING   ACCEPT [0:0] # TODO: Harden?\n:INPUT        ACCEPT [0:0] # TODO: Harden?\n:OUTPUT       ACCEPT [0:0] # TODO: Harden?\n:POSTROUTING  ACCEPT [0:0] # TODO: Harden?\n\n# Portfowarding port 2200 to port 22 on the airlock\n-A PREROUTING -i eth0 -m tcp -p tcp --dport 2200 -j DNAT --to 10.0.0.100:22\n\n# Handle the NAT routing\n-A POSTROUTING -s 10.0.0.0/24 -o eth0 -j MASQUERADE\n\nCOMMIT\n```\n\nLXC /proc/sys fix can be accomplished by:\n\n```\ncat \u003c\u003c EOF \u003e /etc/rc.d/rc.local\n#!/bin/sh\numount /proc/sys\nsystemctl start systemd-sysctl.service\nEOF\nchmod +x /etc/rc.d/rc.local\n```\n","created_at":-62135596800,"fuzzy_word_count":600,"path":"/notes/router/","published_at":1507587428,"reading_time":3,"tags":null,"title":"Router","type":"notes","updated_at":1507587428,"weight":0,"word_count":503},{"cid":"0759d1066ef8be415228ca39a9b3239ed17a4ee5","content":"\n***Note: This page is quite old and is likely out of date. My opinions may have\nalso changed dramatically since this was written. It is here as a reference\nuntil I get around to updating it.***\n\nSources:\n\n* https://fedoraproject.org/wiki/How_to_create_an_RPM_package\n* https://fedoraproject.org/wiki/Packaging:Guidelines\n\nCreate a build user whose sole purpose is to handle building packages. Do not\nuse root under any circumstances.\n\nAs root:\n\n```\nuseradd mockbuild\necho -n 'mockbuild:password' | chpasswd\n```\n\nIts valuable to grant the mockbuild user sudo permissions to allow that user to\ninstall missing development packages.\n\n```\nyum install sudo -y\ngroupadd -g 400 sudoers\nusermod -a -G sudoers mockbuild\necho '%sudoers ALL=(ALL) NOPASSWD: ALL' \u003e\u003e /etc/sudoers\n```\n\nAnd install the requisite tools:\n\n```\nyum install @development-tools fedora-packager wget -y\n```\n\nLogin as the build user and setup the build environment.\n\n```\nrpmdev-setuptree\necho 'timestamping = on' \u003e .wgetrc\necho '-R' \u003e .curlrc\n```\n\nThe latter bits setup preservation of files when downloading the packages with\nwget and curl.\n\n## Downloading existing source packages\n\n```\nyumdownloader --source nginx\nrpm -ivh nginx*.src.rpm\nrm nginx*.src.rpm\n```\n\n## Running a spec\n\n```\nrpmbuild -ba ~/rpmbuild/SPECS/nginx.spec\n```\n\nWhen running the above command you will probably encounter missing\ndependencies.\n\nFor nginx these were:\n\n```\nsudo yum install GeoIP-devel gd-devel gperftools-devel libxslt-devel \\\n  openssl-devel pcre-devel perl-devel perl zlib-devel perl-ExtUtils-Embed -y\n```\n\n## Modifying Nginx to include Lua support\n\nSince we are going to be adding lua we need to make sure it is represented as\ndependencies by adding them to the spec files.\n\n```\nBuildRequires:     lua-devel\nRequires:          lua\n```\n\nDownload the ngx_devel_kit which is a module that provides common functions and\nuseful utilities for other modules.\n\n```\ncd ~/rpmbuild/SOURCES\nwget https://github.com/simpl/ngx_devel_kit/archive/v0.2.19.tar.gz\nwget https://github.com/chaoslawful/lua-nginx-module/archive/v0.9.0.tar.gz\n```\n\nAdd the following sources:\n\n```\nSource2: https://github.com/simpl/ngx_devel_kit/archive/v0.2.19.tar.gz\nSource3: https://github.com/chaoslawful/lua-nginx-module/archive/v0.9.0.tar.gz\n```\n\nWe need to make sure the additional tar balls are extracted. Add the following\nlines after the `%setup -q` line:\n\n```\n%setup -T -D -a 2 -n ngx_devel_kit-0.2.19\n%setup -T -D -a 3 -n lua-nginx-module-0.9.0\n```\n\nFor future reference the `-D` means dont delete the prior directories before\nuncompressing. The `-a` flag indicates the source number that should be\nprocessed. Finally the `-T` flag prevents the original tar ball from being\nextracted again. The `-n` flag fixes the name of the folder that the package\ngets uncompressed into.\n\nAt the beginning of the `%build` section we need to add a few additional\nexports.\n\n```\nexport LUA_LIB=/usr/lib64\nexport LUA_INC=/usr/share/lua/5.1\n```\n\nFinally we need to add the compile flags to include the modules. Near the end\nof the ./configure flags (I Choose after the --with-pcre flag) add the\nfollowing lines:\n\n```\n  --add-module=./ngx_devel_kit-0.2.19 \\\n  --add-module=./lua-nginx-module-0.9.0 \\\n```\n\n```\nyum install lua lua-socket lua-devel -y\n```\n\n## Signing Packages\n\nSource:\n\n* http://www.question-defense.com/2010/03/04/generate-a-gpg-key-to-sign-rpm-packages-created-using-rpmbuild-on-centos-linux\n","created_at":-62135596800,"fuzzy_word_count":400,"path":"/notes/rpm-build/","published_at":1508540507,"reading_time":2,"tags":null,"title":"RPM Build","type":"notes","updated_at":1508540507,"weight":0,"word_count":396},{"cid":"9cec44df70dfc92dae4991a270e608e596f50a57","content":"\n***Note: This page is quite old and is likely out of date. My opinions may have\nalso changed dramatically since this was written. It is here as a reference\nuntil I get around to updating it.***\n\n# RSyslog\n\nRSyslog is a more advanced replacement for the aging klogd and syslogd. It\nsupports useful features such as attribute filtering, multiple protocol\nsupport, and logging to databases.\n\n## Security Notes\n\nRSyslog can log some sensitive information or very useful information for any\npotential intruder including service names, versions, valid usernames, logon\ntimes and quite a bit more.\n\nAttempts are usually made to alter or destroy logs by various malicious actors\nto cover their tracks. As such it is highly recommended that logs are sent to a\ncentral hardened host that can maintain a backup of any logs in case a server\nis compromised as well as providing a central place for logs to be processed\nand manipulated.\n\nOther software will need to be used to handle the correlation of logs, RSyslog\nitself only provides mechanisms for aggregations.\n\n## Firewall Adjustments\n\nBy default RSyslog doesn't need any ports opened. If you intend to send your\nlogs over the network, it will depend on how you're doing it. UDP is the most\nerror prone but the least processor intensive, you could also use UDP across a\none-way data path which can be valuable in some high security situations.\n\nTCP is more reliable but does require more processing and memory as connections\nneed to be tracked. This overhead is usually trivial amounts in small to medium\nlogging situations and can probably be safely ignored in most cases.\n\nRSyslog also provides a mechanism to encrypt syslog messages over TCP using\nTLS. This however requires there to be a trusted [Certificate_Authority][1] to\ncreate and sign certificates. You could potentially use [StartSSL][5]\ncertificates but using others could do the same thing and correctly\nauthenticate to your log server. There would be an audit trail linking someone\nto the offending certificate, getting access to that information would require\nassistance from the issuing authority in the event of a breach.\n\nMany network appliances, switches, and routers don't support TCP or TLS syslog\nreporting. This frequently means you'll want to operate in a mixed mode setup,\nproviding a secure connection to clients that support it while still allowing\nolder clients to send you logs over UDP.\n\nThese logs generally travel unencrypted over the network unless configured to\ndo otherwise.\n\n## Firewall Adjustments\n\nIf you're sending or receiving log messages between machines you'll need to\nadjust your firewall rules to allow the traffic. Syslog has an IANA assigned\nport number of *514* for both TCP and UDP.\n\nIt is common and more widely supported by endpoint devices to support UDP as it\nrequires less processing but is more prone to message loss. TCP is\nsignificantly more reliable for message delivery, at the cost of having more\nprocessing overhead.\n\nRSyslog also supports a homegrown protocol referred to as RELP (the \"Reliable\nEvent Logging Protocol\"). This does not have an IANA standard port assigned to\nit but defaults to port *2514/tcp*.\n\nIt is a good idea to limit the hosts that can send log messages to your central\nlog servers or implement a form of authentication to prevent malicious\nattackers from flooding / overloading your log server.\n\n## Forwarding Windows Events\n\nThere is a handy service that can forward syslog events from the Windows Event\nsubsystem built into windows to a syslog server. It's called\n[eventlog-to-syslog][2] and seems to be under active development (good thing).\n\n## Modular Configuration\n\nModular configuration of services has become increasingly important with the\nrise of configuration management systems. By making use of the modular imports,\nconfiguration of systems can be mostly standardized with only the variations\nbased on the system's role adding or removing individual files without worrying\nabout the state of others.\n\nI've updated my example configurations for RSyslog to make full use of the\nmodular configuration mechanism.\n\n***Compatibility warning:*** I've switched from using the \"legacy\" syslog.conf\nsyntax to RSyslog's \"RainerScript\" as much as possible. This sample\nconfiguration works at version 7.4.8 of RSyslog on Fedora 20. I've had serious\ncompatibility issues going to new and older minor releases of RSyslog with\ncertain subsets of the new syntax, all of which I've left out of these examples\nto the best of my abilities.\n\nMy base `/etc/rsyslog.conf` file.\n\n```\nmodule(load=\"imuxsock\")\nmodule(load=\"imjournal\" statefile=\"imjournal.state\")\nmodule(load=\"imklog\")\nmodule(load=\"immark\" interval=\"3600\")\n\n$WorkDirectory /var/lib/rsyslog\n$IncludeConfig /etc/rsyslog.d/*.conf\n```\n\nIf you prefer not to have time based mark messages periodically added to your\nlog files you'll want to remove or comment out that line or change the interval\nthat they're logged at (I've found hourly to be pretty balanced and useful).\n\nThe first important modular configuration file is for logging messages to local\nfiles. All my systems get this file which I place at\n`/etc/rsyslog.d/local_logging.conf`.\n\nThere is one weird condition on only logging to local files if the log message\nitself didn't come from a file, the reason for this will become apparent later\non.\n\n```\nif $inputname != 'imfile' then {\n  local7.*                                  action(type=\"omfile\" file=\"/var/log/boot.log\")\n  cron.*                                    action(type=\"omfile\" file=\"/var/log/cron\")\n  mail.*                                    action(type=\"omfile\" file=\"/var/log/maillog\")\n  *.info;mail.none;authpriv.none;cron.none  action(type=\"omfile\" file=\"/var/log/messages\")\n  authpriv.*                                action(type=\"omfile\" file=\"/var/log/secure\")\n  uucp,news.crit                            action(type=\"omfile\" file=\"/var/log/spooler\")\n  *.emerg                                   action(type=\"omusrmsg\" users=\"*\")\n}\n```\n\nYou can stop at this point if you're not going to send your logs to a central\nlocation. On the systems that will be receiving logs you'll want to add the\nfollowing configuration file at `/etc/rsyslog.d/server.conf`.\n\nThis is one of the places where I couldn't find a RainerScript equivalent\n(around the definition of templates). The RELP module also doesn't support the\n'ruleset' attribute, otherwise, I'd probably include the module just for\ncompleteness.\n\n```\nmodule(load=\"imudp\")\nmodule(load=\"imtcp\")\n\n$template remoteAudit,    \"/var/log/remote/%fromhost-ip%/audit.log\"\n$template remoteBoot,     \"/var/log/remote/%fromhost-ip%/boot.log\"\n$template remoteCron,     \"/var/log/remote/%fromhost-ip%/cron\"\n$template remoteMail,     \"/var/log/remote/%fromhost-ip%/maillog\"\n$template remoteMessages, \"/var/log/remote/%fromhost-ip%/messages\"\n$template remoteSecure,   \"/var/log/remote/%fromhost-ip%/secure\"\n$template remoteSpooler,  \"/var/log/remote/%fromhost-ip%/spooler\"\n\n$template auditFormat, \"%msg%\\n\"\n\nruleset(name=\"remote\") {\n  if $pri-text == 'local6.info' and $programname == 'auditd-sender' then {\n    local6.info   action(type=\"omfile\" dynaFile=\"remoteAudit\" template=\"auditFormat\")\n  } else {\n    local7.*                                  action(type=\"omfile\" dynaFile=\"remoteBoot\")\n    cron.*                                    action(type=\"omfile\" dynaFile=\"remoteCron\")\n    mail.*                                    action(type=\"omfile\" dynaFile=\"remoteMail\" sync=\"off\")\n    *.info;mail.none;authpriv.none;cron.none  action(type=\"omfile\" dynaFile=\"remoteMessages\")\n    authpriv.*                                action(type=\"omfile\" dynaFile=\"remoteSecure\")\n    uucp,news.crit                            action(type=\"omfile\" dynaFile=\"remoteSpooler\")\n  }\n}\n\ninput(type=\"imudp\" port=\"514\" ruleset=\"remote\")\ninput(type=\"imtcp\" port=\"514\" ruleset=\"remote\")\n```\n\nThis will maintain indepedant log file for each client that sends the server\nlogs broken down by IP address in the `/var/log/remote` directory. Each\ndirectory will match the one you'd expect to see on that systems `/var/log`\ndirectory.\n\nIf you look closely you can see that I've got some special logic in place for\nhandling one special type of log, the output of 'auditd'. We'll get to that\nsoon.\n\nWe need to now configure our clients to send the logs to our server. On all\nmachines (I do this on the log servers as well) I add the following\nconfiguration file at `/etc/rsyslog.d/remote.conf`.\n\n```\n*.* action(\n  type=\"omfwd\"\n  target=\"logserver.example.tld\"\n  port=\"514\"\n  protocol=\"tcp\"\n\n  action.resumeretrycount=\"-1\"\n\n  queue.filename=\"log01\"\n  queue.size=\"1g\"\n  queue.type=\"LinkedList\"\n  queue.saveonshutdown=\"on\"\n)\n```\n\nYou'll want to replace the value of the target attribute with the name of your\nlogserver. I've never had issues with using hostnames though generally the best\npractice for logging is to use IP addresses. This name will only be looked up\nwhen the configuration is evaluated.\n\nI have two separate log servers that I send logs to for redundancy, this does\nmake log processing later on a smidge more difficult due to duplicaitons but\nI've found the benefits far outweigh the added effort.\n\nThe final link in this log chain is sending our audit log files to the central\nserver. This is why I've added a conditional statement to the local logging as\nrsyslog isn't responsible for generating or locally logging this data and why\nthe server file has conditions for dealing with the audit log. I keep this\nconfiguration in `/etc/rsyslog.d/audit_input.conf`.\n\n```\nmodule(load=\"imfile\")\ninput(type=\"imfile\"\n      file=\"/var/log/audit/audit.log\"\n      statefile=\"imfile-state:-var-log-audit-audit.log\"\n      tag=\"auditd-sender\"\n      severity=\"info\"\n      facility=\"local6\")\n```\n\nEnsure you restart the syslog server after making these changes:\n\n```\nsystemctl restart rsyslog.service\n```\n\n## Log Rotation for Servers\n\nThe last component in the log server is too ensure log files are regularly\nrotated and compressed. My per server log volume is relatively small so I\nrotate my logs weekly and keep a years worth for each machine.\n\nI stick the following configuration file at\n`/etc/logrotate.d/rsyslog_server.conf`. This assumes you already have\n[logrotate][6] configured.\n\n```\n/var/log/remote/*/audit.log\n/var/log/remote/*/boot.log\n/var/log/remote/*/cron\n/var/log/remote/*/maillog\n/var/log/remote/*/messages\n/var/log/remote/*/secure\n/var/log/remote/*/spooler {\n  # Keep three years worth of logs (52 weeks * 3 years)\n  weekly\n  rotate 52\n\n  # If the file is missing, or empty just skip it and don't bother creating a\n  # file if it's missing, rsyslog can handle it.\n  missingok\n  notifempty\n  nocreate\n\n  # Append the date to any rotated log files instead of an arbitrary number and\n  # compress any rotated logs (with the exception of the most recent one).\n  compress\n  delaycompress\n  dateext\n  dateformat .%Y%m%d\n\n  # When all the logs are done being rotated notify Rsyslog that it needs to\n  # reopen all it's file descriptors.\n  sharedscripts\n  postrotate\n    [ -s /var/run/syslogd.pid ] \u0026\u0026 kill -HUP $(cat /var/run/syslogd.pid) \u0026\u003e /dev/null || true\n  endscript\n}\n```\n\n## RELP\n\nRELP is a custom log protocol created by the maintainers of RSyslog attempting\nto ensure logs are reliably delivered to particular servers. In practice I've\nhad significant issues RELP, mostly around it's reliability (ironically) at\nsending logs when all systems are operating normally and at low volumes.\n\nRELP was never able to pass any of my deployment tests reliably enough for me\nto be able to make use of it in production and I highly encourage it not be\nused.\n\n## Signature Providers\n\nRSyslog has integrated [signature provider support][3] through a third party\nservice called [GuardTime][4]. The Rsyslog developers have claimed other\nsignature provider modules will be provided in the future but give no\nindications on possible timelines or how they might be implemented.\n\nGuardTime itself as a service isn't well documented, their entire site being a\nshrine to abstract meaningless buzz words. What I have been able to gather is\nthat they implement some form of a block chain (it is unclear whether it is\npublic or not) and they use the aggregated signatures of everyone requesting\nsignatures of hashes as part of this block chain. I could be *way* off on this\nas the documentation is very sparse, old and is largely based off of jumps of\nintuition.\n\nGuardTime itself seems to have been having issues. They shut down their API due\nto DoS attacks now requiring an account. It also requires trusting them as a\nthird party, even though they claim this is not the case (they provide no\ntechnical information backing this claim).\n\nSigning logs is not an easy problem. Systemd is also attempting to solve the\nproblem in their own way, but there is a lot of criticism and valid arguments\nover their implementation.\n\nI work in environments that by definition don't trust third parties, so until\nthere is a self-hosted solution for providing signatures, I can't recommend\nthis.\n\n## Personal Views\n\nI use RSyslog because it is IMHO the best open source log daemon available. I\nam appalled by the developers compatibility breaking changes between minor\nversions that I've experienced, the self-aggrandizing ('RainerScript'...),\ninconsistent documentation, and general lack of support that I've witnessed.\n\nThat being said, I have yet to see even a commercial offering that works as\nwell as RSyslog. The developers are continuing to develop in a security\nconcious way (such as integrating encryption, and third party message\nsignatures) and stability (such as log queues).\n\n[1]: {{\u003c ref \"./certificate_authority.md\" \u003e}}\n[2]: http://code.google.com/p/eventlog-to-syslog/\n[3]: http://www.rsyslog.com/doc/sigprov_gt.html\n[4]: http://guardtime.com/\n[5]: https://www.startssl.com/\n[6]: {{\u003c ref \"./logrotate.md\" \u003e}}\n","created_at":-62135596800,"fuzzy_word_count":1900,"path":"/notes/rsyslog/","published_at":1540945455,"reading_time":9,"tags":null,"title":"RSyslog","type":"notes","updated_at":1540945455,"weight":0,"word_count":1844},{"cid":"23e66c6205194360dcf2148f3be8ea69e6eaf91d","content":"\n***Note: This page is quite old and is likely out of date. My opinions may have\nalso changed dramatically since this was written. It is here as a reference\nuntil I get around to updating it.***\n\n## RVM\n\nIf using Gnome terminal, you have to set the 'Run command as login shell' check\nbox on the Title and Command tab inside of gnome-terminal's Settings page to\nrun the bashrc file upon opening the terminal. If you don't do this you'll\nreceive \"RVM is not a function\" error message.\n\nInstall all the packages needed to compile ruby...\n\n```\nyum install gcc-c++ patch readline readline-devel zlib zlib-devel \\\n  libyaml-devel libffi-devel openssl-devel make bzip2 autoconf automake \\\n  libtool bison libxml2 libxml2-devel sqlite sqlite-devel libxslt \\\n  libxslt-devel -y\n```\n\nInstall RVM...\n\n```\ncurl https://raw.github.com/wayneeseguin/rvm/master/binscripts/rvm-installer | bash -s stable\n```\n\n```\nrvm install ruby-1.9.3\nrvm use 1.9.3 --default\n```\n\nCreate a global gemset:\n\n```\nrvm gemset create global\nrvm gemset use global\n```\n\nAnd install a few gems useful globally:\n\n```\ngem install bundler pry\n```\n\nTo make sure these get installed automatically for any other ruby versions that\nget installed you can use some nifty information pulled from [RVM's\nDocumentation][1].\n\n```\necho -e \"bundler\\npry\" \u003e ~/.rvm/gemsets/global.gems\n```\n\n### Ruby Optimization\n\nBy default RVM compiles ruby with no optimization flags. The flags will\nincrease the time it takes to compile a specific instance of Ruby, however, it\nwill also save running time with every execution of the code. For me this is a\nvery useful trade off as I run various Ruby commands hundreds of times a day\nwhen I'm working on various programs. You can add the compilization flags by\ncreating an \".rvmrc\" file in the root of your home directory and include the\nfollowing:\n\n```\nrvm_configure_env=(CFLAGS=\"-march=native -O2 -pipe\")\n```\n\nThese are \"safe\" optimization flags and will make optimizations automatically\nbased on the processor of the system that compiled it. This has the downside of\nmaking the binaries non-portable (as in you can only move the compiled binaries\nto systems with the same or newer processors).\n\nAs an additional level of optimizations rather than using the stock Ruby binary\nyou can use the \"turbo\" branch with the falcon patch which has decreased rails\nload time by half or more. You install this version like so:\n\n```\nrvm install 1.9.3-turbo --patch falcon\n```\n\nIf you encounter any errors about CFlags or compilation errors your RVM is\nprobably out of date and needs to be updated. On one machine I received the\nfollowing error:\n\n```\nError running 'CFLAGS=-march=native -O2 ./configure\n--prefix=/home/user/.rvm/rubies/ruby-1.9.3-p194-turbo --enable-shared\n--disable-install-doc --with-libyaml --with-opt-dir=/home/user/.rvm/usr ',\nplease read /home/user/.rvm/log/ruby-1.9.3-p194-turbo/configure.logThere has\nbeen an error while running configure. Halting the installation.\n```\n\nWhich was quickly solved by running the following command and trying again:\n\n```\nrvm get stable\n```\n\nI did also encounter an error when that ran for me, but it fixed the issue and\nI suspect the error was unrelated.\n\n## RBEnv\n\nhttp://hmarr.com/2012/nov/08/rubies-and-bundles/\n\n[1]: https://rvm.io/gemsets/initial/\n","created_at":-62135596800,"fuzzy_word_count":500,"path":"/notes/ruby/","published_at":1507584890,"reading_time":3,"tags":null,"title":"Ruby","type":"notes","updated_at":1507584890,"weight":0,"word_count":451},{"cid":"5a300f2858d4a89bfb8f06d4203ab1456b38a1e9","content":"\n***Note: This page is quite old and is likely out of date. My opinions may have\nalso changed dramatically since this was written. It is here as a reference\nuntil I get around to updating it.***\n\nFEAR FOR THOSE WHO TREAD THESE WATERS FOR THEY ARE DEEP AND EVIL.\n\n## SELinux Adjustments\n\nSELinux hates Samba and with good reason. To allow authenticated clients to\naccess the contents of a samba share you need to label the share. The easiest\nway to make this persistent is by adjusting the targeted SELinux config. In the\nfollowing example the only samba share is location at /media/sharing.\n\nAdd this to `/etc/selinux/targeted/contexts/files/file_contexts`:\n\n```\n/media/storage(/.*)?    system_u:object_r:samba_share_t:s0\n```\n\nThen run this command:\n\n```\n[root@localhost ~]# restorecon -R /media/storage\n```\n\n## On Anonymous Access\n\nIf you intend to allow anonymous browsing of shares and/or access to a share,\nyou will need to create a 'nobody' user with no password and add a few lines to\nthe configuration file. Add the user by following the instructions in 'Managing\nUsers'\n\n## Managing Users\n\nTo add a samba user make sure they have an account on the local machine, and\nrun the command:\n\n```\n[root@localhost ~]# pdbedit -a \u003cusername\u003e\n```\n\nTo list samba users:\n\n```\n[root@localhost ~]# pdbedit -L\n```\n\nTo delete a samba user:\n\n```\n[root@localhost ~]# pdbedit -x \u003cusername\u003e\n```\n\n## Configuration\n\n### /etc/samba/smb.conf\n\n```ini\n[global]\n# ----------------------- Network-Related Options -------------------------\n\n        workgroup = Pantheon\n        server string = For Sexual Favors\n        netbios name = Moirae\n        interfaces = eth0:0\n        bind interfaces only = yes\n        hosts allow = 10.13.37.0/24\n        guest account = nobody\n        map to guest = bad user\n        force directory mode = 0777\n        force create mode = 0666\n\n        # Force master browser elections to choose this server\n        domain master = no\n        local master = yes\n        preferred master = yes\n        os level = 60\n\n        socket options = TCP_NODELAY\n        read raw = yes\n        write raw = yes\n\n        security = user\n        passdb backend = tdbsam\n\n# --------------------------- Logging Options -----------------------------\n\n        # log files split per-machine:\n        log file = /var/log/samba/log.%m\n\n        # maximum size of 50KB per log file, then rotate:\n        max log size = 50\n\n# --------------------------- Printing Options -----------------------------\n# Disable printing altogether\n\n        load printers = no\n        printcap name = /dev/null\n        printing = bsd\n        disable spoolss = yes\n\n# --------------------------- File System Options ---------------------------\n\n;       map archive = no\n;       map hidden = no\n;       map read only = no\n;       map system = no\n;       store dos attributes = yes\n\n\n#============================ Share Definitions ==============================\n\n[Hidden]\n        comment = I'm a secret!\n        browsable = no\n        path = /media/storage/Dropbox\n        public = no\n        writable = no\n        printable = no\n        write list = @users\n        guest ok = no\n\n[Anonymous Dropbox]\n        comment = Public File Drop Be Nice\n        browsable = yes\n        path = /media/storage/Dropbox\n        public = yes\n        writable = yes\n        printable = no\n        guest ok = yes\n\n[Media]\n        comment = Public Media Shares\n        browsable = yes\n        path = /media/storage/Media\n        public = yes\n        writable = no\n        printable = no\n        write list = @users\n        guest ok = yes\n```\n","created_at":-62135596800,"fuzzy_word_count":600,"path":"/notes/samba/","published_at":1507587428,"reading_time":3,"tags":null,"title":"Samba","type":"notes","updated_at":1507587428,"weight":0,"word_count":554},{"cid":"46365b05c9d1f644c0cfc4c953b0c7b3057df4f5","content":"\nThank you for taking the time to consider the security of other people! If you've found a security vulnerability with any of my infrastructure, I'd greatly appreciate an [email](mailto:sam@stelfox.net). If the vulnerability is sensitive enough, please consider encrypting the contents with my [public key](/publickey.gpg) (Key ID 0x30856D4EA0FFBA8F).\n\nAs soon as I receive any reports, I'll respond as soon as possible then begin confirming it myself. Once I've addressed the issue I'll publish both an acknowledgement on this page and write a post on addressing the vulnerability. You're welcome (and I encourage you) to publish your analysis and I will happily link to it in both places.\n\nAlternatively, if one the configs I've published on my site has a flaw I'd love to hear about that as well and will also give you an acknowledgement here and on the page with the effected config.\n\n## Acknowledgements\n\n*Currently, I haven't received any security reports. Thanks for your consideration*\n","created_at":-62135596800,"fuzzy_word_count":200,"path":"/security/","published_at":1721005407,"reading_time":1,"tags":null,"title":"Security Acknowledgements","type":"page","updated_at":1721005407,"weight":0,"word_count":154},{"cid":"722c96871fb25707693a5cf768bdc52a851a30a6","content":"\n***Note: This page is quite old and is likely out of date. My opinions may have\nalso changed dramatically since this was written. It is here as a reference\nuntil I get around to updating it.***\n\nConsider [Postfix][1] instead. It was written with security in mind.\n\n* http://www.sendmail.com/sm/open_source/docs/m4/tweaking_config.html\n* http://etutorials.org/Linux+systems/red+hat+linux+bible+fedora+enterprise+edition/Part+IV+Red+Hat+Linux+Network+and+Server+Setup/Chapter+19+Setting+Up+a+Mail+Server/Configuring+sendmail/\n* http://www.fredshack.com/docs/sendmail.html\n\n## Installation\n\n```\nyum install sendmail -y\n```\n\n## Configure\n\nAny configuration changes need to be `re-compiled` by the m4 processor. This is\navailable in the `sendmail-cf` package which can be install using the following\ncommand:\n\n```\nyum install sendmail-cf -y\n```\n\n[1]: {{\u003c ref \"./postfix.md\" \u003e}}\n","created_at":-62135596800,"fuzzy_word_count":100,"path":"/notes/sendmail/","published_at":1540945455,"reading_time":1,"tags":null,"title":"Sendmail","type":"notes","updated_at":1540945455,"weight":0,"word_count":82},{"cid":"1c1354826e33da3ce81d4dea48bb1c660a9e69f4","content":"\n***Note: This page is quite old and is likely out of date. My opinions may have\nalso changed dramatically since this was written. It is here as a reference\nuntil I get around to updating it.***\n\n## Bash Builtins\n\nThe script utility which I've found in Fedora as a default package (or it's a\nbash builtin). You can record a session and the timing data about it exit and\nthen view it like an in console video.\n\nRecord a session with it's timing data...\n\n```\n[user@host ~]$ script -t 2\u003e example.timing -a example.session\nScript started, file is example.session\n[user@host ~]$ # DO STUFF\n[user@host ~]$ exit\nScript done, file is example.session\n```\n\nAnd replay it later:\n\n```\n[user@host ~]$ scriptreplay example.timing example.session\n```\n\n","created_at":-62135596800,"fuzzy_word_count":200,"path":"/notes/session-recording/","published_at":1507587428,"reading_time":1,"tags":null,"title":"Session Recording","type":"notes","updated_at":1507587428,"weight":0,"word_count":116},{"cid":"6d7231705c8889551209c3fd10ff4bbff4ebb42d","content":"\nUnless otherwise noted all code samples on this site are licensed under [the MIT license](https://github.com/sstelfox/stelfox.net/blob/master/LICENSE.mit). Code referenced on other sites or where attribution is directly provided may be under alternative licenses and it is up to the consumer of the content to verify they are compliant with the external licenses.\n\nPost and page prose of this site is licensed under the [Creative Commons Attribution 4.0 International License](http://creativecommons.org/licenses/by/4.0/).\n","created_at":-62135596800,"fuzzy_word_count":100,"path":"/licenses/","published_at":1721005407,"reading_time":1,"tags":null,"title":"Site License Information","type":"page","updated_at":1721005407,"weight":0,"word_count":67},{"cid":"b14bab695d572e08174d76afccbca8acf2d42c12","content":"\n***Note: This page is quite old and is likely out of date. My opinions may have\nalso changed dramatically since this was written. It is here as a reference\nuntil I get around to updating it.***\n\nSuricata might be a better option and has packages for Fedora 15.\n\nUseful software that can complement an IDS/IPS: OpenFPC, Snorby\n\n## Notes\n\nAlright this is frustrating, Fedora 15 doesn't have snort packages so we're\ncompiling from source. Here's what we need to do:\n\nDownload snort \u0026 daq source (both available on snort.org)\n\nExtract them\n\nInstall the following fedora packages and their dependencies:\n\n```\nyum bison flex make gcc libpcap libpcap-devel libdnet libdnet-devel zlib \\\n  zlib-devel mysql mysql-devel -y\n```\n\ncd into the daq directory\n\n```\n./configure \u0026\u0026 make \u0026\u0026 make install\n```\n\nCreate the file `/etc/ld.so.conf.d/snort-i386.conf` with the following\ncontents: `/usr/local/lib/daq`\n\ncd into the snort source directory\n\n```\n./configure --with-mysql --enable-dynamicplugin \u0026\u0026 make \u0026\u0026 make install\nmkdir -p /etc/snort/rules\nmkdir -p /var/log/snort\ncd etc/\ncp * /etc/snort/\n```\n","created_at":-62135596800,"fuzzy_word_count":200,"path":"/notes/snort/","published_at":1508540507,"reading_time":1,"tags":null,"title":"Snort","type":"notes","updated_at":1508540507,"weight":0,"word_count":155},{"cid":"f1f2579b1bcf8119869dd300493bde32c555f88d","content":"\n***Note: This page is quite old and is likely out of date. My opinions may have\nalso changed dramatically since this was written. It is here as a reference\nuntil I get around to updating it.***\n\n## Installation\n\nThe mdadm package is required for software RAID:\n\n```\nyum install mdadm -y\n```\n\n## Array Creation\n\n```\nmdadm --create /dev/md0 --verbose --level=1 --raid-devices=2 /dev/sda /dev/sdb\n```\n\nShorthand:\n\n```\nmdadm -Cv /dev/md0 -l1 -n2 /dev/sd[ab]\n```\n\nNote: If you use a RAID 0 and want to put ZFS on top of be sure to set a chunk\nsize \u003c= 256 (maybe even 128 if you're still getting errors) otherwise ZFS will\nwarn about issues creating it's partitions.\n\n## Troubleshooting\n\n## Status Check\n\nYou can view the status of the RAID by cat'ing `/proc/mdstat`. You can see more\ndetails by using the mdadm utility like so:\n\n```\nmdadm --misc --detail /dev/md0\n```\n\nAs long as the state is clean you're golden.\n\n## Recovery\n\n### Configuration File\n\nDuring initial setup the /etc/mdadm.conf is created automatically. All this\ndata exists in the metadata on the disks and can be rebuilt with the mdadm tool\nlike so:\n\n```\nmdadm --examine --scan \u003e /etc/mdadm.conf\n```\n\n### Remove Disk from Array\n\nA disk needs to be failed before it can be removed from an array, if it isn't\nalready you'll need to fail it manually:\n\n```\nmdadm /dev/md0 --fail /dev/sda\n```\n\nThen remove it:\n\n```\nmdadm /dev/md0 --remove /dev/sda\n```\n\nOr in a single step:\n\n```\nmdadm /dev/md0 --fail /dev/sda --remove /dev/sda\n```\n\n### Adding a Disk to an Existing Array\n\nProbably useful for replacing a failed disk:\n\n```\nmdadm /dev/md0 --add /dev/sda\n```\n\n## Delete an Array\n\nYou'll lose all data... don't say I didn't warn you...\n\n```\nmdadm --stop /dev/md0\n```\n\nI didn't need the second command but you'll want to run it if the device is\nstill kicking around:\n\n```\nmdadm --remove /dev/md0\n```\n\nAnd blow away the super block on all of the drives:\n\n```\nmdadm --zero-superblock /dev/sd[ab]\n```\n","created_at":-62135596800,"fuzzy_word_count":300,"path":"/notes/software-raid/","published_at":1507587428,"reading_time":2,"tags":null,"title":"Software RAID","type":"notes","updated_at":1507587428,"weight":0,"word_count":282},{"cid":"f5946d3f3078c473f5c0c767b90c76521781c8b2","content":"\n***Note: This page is quite old and is likely out of date. My opinions may have\nalso changed dramatically since this was written. It is here as a reference\nuntil I get around to updating it.***\n\n* http://klaubert.wordpress.com/2008/01/09/squid-kerberos-authentication-and-ldap-authorization-in-active-directory/\n* http://gofedora.com/how-to-configure-squid-proxy-server/\n* http://rosincore.blogspot.com/2010/01/dansguardian-content-filtering-with-ad.html\n* http://eu.squid-cache.org/ConfigExamples/Authenticate/Kerberos\n* http://wiki.squid-cache.org/Features/NegotiateAuthentication\n* http://www.squid-cache.org/Versions/v3/3.2/cfgman/\n\nFor URL filtering please refer to the [SquidGuard][1] page.\n\n```\nyum install squid -y\n```\n\nBefore configuring squid we mind as well get the authentication credentials in\nplace. To do this we make use of the `htpasswd` utility which can be installed\nthrough the `httpd-tools` package.\n\nFor the first account:\n\n```\nhtpasswd -b -c /etc/squid/accounts username password\n```\n\nAnd subsequent accounts:\n\n```\nhtpasswd -b /etc/squid/accounts username2 password2\n```\n\nLets drop the config into place, this is fairly generic there are just a few IP\naddresses, networks, and emails to change. It should live at\n`/etc/squid/squid.conf`.\n\n```sh\n# Configuration reference:\n# http://www.squid-cache.org/Versions/v3/3.2/cfgman/\n# https://calomel.org/squid.html\n# Note to self search for \"TODO\" and \"TEST\"\n\n# Bind to the standard squid part (3128)\nhttp_port [::]:3128\n\n### GENERAL CONFIGURATION\n\n# Email address of the local cache manager in case an issue crops up. This will\n# also show up in error messages.\ncache_mgr cache-admin@proxy-01.i.0x378.net\n\n# For security and stability reasons Squid can check hostnames for Internet\n# standard RFC compliance.\ncheck_hostnames on\n\n# Leave coredumps in the base cache directory\ncoredump_dir /var/spool/squid\n\n# Automatically close broken persistent connections\ndetect_broken_pconn on\n\n# Using the transparent option prevent squid from manipulating this header,\n# turning it off just sets the client's address do \"unknown\".\nforwarded_for transparent\n\n# Prevent Squid from announcing it's version information\nhttpd_suppress_version_string on\n\n# Minimum umask to be enforced for all written files, generally this should be\n# public information (cached content) but it can't hurt to  restrict it.\numask 027\n\n# Hostname that is visible in error messages, and if used in a cluster used to\n# detect forwarding loops. If your cluster needs to use the same\n# visible_hostname investigate the related option unique_hostname.\nvisible_hostname proxy-01.internal.private.web\n\n### TIMEOUTS\n\n# How long to wait for TCP connect to a requested server or peer to complete\n# before Squid should attempt to find another path to forward the request.\nconnect_timeout 5 second\n\n# If no response is received to a DNS query within this time all DNS servers\n# for the queried domain are assumed to be unavailable. This may cause\n# transient resolution errors to be cached but I haven't verified that.\ndns_timeout 5 second\n\n# Max length of time Squid should attempt to find a forwarding path for a\n# request before giving up.\nforward_timeout 10 second\n\n# How long to wait for complete HTTP request headers after initial connection\n# establishment.\nrequest_timeout 10 second\n\n### AUTHENTICATION\n\n# Ensure usernames are case sensitive\nauth_param basic casesensitive on\n\n# Length of time to consider credentials authenticated through the external\n# program valid.\nauth_param basic credentialsttl 2 hours\n\n# Use basic htpasswd style files to handle authentication.\nauth_param basic program /usr/lib64/squid/basic_ncsa_auth /etc/squid/accounts\n\n# The realm to display to any inquistive clients\nauth_param basic realm Proxy Access Requires Authentication\n\n### CACHE CONFIGURATION\n\n# \u003cstore type\u003e \u003cstore location\u003e \u003ccache size in MB\u003e \u003c# of first level\n# directories\u003e \u003c# of second level directories\u003e aufs is the non-blocking disk\n# storage option\ncache_dir aufs /var/spool/squid 1024 16 256\n\n# This represents the maximum amount of ram that squid will utilize to keep\n# cached objects in memory. Squid requires about 100Mb of RAM per Gb of cache\n# storage. If you have a 10gb cache, Squid will use ~1Gb just to handle that.\n# Make sure that cache_mem + (cache_dir size limit * 100Mb) is less than your\n# available RAM.\ncache_mem 192 MB\n\n# Least Frequently Used with Dynamic Aging (keeps popular objects in cache\n# regardless of their size and thus optimizes byte hit rate at the expense of\n# hit rate since one large, popular object will prevent many smaller, slightly\n# less popular objects from being cached.\ncache_replacement_policy heap LFUDA\n\n# Limit the size of file to hold in the cache, 10 MB is about the largest file\n# I expect multiple of my client to pull down.\nmaximum_object_size 10 MB\n\n# Tell squid to release memory it's not using\nmemory_pools off\n\n# Cacheing rules for content that doesn't have an explicit expire time.\nrefresh_pattern ^ftp:             1440 20%    10080\nrefresh_pattern -i (/cgi-bin/|\\?) 0    0%     0\nrefresh_pattern .                 0    20%    4320\n\n# Specifies how long squid should wait when it's service is asked to stop.\n# Generally this to allow client requests to complete, but I find it more\n# useful to make this fast.\nshutdown_lifetime 1 second\n\n# This normally evaluate how much data is left in a transfer when a client\n# disconnects before it's completed, and if the remaining data falls in the\n# configured range, Squid will finish downloading the file and cache it anyway.\n# This is cool but not useful in my environment.\nquick_abort_min 0 KB\nquick_abort_max 0 KB\n\n### SSL BUMP\n\n# TODO: This is exactly worst practices in most cases, however, for a highly\n# restricted network of servers using this as their only means of accessing the\n# internet... It can be an important security measure.\n\n### CUSTOM ERROR MESSAGES\n\n# TODO: Useful for client facing servers\n\n### SQUIDGUARD CONFIG\n\n#url_rewrite_program /usr/bin/squidGuard -c /etc/squid/squidGuard.conf\n#url_rewrite_bypass off\n\n### LOGGING\n\n# \u003cClient IP\u003e \u003cUsername\u003e [\u003cLocal Time\u003e] \"\u003cRequest Method\u003e \u003cRequest URL\u003e\n# HTTP/\u003cProtocol Version\u003e \u003cResponse Status Code\u003e \u003cSent reply size (with\n# headers)\u003e \u003cReferer\u003e \u003cUser Agent\u003e \u003cSquid Request Status\u003e:\u003cSquid Hierarchy\n# Status\u003e\nlogformat        combined %\u003ea %un [%tl] \"%rm %ru HTTP/%rv\" %\u003eHs %\u003cst \"%{Referer}\u003eh\" \"%{User-Agent}\u003eh\" %Ss:%Sh\n\n# \u003cClient IP\u003e \u003cUsername\u003e [\u003cLocal Time\u003e] \"\u003cSent Headers\u003e\" \"\u003cReply Headers\u003e\"\nlogformat        debugheaders %\u003ea %un [%tl] \"%\u003eh\" \"%\u003ch\"\n\n# Send our neat logs through to the local syslog server\naccess_log       syslog:local3.info combined\n\n# Useful for debugging\n#access_log      syslog:local3.debug debugheaders\n#access_log      stdio:/var/log/squid/headers.log debugheaders\n#cache_store_log stdio:/var/log/squid/store.log\n#cache_log       stdio:/var/log/squid/cache.log\n#logfile_rotate  3\n\n### BEGIN ACL ALIASES\n\nacl CONNECT method CONNECT\n\n# An ACL to match authenticated users\nacl authenticated_user proxy_auth REQUIRED\n\n# ACL covering clients allowed to use this proxy\nacl allowed_hosts src 10.0.0.0/24\nacl allowed_hosts src fc00::/7       # RFC 4193 local private network range\nacl allowed_hosts src fe80::/10      # RFC 4291 link-local (directly plugged) machines\n\n# ACLs covering restricted destinations\nacl to_local_network dst 10.0.0.0/24\n\n# Limit individual matching hosts to 500 connections\nacl proxy_connection_limit maxconn 500\n\n# Which ports to allow raw connects too (For SSL mostly)\nacl SSL_ports port 443 \t        # https\n\n# Which ports to allow regular unencrypted procols to connect to\nacl allowed_ports port 80          # http\nacl allowed_ports port 21          # ftp\nacl allowed_ports port 443         # https\n#acl allowed_ports port 1025-65535  # unregistered ports\n\n### BEGIN ACLS\n\n# Only allow cachemgr access from localhost\nhttp_access allow manager localhost\nhttp_access deny manager\n\n# Access to local web based services should be explictely and directly allowed.\n# Deny access to them through this proxy.\nhttp_access deny to_local_network\n\n# Deny requests from clients that go over the connection limits\nhttp_access deny proxy_connection_limit\n\n# Allow CONNECT to only the explicitely allowed ports\nhttp_access allow CONNECT SSL_ports\nhttp_access deny CONNECT\n\n# Deny requests to any port we haven't whitelisted\nhttp_access deny !allowed_ports\n\n# Allow access to this squid server from our local whitelisted network... as\n# long as they're authenticated.\nhttp_access allow allowed_hosts authenticated_user\n\n# Deny any requests we haven't allowed by this point\nhttp_access deny all\n\n### PRIVACY / REQUEST MANGLING\n\n# TODO: Client information can leak out through headers including information\n# about services and OS's running inside the network. Mangling the requests and\n# responses and whitelisting the various headers of value can help mitigate\n# this, and provide meaningful dis-information.\n\n#request_header_replace User-Agent Mozilla/5.0 (Windows; U; Windows NT 5.1; en-US; rv:1.9.1.3) Gecko/20090824 Firefox/3.5.3 (.NET CLR 3.5.30729)\n#request_header_replace Accept */*\n#request_header_replace Accept-Encoding gzip # May cause issues should TEST\n#request_header_replace Accept-Language en\n#\n## Some of these should be reply_header_access\n#request_header_access Accept allow all\n#request_header_access Accept-Encoding allow all\n#request_header_access Accept-Language allow all\n#request_header_access Authorization allow all\n#request_header_access Cache-Control allow all\n##request_header_access Content-Disposition allow all\n#request_header_access Content-Encoding allow all\n#request_header_access Content-Length allow all\n##request_header_access Content-Location allow all\n#request_header_access Content-Range allow all\n#request_header_access Content-Type allow all\n#request_header_access Cookie allow all\n#request_header_access Expires allow all\n#request_header_access Host allow all\n#request_header_access If-Modified-Since allow all\n##request_header_access Location allow all\n#request_header_access Range allow all\n#request_header_access Referer allow all\n#request_header_access Set-Cookie allow all\n#request_header_access User-Agent deny all\n#request_header_access WWW-Authenticate allow all\n#request_header_access All deny all\n```\n\nTo enable syslog logging we need to add the `-s` flag to `SQUID_OPTS` in\n`/etc/sysconfig/squid`.\n\n```\n# Squid options (additionally log to syslog)\nSQUID_OPTS=\"-s\"\n\n# Time to wait for Squid to shut down when asked. Should not be necessary\n# most of the time.\nSQUID_SHUTDOWN_TIMEOUT=100\n\n# default squid conf file\nSQUID_CONF=\"/etc/squid/squid.conf\"\n```\n\nEnable and start the service:\n\n```\nsystemctl enable squid.service\nsystemctl start squid.service\n```\n\n## Firewall\n\nAllow access to the squid proxy from within the airlock, and the normal ports\noutbound.\n\n```\n# Allow access to the local squid proxy server (mostly for updates)\n-A INPUT  -m tcp -p tcp --dport 3128 -s 10.0.0.0/24 -j ACCEPT\n\n# Allow squid to make FTP/HTTP(S) requests on behalf of the other machines in\n# the network\n-A OUTPUT -m tcp -p tcp --dport 21  -j ACCEPT\n-A OUTPUT -m tcp -p tcp --dport 80  -j ACCEPT\n-A OUTPUT -m tcp -p tcp --dport 443 -j ACCEPT\n```\n\nOther clients on the network will need to be able to talk to the squid port.\n\n```\n-A OUTPUT -m tcp -p tcp --dport 3128 -d 10.0.0.170 -j ACCEPT\n```\n\n## A Note on Non-Standard Ports\n\nIt is highly recommended to run squid on a non-standard port if it is going to\nbe exposed to the internet. The catch is that this will throw SELinux for a\nloop. The following command will tell SELinux to be ok with squid running on\nport 11637.\n\n```\n[root@localhost ~]# semanage port -a -t http_cache_port_t -p tcp 11637\n```\n\nThis may be necessary if squid needs to listen on multiple ports (for example\nfor dansGuardian with kerberos authentication).\n\n## Viewing Statistics\n\nOn the squid server you can run a report to see the status of squid with the\n\"squidclient\" command like so:\n\n```\nsquidclient -h 127.0.0.1 mgr:info\n```\n\n## Testing\n\nYou can confirm the squid works like so:\n\n```\ncurl -x 10.0.0.170:3128 https://www.google.com/\n```\n\nWith authentication:\n\n```\ncurl --proxy-basic --proxy-user user:pass -x 10.0.0.170:3128 https://www.google.com/\n```\n\nTo get yum working with the proxy add the following line to the `[main]`\nsection of /etc/yum.conf file.\n\n```ini\nproxy=http://10.0.0.170:3128\n```\n\nIf authentication is required the two following lines can always be added:\n\n```ini\nproxy_username=username\nproxy_password=password\n```\n\nTo setup general proxy use on the system for various command line utilities\nlike `wget`, `curl`, and `lynx`, there are a set of common environment variable\nto set. I created a file `/etc/profile.d/proxy.sh` with the following contents\nand make it executable.\n\n```\n# General proxy configuration for system tools\nexport http_proxy=squid.local.tld:3128\nexport https_proxy=squid.local.tld:3128\nexport ftp_proxy=squid.local.tld:3128\nexport ALL_PROXY=squid.local.tld:3128\nexport no_proxy=.i.0x378.net\nexport NO_PROXY=.i.0x378.net\n```\n\nYou can include credentials by prepending the `user:pass@` to the hosts of the\nprotocols above.\n\n[1]: {{\u003c ref \"./squid_guard.md\" \u003e}}\n","created_at":-62135596800,"fuzzy_word_count":2100,"path":"/notes/squid/","published_at":1540945455,"reading_time":10,"tags":null,"title":"Squid","type":"notes","updated_at":1540945455,"weight":0,"word_count":2033},{"cid":"21450ff2a564c16c71368a12420217901276ba15","content":"\n***Note: This page is quite old and is likely out of date. My opinions may have\nalso changed dramatically since this was written. It is here as a reference\nuntil I get around to updating it.***\n\nSquid Guard is officially a \"URL Rewrite Program\". It takes information about\nthe requested page coming in and checks it against a series of ACLs,\nblacklists, and whitelists in an order specified by the administrator. The\nadministrator can redirect offending pages to a block page (which has to be a\nhosted site outside of squid).\n\n## Installation\n\nDon't use the \"squidGuard\" service, it is incorrectly defined, looking\nfor non-existant files, and is poorly implemented. Call squidGuard directly\nfrom the squid config (see [Squid][1]).\n\nNOTICE: To use squid guard at all you need to have a web server setup to\nredirect sites that are denied. For this purpose I decided to install apache,\ncreate a folder \"guard\" and within it a file named blocked.php that simply\nprints out the contents of the $_GET variable. With the config provided this\nwill show the client's address, name, username, group, category of the\noffending URL and the URL itself. This can be used to properly display an error\nmessage. I am quite annoyed by how this works... I'd much prefer to just deny\nthe requests...\n\n```\nyum install squidGuard -y\n```\n\n### Black \u0026 White Lists\n\nRemove the default blacklist file as we're going to use another one. This is a\nnecessary step to continue using the rest of these instructions. If you don't\nwan to use the Shalla Lists, you'll have to adapt to the instructions given\nhowever they shouldn't differ much between different lists.\n\n```\n[root@localhost ~]# rm -f /var/squidGuard/blacklists.tar.gz\n[root@localhost ~]# mkdir -p /var/squidGuard/blacklists\n[root@localhost ~]# mkdir -p /var/squidGuard/whitelists/custom/\n[root@localhost ~]# touch /var/squidGuard/whitelists/custom/domains\n[root@localhost ~]# touch /var/squidGuard/whitelists/custom/urls\n```\n\nDownload a blacklist file, [Shalla List][2] seems to be the defacto/good one.\n\n```\n[root@localhost ~]# curl http://www.shallalist.de/Downloads/shallalist.tar.gz -o /var/squidGuard/blacklists/shallalist.tar.gz 2\u003e/dev/null\n[root@localhost ~]# curl http://www.shallalist.de/Downloads/shallalist.tar.gz.md5 -o /var/squidGuard/blacklists/shallalist.tar.gz.md5 2\u003e /dev/null\n[root@localhost ~]# cat /var/squidGuard/blacklists/shallalist.tar.gz.md5 \u0026\u0026 md5sum /var/squidGuard/blacklists/shallalist.tar.gz\nc50fdc59593bc3e5c5863e2b120f6fed  shallalist.tar.gz\nc50fdc59593bc3e5c5863e2b120f6fed  /var/squidGuard/blacklists/shallalist.tar.gz\n```\n\nNote: the md5sums are the same (we received an intact download, the sums will\nbe different everytime it updates).\n\nNow that we've verified the blacklists lets extract them.\n\n```\n[root@localhost ~]# cd /var/squidGuard\n[root@localhost squidGuard]# tar -xzvf shallalist.tar.gz --strip-components 1\n[root@localhost squidGuard]# chown -R squid:squid /var/squidGuard/\n```\n\n## Configuration\n\nBy default, squidGuard tries to store it's logfiles in `/var/log/squidGuard`,\nhowever, since squid starts up squidGuard, squidGuard will be running under the\nsquid user. This is important as the squid user won't have permission to\n`/var/log/squidGuard`. We should switch it too `/var/log/squid`, and ensure\nthat `/var/squidGuard/blacklists` are owned by squid with the squid group.\n\n```\n# Configure locations for the rest of squidGuard's files\ndbhome /var/squidGuard\nlogdir /var/log/squid\n\n# Time aliases\n# s = sun, m = mon, t =tue, w = wed, h = thu, f = fri, a = sat\n\n# Hours during the work day\ntime workhours {\n  weekly mtwhf 08:00 - 12:00\n  weekly mtwhf 13:00 - 16:30\n}\n\n# Source ACLs\nsrc localnets {\n  ip 155.42.89.0/24\n}\n\n# Destination whitelists (overrides)\ndest whitelists_custom {\n  domainlist  whitelists/custom/domains\n  urllist   whitelists/custom/urls\n}\n\n# Destination blacklists - Descriptions from shallalist.de\n\n# All about advertising: This includes sites offering banners and banner\n# creation as well as sites delivering banners to be shown in webpages.\n# Advertising companies are listed, too.\ndest adv {\n  domainlist  blacklists/adv/domains\n  urllist   blacklists/adv/urls\n}\n\n# Sites of obvious aggressive content. This coveres hate speech and all\n# kinds of racism.\ndest aggressive {\n  domainlist  blacklists/aggressive/domains\n  urllist   blacklists/aggressive/urls\n}\n\n# Sites of breweries, wineries and destilleries. This category also covers\n# sites that explain howto make beer, wines and spirits.\ndest alchohol {\n  domainlist  blacklists/alcohol/domains\n  urllist   blacklists/alcohol/urls\n}\n\n# This category covers sites providing vpn services to the public. The\n# focus is on vpn sites used to hide the origin of the traffic like\n# tor nodes. The category does not include company vpn accesses.\ndest anonvpn {\n  domainlist  blacklists/anonvpn/domains\n  urllist   blacklists/anonvpn/urls\n}\n\n# All around motorcycles. Included are vendor sites, resellers, fan and\n# hobby pages as well as and suppliers. Scooters included.\ndest automobile_bikes {\n  domainlist  blacklists/automobile/bikes/domains\n  urllist   blacklists/automobile/bikes/urls\n}\n\n# All around motorboats. Included are vendor sites, resellers, fan and\n# hobby pages as well as and suppliers. Not included are travel tips\n# (this can be found in recreation/travel).\ndest automobile_boats {\n  domainlist  blacklists/automobile/boats/domains\n  urllist   blacklists/automobile/boats/urls\n}\n\n# All around cars. Included are automobile companies and automotive\n# suppliers.\ndest automobile_cars {\n  domainlist  blacklists/automobile/cars/domains\n  urllist   blacklists/automobile/cars/urls\n}\n\n# All around planes ranging from small one and two seaters up to the\n# large traffic planes, old and new, private, commercial and military.\n# Vendors and supplier are included (airports are not). Helicopter\n# sites are included as well.\ndest automobile_planes {\n  domainlist  blacklists/automobile/planes/domains\n  urllist   blacklists/automobile/planes/urls\n}\n\n# Sites for realtime chatting and instant messaging. Everything that\n# is not realtime is included in -\u003e forum.\ndest chat {\n  domainlist  blacklists/chat/domains\n  urllist   blacklists/chat/urls\n}\n\n# Sites that lure with free of charge services but then give you a\n# costly abbonement (written somewhere in tiny letters nearly\n# unreadable).\ndest costtraps {\n  domainlist  blacklists/costtraps/domains\n  urllist   blacklists/costtraps/urls\n}\n\n# Sites to contact people for love and live together. He seeks her,\n# she seeks him and so on.\ndest dating {\n  domainlist  blacklists/dating/domains\n  urllist   blacklists/dating/urls\n}\n\n# This covers mostly filesharing, p2p and torrent sites. Other\n# download sites (for software, wallpapers, ..) are included as well.\ndest downloads {\n  domainlist  blacklists/downloads/domains\n  urllist   blacklists/downloads/urls\n}\n\n# Sites offering drugs or explain how to make drugs (legal and non\n# legal). Covers tabacco as well as viagra and similar substances.\ndest drugs {\n  domainlist  blacklists/drugs/domains\n  urllist   blacklists/drugs/urls\n}\n\n# All domains where people log in from one obtaining a dynmic IP\n# address. Dynamic sites can be most harmless as well as carry\n# redirecting proxies to bypass the webfilter or porn, games or\n# anything else why may be inappropiate.\ndest dynamic {\n  domainlist  blacklists/dynamic/domains\n  urllist   blacklists/dynamic/urls\n}\n\n# Home pages of schools, colleges and universities.\ndest education {\n  domainlist  blacklists/education/schools/domains\n  urllist   blacklists/education/schools/urls\n}\n\n# Home page of banking companies are listed here. This is not\n# restricted to online banking.\ndest finance_banking {\n  domainlist  blacklists/finance/banking/domains\n  urllist   blacklists/finance/banking/urls\n}\n\n# Sites of insurance companies, about information about insurances\n# and link collections concering this subject.\ndest finance_insurance {\n  domainlist  blacklists/finance/insurance/domains\n  urllist   blacklists/finance/insurance/urls\n}\n\n# Sites one can apply for loans and mortgages or can obtain\n# information about this business.\ndest finance_moneylending {\n  domainlist  blacklists/finance/moneylending/domains\n  urllist   blacklists/finance/moneylending/urls\n}\n\n# Sites about all types of realestate, bying and selling homes,\n# finding appartments for rent and selling.\ndest finance_realestate {\n  domainlist  blacklists/finance/realestate/domains\n  urllist   blacklists/finance/realestate/urls\n}\n\n# Sites about the stock exchange market, trading of stocks and stock\n# options as well as sites related to this subject.\ndest finance_trading {\n  domainlist  blacklists/finance/trading/domains\n  urllist   blacklists/finance/trading/urls\n}\n\n# All financial pages that do not fit in the financial categories\n# above.\ndest finance_other {\n  domainlist  blacklists/finance/other/domains\n  urllist   blacklists/finance/other/urls\n}\n\n# All sites about astrology, horoscopes, numerology, palm reading\n# and so on; sites that offer services to forsay the future.\ndest fortunetelling {\n  domainlist  blacklists/fortunetelling/domains\n  urllist   blacklists/fortunetelling/urls\n}\n\n# Discussion sites. Covered explicit forum sites and some blogs.\n# Sites where people can discuss and share information in a non\n# interactive/realtime way. Realtime discussions are covered with\n# chat.\ndest forum {\n  domainlist  blacklists/forum/domains\n  urllist   blacklists/forum/urls\n}\n\n# Sites offering the possibility to win money. Poker, Casino,\n# Bingo and other chance games as well as betting sites. Differs\n# from -\u003e hobby/games in the aspect of winning or loosing money\n# or being lured to do so.\ndest gamble {\n  domainlist  blacklists/gamble/domains\n  urllist   blacklists/gamble/urls\n}\n\n# Sites belonging to the goverment of a country, county or city.\ndest government {\n  domainlist  blacklists/government/domains\n  urllist   blacklists/government/urls\n}\n\n# Sites with information and discussions about security weaknesses\n# and how to exploit them. Sites offering exploits are listed as\n# well as sites distributing programs that help to find security\n# leaks.\ndest hacking {\n  domainlist  blacklists/hacking/domains\n  urllist   blacklists/hacking/urls\n}\n\n# Sites concering food and food preparation.\ndest hobby_cooking {\n  domainlist  blacklists/hobby/cooking/domains\n  urllist   blacklists/hobby/cooking/urls\n}\n\n# Sites related to games. This includes descriptions, news and\n# general information about games. No gamble sites.\ndest hobby_games-misc {\n  domainlist  blacklists/hobby/games-misc/domains\n  urllist   blacklists/hobby/games-misc/urls\n}\n\n# Sites about online games (all kinds of browserbased games). The\n# games are for fun only (no gamble).\ndest hobby_games-online {\n  domainlist  blacklists/hobby/games-online/domains\n  urllist   blacklists/hobby/games-online/urls\n}\n\n# Sites about gardening, grewing plants, fighting bugs and\n# everything else related to gardening.\ndest hobby_gardening {\n  domainlist  blacklists/hobby/gardening/domains\n  urllist   blacklists/hobby/gardening/urls\n}\n\n# Sites about all topics concerning pets: description, raise,\n# food, looks, fairs, favorite pet stories and so on.\ndest hobby_pets {\n  domainlist  blacklists/hobby/pets/domains\n  urllist   blacklists/hobby/pets/urls\n}\n\n# Sites about everything required to create a cozy home (interior\n# design and assesoirs).\ndest homestyle {\n  domainlist  blacklists/homestyle/domains\n  urllist   blacklists/homestyle/urls\n}\n\n# Sites of hospitals and medical facilities.\ndest hospitals {\n  domainlist  blacklists/hospitals/domains\n  urllist   blacklists/hospitals/urls\n}\n\n# Sites specialized on hosting images, photogalleries and so on.\ndest imagehosting {\n  domainlist  blacklists/imagehosting/domains\n  urllist   blacklists/imagehosting/urls\n}\n\n# Home pages of Internet Service Providers. Site of companies\n# offering webspace only are now being added, too.\ndest isp {\n  domainlist  blacklists/isp/domains\n  urllist   blacklists/isp/urls\n}\n\n# Portals for job offers and job seekers as well as the career\n# and work-for-us pages of companies.\ndest jobsearch {\n  domainlist  blacklists/jobsearch/domains\n  urllist   blacklists/jobsearch/urls\n}\n\n# Online libraries and sites where you can obtain and/or read\n# e-books. Book shops are not listed here but under shopping.\ndest library {\n  domainlist  blacklists/library/domains\n  urllist   blacklists/library/urls\n}\n\n# Sites of military facilites or related to the armed forces.\ndest military {\n  domainlist  blacklists/military/domains\n  urllist   blacklists/military/urls\n}\n\n# Model agency, model and supermodel fan pages and other model\n# sites presenting model photos. No porn pictures.\ndest models {\n  domainlist  blacklists/models/domains\n  urllist   blacklists/models/urls\n}\n\n# Sites offering cinema programs, information about movies and actors.\n# Sites for downloading video clips/movies (as long it is legal) are\n# included as well.\ndest movies {\n  domainlist  blacklists/movies/domains\n  urllist   blacklists/movies/urls\n}\n\n# Sites that offer the download of music, information about music\n# groups or music in general.\ndest music {\n  domainlist  blacklists/music/domains\n  urllist   blacklists/music/urls\n}\n\n# Sites presenting news. Homepages from newspapers, magazines and\n# journals as well as some blogs.\ndest news {\n  domainlist  blacklists/news/domains\n  urllist   blacklists/news/urls\n}\n\n# Sites offering podcasts or podcast services.\ndest podcasts {\n  domainlist  blacklists/podcasts/domains\n  urllist   blacklists/podcasts/urls\n}\n\n# Sites of political parties, political organisations and associations;\n# sites with political discussions.\ndest politics {\n  domainlist  blacklists/politics/domains\n  urllist   blacklists/politics/urls\n}\n\n# Sites about all kinds of sexual content ranging from bare bosoms to\n# hardcore porn and sm.\ndest porn {\n  domainlist  blacklists/porn/domains\n  urllist   blacklists/porn/urls\n}\n\n# Domains and urls of TV and radio stations, regardless whether they\n# offer any programs on the site or just displaying a static page. The\n# sites offering streams are still collected in webradio and webtv,\n# respectively.\ndest radiotv {\n  domainlist  blacklists/radiotv/domains\n  urllist   blacklists/radiotv/urls\n}\n\n# Humorous pages, comic strips, funny stories, everything which makes\n# people laugh.\ndest recreation_humor {\n  domainlist  blacklists/recreation/humor/domains\n  urllist   blacklists/recreation/humor/urls\n}\n\n# Sites dedicated to martial arts such karate, kung fu, taek won do as\n# well as fighting sports sites like ufc. All site listed in this\n# category are also part of sports. This category is meant for users\n# who wish to allow sports but no \"aggressive\" kind of sports.\ndest recreation_martialarts {\n  domainlist  blacklists/recreation/humor/domains\n  urllist   blacklists/recreation/humor/urls\n}\n\n# Sites of restaurants as well as restaurant descriptions and\n# comentaries.\ndest recreation_restaurants {\n  domainlist  blacklists/recreation/humor/domains\n  urllist   blacklists/recreation/humor/urls\n}\n\n# All about sports: sports teams, sport discussions as well as\n# information about sports people and the varios sports themselves.\ndest recreation_sports {\n  domainlist  blacklists/recreation/humor/domains\n  urllist   blacklists/recreation/humor/urls\n}\n\n# Sites with information about foreign countries, travel companies,\n# travel fares, accomondations and everything else that has to do\n# with travel including travel blogs.\ndest recreation_travel {\n  domainlist  blacklists/recreation/humor/domains\n  urllist   blacklists/recreation/humor/urls\n}\n\n# Sites about treatments for feeling internally and externally healthy\n# and beautiful again.\ndest recreation_wellness {\n  domainlist  blacklists/recreation/wellness/domains\n  urllist   blacklists/recreation/wellness/urls\n}\n\n# Sites that actively help to bypass url filters by accepting urls via\n# webform and play a proxing and redirecting role.\ndest redirector {\n  domainlist  blacklists/redirector/domains\n  urllist   blacklists/redirector/urls\n}\n\n# Sites with religious content: all kind of churches, sects, religious\n# interpretations and so on.\ndest religion {\n  domainlist  blacklists/religion/domains\n  urllist   blacklists/religion/urls\n}\n\n# Sites offering the service to remotely access computers, expecially (but not\n# limited to going) through firewalls. This includes using a third party\n# computer. Traditional VPN is not covered.\ndest remotecontrol {\n  domainlist  blacklists/remotecontrol/domains\n  urllist   blacklists/remotecontrol/urls\n}\n\n# Sites that offer the download of ringtones or present other informations\n# about ringtones.\ndest ringtones {\n  domainlist  blacklists/ringtones/domains\n  urllist   blacklists/ringtones/urls\n}\n\n# Sites of institutions as well as of amateurs about all topics of astronomy.\ndest science_astronomy {\n  domainlist  blacklists/science/astronomy/domains\n  urllist   blacklists/science/astronomy/urls\n}\n\n# Sites of institutions as well as of amateurs about all topics of chemistry.\ndest science_chemistry {\n  domainlist  blacklists/science/chemistry/domains\n  urllist   blacklists/science/chemistry/urls\n}\n\n# Collection of seach engines and directory sites.\ndest searchengines {\n  domainlist  blacklists/searchengines/domains\n  urllist   blacklists/searchengines/urls\n}\n\n# Sites explaining the biological functions of the body concerning sexuality\n# as well as sexual health; this, too, covers sites for teenagers with\n# questions about firstlove, first sex, and subjects related to this topics.\n# This category does not cover porn.\ndest sex_education {\n  domainlist  blacklists/sex/education/domains\n  urllist   blacklists/sex/education/urls\n}\n\n# Sites selling and presenting sexy lingerie or lingerie in a sexy manner.\ndest sex_lingerie {\n  domainlist  blacklists/sex/lingerie/domains\n  urllist   blacklists/sex/lingerie/urls\n}\n\n# Sites offering online shopping and price comparisons.\ndest shopping {\n  domainlist  blacklists/shopping/domains\n  urllist   blacklists/shopping/urls\n}\n\n# Sites bringing people together (social networking) be it for friendship\n# or for business.\ndest socialnet {\n  domainlist  blacklists/socialnet/domains\n  urllist   blacklists/socialnet/urls\n}\n\n# Sites that tries to actively try to install software (or lure the user\n# in doing so) in order to spy the surfig behaviour (or worse). This\n# category includes trojan and phishing sites. The homecalling site\n# where the collecting information is sent are listed, too.\ndest spyware {\n  domainlist  blacklists/spyware/domains\n  urllist   blacklists/spyware/urls\n}\n\n# Site keeping an eye on where you surf and what you do in a passive. Covers\n# web bugs, counters and other tracking mechanism in web pages that do not\n# interfere with the local computer yet collecting information about the\n# surfing person for later analysis. Sites actively spying out the surfer\n# by installing software or calling home sites are not covered with tracker\n# but with -\u003e spyware.\ndest trackers {\n  domainlist  blacklists/tracker/domains\n  urllist   blacklists/tracker/urls\n}\n\n# Kind of white list to allow necessary downloads from vendors. Thought as\n# a correction to the downloads category.\ndest updatesites {\n  domainlist  blacklists/updatesites/domains\n  urllist   blacklists/updatesites/urls\n}\n\n# Domains that can be used to shorten long URLs. The orginal (long) URL will\n# be accessed after the the short URL has been requested from the shortener.\n# This distinguishes this category from redirector where the orginal URL is\n# never accessed directly.\ndest urlshortener {\n  domainlist  blacklists/urlshortener/domains\n  urllist   blacklists/urlshortener/urls\n}\n\n# Sites about killing and harming people. Covers anything about brutality\n# and beastiality.\ndest violence {\n  domainlist  blacklists/violence/domains\n  urllist   blacklists/violence/urls\n}\n\n# Collection of sites offering programs to break licence keys, licence\n# keys themselves, cracked software and other copyrighted material.\ndest warez {\n  domainlist  blacklists/warez/domains\n  urllist   blacklists/warez/urls\n}\n\n# Sites offering all kinds of weapons or accessories for weapons: Firearms,\n# knifes, swords, bows,... . Armory shops are included as well as sites\n# holding general information about arms (manufacturing, usage).\ndest weapons {\n  domainlist  blacklists/weapons/domains\n  urllist   blacklists/weapons/urls\n}\n\n# Sites that offer web-based email services.\ndest webmail {\n  domainlist  blacklists/webmail/domains\n  urllist   blacklists/webmail/urls\n}\n\n# Sites that enable user to phone via Internet/WWW. Any site where users\n# can voice-chat with each other (normal chat sites, where users type their\n# messages are part of chat, not webphone).\ndest webphone {\n  domainlist  blacklists/webphone/domains\n  urllist   blacklists/webphone/urls\n}\n\n# Sites that offer listening to music and radiostreams.\ndest webradio {\n  domainlist  blacklists/webradio/domains\n  urllist   blacklists/webradio/urls\n}\n\n# Collection of site offering TV streams via world wide web.\ndest webtv {\n  domainlist  blacklists/webtv/domains\n  urllist   blacklists/webtv/urls\n}\n\n\nacl {\n  localnets within workhours {\n    pass whitelists_custom !adv !aggressive !alchohol !anonvpn !automobile_bikes !automobile_boats !automobile_cars !automobile_planes !chat !costtraps !dating !downloads !drugs !dynamic !fortunetelling !gamble !hobby_games-misc !hobby_games-online !hobby_gardening !hobby_pets !homestyle !hospitals !in-addr !imagehosting !isp !models !movies !music !podcasts !politics !porn !radiotv !recreation_humor !recreation_martialarts !recreation_restaurants !recreation_sports !recreation_travel !recreation_wellness !redirector !religion !remotecontrol !ringtones !sex_education !sex_lingerie !socialnet !spyware !trackers !violence !warez !weapons !webphone !webtv all\n    redirect http://127.0.0.1/guard/blocked.php?clientaddr=%a\u0026clientdomain=%n\u0026clientuser=%i\u0026clientgroup=%s\u0026targetgroup=%t\u0026url=%u\u0026uri=%p\n  } else {\n    pass whitelists_custom !adv !costtraps !in-addr !redirector !ringtones !spyware !trackers all\n    redirect http://127.0.0.1/guard/blocked.php?clientaddr=%a\u0026clientdomain=%n\u0026clientuser=%i\u0026clientgroup=%s\u0026targetgroup=%t\u0026url=%u\u0026uri=%p\n  }\n\n  default {\n    pass updatesites none\n    redirect http://127.0.0.1/guard/blocked.php?clientaddr=%a\u0026clientdomain=%n\u0026clientuser=%i\u0026clientgroup=%s\u0026targetgroup=%t\u0026url=%u\u0026uri=%p\n  }\n}\n```\n\n## Live Config Changes\n\nThe squid command \"squid -k reconfigure\" DOES re-evaluate squidGuard config\nchanges as well. This make it very easy to quickly adjust things all over the\nserver without a significant service interruption.\n\n## Testing and Diagnostics\n\nSo what I've found in my experimentation is that if there is an issue with\nsquidGuard starting up squid will still success and there will probably not be\nany errors anywhere, it will simply not block anything. This is obviously not\nan ideal situation. To test and make sure that your configuration is good (or\npossibly see the errors that may be preventing the filtering from working) run\nthe following command:\n\n```\n[root@localhost ~]# squidGuard -c /etc/squid/squidGuard.conf -C all\n```\n\nIt will go through and build the database files for every category defined in\nthe configuration then verify that everything is in order. If an error does\noccur you'll see the message:\n\n```\n2011-09-20 12:15:50 [7140] Going into emergency mode\n```\n\nLook on the line above it and it you'll have to figure out the issue based on\nthat message.\n\nNOTE: After performing this command all of the database files will be owned by\nroot! This won't work, fix the issue running the following command:\n\n```\n[root@localhost ~]# chown -R squid:squid /var/squidGuard/*\n```\n\nAdditional permission issues can (usually) be resolved with the following\ncommands:\n\n```\n[root@localhost ~]# chown -R root:squid /etc/squid\n[root@localhost ~]# chmod 640 /etc/squid/*\n```\n\nTo test individual rules to make sure things are being redirected or blocked\ncorrectly you can test squidGuard with something along the following lines:\n\n```\n[root@localhost ~]# su squid -s /bin/bash -\nbash-4.2$ echo \"http://porn.com 10.0.0.1/ - - GET\" | squidGuard -d -c /etc/squid/squidGuard.conf\n```\n\nIt will either return nothing (passes the check) or return a URL to redirect\nthe user too (failed the check). You can replace the 10.0.0.1 with an IP of a\nmachine you'd like to test the rules with.\n\n## Security Notes\n\nsquidGuard is an additional layer of protection against known bad sites. It can\nnot possibly catch all of the bad sites but it does help. For any general\nbrowsing that you don't want to restrict the content of, I strongly suggest\nincluding the following groups as exclusions. They will help promote anonymous\nsafe browsing.\n\n* adv\n* costtraps\n* in-addr\n* redirector\n* ringtones\n* spyware\n* trackers\n\nThe only one that may cause issues is the \"in-addr\" one which blocks going to\nIP addresses directly. I personally get around this by including the sites I\nwant to access directly in my whitelist file.\n\n## Sample Error Page\n\n```php\n\u003c?php\n  function uuid() {\n    return sprintf( '%04x%04x-%04x-%04x-%04x-%04x%04x%04x',\n      // 32 bits for \"time_low\"\n      mt_rand( 0, 0xffff ), mt_rand( 0, 0xffff ),\n\n      // 16 bits for \"time_mid\"\n      mt_rand( 0, 0xffff ),\n\n      // 16 bits for \"time_hi_and_version\",\n      // four most significant bits holds version number 4\n      mt_rand( 0, 0x0fff ) | 0x4000,\n\n      // 16 bits, 8 bits for \"clk_seq_hi_res\",\n      // 8 bits for \"clk_seq_low\",\n      // two most significant bits holds zero and one for variant DCE1.1\n      mt_rand( 0, 0x3fff ) | 0x8000,\n\n      // 48 bits for \"node\"\n      mt_rand( 0, 0xffff ), mt_rand( 0, 0xffff ), mt_rand( 0, 0xffff )\n    );\n  }\n?\u003e\n\u003c!DOCTYPE\u003e\n\u003chtml\u003e\n  \u003chead\u003e\n    \u003ctitle\u003eThis Site Has Been Blocked\u003c/title\u003e\n    \u003clink href=\"data:image/x-icon;base64,AAABAAEAEBAAAAAAAABoBQAAFgAAACgAAAAQAAAAIAAAAAEACAAAAAAAAAEAAAAAAAAAAAAAAAEAAAAAAAAAAAAAeRtNANCRtwCwXowA0Z66ANSZvQD00eUA9dbrAP/7/wCpT4QAlzttAKpShACoVYQA9N/rAKZbhACrVocApl2KAOzF3QCxW40A4q7PAHEEQAC1Z5YAjSRgAOzC3gDuwt4AjiVjAK9aiwD86PUAsl+RAH4ZUgCzX5EA0Zq5AJQzaQDHgKsA//T+AJcybwCFHlgAplSDAMiErgD41u0Aq1KGAKtVhgDcpcgAmUVyAL5woAC+c6AAeBRNAM6StABnBzkA0pS6AHwhUADDgaYAoVJ+ALdjlQDHgqkAplWHAMiHrwCnV4QAjBxcAOq92gCbQnMArFyKAKBAdgDOk7gAijNiANGTuAB6HU4A78zjAKBKeQCDGVQAtGiTAJUzawD10ekAqFCCAJg3bgCFJV0AhSdaANunxwD54/IAvXWfAN+oygDCdqIAjjBjAIIVUgCjS30Axn6oAPTQ5wCEHFUAt2aXAMiCqwDIhq4AuXaaAL9yoACOJmEAjydkAKFEewCeTHgAoEd7AJAwZAC3ZZUAlTVqANabwACnUYcAqVaEAHgLSADJj68A+N/xALh4oQDcqcYAzY61AJAmYgCgQnYAr1+NAMJ5pADjr88As2OQAIEbVACSNGgA5LbSANedvgCnT4IA99PpAJk0cQDbo8cAyoywAL96nwDDeaUA/er4AOKxzQCCGFUAtmSUANWavwD+9PsA1p2/ANaewgD20+oAxYerALdrlwCHHVsAyYWuAMiIrgD41+0At3WaAOnB2QB3EEcA3abIAIwoYQCfP3gAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAXW5vDy8gVx0AAAAAAAAAKBYfaQSEaYgkUwAAAAAAOmF1OQ59DY5gTDAuAAAAAGhFZFQrNHMDQDKQFAAAAF6BAVt4RHBcInwzPSVLAACKklI8bHGLj4A1SWJ0IwAAGUopDIlDfyoFTWMKQnoAAJM+Z0YFBwg7coOMC0dmAAA3X4ImUFaHjQY4LRpPAAAAABWFdhtIJ3kTWkE2HAAAAAAQEllOahgXezF+CQAAAAAAAGtYd22REWVRHgAAAAAAAAAAAIYsIVUCPwAAAAAAAAAAAAAAAAAAAAAAAAAAAP//AAD//wAA8A8AAOAHAADAAwAAwAMAAIABAACAAQAAgAEAAIABAACAAwAAwAMAAMAHAADgDwAA+B8AAP//AAA=\" rel=\"icon\" type=\"image/x-icon\" /\u003e\n    \u003clink href=\"data:image/x-icon;base64,AAABAAEAEBAAAAAAAABoBQAAFgAAACgAAAAQAAAAIAAAAAEACAAAAAAAAAEAAAAAAAAAAAAAAAEAAAAAAABNOB4ATzobAH5kNABSORgAUTobALl+SgBSOhsAUjwYAFA8IQBUPBgATj4nAFY8GACyilMAWD0bAIZmOgBZQR4AW0AbAEEuEwBCLxYAXUMbAFxEHgDounUAX0QeAGRIGABLNxwASjgfAPDCewBPORkATzocAJxsQwBTPBkAVDwZAFU8GQBWPBkAhGY7AMikawBaQBwA/cl+AFlCIgBFMhcAoYFVAF9IIgBfRiUASDMaAIpyTQBJNhoATjkaAE46HQBQORoAUTkaAFI5GgBPOyAAaUwuAFI8GgBkUTQAUj0dAFM9HQBVPBoAVjwaAFQ9HQCbekoAWT8aAF5HIABeRSMASjYbAEs2GwBKNx4ATDgYAHteNwBNOBgATzgYAE45GwBOOh4AUDkbAE86HgBRORsAUjsYAMuWXgD0xX0A27JyAFM9HgBUPR4AWj4YAFk/GwBaPxsAVkEhAFhCJACKZEAAVUQqAFpEIQBeQx4ARDEWAGFFGwBiRh4AYUgkANesagBNOBkATzkcAE46HwBTOxkAVDsZAFY+GQBWPxwAblQtAFVBIgD7y34AVkEiALKPXQBbQxYAV0EiAJZrFwBcRCIA6b15AEg1GgBLOBoAZU4lAEs6IABSOxoAUzsaAIBmNgBpUCsAVDsaAFM8HQBWOxoAVz4aAFY/HQBYPhoA+Mt/AFo+GgD8yH8AhGk/AFtBGgBCMBUAQzAVAEQwFQCHa0UAYkYXAEg0GABMNxgAtHtHAEk5HgBKOR4ATDgbAEs5HgBMOR4AUToYAFE7GwBSOxsAhl46AFc9GAD3yIAAWkAYAFxBGwBcQh4AXkkhAEczFgB5Vy8ARzQZAEo0GQBMNxkAfGIsAE05HwB9YDgAUDoZAFE6GQBSOhkAUzoZAFM9GQBQPiUA+Md+AFc9GQCyj1cAbVQwAOmzcABcQRwAW0QcAFpDIgBGMxcAY0YZAEczFwBeRisAZUkZAEs5IABJOyYATjkgAFA6GgBROhoAUjoaAFU9GgBWPRoAVz0aAFg9GgBZQBoA4bd0AFZAIwCXahgAQS8VAEMvFQBbRSAAYEMaAFxDIwCecBgAlmY5AHpiLgBMNhgASjgeAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAtkKQAIzNobiPjUpIYi9ICLESgqKJhpeItRfGPpopWY4rxSJ/FXhsw27JoHcCy3OfnYTErKmWZ1KVslwUwK81QXEnW4U8JXAtEZteDqtrcmFAs7OzRIFpGl9PwSMst55HN7m5YAxOKDZYqHQcJL8efTe6MS6thwEDpR9MHlMQIWZQpDGcTTMEMLqmCWM6VJgPerlJyldLkrm6uh8LeXvHWlG5RYu0pLkGk5N1vb5jgF07uYoFGTGjk6c1k3ZUVFNaOLpDHRikuZMHZGQgOYOumTu5zJQbMrkxuzoNfr2qE1p6o0Y0kTG6MQZkvHx8ZT0WVm1qCmhVwlVtJrBvP2/IKgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA=\" rel=\"icon\" type=\"image/x-icon\" /\u003e\n    \u003cstyle type=\"text/css\"\u003e\n      @font-face {\n        font-family: 'Tangerine';\n        src: local('Tangerine'), url('http://themes.googleusercontent.com/static/fonts/tangerine/v1/HGfsyCL5WASpHOFnouG-RKCWcynf_cDxXwCLxiixG1c.ttf') format('truetype');\n      }\n      body {\n        font-family: serif;\n        padding: 0;\n        margin: 0;\n        background: #FFF8DC;\n      }\n      #page {\n        border: 1px solid #CC9;\n        border-top: 0px;\n        position: relative;\n        width: 960px;\n        margin: auto;\n        background: #FFF;\n        height: 95%;\n        box-shadow: 0 0 2em #CC9;\n        overflow: hidden;\n        border-bottom-left-radius: 1em;\n        border-bottom-right-radius: 1em;\n      }\n      #main {\n        position: relative;\n        overflow: hidden;\n      }\n      #content {\n        overflow: auto;\n        padding: 1em 2em;\n        box-shadow: inset 0 0 0.5em #CC9;\n        height: 100%;\n      }\n      header {\n        background: #CFC;\n        font-family: 'Tangerine', serif;\n        font-size: 2em;\n        max-height: 80px;\n        padding-top: 0.25em;\n      }\n      header h1,h4 {\n        padding: 0;\n        margin: 0;\n        margin-left: 1.0em;\n        display: inline-block;\n        text-decoration: underline;\n      }\n      footer {\n        background: #CFC;\n        border-bottom-left-radius: 1em;\n        border-bottom-right-radius: 1em;\n        padding: 0.3em 0;\n        font-size: 0.7em;\n        position: absolute;\n        width: 100%;\n        bottom: 0px;\n        text-align: center;\n      }\n      nav p {\n        margin-bottom: 0;\n      }\n      nav {\n        background: #CFC;\n        margin: 0;\n        padding: 0 0 0 1em;\n        font-size: 0.8em;\n        width: 16%;\n        height: 100%;\n        float: left;\n        box-shadow: 0 4px 6px 0 #CC9;\n      }\n      nav ul {\n        list-style: none;\n        margin: 0;\n        padding: 0;\n      }\n      nav ul li {\n        padding-left: 1em;\n      }\n      nav ul li a {\n        color: #552;\n        text-decoration: none;\n      }\n      nav ul li a:hover {\n        text-decoration: underline;\n      }\n      table {\n        margin: auto;\n        font-size: 0.8em;\n        font-family: sans-serif;\n      }\n      th {\n        text-align: left;\n        vertical-align: top;\n        width: 100px;\n        font-weight: normal;\n      }\n    \u003c/style\u003e\n  \u003c/head\u003e\n  \u003cbody\u003e\n    \u003cdiv id='page'\u003e\n      \u003cheader\u003e\n        \u003ch1\u003eBedroomProgrammers.net\u003c/h1\u003e\n        \u003ch4\u003eIntricacy requires well rested minds\u003c/h4\u003e\n      \u003c/header\u003e\n      \u003cdiv id=\"main\"\u003e\n        \u003cnav\u003e\n          \u003cp\u003eInformative Links:\u003c/p\u003e\n          \u003cul\u003e\n            \u003cli\u003e- \u003ca href=\"#\"\u003eAcceptable Use Policy\u003c/a\u003e\u003c/li\u003e\n          \u003c/ul\u003e\n        \u003c/nav\u003e\n        \u003cdiv id=\"content\"\u003e\n          \u003csection\u003e\n            \u003ch2\u003eThis Page Has Been Blocked\u003c/h2\u003e\n            \u003cp\u003eThis could have happened because it is currently business hours and you are trying to get too a site that is in our business hours blacklist, it could have been blocked for safety reasons (for example the site is known to host malware or attempt to phish information from unsuspecting users), or it could be a mistake.\u003c/p\u003e\n            \u003cp\u003eIf this is a mistake and you feel that this site should be removed from the blacklist, please email the administrator with the information provided below. Please note how urgent the access is required as it can take up to 48 hours for a request to be processed. The administrator reserves the right to reject requests for removal if they deem the site should not be accessed or information is missing from the request.\u003c/p\u003e\n            \u003cp\u003ePlease note, that business hours are Monday-Friday 8:00am to 4:30pm. The business hours blacklist is disabled automatically during lunch time (12:00pm to 1:00pm) and outside of business hours. IT doesn't take holidays... neither does this system.\u003c/p\u003e\n          \u003c/section\u003e\n          \u003chr /\u003e\n          \u003csection\u003e\n            \u003ctable\u003e\n              \u003ctr\u003e\n                \u003cth\u003eTimestamp\u003c/th\u003e\n                \u003ctd\u003e\u003c?php echo date(DATE_W3C); ?\u003e\u003c/td\u003e\n              \u003c/tr\u003e\n              \u003ctr\u003e\n                \u003cth\u003eRequest ID\u003c/th\u003e\n                \u003ctd\u003e\u003c?php echo uuid(); ?\u003e\n              \u003c/tr\u003e\n              \u003ctr\u003e\n                \u003cth\u003eIP Address\u003c/th\u003e\n                \u003ctd\u003e\u003c?php echo $_GET['clientaddr']; ?\u003e\u003c/td\u003e\n              \u003c/tr\u003e\n              \u003ctr\u003e\n                \u003cth\u003eHostname\u003c/th\u003e\n                \u003ctd\u003e\u003c?php echo $_GET['clientname']; ?\u003e\u003c/td\u003e\n              \u003c/tr\u003e\n              \u003ctr\u003e\n                \u003cth\u003eUser\u003c/th\u003e\n                \u003ctd\u003e\u003c?php echo $_GET['clientuser']; ?\u003e\u003c/td\u003e\n                \u003c/tr\u003e\n              \u003ctr\u003e\n                \u003cth\u003eClient Group\u003c/th\u003e\n                \u003ctd\u003e\u003c?php echo $_GET['clientgroup']; ?\u003e\u003c/td\u003e\n              \u003c/tr\u003e\n              \u003ctr\u003e\n                \u003cth\u003eURL Group\u003c/th\u003e\n                \u003ctd\u003e\u003c?php echo $_GET['targetgroup']; ?\u003e\u003c/td\u003e\n              \u003c/tr\u003e\n              \u003ctr\u003e\n                \u003cth\u003eBlacklisted URL\u003c/th\u003e\n                \u003ctd\u003e\u003c?php echo $_GET['url'] . $_GET['uri']; ?\u003e\u003c/td\u003e\n              \u003c/tr\u003e\n            \u003c/table\u003e\n          \u003c/section\u003e\n        \u003c/div\u003e\n      \u003c/div\u003e\n      \u003cfooter\u003e\u0026copy 2011 Me!\u003c/footer\u003e\n    \u003c/div\u003e\n  \u003c/body\u003e\n\u003c/html\u003e\n```\n\n[1]: {{\u003c ref \"./squid.md\" \u003e}}\n[2]: http://www.shallalist.de/\n","created_at":-62135596800,"fuzzy_word_count":4e3,"path":"/notes/squid-guard/","published_at":1540945455,"reading_time":19,"tags":null,"title":"Squid Guard","type":"notes","updated_at":1540945455,"weight":0,"word_count":3994},{"cid":"4e3452d6c43f78ec2dbb8b5b831efe68cddb37c7","content":"\n***Note: This page is quite old and is likely out of date. My opinions may have\nalso changed dramatically since this was written. It is here as a reference\nuntil I get around to updating it.***\n\nSecure Shell or SSH is a network protocol that allows data to be exchanged\nusing a secure channel between two networked devices. Used primarily on\nGNU/Linux and Unix based systems to access shell accounts, SSH was designed as\na replacement for Telnet and other insecure remote shells, which send\ninformation, notably passwords, in plaintext, rendering them susceptible to\npacket analysis. The encryption used by SSH provides confidentiality and\nintegrity of data over an insecure network, such as the Internet.\n\n## Security Notes\n\nSSH is a crux service. An improperly configured SSH could be equivalent to\nletting an attacker have physical access to the box.\n\nGenerally it's recommended to run any instances of SSH exposed to the public\ninternet on an alternate port. Alternate ports won't protect you against\nvulnerabilities but it will drastrically reduce the number of automated and\nscripted attacks your server will be subjected to. If nothing else it will allow\nreal attacks to stand out more clearly in your logs.\n\n## Firewall Adjustments\n\nMy default [IPTables][1] firewall already has the following rules in place to\nallow access to SSH by default while still providing a modicum of protection\nfrom attackers.\n\n```\n# Allow SSH, but no more than 5 new connections every minute Note: This has\n# extreme repercussions if I was to use sftp as each file transfer initiates a\n# new connection. Since it's rare for me to use sftp this isn't really an\n# issue, however, when I do want to use it this rule will be the cause of\n# failed transfers. Hopefully I will save myself the diagnostic nightmare\n# scenario I went through last time\n-A INPUT -m tcp -p tcp --dport 22 -m state --state NEW -m recent --name SSH --set\n-A INPUT -m tcp -p tcp --dport 22 -m state --state NEW -m recent --update --seconds 60 --hitcount 5 --rttl --name SSH -j LOG --log-prefix \"SSH Brute Force\"\n-A INPUT -m tcp -p tcp --dport 22 -m state --state NEW -m recent --update --seconds 60 --hitcount 5 --rttl --name SSH -j DROP\n-A INPUT -m tcp -p tcp --dport 22 -m state --state NEW -j ACCEPT\n```\n\n## Configuration\n\n### /etc/ssh/sshd_config\n\nThe following is a minimal SSHd config relying mostly on the defaults. It\nrequires a group named `sshers` to be created before hand and any user that\nshould have legitimate access via SSH should be added to the group.\n\n```\nBanner /etc/issue.net\n\nClientAliveInterval 60\nTCPKeepAlive no\n\nSyslogFacility AUTHPRIV\nUseDNS no\n\nPermitRootLogin no\nPasswordAuthentication no\nUsePAM yes\n\nAllowTcpForwarding no\nAllowGroups sshers\n\n# Accept locale-related environment variables\nAcceptEnv LANG LC_CTYPE LC_NUMERIC LC_TIME LC_COLLATE LC_MONETARY LC_MESSAGES\nAcceptEnv LC_PAPER LC_NAME LC_ADDRESS LC_TELEPHONE LC_MEASUREMENT\nAcceptEnv LC_IDENTIFICATION LC_ALL LANGUAGE\n```\n\nTo create the group and add the user 'test' to it you can run the following\ncommands:\n\n```\ngroupadd sshers\nusermod -a -G sshers test\n```\n\n### /etc/issue.net\n\nThe net issue file is what gets displayed before a user logs in. While I'm\nunder no false pretense that having a policy displayed to a potential attacker\nwill prevent them from their malicious activity, it is generally recommended by\nlaw enforcement to ensure that notice of unauthorized access was given prior.\n\nAt a minimum it will show the server has been professionally configured and\nthat they are not dealing with a service that was left unconfigured.\n\n```\n************************************************************************\nThis system is privately owned. If you are not authorized to access this\nsystem, exit immediately. Unauthorized access to this system is\nforbidden by organization policies, national, and international laws.\nUnauthorized users are subject to criminal and civil penalties as well\nas organization initiated disciplinary proceedings.\n\nBy entry into this system you acknowledge that you are authorized to\naccess and have been granted the level of privileges you will\nsubsequently execute on this system. You further acknowledge that by\nentry into this system you expect no privacy from monitoring.\n************************************************************************\n```\n\n### Default SSH Client Configuration\n\n```\n# /etc/ssh/ssh_config\nHost *\n  Compression yes\n  HashKnownHosts yes\n  Protocol 2\n  SendEnv LC_ALL\n  VerifyHostKeyDNS ask\n  VisualHostKey yes\n```\n\n### SSH Authorized Keys\n\nSSH authorized keys provide a considerably stronger authentication method than\na user's password as long as the key is protected by a pass-phrase. If the key\nis left without a pass-phrase, anyone who manages to get access to the system\ncan immediately login as that user anywhere they have deployed the key. Ensure\nthere is a strong pass-phrase on the key.\n\nThe keys are normally created as `~/.ssh/id_rsa` and `~/.ssh/id_rsa.pub`,\nhowever I like to physically separate my keys from my computers so I'll give my\nkeys a descriptive name of `username@host.key` and put them on a pendrive\nlocated at `/media/pendrive`. Personally I to use keys with 4096 bits. The\nfollowing command will create the public / private keys:\n\n```\n[user@localhost ~]$ ssh-keygen -t rsa -b 4096 -f /media/pendrive/username@host.key\nGenerating public/private rsa key pair.\nEnter passphrase (empty for no passphrase):\nEnter same passphrase again:\nYour identification has been saved in /media/pendrive/username@host.key.\nYour public key has been saved in /media/pendrive/username@host.key.pub.\nThe key fingerprint is:\nxx:xx:xx:xx:xx:xx:xx:xx:xx:xx:xx:xx:xx:xx:xx:xx user@localhost\n```\n\nNow that we have a strong key we just need to place it on the servers we want\nto access. Conveniently enough there is a utility that makes this quick and\nsimple, if for some reason you don't trust this utility or want to do it by\nhand just copy the contents of username@host.key.pub into\n`~/.ssh/authorized_keys` on the remote host and make sure the file's\npermissions are set to 0400.\n\nYou can use the utility by performing the following command, replacing\n`remoteuser@remotehost` with a valid username and hostname for the remote host.\n\n```\n[user@localhost ~]$ ssh-copy-id -i /media/pendrive/username@host.key.pub remoteuser@remotehost\nremoteuser@remotehost's password: \nNow try logging into the machine, with \"ssh 'remoteuser@remotehost'\", and check in:\n\n  .ssh/authorized_keys\n\nto make sure we haven't added extra keys that you weren't expecting.\n\n[user@localhost ~]$ ssh -i /media/pendrive/username@host.key remoteuser@remotehost\nEnter passphrase for key '/media/pendrive/username@remotehost.key':\n[remoteuser@remotehost ~]$\n```\n\nOnce your sure that the key based login is working you can safely disable\npassword based logins through ssh to the system entirely by changing the\nfollowing line in `/etc/ssh/sshd_config`:\n\n```\nPasswordAuthentication yes\n---- Replace with ----\nPasswordAuthentication no\n```\n\nThis replaces the 'something you know' of a password based login, with\n'something you have' (the key, especially if it's on a pendrive) and 'something\nyou know' (the key's pass-phrase).\n\n## User Jails\n\nJails can isolate users from sensitive parts of the system, creating a 'fake'\nenvironment for them to execute their programs. In the event that a user's\naccount becomes compromised, the jail can limit what an attacker has access too\nthough it should be considered a delay rather than a security measure since a\ndetermined hacker might be able to break out of an otherwise secure jail.\n\nTo start off we're going to need to setup an environment for our jail. The\nfollowing commands create the base directories and the /dev/null device within\nthe jail.\n\n```\nmkdir -p /var/jail/{dev,etc,lib,lib64,usr/bin,bin,home}\nchown -R root:root /var/jail/\nmknod -m 666 /var/jail/dev/null c 1 3\n```\n\nThere is something serious to note here. The [partitioning][2] security\nguidelines add two flags to the mount options of the /var partition.  You will\nneed to remove the 'nosuid', and 'noexec' options from this partition to be\nable use a chroot jail.\n\nNext we need to copy a few minimum files into the jail's etc directory. Hard\nlinks may work better for these as they will get properly updated with the rest\nof the system, however, I am unsure whether or not that would actually work.\n\n```\ncp /etc/ld.so.cache /etc/ld.so.conf /etc/nsswitch.conf /etc/hosts /var/jail/etc/\n```\n\nAt this point you need to decide what commands you want you're user to have\naccess too and copy the appropriate binaries into place. You can use the\n`which` command to locate a binary and then copy it into the same directory\nwithin the jail. The following is an example for moving `bash` over.\n\n```\n[root@localhost]# which bash\n/bin/bash\n[root@localhost]# cp /bin/bash /var/jail/bin/\n```\n\nThe trickiest part of setting up a jail is the required libraries for the\nallowed executables. Conveniently, all the Linux distributions that I have\ntried come with a tool too determine what libraries are required. `ldd`. Here's\na little snippet that will give you a run down of the counts for the number of\ntimes each library is used by a system:\n\n```\nfind /bin -type f -perm /a+x -exec ldd {} \\; \\\n| grep so \\\n| sed -e '/^[^\\t]/ d' \\\n| sed -e 's/\\t//' \\\n| sed -e 's/.*=..//' \\\n| sed -e 's/ (0.*)//' \\\n| sort \\\n| uniq -c \\\n| sort -n\n```\n\nIf for some reason ldd isn't reporting all of the libraries, while the app is\nrunning you can use this command to track down what it has actually loaded:\n\n```\nlsof -P -T -p Application_PID\n```\n\nA few references:\n\n* http://www.fuschlberger.net/programs/ssh-scp-sftp-chroot-jail/\n* http://allanfeid.com/content/creating-chroot-jail-ssh-access\n\n## Multiplex-Sessions \u0026 Signed PubKeys\n\n* http://comments.gmane.org/gmane.network.openssh.general/8269\n\n## Published SSH Host Keys\n\n* https://www.rfc-editor.org/rfc/rfc4255.txt\n* http://www.dnorth.net/2007/12/16/sshfp-howto/\n\n## LXC Specific\n\nIf you want to run sshd in a LXC container you will need to disable\n`pam_loginuid.so` in `/etc/pam.d/sshd` by commenting out the appropriate line.\n\n## Misc\n\nI've play around with implementing a [gatekeeper style script][3] to provide an\nadditional layer of security. In practice real multi-factor authentication is\nmore reliable and should be used instead.\n\n[1]: {{\u003c ref \"./iptables.md\" \u003e}}\n[2]: {{\u003c ref \"./partitioning.md\" \u003e}}\n[3]: {{\u003c ref \"./sshd_gatekeeper.md\" \u003e}}\n","created_at":-62135596800,"fuzzy_word_count":1600,"path":"/notes/sshd/","published_at":1540945455,"reading_time":8,"tags":null,"title":"SSHd","type":"notes","updated_at":1540945455,"weight":0,"word_count":1515},{"cid":"4cd787af8731e4b87db08c269cfa9f4d4d947dc3","content":"\n***Note: This page is quite old and is likely out of date. My opinions may have\nalso changed dramatically since this was written. It is here as a reference\nuntil I get around to updating it.***\n\n## Installation\n\n```\nyum install suricata -y\n```\n\n## Configuration\n\n### Network Configuration for IPS\n\nI purchased a dedicated two NIC PCIe card to make use of Suricata in IPS mode.\nWhen connected these interfaces were reported to me as devices p1p1 and p2p1. I\nwant the IPS to show up transparently, at no point should the IPS be\naddressable on it's sniffing interfaces. The bridge interface that will handle\nthe filtering, sniffing etc will be br256. Here are the various configuration\nfiles:\n\n#### /etc/sysconfig/network-scripts/ifcfg-p1p1\n\n```\nDEVICE=\"p1p1\"\nNM_CONTROLLED=\"no\"\nHWADDR=\"00:0A:CD:20:86:EC\"\nONBOOT=\"yes\"\nBOOTPROTO=\"none\"\n\nBRIDGE=br256\n\nIPV6INIT=no\nIPV6_AUTOCONF=no\n\nNAME=\"Transparent TAP\"\n```\n\n#### /etc/sysconfig/network-scripts/ifcfg-p2p1\n\n```\nDEVICE=\"p2p1\"\nNM_CONTROLLED=\"no\"\nHWADDR=\"00:0A:CD:20:86:ED\"\nONBOOT=\"yes\"\nBOOTPROTO=\"none\"\n\nBRIDGE=br256\n\nIPV6INIT=no\nIPV6_AUTOCONF=no\n\nNAME=\"Transparent TAP\"\n```\n\n#### /etc/sysconfig/network-scripts/ifcfg-br256\n\n```\n# br256 - No VLAN - Transparent Intercept Bridge\nDEVICE=br256\nNM_CONTROLLED=no\nONBOOT=yes\nTYPE=Bridge\n\nSTP=off\nDELAY=0\n\nBOOTPROTO=none\n\nIPV6INIT=no\nIPV6_AUTOCONF=no\n\nNAME=\"Transparent Intercept Bridge\"\n```\n\n### Sysctl Configuration Options\n\nThe following changes will need to be made to sysctl settings to allow\nfiltering of traffic across bridges:\n\n```\n# Ensure all traffic is forwarded where appropriate\nnet.ipv4.ip_forward = 1\nnet.ipv6.conf.default.forwarding = 1\nnet.ipv6.conf.all.forwarding = 1\n\n# Enable netfilter on bridges.\nnet.bridge.bridge-nf-call-ip6tables = 1\nnet.bridge.bridge-nf-call-iptables = 1\nnet.bridge.bridge-nf-call-arptables = 0\nnet.bridge.bridge-nf-filter-pppoe-tagged = 0\nnet.bridge.bridge-nf-filter-vlan-tagged = 0\nnet.bridge.bridge-nf-pass-vlan-input-dev = 1\n```\n\n### Initial Firewall Rules\n\nInitially we want to allow all the traffic across the bridges.\n\nFirst iptables:\n\n```\n# Traffic across the transparent bridge\n-A FORWARD -i br256 -o br256 -j ACCEPT\n```\n\nAnd then ip6tables:\n\n```\n# Traffic across the transparent bridge\n-A FORWARD -i br256 -o br256 -j ACCEPT\n```\n\nAfter restarting the network, applying the sysctl file, connecting the card's\nports and bringing up the interfaces you can start watching traffic with\ntshark:\n\n```\ntshark -i br256\n```\n","created_at":-62135596800,"fuzzy_word_count":300,"path":"/notes/suricata/","published_at":1507587794,"reading_time":2,"tags":null,"title":"Suricata","type":"notes","updated_at":1507587794,"weight":0,"word_count":278},{"cid":"c3b87abde14d045792812c3f092d6d06ab9a41fa","content":"\n***Note: This page is quite old and is likely out of date. My opinions may have\nalso changed dramatically since this was written. It is here as a reference\nuntil I get around to updating it.***\n\n## Security\n\nIf at any point a decryption key is needed to access a file, it will be stored\nin RAM. If an otherwise encrypted file is edited it will get stored in RAM\nbefore being saved. Without warning at any time that RAM can be swapped out to\ndisk, at which point there is now a recoverable copy of an encryption key or\nsections of an encrypted document plain text on disk that [can be\nrecovered][1].\n\nTo prevent this I strongly recommend encrypting the swap. During a standard\ninstall of Fedora if you want to accomplish this you have to use dirty\nVolGroups and LVMs. I hate VolGroups and LVMs and on top of that it will be\nusing a set static key that I'm sure will eventually be open to static\nanalysis. While most people won't care about the latter I like to avoid the\nwhole thing if I can do it with a fast simple solution.\n\nMy solution is to take swap out of the hands of fstab and mount it myself after\ngenerating a one-time key for that session. When the computer turns off the key\nwill be completely lost and a new one will be generated the next time the\ncomputer boots. I don't want to do this by hand every time the computer boots\nso I'll make sure it does this automatically.\n\nThe first step is to prevent Linux from mounting the swap on it's own. To do\nthis comment out all the lines in fstab that have to do with swap. They will\nlook like the following:\n\n```\n/dev/sda3       swap                swap        defaults        0 0\n```\n\nYour going to need to know what partitions your swap lives on. In the above\nexample swap lives on `/dev/sda3`, it is very likely that your's will live on a\ndifferent device/partition.\n\nFirst we're going to want to fill the swap space with junk to make sure that\nthere isn't anything useful left on there to be recovered. Your going to want\nto make sure that your system isn't currently using the swap, so first turn it\noff before writing over the swap space. The latter command is very dangerous if\nyou do it on the wrong partition! Double check before executing it!\n\n```\n[root@localhost ~]# swapoff -a\n[root@localhost ~]# dd if=/dev/urandom of=/dev/sda3\n```\n\nNext we'll want to make one of the boot scripts initialize encryption and mount\nthe swap, there is a specific boot script created explicitly for user\nmodification that other packages won't touch. We're going to use that one which\nis `/etc/rc.d/rc.local`. You'll want to add the following lines to it.\n\n```\ncryptsetup -c blowfish -h sha256 -d /dev/urandom create swap /dev/sda3\nmkswap -f /dev/mapper/swap\nswapon /dev/mapper/swap\n```\n\nThe cipher in the above command can be swapped around, blowfish is a strong\nencryption with a fairly good throughput however it seems that 256bit\naes-xts-plain is a bit faster without sacrificing too much security. If you'd\nprefer to use that instead, replace the cryptsetup invocation with the\nfollowing one:\n\n```\ncryptsetup -c aes-xts-plain -s 256 -h sha256 -d /dev/urandom create swap /dev/sda3\n```\n\nFedora has device mapper support built into the kernel, however if your\ndistribution doesn't have device mapper support active by default, in the line\nright before the cryptsetup invocation add the line \"modprobe dm-mod\" to load\nthe kernel module. Otherwise you'll get a \"Command failed: Incompatible\nlibdevmapper\" error.\n\n[1]: {{\u003c ref \"./data_recovery.md\" \u003e}}\n","created_at":-62135596800,"fuzzy_word_count":600,"path":"/notes/swap/","published_at":1540945455,"reading_time":3,"tags":null,"title":"Swap","type":"notes","updated_at":1540945455,"weight":0,"word_count":587},{"cid":"7a667d363ac83e30aa7588a07b5211b7f8bb6294","content":"\nThis is relevant for TAILS version 2.5 and basically follows their install\nguide.\n\nI needed 2 USB sticks each at least 4Gb in size, and a spare laptop to take\nthese notes on and read the instructions.\n\nI downloaded a copy of the Tails GPG key from several machines located on\nwildly disparate internet connections like so:\n\n```sh\ncurl -s https://tails.boum.org/tails-signing.key -o tails-signing-local.key\nfor i in ircp io proc; do\n  ssh $i 'curl -s https://tails.boum.org/tails-signing.key' \u003e tails-signing-$i.key\ndone\n```\n\nI pulled them all on to the machine I'm installing from and ensured all their\ncontents matched:\n\n```sh\ndiff -qs --from-file tails-signing-*.key\n```\n\nWith the key validated I can relatively safely import it (the Key ID was this:\n0xDBB802B258ACD84F):\n\n```sh\ngpg --import tails-signing-local.key\ngpg --refresh-keys\n```\n\nNow I need the ISO, the safest method is via torrent.\n\n```sh\ncurl -s https://tails.boum.org/torrents/files/tails-i386-2.5.torrent -o tails-i386-2.5.torrent\n```\n\nAfter downloading the torrent file's content, it's signature needs to be\nverified then installed directly to the first USB stick (/dev/sdb) like so:\n\n```sh\ngpg --keyid-format 0xlong --verify tails-i386-2.5.iso.sig tails-i386-2.5.iso\ndd if=tails-i386-2.2.1.iso of=/dev/sdb bs=1M oflag=sync\n```\n\nWith this setup, we need to boot into the TAILS usb drive. Start it up and\nchoose the 'Live' boot menu option.\n\nWait for the tails greeter appears and click Login.\n\nWhen the desktop appears plug in the second USB stick. Open up 'Applications'\n-\u003e 'Tails' -\u003e 'Tails Installer'. Choose 'Install by cloning'. Choose the other\nUSB drive and begin the installation. Boot into the second TAILS usb drive.\n\nWe want to create an encrypted persistent storage as well once booted back in.\nChoose 'Applications' \u003e 'Tails' \u003e 'Configure persistent volume'. Specify a long\nstrong passphrase and click 'Create'. Active 'Personal Data', consider the\nother options but they're not officially recommended.\n\nRestart into the USB drive again, but activate the 'Use persistence?' flag and\nenter your passphrase before clicking on 'Login'. Anything stored in the Places\n-\u003e Persistent location will be saved between environments.\n","created_at":-62135596800,"fuzzy_word_count":400,"path":"/notes/tails-setup-notes/","published_at":1508540507,"reading_time":2,"tags":["linux","security"],"title":"Tails Setup Notes","type":"notes","updated_at":1508540507,"weight":0,"word_count":323},{"cid":"bb433d99f57a1b264f270f04bd6e10fe1f9a270d","content":"\n***Note: This page is quite old and is likely out of date. My opinions may have\nalso changed dramatically since this was written. It is here as a reference\nuntil I get around to updating it.***\n\n## Installation\n\n```\nyum install tmux -y\n```\n\n## Shared Session Configuration\n\nThe idea behind this was to allow easy pair programming in a fairly secure way\nusing a separate unprivileged account to host the session. I've used a\ncombination of tmux, some bash scripts, and sudo to accomplish this\nenvironment. Please note that I don't add the shared user to the 'sshers' group\nso there is only local access to the account.\n\n### User Setup\n\nCreate a new user, I used \"shared\" for the user name as shown in the example\nbelow. This will also create the \"shared\" group.\n\n```\nuseradd shared\npassword shared\n```\n\nSet some rediculous password for the user as no one will be logging directly\nin.\n\nAdd any users that you want to participate in the shared environment to the\nshared group like so:\n\n```\nusermod -a -G shared \u003cusername\u003e\n```\n\nAdd this line to the `/etc/sudoers` to allow any user added to the shared group\nto be able to switch to it without a password:\n\n```\n%shared   ALL=(ALL) NOPASSWD: /bin/su shared -\n```\n\n### Shared Session Setup\n\nSwitch to the shared user's account from a user that is a member of the shared\ngroup and create a .bin directory in their home.\n\n```\nmkdir /home/shared/.bin\n```\n\nInside of that create a file \"shared-tmux\" with the following contents and make\nit executable. This is a simplified version of a more flexible shared tmux\nsession script I found on the [Arch Linux Wiki][1] which has a ridiculous\namount of very good information that I strongly recommend people reference with\nany questions.\n\n```\n#!/bin/bash\n\nbase_session=\"shared-session\"\ntmux_session_count=`tmux ls | grep \"^$base_session\" | wc -l`\n\nif [[ \"$tmux_session_count\" == \"0\" ]]; then\n  echo \"Launching tmux base session $base_session ...\"\n  tmux new-session -s $base_session\nelse\n  # Make sure we are not already in a tmux session\n  if [[ -z \"$TMUX\" ]]; then\n    # Kill defunct sessions first\n    old_sessions=$(tmux ls 2\u003e/dev/null | egrep \"^[0-9]{14}.*[0-9]+\\)$\" | cut -f 1 -d:)\n    for old_session_id in $old_sessions; do\n        tmux kill-session -t $old_session_id\n    done\n\n    echo \"Launching copy of base session $base_session ...\"\n\n    # Session is is date and time to prevent conflict\n    session_id=`date +%Y%m%d%H%M%S`\n\n    # Create a new session (without attaching it) and link to base session \n    # to share windows\n    tmux new-session -d -t $base_session -s $session_id\n    \n    # Attach to the new session\n    tmux attach-session -t $session_id\n\n    # When we detach from it, kill the session\n    tmux kill-session -t $session_id\n  fi\nfi\n```\n\nI added the following to the shared user's .bashrc file so the shared session\npops open automatically as soon as a user has opened a shell and it will drop\nthem out of the user's account as soon as they leave tmux.\n\n```\nif [[ -z \"$TMUX\" ]]; then\n  export PS1=\"\\W\\$(__git_ps1)\\$ \"\n  shared-session\n  # The following is needed because funny shit happens when the server drops\n  reset\n  exit\nelse\n  export PS1=\"[\\u@\\h \\W]\\$(__git_ps1)\\$ \"\nfi\n```\n\nI also created a small shell script to make it easier for user's to make use of\nthe shared session in `/bin/shared`. The contents are below, make sure to make\nit executable:\n\n```\n#!/bin/bash\n\nsudo su shared -\nclear\necho \"Left shared environment\"\n```\n\n#### VIM \u0026 Tmux\n\nI made a few compromises on my vim and tmux configuration to work with a\nco-worker and tried to keep it simple at the same time, so I'm including their\nconfiguration.\n\nHere is the tmux configuration `/home/shared/.tmux.conf`, there are two\nconfiguration items that are relevant to the shared session configuration and\nshould be included even if you don't use any of the rest of it. The two\nrelevant configuration lines are the \"setw -g aggressive-resize on\" and the\n\"set-option destroy-unattached on\".\n\n```\nset-option -g prefix C-a\nset-option destroy-unattached on\nbind r source-file ~/.tmux.conf\nbind C-a last-window\nbind a send-prefix\nset-window-option -g mode-keys vi\nbind-key k select-pane -U\nbind-key j select-pane -D\nbind-key h select-pane -L\nbind-key l select-pane -R\nsetw -g aggressive-resize on\nbind \\ split-window -h\nbind - split-window -v\nset -g base-index 1\nset -g visual-activity on\nset -g monitor-activity on\nset -g history-limit 100000\nset -g status-bg black\nset -g status-fg green\nset -g status-interval 30\nset -g status-left-length 40\nset -g status-left '#[fg=green](#S) #(whoami)@#H#[default]'\nset -g status-right '#[fg=yellow]#(cut -d \" \" -f 1-3 /proc/loadavg)#[default] #[fg=cyan]%H:%M#[default]'\n```\n\nHere is the vim configuration I use /home/shared/vimrc, nothing specifically\nrelated to the shared session in this one:\n\n```\nset history=500\nset nocompatible\nautocmd! bufwritepost vimrc source ~/.vimrc\nset modelines=10\nset autoread\nset scrolloff=5\nset wildmenu\nset ruler\nset cmdheight=2\nset nostartofline\nset confirm\nset pastetoggle=\u003cF11\u003e\nnnoremap \u003cC-L\u003e :nohl\u003cCR\u003e\u003cC-L\u003e\nset shiftwidth=2 tabstop=2\nset expandtab\nset ai\nset smarttab\nset ignorecase\nset hlsearch\nset magic\nset showmatch\nset noerrorbells\nset visualbell\nset number\nset hidden\nsyntax on\nset ai\nset encoding=utf8\ntry\n  lang en_US\ncatch\nendtry\nset ffs=unix,dos,mac\nset lbr\nset laststatus=2\n```\n\n[1]: https://wiki.archlinux.org/index.php/Tmux#Clients_simultaneously_interacting_with_various_windows_of_a_session\n","created_at":-62135596800,"fuzzy_word_count":900,"path":"/notes/tmux/","published_at":1507587428,"reading_time":4,"tags":null,"title":"Tmux","type":"notes","updated_at":1507587428,"weight":0,"word_count":800},{"cid":"e3182b08bd64c7d7641da00bc2a335dc2aa887e6","content":"\n***Note: This page is quite old and is likely out of date. My opinions may have\nalso changed dramatically since this was written. It is here as a reference\nuntil I get around to updating it.***\n\n## Configuration\n\nFirst install the transmission-daemon like so:\n\n```\n[root@localhost ~]# yum install transmission-daemon -y\n```\n\nThe init script that comes with transmission-daemon uses the utility `which`\nwhich is not in the minimal install. As such you'll need to edit the init file\nto point at the binary as it should be doing in the first place. Find the\nfollowing line:\n\n```\nDAEMON=$(which $NAME)\n```\n\nReplace it with:\n\n```\nDAEMON=/usr/bin/transmission-daemon\n```\n\nOpen up `/etc/sysconfig/transmission-daemon`. We're going to make a few changes\nhere. First off the defaults listed in this file are not actually the defaults\nthat the transmission-daemon will start up with. I don't like ambiguity so\nwe're going to replace them with the real values.\n\nThe tranmission-daemon runs as user \"tranmission\" and it's home directory is\n`/var/lib/transmission`. We want to prefer encryption whenever it is available.\n\nI don't like defining configuration information through the service startup\nunless they don't have an equivalent setting (encryption preferred - you can\nturn encryption support on in the config but not the 'prefer' part) so we get\nrid of the blacklist setting.\n\nWith those changes the file should look like the following:\n\n```\nTRANSMISSION_HOME=/var/lib/transmission\nDAEMON_USER=\"transmission\"\nDAEMON_ARGS=\"-ep -g $TRANSMISSION_HOME/.config/transmission-daemon\"\n```\n\nThe install doesn't set up a default configuration, so we need to quickly start\nand stop the daemon so one is generated for us to edit like so:\n\n```\n[root@localhost ~]# service transmission-daemon start\nStarting transmission-daemon (via systemctl):              [  OK  ]\n[root@localhost ~]# service transmission-daemon stop\nStopping transmission-daemon (via systemctl):              [  OK  ]\n```\n\nAlright now that we have a config lets open it up and make some changes, it can\nbe found at `/var/lib/transmission/.config/transmission-daemon/settings.json`.\n\nProvided below is my configuration file AFTER I've made changes to the\ndefaults.\n\nNote to self: This isn't cleaned up and is not fit for public consumption. The\nonly change that has been made is the removal of the contents of rpc-password\nhash.\n\nAfter entering a password into the rpc-password field, start and stop the\nservice again to have it in hashed form. Also note that I've picked a random\nbut static port in this case 37288. Make sure that it is allowed through the\nfirewall and forwarded.\n\n```\n{\n  \"alt-speed-down\": 200, \n  \"alt-speed-enabled\": false, \n  \"alt-speed-time-begin\": 540, \n  \"alt-speed-time-day\": 127, \n  \"alt-speed-time-enabled\": false, \n  \"alt-speed-time-end\": 1020, \n  \"alt-speed-up\": 200, \n  \"bind-address-ipv4\": \"0.0.0.0\",\n  \"bind-address-ipv6\": \"::\", \n  \"blocklist-enabled\": true, \n  \"blocklist-url\": \"http://list.iblocklist.com/?list=bt_level1\u0026fileformat=p2p\u0026archiveformat=gz\", \n  \"cache-size-mb\": 10, \n  \"dht-enabled\": true, \n  \"download-dir\": \"/media/storage/Torrents/Downloads/\", \n  \"encryption\": 1, \n  \"idle-seeding-limit\": 30, \n  \"idle-seeding-limit-enabled\": false, \n  \"incomplete-dir\": \"/media/storage/Torrents/Incomplete\", \n  \"incomplete-dir-enabled\": true, \n  \"lazy-bitfield-enabled\": true, \n  \"lpd-enabled\": false, \n  \"message-level\": 2, \n  \"open-file-limit\": 32, \n  \"peer-congestion-algorithm\": \"\", \n  \"peer-limit-global\": 240, \n  \"peer-limit-per-torrent\": 60, \n  \"peer-port\": 37288, \n  \"peer-port-random-high\": 65535, \n  \"peer-port-random-low\": 49152, \n  \"peer-port-random-on-start\": false, \n  \"peer-socket-tos\": \"default\", \n  \"pex-enabled\": true, \n  \"port-forwarding-enabled\": true, \n  \"preallocation\": 0, \n  \"prefetch-enabled\": 1, \n  \"ratio-limit\": 15, \n  \"ratio-limit-enabled\": false, \n  \"rename-partial-files\": true, \n  \"rpc-authentication-required\": true, \n  \"rpc-bind-address\": \"10.13.37.52\", \n  \"rpc-enabled\": true, \n  \"rpc-password\": \"\", \n  \"rpc-port\": 9091, \n  \"rpc-url\": \"/transmission/\", \n  \"rpc-username\": \"torrentadmin\", \n  \"rpc-whitelist\": \"127.0.0.1,10.13.37.*\", \n  \"rpc-whitelist-enabled\": true, \n  \"script-torrent-done-enabled\": true,\n  \"script-torrent-done-filename\": \"/var/lib/transmission/.config/transmission-daemon/torrent-completed.sh\",\n  \"speed-limit-down\": 100, \n  \"speed-limit-down-enabled\": false, \n  \"speed-limit-up\": 100, \n  \"speed-limit-up-enabled\": false, \n  \"start-added-torrents\": true, \n  \"trash-original-torrent-files\": true, \n  \"umask\": 18, \n  \"upload-slots-per-torrent\": 14,\n  \"watch-dir\": \"/media/storage/Dropbox/TorrentDrop\",\n  \"watch-dir-enabled\": true\n}\n```\n\nWith the config above I've configured it too run a script\n`/var/lib/transmission/.config/transmission-daemon/torrent-completed.sh`\nwhenever a torrent finishes downloading. This file needs to at least exist and\nbe executable if you don't turn it off above. You can do this with the\nfollowing commands:\n\n```\n[root@localhost ~]# touch /var/lib/transmission/.config/transmission-daemon/torrent-completed.sh\n[root@localhost ~]# chmod +x /var/lib/transmission/.config/transmission-daemon/torrent-completed.sh\n```\n\nI've personally set it up to email me whenever a torrent completes by putting\nthe following script in that file:\n\n```\n#!/bin/bash\n\n# Transmission populates the following variables for use in the script:\n# TR_APP_VERSION\n# TR_TIME_LOCALTIME\n# TR_TORRENT_DIR\n# TR_TORRENT_HASH\n# TR_TORRENT_ID\n# TR_TORRENT_NAME\n\n# Setup the message to be sent\nTO_ADDR=admin@example.com\nSUBJECT=\"Torrent Completed\"\nFROM_ADDR=\"transmission@example.com\"\nBODY=\"Transmission finished downloading \\\"$TR_TORRENT_NAME\\\" on $TR_TIME_LOCALTIME\"\n\n# Send the email\necho $BODY | mailx -s \"$SUBJECT\" -r \"$FROM_ADDR\" $TO_ADDR\n```\n\nAlright now this part is strange. If for some reason transmission doesn't close\nproperly or some other funniness happens, it DELETES it's config file and\nresets it back to the defaults. To prevent this we're going to remove\ntranmissions ability to change it's config file. The catch with this is that it\nstarts up with root privileges and will just rewrite the config file with\nread/write permissions so you need to make the file immutable like so:\n\n```\n[root@localhost ~]# chattr +i /var/lib/transmission/.config/transmission-daemon/settings.json\n```\n\nEnsure that transmission's group has write privileges over it's Download,\nIncomplete, and TorrentDrop directory.\n\nNow lets start it up for real and make sure it starts up correctly on boot:\n\n```\n[root@localhost ~]# chkconfig transmission-daemon on\n[root@localhost ~]# service transmission-daemon start\n```\n\nYou can then use the utility transmission-remote to check on the status of\ntransmission like so:\n\n```\n[root@localhost ~]# transmission-remote transmission-server.example.com -l\nID     Done       Have  ETA           Up    Down  Ratio  Status       Name\nSum:           Unknown               0.0     0.0\n```\n\n## Making Torrents from the Command Line\n\nFirst you need to install the utility \"mktorrent\" like so:\n\n```\n[root@localhost ~]# yum install mktorrent -y\n```\n\nI use a private tracker so these instructions are going to be specific for\nthem.\n\nFirst things first we're going to figure out the best size for the pieces in\nthe torrent. Generally I default to 512Kb (2^19), if the torrent is around the\nsize of a DVD I'll increase the size of the pieces to 1Mb (2^20). Anything\nlarger than that probably doesn't belong in a single torrent. We need to know\nthis as the exponent is one of the flags passed (specifically -l) so for my\ndefault I'll use the flag \"-l 19\".\n\nSince I use a private tracker I need to set the private flag (-p). You should\nreally give the torrent a meaningful name with the \"-n\" flag. Additionally a\ncomment on the torrent is usually welcome with \"-c\".\n\nWith my current private tracker when creating a torrent you get a unique\ntracker link that looks something like `http://tracker.\u003csite\u003e.org:34000/\u003cYour\nprivate 32 character ID string\u003e/announce`, so you need to get that before you\ncan finish setting up the torrent. Pass it as an option to the \"-a\" flag\n\nMake sure all the files you want in the torrent are in the same directory\n(we're going to use `/media/example/MySpecialTorrent/`) in this example:\n\n```\n[root@localhost ~]# mktorrent -l 19 -p -n \"My Special Torrent\" -c \"A very special torrent that I'm using as an example\" \\\n\u003e -a \"\u003ctracker address\u003e\" \\\n\u003e /media/example/MySpecialTorrent/*\nmktorrent 1.0 (c) 2007, 2009 Emil Renner Berthing\n\nHashed 40 of 40 pieces.\nWriting metainfo file... done.\n```\n\nYou will then have a .torrent file in your current directory named after what\nyou gave mktorrent with the -n flag.\n\n## Security Notes\n\n* The transmission-daemon doesn't support multiple users, so either a shared\n  password needs to be provided, none configured or a reverse proxy needs to be\n  setup in front of it that has basic authentication.\n  * A lot of the clients can support basic authentication by using the format:\n    `http://username:password@torrentbox.local/` for the \"host\".\n  * A reverse proxy can also be used to provide HTTPS access to the interface\n  * A reverse proxy can safely make the interface available to the outside\n    world by putting restrictions on it\n* The transmission daemon can restrict access to certain things based on IP\n  masks, these should be configured appropriately\n* Blocklists can be used to eliminate potential corporation spies that'll do\n  bad things to you\n* I strongly encourage preferring or enforcing encryption as it will reduce\n  what is visible to ISPs\n* All of the torrent traffic could potentially be pushed through an anonymising\n  VPN such as that provided by xerobank.com\n\n## Firewall Adjustments\n\n```\n# Allow other torrenter's to connect to us\n-A SERVICE -m tcp -p tcp --dport 37288 -j ACCEPT\n# Allow local access to the web interface\n-A SERVICE -s 10.13.37.0/24 -m tcp -p tcp --dport 9091 -j ACCEPT\n```\n","created_at":-62135596800,"fuzzy_word_count":1300,"path":"/notes/transmission-daemon/","published_at":1507587428,"reading_time":6,"tags":null,"title":"Transmission Daemon","type":"notes","updated_at":1507587428,"weight":0,"word_count":1254},{"cid":"d49b9776c4a1b08a7e56c7dffb609b105a79655d","content":"\n***Note: This page is quite old and is likely out of date. My opinions may have\nalso changed dramatically since this was written. It is here as a reference\nuntil I get around to updating it.***\n\n## Configuring\n\nThis page is referring to TRIM support for SSDs. To enable it add\n`noatime,discard` to the options on all of the fstab partition entries that\nlive on the drive. Be sure to disable any swap partitions that live on that\ndrive as well.\n\n### Note on Encrypted Drives\n\nTurning TRIM support on for an encrypted hard drive will not actually do\nanything. `discard` is a file system level option and full disk encryption\nlives below that. It makes sense that it doesn't work, writing a sector full of\n0s to an encrypted partition will result in a sector of encrypted 0s which will\nlook more or less like random garbage.\n\nThere is a trick to allow this to work but I'm not will to go that route due to\nthe security implications of allowing attackers to see where the data lives on\nmy hard drive and where it doesn't, but for completeness, I'm documenting\n(untested) what needs to be done to enable this.\n\nYou'll need to replace \u003cyour_device\u003e with the DM mapper crypted partition such\nas `/dev/mapper/vg_desktop-lv_root`. You'll want to do this for all of the\nencrypted partitions on the drive.\n\n```\ndmsetup table \u003cyour_device\u003e --showkeys\ndmsetup load \u003cyour_device\u003e --\"\u003cline above\u003e 1 allow_discards\"\ndmsetup resume \u003cyour_device\u003e\n```\n\nEven without TRIM support noatime and no swap partitions are still good\nperformance improvements.\n\n## Testing\n\nThis requires you install the `hdparm` package on Fedora.\n\nTo test and make sure that trim support is working, make sure you're in a\ndirectory that lives on a partition on the SSD, then create a test file like\nso:\n\n```\nseq 1 1000 \u003e trimtest\nsync\nhdparm --fibmap trimtest\n```\n\nYou will get output like this:\n\n```\ntrimtest:\n filesystem blocksize 4096, begins at LBA 0; assuming 512 byte sectors.\n byte_offset  begin_LBA    end_LBA    sectors\n           0   25565616   25565623          8\n```\n\nThe important thing to take into account is the `begin_LBA` column make note of\nthis number and use it in place in the following commands. You'll need to\nreplace `/dev/sda` with the device name matching your SSD.\n\n```\nhdparm --read-sector 25565616 /dev/sda\n```\n\nYou'll receive output that looks a lot like the following:\n\n```\n/dev/sda:\nreading sector 25565616: succeeded\nbb2d 06dc 87c3 6bb6 c43c fab7 a0a9 12db\n9e35 2246 4ae3 9eaf 373e 3ce3 ea75 2657\n4bb9 e299 c9cf eee9 c74b bb86 9b25 b36c\n5214 bf13 7f2b 0cf3 9866 6b7e 0344 8f33\n4ea4 0cdc 6ad3 f3e2 8a72 4032 b0d5 7c8c\n2002 82cc d631 6f80 0967 e698 5e7c b9c3\nc660 9953 ed1e 832f 29a2 7c46 5fab f7e0\n8705 6c74 10d2 3649 6b4c 136e e3ba b36f\n21dd f96e 9d7c 7c9e 8596 0c0b 67df 76fe\n6797 608c 0f42 145e b381 4efe 754f bd59\n8575 d75e 95b0 7281 f454 c364 43ab 11dd\n79a4 e3b2 1b11 67fa ec04 5d73 6ed9 d77b\nbebe 3df2 086c 26e3 ac8e c6b3 0591 370f\n5c9c ec7f 777e 15de 23e0 d432 5b8a 460a\n132c d09c a81d 3683 79ff c7de 4631 f88f\nfa53 5aa6 0286 1817 0dde 85c8 84bc 3a03\nf8b1 60eb e1ca 5f1e 2094 eba9 ec61 6fe8\n7214 1a0c 88e1 4dac 5f3c f3b7 9d29 7dfd\n2c2a 6539 a675 a755 482b 31c8 755c 832c\ne166 25b9 539d e0a2 2360 217b 79ec a7d9\n9930 e7a7 3af7 d318 83aa 1098 81c2 161f\ne55c 6c45 4641 b4e8 8aeb 58e3 e199 74d5\nb6ff 93a1 da35 11e9 c3f3 e84d 10e8 ee5b\n094f f905 af0e 2872 a683 4836 9d7b 1dba\nc528 e972 f72d 1575 a116 bc3d 44b4 970c\ndb54 690e 2434 3c4e 9eec 2b60 6ccf 4765\n6f90 b0bc 7f15 9bd5 6213 3b4b c477 9972\ncb27 d777 afcb d95e 1528 2ecd 5d2e 63c9\na25e 9132 39af dab4 81db ea9e a168 204e\nf162 8616 7c6a c7e3 bdb2 cb5c 7620 fd43\n17af b869 978d 6b58 c54f fdc9 8a37 01d2\nf82e b721 631b 3888 bb27 874c 872a 28e0\n```\n\nNow we'll remove the file, sync the filesystem and check that sector again:\n\n```\nrm -f trimtest\nsync\nhdparm --read-sector 25565616 /dev/sda\n```\n\nIf trim support is working the sector read should be all 0s.\n","created_at":-62135596800,"fuzzy_word_count":700,"path":"/notes/trim/","published_at":1507587794,"reading_time":4,"tags":null,"title":"Trim","type":"notes","updated_at":1507587794,"weight":0,"word_count":667}]